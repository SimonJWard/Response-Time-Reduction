{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d68f83",
   "metadata": {},
   "source": [
    "<a id='AlternativeModelsEnsemble-RNN-GRUTop'></a>\n",
    "# Train Ensembles of Alternative Recurrent Neural Network Models\n",
    "\n",
    "Comparing performance of alterative recurrent neural network (RNN) models, the [traditional RNN](#RNN) and [gated recurrent unit (GRU)](#GRU).\n",
    "\n",
    "- Architecture, kept constant across LSTM, GRU and RNN models for a fair comparison, is:\n",
    "    - input layer - 300\n",
    "    - hidden layer - 100\n",
    "    - output layer - 2 (softplus activation, to ensure variance predictions are positive)\n",
    "- batch size = 32\n",
    "- Number of base learners in ensemble = 10\n",
    "- 3:1:1 train:validation:test dataset split\n",
    "- 2000 epochs\n",
    "\n",
    "The model from the epoch which gives the best validation loss is saved for future use, for making predictions on the previously unseen experimental test set\n",
    "\n",
    "Minimum loss is -ln(minimum_variance)/2 = -6.91 (for a minimum variance chosen to be 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6498e4",
   "metadata": {},
   "source": [
    "<a id='GRU'></a>\n",
    "# Train Ensemble of Gated Recurrent Unit (GRU) Networks\n",
    "[return to top](#AlternativeModelsEnsemble-RNN-GRUTop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from LSTMutils import MeanVarianceLogLikelyhoodLoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import LSTMutils\n",
    "\n",
    "# input parameters\n",
    "NumEnsemble = 10\n",
    "SequenceLength = 250\n",
    "validation_split = 0.25\n",
    "batch_size = 32\n",
    "NumEpochs = 2000\n",
    "test_split = 0.2\n",
    "\n",
    "# set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# read experimental dataset\n",
    "ExperimentalData = LSTMutils.ExperimentalData(SequenceLength=SequenceLength)\n",
    "unused, concentrations, df_data, unused = ExperimentalData.ReadData()\n",
    "\n",
    "# split data into stratified train and test sets, size defined by the test_split variable\n",
    "# the split will always be the same provided the data is in the same order, the same random_state is used,\n",
    "# and strangely the labels used for stratification are always the same type (str is used here)\n",
    "df_train, df_test = train_test_split(df_data, test_size=test_split, train_size=1-test_split, random_state=42, shuffle=True, stratify=concentrations)\n",
    "\n",
    "# normalise time series data\n",
    "df_norm_train, df_norm_test, unused = ExperimentalData.NormalizeData(df_train,df_test)\n",
    "    \n",
    "# Define y as the last element in X, and ensure X and y are the correct shape\n",
    "X_train, y_train = ExperimentalData.Shape(df_norm_train)\n",
    "\n",
    "# train NumEnsemble base learners, minimizing negative log likelyhood loss for mean and variance predictions\n",
    "# implementation follows this work: doi.org/10.48550/arXiv.1612.01474\n",
    "for i in range(NumEnsemble):\n",
    "    \n",
    "    model = keras.models.Sequential([keras.layers.GRU(300, input_shape=(SequenceLength,1), return_sequences=True)\n",
    "                                  , keras.layers.GRU(100, return_sequences=True)\n",
    "                                 , keras.layers.GRU(2, activation='softplus',return_sequences=True)])\n",
    "    \n",
    "    # save the model at the epoch which gives the lowest loss predictions on the validataion dataset\n",
    "    checkpoint_filepath = r\"../Models/AlternativeModels/GRU/EnsembleModel\" + str(i+1)\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "        \n",
    "    model.compile(optimizer=\"adam\",loss = MeanVarianceLogLikelyhoodLoss)\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, validation_split=validation_split, epochs=NumEpochs, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "    # plot loss vs epochs\n",
    "    Evaluation = LSTMutils.ModelTrainingEvaluation()\n",
    "    Evaluation.PlotLossHistory(history)\n",
    "\n",
    "    # load and evaluate the best model, in terms of validation loss\n",
    "    bestModel = keras.models.load_model(checkpoint_filepath, custom_objects={\"MeanVarianceLogLikelyhoodLoss\": MeanVarianceLogLikelyhoodLoss})\n",
    "    bestModel.evaluate(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e10778",
   "metadata": {},
   "source": [
    "<a id='RNN'></a>\n",
    "# Train Ensemble of RNN Networks\n",
    "[return to top](#AlternativeModelsEnsemble-RNN-GRUTop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from LSTMutils import MeanVarianceLogLikelyhoodLoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# input parameters\n",
    "NumEnsemble = 10\n",
    "SequenceLength = 250\n",
    "validation_split = 0.25\n",
    "batch_size = 32\n",
    "NumEpochs = 2000\n",
    "test_split = 0.2\n",
    "\n",
    "# set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# read experimental dataset\n",
    "ExperimentalData = LSTMutils.ExperimentalData(SequenceLength=SequenceLength)\n",
    "unused, concentrations, df_data, unused = ExperimentalData.ReadData()\n",
    "\n",
    "# split data into stratified train and test sets, size defined by the test_split variable\n",
    "# the split will always be the same provided the data is in the same order, the same random_state is used,\n",
    "# and strangely the labels used for stratification are always the same type (str is used here)\n",
    "df_train, df_test = train_test_split(df_data, test_size=test_split, train_size=1-test_split, random_state=42, shuffle=True, stratify=concentrations)\n",
    "\n",
    "# normalise time series data\n",
    "df_norm_train, df_norm_test, unused = ExperimentalData.NormalizeData(df_train,df_test)\n",
    "    \n",
    "# Define y as the last element in X, and ensure X and y are the correct shape\n",
    "X_train, y_train = ExperimentalData.Shape(df_norm_train)\n",
    "\n",
    "# train NumEnsemble base learners, minimizing negative log likelyhood loss for mean and variance predictions\n",
    "# implementation follows this work: doi.org/10.48550/arXiv.1612.01474\n",
    "for i in range(NumEnsemble):\n",
    "    \n",
    "    model = keras.models.Sequential([keras.layers.SimpleRNN(300, input_shape=(SequenceLength,1), return_sequences=True)\n",
    "                                  , keras.layers.SimpleRNN(100, return_sequences=True)\n",
    "                                 , keras.layers.SimpleRNN(2, activation='softplus',return_sequences=True)])\n",
    "    \n",
    "    # save the model at the epoch which gives the lowest loss predictions on the validataion dataset\n",
    "    checkpoint_filepath = r\"../Models/AlternativeModels/RNN/EnsembleModel\" + str(i+1)\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "        \n",
    "    model.compile(optimizer=\"adam\",loss = MeanVarianceLogLikelyhoodLoss)\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, validation_split=validation_split, epochs=NumEpochs, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "    # plot loss vs epochs\n",
    "    Evaluation = LSTMutils.ModelTrainingEvaluation()\n",
    "    Evaluation.PlotLossHistory(history)\n",
    "    \n",
    "    # load and evaluate the best model, in terms of validation loss\n",
    "    bestModel = keras.models.load_model(checkpoint_filepath, custom_objects={\"MeanVarianceLogLikelyhoodLoss\": MeanVarianceLogLikelyhoodLoss})\n",
    "    bestModel.evaluate(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7bb2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
