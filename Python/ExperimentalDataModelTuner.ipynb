{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c0f2c5",
   "metadata": {},
   "source": [
    "<a id='ExperimentalDataModelTunerTop'></a>\n",
    "# Hyperameter Tuning for Single LSTM Model Trained on Experimental Data using Keras Tuner\n",
    "\n",
    "Hyperparameter tuning of single LSTM model on the experimental dataset using keras tuner.\n",
    "\n",
    "- Tunable parameters are:\n",
    "    - number of nodes in each layer (5-1000)\n",
    "    - number of hidden layers (2-9)\n",
    "    - batch size (2-128)\n",
    "- Optional tunable parameters are:\n",
    "    - regularization\n",
    "    - dropout\n",
    "    - custom learning rate\n",
    "\n",
    "Hyperparameter tuning parameters\n",
    "- max epoch = 2000\n",
    "- random search\n",
    "- early stopping after 500 epochs of no improvement, or validation loss is nan\n",
    "\n",
    "The hyperparameters and validation loss from the epoch and model which resulted in the best validation loss is saved, informing the hyperparameters of the [model](ExperimentalDataEnsembleModels.ipynb#ExperimentalDataEnsembleModelsTop)\n",
    "\n",
    "Minimum loss is -ln(minimum_variance)/2 = -6.91 (for a minimum variance chosen to be 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from LSTMutils import MeanVarianceLogLikelyhoodLoss\n",
    "import LSTMutils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_tuner as kt\n",
    "\n",
    "# input parameters\n",
    "SequenceLength = 250\n",
    "validation_split = 0.25\n",
    "NumEpochs = 2000\n",
    "test_split = 0.2\n",
    "AddDropout = False\n",
    "AddRegularization = False\n",
    "CustomLearningRate = False\n",
    "EarlyStoppingPatience = 500\n",
    "NumTopResults = 30\n",
    "\n",
    "#set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# read experimental dataset\n",
    "ExperimentalData = LSTMutils.ExperimentalData(SequenceLength=SequenceLength)\n",
    "time, concentrations, df_data, unused = ExperimentalData.ReadData()\n",
    "\n",
    "# split data into stratified train and test sets, size defined by the test_split variable\n",
    "# the split will always be the same provided the data is in the same order, the same random_state is used,\n",
    "# and strangely the labels used for stratification are always the same type (str is used here)\n",
    "df_train, df_test = train_test_split(df_data, test_size=test_split, train_size=1-test_split, random_state=42, shuffle=True, stratify=concentrations)\n",
    "\n",
    "# split data into stratified train and validation sets, size defined by the validation_split variable\n",
    "train_concentrations = df_train.iloc[:,0]\n",
    "df_train, df_val = train_test_split(df_train, test_size=validation_split, train_size=1-validation_split, random_state=42, shuffle=True, stratify=train_concentrations)\n",
    "\n",
    "# normalise time series data\n",
    "df_norm_train, df_norm_test, df_norm_val = ExperimentalData.NormalizeData(df_train,df_test,df_val)\n",
    "\n",
    "# Define y as the last element in X, and ensure X and y are the correct shape\n",
    "X_train, y_train = ExperimentalData.Shape(df_norm_train)\n",
    "X_val, y_val = ExperimentalData.Shape(df_norm_val)\n",
    "\n",
    "# implementation informed by https://github.com/keras-team/keras-tuner/issues/122\n",
    "# set up class to enable batch size to be searched with the rest of the hyperparameters\n",
    "class LSTMHyperModel(kt.HyperModel):\n",
    "    \n",
    "    def build(self,hp):\n",
    "        \n",
    "        # if needed, add L2 regularizer\n",
    "        if AddRegularization:\n",
    "            Regularizer = keras.regularizers.L2(hp.Choice(\"L2Regularizer1\", [0., 1e-4]))\n",
    "        else:\n",
    "            Regularizer = None\n",
    "        \n",
    "        # define network architecture of input and first hidden layer\n",
    "        model = keras.models.Sequential(keras.layers.LSTM(hp.Choice(\"layer_1_units\", [5,10,20,50,100,200,500,750,1000])\n",
    "                                        , input_shape=(SequenceLength,1), return_sequences=True\n",
    "                                        , kernel_regularizer=Regularizer))\n",
    "        \n",
    "        # if needed add dropout\n",
    "        if AddDropout:\n",
    "            if hp.Boolean(\"layer_1_dropout\"):\n",
    "                model.add(keras.layers.Dropout(rate=0.25))\n",
    "        \n",
    "        # define network architecture of subsequent hidden layers\n",
    "        for i in range(hp.Int(\"LSTM Layers\", min_value = 1, max_value = 8)):\n",
    "            model.add(keras.layers.LSTM(hp.Choice(f\"layer_{i+2}_units\", [5,10,20,50,100,200,500,750,1000])\n",
    "                                        , return_sequences=True, kernel_regularizer=Regularizer))\n",
    "            \n",
    "            # if needed add dropout\n",
    "            if AddDropout:\n",
    "                if hp.Boolean(f\"layer_{i+2}_dropout\"):\n",
    "                    model.add(keras.layers.Dropout(rate=0.25))\n",
    "        \n",
    "        # if needed specify learning rate schedule            \n",
    "        if CustomLearningRate:\n",
    "            learning_rate = hp.Float(\"lr\", min_value=1e-5, max_value=1e-2, sampling=\"log\")\n",
    "        \n",
    "        # add output layer with softmax activation to ensure variance predictions are positive\n",
    "        model.add(keras.layers.LSTM(2, activation='softplus',return_sequences=True))\n",
    "        \n",
    "        # adam optimizer and custom negative log likelyhood loss\n",
    "        model.compile(optimizer=\"adam\",loss = MeanVarianceLogLikelyhoodLoss)\n",
    "    \n",
    "        return model\n",
    "\n",
    "    # define custom fit function called by keras tuner that includes batch size as a hyperparameter\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\",[2,4,8,16,32,64,128]),**kwargs)\n",
    "\n",
    "# set up tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    LSTMHyperModel(),\n",
    "    objective='val_loss',\n",
    "    overwrite = False,\n",
    "    max_trials = 300,\n",
    "    seed = 42,\n",
    "    directory = \"../Models/ExperimentalDataModelTuner\")\n",
    "\n",
    "# run random hyperparameter search, with early stopping if there is no decrease in validation loss\n",
    "# in 'EarlyStoppingPatience' epochs, or the loss is nan\n",
    "tuner.search(X_train, y_train, epochs=NumEpochs, validation_data=(X_val,y_val)\n",
    "             , callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',patience=EarlyStoppingPatience)\n",
    "             ,keras.callbacks.TerminateOnNaN()])\n",
    "\n",
    "# get and evaluate model from the epoch that achieved the best validation loss\n",
    "best_model = tuner.get_best_models()[0]\n",
    "best_model.summary()\n",
    "best_model.save(\"../Models/ExperimentalDataModelTuner\")\n",
    "\n",
    "# show summary of top 'NumTopResults' candidate sets of hyperparameters, ordered in terms of lowest validation loss\n",
    "tuner.results_summary(NumTopResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c27414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
