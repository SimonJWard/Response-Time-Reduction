{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d68f83",
   "metadata": {},
   "source": [
    "<a id='ExperimentalDataTransferLearningReplacingOutputLayerEnsembleModelsTop'></a>\n",
    "# Fine Tune Ensemble of LSTM Networks on Experimental Data\n",
    "\n",
    "Investigating whether starting with a model pretrained on simulated data, and using transfer learning, removing the output layer from the pretrained model and adding 4 more layers to train on the experimental dataset, improves performance\n",
    "\n",
    "- Architecture, informed by limited [hyperparameter tuning](ExperimentalDataTransferLearningReplacingOutputLayerModelTuner.ipynb#ExperimentalDataTransferLearningReplacingOutputLayerModelTunerTop) with keras tuner, is:\n",
    "    - input layer - 500\n",
    "    - hidden layer - 5\n",
    "    - hidden layer - 200\n",
    "    - output layer - 2 (softplus activation, to ensure variance predictions are positive)\n",
    "- batch size = 16\n",
    "- Number of base learners in ensemble = 10\n",
    "- 3:1:1 train:validation:test dataset split\n",
    "- 1500 epochs\n",
    "\n",
    "The model from the epoch which gives the best validation loss is saved for future use, for making predictions on the previously unseen experimental test set\n",
    "\n",
    "Minimum loss is -ln(minimum_variance)/2 = -6.91 (for a minimum variance chosen to be 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from LSTMutils import MeanVarianceLogLikelyhoodLoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import LSTMutils\n",
    "\n",
    "# input parameters\n",
    "NumEnsemble = 10\n",
    "SequenceLength = 250\n",
    "validation_split = 0.25\n",
    "batch_size = 16\n",
    "NumEpochs = 1500\n",
    "test_split = 0.2\n",
    "\n",
    "# set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "reconstructed_model = keras.models.load_model(\"../Models/SimulatedDataPretrainedModel\",custom_objects={\"MeanVarianceLogLikelyhoodLoss\": MeanVarianceLogLikelyhoodLoss})\n",
    "reconstructed_model.trainable = False\n",
    "\n",
    "# read experimental dataset\n",
    "ExperimentalData = LSTMutils.ExperimentalData(SequenceLength=SequenceLength)\n",
    "unused, concentrations, df_data, unused = ExperimentalData.ReadData()\n",
    "\n",
    "# split data into stratified train and test sets, size defined by the test_split variable\n",
    "# the split will always be the same provided the data is in the same order, the same random_state is used,\n",
    "# and strangely the labels used for stratification are always the same type (str is used here)\n",
    "df_train, df_test = train_test_split(df_data, test_size=test_split, train_size=1-test_split, random_state=42, shuffle=True, stratify=concentrations)\n",
    "\n",
    "# split data into stratified train and validation sets, size defined by the validation_split variable\n",
    "train_concentrations = df_train.iloc[:,0]\n",
    "df_train, df_val = train_test_split(df_train, test_size=validation_split, train_size=1-validation_split, random_state=42, shuffle=True, stratify=train_concentrations)\n",
    "\n",
    "# normalise time series data\n",
    "df_norm_train, df_norm_test, df_norm_val = ExperimentalData.NormalizeData(df_train,df_test,df_val)\n",
    "\n",
    "# train NumEnsemble base learners, minimizing negative log likelyhood loss for mean and variance predictions\n",
    "# implementation follows this work: doi.org/10.48550/arXiv.1612.01474\n",
    "for i in range(NumEnsemble):\n",
    "    \n",
    "    # randomly shuffle data to achieve sufficient base learner diversity \n",
    "    df_norm_train = df_norm_train.sample(frac=1)\n",
    "    df_norm_val = df_norm_val.sample(frac=1)\n",
    "    \n",
    "    # Define y as the last element in X, and ensure X and y are the correct shape\n",
    "    X_train, y_train = ExperimentalData.Shape(df_norm_train)\n",
    "    X_val, y_val = ExperimentalData.Shape(df_norm_val)\n",
    "    \n",
    "    print(\"model number \" + str(i+1))\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # define network architecture, prepending the pretrained model without the output layer\n",
    "    for layer in reconstructed_model.layers[:-1]:\n",
    "            model.add(layer)\n",
    "    for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "    model.add(keras.layers.LSTM(500, return_sequences=True,stateful=False, name = \"first_lstm_layer\"))\n",
    "    model.add(keras.layers.LSTM(5, return_sequences=True,stateful=False, name = \"second_lstm_layer\"))\n",
    "    model.add(keras.layers.LSTM(200, return_sequences=True,stateful=False, name = \"third_lstm_layer\"))\n",
    "    model.add(keras.layers.LSTM(2, activation='softplus',return_sequences=True,stateful=False, name = \"output_layer\"))\n",
    "    \n",
    "    # save the model at the epoch which gives the lowest loss predictions on the validataion dataset\n",
    "    checkpoint_filepath = r\"../Models/ExperimentalDataTransferLearningEnsembleModels/ExperimentalDataTransferLearningEnsembleModel\" + str(i+1)\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "        \n",
    "    model.compile(optimizer=\"adam\",loss = MeanVarianceLogLikelyhoodLoss)\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_val,y_val), epochs=NumEpochs, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "    # plot loss vs epochs\n",
    "    Evaluation = LSTMutils.ModelTrainingEvaluation()\n",
    "    Evaluation.PlotLossHistory(history)\n",
    "    \n",
    "    # load and evaluate the best model, in terms of validation loss\n",
    "    bestModel = keras.models.load_model(checkpoint_filepath, custom_objects={\"MeanVarianceLogLikelyhoodLoss\": MeanVarianceLogLikelyhoodLoss})\n",
    "    bestModel.evaluate(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7542e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
