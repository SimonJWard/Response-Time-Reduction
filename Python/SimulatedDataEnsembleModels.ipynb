{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d68f83",
   "metadata": {},
   "source": [
    "# Train Ensemble of LSTM Networks on Simulated Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7691bd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.2747"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 34s 263ms/step - loss: -1.2747 - val_loss: -1.4664\n",
      "Epoch 2/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.2731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -2.2731 - val_loss: -2.4835\n",
      "Epoch 3/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -2.7825 - val_loss: -2.4663\n",
      "Epoch 4/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.1102 - val_loss: -1.2524\n",
      "Epoch 5/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.2465"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 243ms/step - loss: -3.2465 - val_loss: -3.0826\n",
      "Epoch 6/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.4907"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 243ms/step - loss: -3.4907 - val_loss: -3.3258\n",
      "Epoch 7/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.6265 - val_loss: -1.0763\n",
      "Epoch 8/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.7078 - val_loss: -2.6624\n",
      "Epoch 9/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.7823 - val_loss: -3.2689\n",
      "Epoch 10/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.8631 - val_loss: -2.1453\n",
      "Epoch 11/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.6168"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -3.6168 - val_loss: -3.4154\n",
      "Epoch 12/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.8738"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -3.8738 - val_loss: -3.4724\n",
      "Epoch 13/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9454 - val_loss: -2.9679\n",
      "Epoch 14/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -3.6707 - val_loss: -2.9112\n",
      "Epoch 15/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.8673 - val_loss: -3.4346\n",
      "Epoch 16/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9829 - val_loss: -1.3262\n",
      "Epoch 17/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.0443 - val_loss: -2.9189\n",
      "Epoch 18/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1509 - val_loss: -3.3645\n",
      "Epoch 19/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1570 - val_loss: -1.8739\n",
      "Epoch 20/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.1303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 245ms/step - loss: -4.1303 - val_loss: -3.8029\n",
      "Epoch 21/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1542 - val_loss: -2.9874\n",
      "Epoch 22/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -4.2234 - val_loss: -3.7412\n",
      "Epoch 23/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1434 - val_loss: -3.0744\n",
      "Epoch 24/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2664 - val_loss: -2.7363\n",
      "Epoch 25/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.2321 - val_loss: -3.6640\n",
      "Epoch 26/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.3382"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -4.3382 - val_loss: -3.8321\n",
      "Epoch 27/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2251 - val_loss: -3.5657\n",
      "Epoch 28/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.2654"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -4.2654 - val_loss: -3.8539\n",
      "Epoch 29/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2969 - val_loss: -3.7602\n",
      "Epoch 30/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3691 - val_loss: -3.7683\n",
      "Epoch 31/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0952 - val_loss: -3.6038\n",
      "Epoch 32/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3727 - val_loss: -0.5989\n",
      "Epoch 33/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3623 - val_loss: -3.7807\n",
      "Epoch 34/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3675 - val_loss: -3.5533\n",
      "Epoch 35/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.4134 - val_loss: -3.3958\n",
      "Epoch 36/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4488"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 237ms/step - loss: -4.4488 - val_loss: -4.0986\n",
      "Epoch 37/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4130 - val_loss: -3.9569\n",
      "Epoch 38/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.4239 - val_loss: -3.9182\n",
      "Epoch 39/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4828 - val_loss: -3.9030\n",
      "Epoch 40/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4790 - val_loss: -4.0192\n",
      "Epoch 41/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.0210 - val_loss: -4.0205\n",
      "Epoch 42/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3245 - val_loss: -3.7981\n",
      "Epoch 43/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3591 - val_loss: -3.7191\n",
      "Epoch 44/2000\n",
      "118/118 [==============================] - 21s 179ms/step - loss: -4.3335 - val_loss: -3.6723\n",
      "Epoch 45/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4706 - val_loss: -4.0302\n",
      "Epoch 46/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.3987 - val_loss: -4.0248\n",
      "Epoch 47/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.4755 - val_loss: -4.0538\n",
      "Epoch 48/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.4726 - val_loss: -3.6642\n",
      "Epoch 49/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -3.7000 - val_loss: -3.8057\n",
      "Epoch 50/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7253 - val_loss: -3.3429\n",
      "Epoch 51/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3053 - val_loss: -3.7695\n",
      "Epoch 52/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3702 - val_loss: -3.8867\n",
      "Epoch 53/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4608 - val_loss: -4.0512\n",
      "Epoch 54/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3164 - val_loss: -3.3022\n",
      "Epoch 55/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4124 - val_loss: -3.3096\n",
      "Epoch 56/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5222 - val_loss: -3.9422\n",
      "Epoch 57/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.4922 - val_loss: -4.0590\n",
      "Epoch 58/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.5654 - val_loss: -3.5465\n",
      "Epoch 59/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4087 - val_loss: -4.0336\n",
      "Epoch 60/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.4717 - val_loss: -2.9150\n",
      "Epoch 61/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5692 - val_loss: -3.8913\n",
      "Epoch 62/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4576 - val_loss: -3.9207\n",
      "Epoch 63/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5836 - val_loss: -3.7108\n",
      "Epoch 64/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6053 - val_loss: -3.8501\n",
      "Epoch 65/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4943 - val_loss: -3.7982\n",
      "Epoch 66/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5947 - val_loss: -3.9244\n",
      "Epoch 67/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5634"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -4.5634 - val_loss: -4.1117\n",
      "Epoch 68/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6087"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 243ms/step - loss: -4.6087 - val_loss: -4.1392\n",
      "Epoch 69/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6555 - val_loss: 3.5009\n",
      "Epoch 70/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6090 - val_loss: -3.7209\n",
      "Epoch 71/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6120 - val_loss: -2.8408\n",
      "Epoch 72/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5801 - val_loss: -1.4937\n",
      "Epoch 73/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0789 - val_loss: -4.1280\n",
      "Epoch 74/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5404"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -4.5404 - val_loss: -4.1733\n",
      "Epoch 75/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5101 - val_loss: -3.9119\n",
      "Epoch 76/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6745"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -4.6745 - val_loss: -4.2092\n",
      "Epoch 77/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4459 - val_loss: -3.7271\n",
      "Epoch 78/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6350 - val_loss: -3.0128\n",
      "Epoch 79/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.5013 - val_loss: -2.9392\n",
      "Epoch 80/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5136 - val_loss: -3.7600\n",
      "Epoch 81/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -4.6113 - val_loss: -3.8333\n",
      "Epoch 82/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5235 - val_loss: -4.1393\n",
      "Epoch 83/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6595 - val_loss: -4.1370\n",
      "Epoch 84/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.5614 - val_loss: -4.0215\n",
      "Epoch 85/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6297 - val_loss: -3.9641\n",
      "Epoch 86/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7090 - val_loss: -4.1140\n",
      "Epoch 87/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6729 - val_loss: -3.2352\n",
      "Epoch 88/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5983 - val_loss: -2.7293\n",
      "Epoch 89/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7357 - val_loss: -3.9514\n",
      "Epoch 90/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9365 - val_loss: -3.1156\n",
      "Epoch 91/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.5111 - val_loss: -3.7809\n",
      "Epoch 92/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.2429 - val_loss: -3.8906\n",
      "Epoch 93/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3989 - val_loss: -0.1863\n",
      "Epoch 94/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4257 - val_loss: -3.8018\n",
      "Epoch 95/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5073 - val_loss: -3.3339\n",
      "Epoch 96/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5230 - val_loss: -4.0742\n",
      "Epoch 97/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3303 - val_loss: -3.9104\n",
      "Epoch 98/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3302 - val_loss: -3.5259\n",
      "Epoch 99/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6124 - val_loss: -3.9956\n",
      "Epoch 100/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6215 - val_loss: -3.3868\n",
      "Epoch 101/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5987 - val_loss: -3.8938\n",
      "Epoch 102/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5644 - val_loss: -4.0994\n",
      "Epoch 103/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -4.6833 - val_loss: -3.3842\n",
      "Epoch 104/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7395"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -4.7395 - val_loss: -4.2856\n",
      "Epoch 105/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3498 - val_loss: -4.1313\n",
      "Epoch 106/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7433 - val_loss: -4.1849\n",
      "Epoch 107/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6266 - val_loss: -4.2115\n",
      "Epoch 108/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5292 - val_loss: -4.0507\n",
      "Epoch 109/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.6461 - val_loss: -3.7771\n",
      "Epoch 110/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3608 - val_loss: -3.4053\n",
      "Epoch 111/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6566 - val_loss: -3.3874\n",
      "Epoch 112/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6009 - val_loss: -3.9678\n",
      "Epoch 113/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6241 - val_loss: -3.6715\n",
      "Epoch 114/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6129 - val_loss: -3.6948\n",
      "Epoch 115/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6257 - val_loss: -3.9698\n",
      "Epoch 116/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6907 - val_loss: -4.0189\n",
      "Epoch 117/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6546 - val_loss: -3.9433\n",
      "Epoch 118/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7233 - val_loss: -2.5236\n",
      "Epoch 119/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4602 - val_loss: -3.7767\n",
      "Epoch 120/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7319 - val_loss: -4.2160\n",
      "Epoch 121/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6729 - val_loss: -3.7752\n",
      "Epoch 122/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7860 - val_loss: -3.8882\n",
      "Epoch 123/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6994 - val_loss: -4.2350\n",
      "Epoch 124/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7644 - val_loss: -3.1398\n",
      "Epoch 125/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7350 - val_loss: -4.0099\n",
      "Epoch 126/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7413 - val_loss: -4.1699\n",
      "Epoch 127/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8063"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -4.8063 - val_loss: -4.2903\n",
      "Epoch 128/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6981 - val_loss: -3.8504\n",
      "Epoch 129/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5114 - val_loss: -3.9146\n",
      "Epoch 130/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.6847 - val_loss: -3.5438\n",
      "Epoch 131/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7322 - val_loss: -4.0923\n",
      "Epoch 132/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7827"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -4.7827 - val_loss: -4.4019\n",
      "Epoch 133/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4884 - val_loss: -3.9318\n",
      "Epoch 134/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7622 - val_loss: -4.3020\n",
      "Epoch 135/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.6049 - val_loss: -4.1439\n",
      "Epoch 136/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7169 - val_loss: -3.3594\n",
      "Epoch 137/2000\n",
      "118/118 [==============================] - 23s 193ms/step - loss: -4.6987 - val_loss: -4.3548\n",
      "Epoch 138/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7504 - val_loss: -4.0732\n",
      "Epoch 139/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8399 - val_loss: -3.7381\n",
      "Epoch 140/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0837 - val_loss: -2.8507\n",
      "Epoch 141/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6012 - val_loss: -3.9398\n",
      "Epoch 142/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8138 - val_loss: -4.1676\n",
      "Epoch 143/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8073 - val_loss: -3.9058\n",
      "Epoch 144/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7881 - val_loss: -4.1784\n",
      "Epoch 145/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4405 - val_loss: -4.0873\n",
      "Epoch 146/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7756 - val_loss: -3.8968\n",
      "Epoch 147/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7929 - val_loss: -4.0759\n",
      "Epoch 148/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.7659 - val_loss: -4.2419\n",
      "Epoch 149/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.7676 - val_loss: -3.6948\n",
      "Epoch 150/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.8157 - val_loss: -3.1680\n",
      "Epoch 151/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7734 - val_loss: -3.8749\n",
      "Epoch 152/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8434 - val_loss: -2.9120\n",
      "Epoch 153/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8258 - val_loss: -4.2040\n",
      "Epoch 154/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8067 - val_loss: -4.2767\n",
      "Epoch 155/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.8528 - val_loss: -3.9057\n",
      "Epoch 156/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8145 - val_loss: -2.6537\n",
      "Epoch 157/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8053 - val_loss: -4.0212\n",
      "Epoch 158/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7942 - val_loss: -4.1537\n",
      "Epoch 159/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8481 - val_loss: -4.0133\n",
      "Epoch 160/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8589 - val_loss: -1.4403\n",
      "Epoch 161/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7387 - val_loss: -2.9057\n",
      "Epoch 162/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7926 - val_loss: -4.0354\n",
      "Epoch 163/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -4.9174 - val_loss: -4.0402\n",
      "Epoch 164/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8153 - val_loss: -4.1982\n",
      "Epoch 165/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8168 - val_loss: -3.5532\n",
      "Epoch 166/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8163 - val_loss: -3.8887\n",
      "Epoch 167/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9105 - val_loss: -4.3551\n",
      "Epoch 168/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8629 - val_loss: -3.9725\n",
      "Epoch 169/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9034 - val_loss: -4.3584\n",
      "Epoch 170/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7834 - val_loss: -4.0892\n",
      "Epoch 171/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9305 - val_loss: -4.2538\n",
      "Epoch 172/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.8563 - val_loss: -3.1868\n",
      "Epoch 173/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8808 - val_loss: -3.9917\n",
      "Epoch 174/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.8026 - val_loss: -4.1467\n",
      "Epoch 175/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8645 - val_loss: -2.8915\n",
      "Epoch 176/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9075 - val_loss: -4.1485\n",
      "Epoch 177/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8564 - val_loss: -3.6064\n",
      "Epoch 178/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9066 - val_loss: -3.9913\n",
      "Epoch 179/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9292 - val_loss: -3.9061\n",
      "Epoch 180/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8502 - val_loss: -4.0671\n",
      "Epoch 181/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9027 - val_loss: -3.6640\n",
      "Epoch 182/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9817 - val_loss: -3.5584\n",
      "Epoch 183/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6828 - val_loss: -3.3095\n",
      "Epoch 184/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8438 - val_loss: -4.1044\n",
      "Epoch 185/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.2828 - val_loss: -2.1491\n",
      "Epoch 186/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.3255 - val_loss: -4.0796\n",
      "Epoch 187/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5821 - val_loss: -3.8125\n",
      "Epoch 188/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.7309 - val_loss: -4.3538\n",
      "Epoch 189/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7757 - val_loss: -4.3787\n",
      "Epoch 190/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7602 - val_loss: -4.2862\n",
      "Epoch 191/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6119 - val_loss: -3.6113\n",
      "Epoch 192/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8542 - val_loss: -3.9946\n",
      "Epoch 193/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8412 - val_loss: -4.1860\n",
      "Epoch 194/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8136 - val_loss: 0.3192\n",
      "Epoch 195/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -3.0027 - val_loss: -3.9157\n",
      "Epoch 196/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2991 - val_loss: -3.6284\n",
      "Epoch 197/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.6960 - val_loss: -3.9204\n",
      "Epoch 198/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7890 - val_loss: -4.2244\n",
      "Epoch 199/2000\n",
      "118/118 [==============================] - 21s 179ms/step - loss: -4.7553 - val_loss: -4.1524\n",
      "Epoch 200/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3029 - val_loss: -4.1305\n",
      "Epoch 201/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6478 - val_loss: -3.9206\n",
      "Epoch 202/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8764 - val_loss: -4.1536\n",
      "Epoch 203/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8144 - val_loss: -3.9917\n",
      "Epoch 204/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7951 - val_loss: -3.6672\n",
      "Epoch 205/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.5524 - val_loss: -3.8113\n",
      "Epoch 206/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7911 - val_loss: -4.0089\n",
      "Epoch 207/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7756 - val_loss: -4.1168\n",
      "Epoch 208/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7391 - val_loss: -4.3794\n",
      "Epoch 209/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8714 - val_loss: -3.7485\n",
      "Epoch 210/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.8115 - val_loss: -4.1694\n",
      "Epoch 211/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8549 - val_loss: -3.6553\n",
      "Epoch 212/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8815 - val_loss: -3.9788\n",
      "Epoch 213/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8135 - val_loss: -3.2370\n",
      "Epoch 214/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8226 - val_loss: -4.2335\n",
      "Epoch 215/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9373 - val_loss: -4.3721\n",
      "Epoch 216/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.9120 - val_loss: -4.3620\n",
      "Epoch 217/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9058 - val_loss: -4.0254\n",
      "Epoch 218/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8822 - val_loss: -4.0992\n",
      "Epoch 219/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8807 - val_loss: -3.3443\n",
      "Epoch 220/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8930 - val_loss: -3.6293\n",
      "Epoch 221/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.7376 - val_loss: -4.2994\n",
      "Epoch 222/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8373 - val_loss: -4.0433\n",
      "Epoch 223/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8881 - val_loss: -4.2347\n",
      "Epoch 224/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.6785 - val_loss: -3.9145\n",
      "Epoch 225/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9157 - val_loss: -4.0011\n",
      "Epoch 226/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9743"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 243ms/step - loss: -4.9743 - val_loss: -4.5845\n",
      "Epoch 227/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9617 - val_loss: -3.8962\n",
      "Epoch 228/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9754 - val_loss: -4.0869\n",
      "Epoch 229/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8989 - val_loss: -2.8008\n",
      "Epoch 230/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8832 - val_loss: -3.8920\n",
      "Epoch 231/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9753 - val_loss: -3.9668\n",
      "Epoch 232/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9318 - val_loss: -4.0622\n",
      "Epoch 233/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8452 - val_loss: -4.3834\n",
      "Epoch 234/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0236 - val_loss: -4.5173\n",
      "Epoch 235/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9392 - val_loss: -4.4545\n",
      "Epoch 236/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0038 - val_loss: -3.9286\n",
      "Epoch 237/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9221 - val_loss: -3.9253\n",
      "Epoch 238/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0089 - val_loss: -3.7913\n",
      "Epoch 239/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9470 - val_loss: -4.3712\n",
      "Epoch 240/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9613 - val_loss: -3.8653\n",
      "Epoch 241/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.0431 - val_loss: -4.2350\n",
      "Epoch 242/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9991 - val_loss: -4.2235\n",
      "Epoch 243/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.9913 - val_loss: -4.4648\n",
      "Epoch 244/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9588 - val_loss: -4.4523\n",
      "Epoch 245/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9060 - val_loss: -3.4859\n",
      "Epoch 246/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0967 - val_loss: -3.7777\n",
      "Epoch 247/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0528 - val_loss: -3.2886\n",
      "Epoch 248/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0229 - val_loss: -3.4185\n",
      "Epoch 249/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0034 - val_loss: -4.2146\n",
      "Epoch 250/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0167 - val_loss: -4.1736\n",
      "Epoch 251/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9817 - val_loss: -4.3750\n",
      "Epoch 252/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9979 - val_loss: -4.0676\n",
      "Epoch 253/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0589 - val_loss: -3.9173\n",
      "Epoch 254/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9091 - val_loss: -3.6810\n",
      "Epoch 255/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0910 - val_loss: -4.3050\n",
      "Epoch 256/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9769 - val_loss: -2.6893\n",
      "Epoch 257/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.0608 - val_loss: -4.0781\n",
      "Epoch 258/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0460 - val_loss: -4.2126\n",
      "Epoch 259/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0375 - val_loss: -4.5161\n",
      "Epoch 260/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9980 - val_loss: -4.2862\n",
      "Epoch 261/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0617 - val_loss: -3.7519\n",
      "Epoch 262/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9871 - val_loss: -4.1249\n",
      "Epoch 263/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.0037 - val_loss: -0.6098\n",
      "Epoch 264/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0104 - val_loss: -2.3225\n",
      "Epoch 265/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.0676 - val_loss: -4.3143\n",
      "Epoch 266/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1028 - val_loss: -4.2894\n",
      "Epoch 267/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7437 - val_loss: -4.1949\n",
      "Epoch 268/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9303 - val_loss: -4.3694\n",
      "Epoch 269/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0802 - val_loss: -3.8906\n",
      "Epoch 270/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9912 - val_loss: -2.1880\n",
      "Epoch 271/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8337 - val_loss: -4.1889\n",
      "Epoch 272/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0245 - val_loss: -4.0745\n",
      "Epoch 273/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7030 - val_loss: -3.9106\n",
      "Epoch 274/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.9406 - val_loss: -4.4603\n",
      "Epoch 275/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0171 - val_loss: -4.2444\n",
      "Epoch 276/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7976 - val_loss: -3.5026\n",
      "Epoch 277/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8371 - val_loss: -3.9243\n",
      "Epoch 278/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0086 - val_loss: -4.0590\n",
      "Epoch 279/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.0767 - val_loss: -4.5256\n",
      "Epoch 280/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0384 - val_loss: -3.5868\n",
      "Epoch 281/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9993 - val_loss: -4.1817\n",
      "Epoch 282/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9260 - val_loss: -4.1696\n",
      "Epoch 283/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3411 - val_loss: -4.2951\n",
      "Epoch 284/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8150 - val_loss: -4.1472\n",
      "Epoch 285/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8727 - val_loss: -4.1534\n",
      "Epoch 286/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9562 - val_loss: -3.9812\n",
      "Epoch 287/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8171 - val_loss: -4.1269\n",
      "Epoch 288/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9108 - val_loss: -3.6825\n",
      "Epoch 289/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6077 - val_loss: -4.4123\n",
      "Epoch 290/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8262 - val_loss: -4.1130\n",
      "Epoch 291/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8140 - val_loss: -4.2056\n",
      "Epoch 292/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9705 - val_loss: -3.8012\n",
      "Epoch 293/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1098 - val_loss: -3.4121\n",
      "Epoch 294/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.0051 - val_loss: -4.2496\n",
      "Epoch 295/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.0326 - val_loss: -4.1516\n",
      "Epoch 296/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.0114 - val_loss: -0.9095\n",
      "Epoch 297/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9287 - val_loss: -3.5890\n",
      "Epoch 298/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0545 - val_loss: -4.3160\n",
      "Epoch 299/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1105 - val_loss: -4.5447\n",
      "Epoch 300/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0521 - val_loss: -4.1017\n",
      "Epoch 301/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9988 - val_loss: -3.7927\n",
      "Epoch 302/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0388 - val_loss: -4.4240\n",
      "Epoch 303/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0554 - val_loss: -4.3695\n",
      "Epoch 304/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0688 - val_loss: -3.9561\n",
      "Epoch 305/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0733 - val_loss: -4.2087\n",
      "Epoch 306/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9989 - val_loss: -4.2264\n",
      "Epoch 307/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.1045 - val_loss: -4.1281\n",
      "Epoch 308/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0701 - val_loss: -4.2919\n",
      "Epoch 309/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1230 - val_loss: -4.3444\n",
      "Epoch 310/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8170 - val_loss: -4.3365\n",
      "Epoch 311/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0741 - val_loss: -4.3309\n",
      "Epoch 312/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1006 - val_loss: -4.4766\n",
      "Epoch 313/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0521 - val_loss: -4.1191\n",
      "Epoch 314/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0308 - val_loss: -4.1628\n",
      "Epoch 315/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.1748 - val_loss: -1.3596\n",
      "Epoch 316/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9796 - val_loss: -4.0047\n",
      "Epoch 317/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0891 - val_loss: -3.6076\n",
      "Epoch 318/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1276 - val_loss: -3.7558\n",
      "Epoch 319/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0880"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -5.0880 - val_loss: -4.6443\n",
      "Epoch 320/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.1128 - val_loss: -4.3294\n",
      "Epoch 321/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9730 - val_loss: -4.3254\n",
      "Epoch 322/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0233 - val_loss: -4.3933\n",
      "Epoch 323/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.0452 - val_loss: -4.4653\n",
      "Epoch 324/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1578 - val_loss: -3.5745\n",
      "Epoch 325/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.0884 - val_loss: -4.0953\n",
      "Epoch 326/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1486"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -5.1486 - val_loss: -4.6607\n",
      "Epoch 327/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0891 - val_loss: -4.1550\n",
      "Epoch 328/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1330 - val_loss: -3.9551\n",
      "Epoch 329/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1240 - val_loss: -4.3009\n",
      "Epoch 330/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0450 - val_loss: -3.7653\n",
      "Epoch 331/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1656 - val_loss: -4.4267\n",
      "Epoch 332/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0990 - val_loss: -4.4186\n",
      "Epoch 333/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0365 - val_loss: -4.4864\n",
      "Epoch 334/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1962 - val_loss: -4.6372\n",
      "Epoch 335/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1090 - val_loss: -4.3498\n",
      "Epoch 336/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0459 - val_loss: -4.3162\n",
      "Epoch 337/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0710 - val_loss: -4.3126\n",
      "Epoch 338/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0695 - val_loss: -4.4984\n",
      "Epoch 339/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1654 - val_loss: -4.4906\n",
      "Epoch 340/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1664 - val_loss: -2.6130\n",
      "Epoch 341/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0695 - val_loss: -2.9104\n",
      "Epoch 342/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0054 - val_loss: -4.3922\n",
      "Epoch 343/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1021 - val_loss: -4.1611\n",
      "Epoch 344/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1416 - val_loss: -4.3982\n",
      "Epoch 345/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1007 - val_loss: -4.0499\n",
      "Epoch 346/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1651 - val_loss: -4.3150\n",
      "Epoch 347/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2012 - val_loss: -3.9182\n",
      "Epoch 348/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0925 - val_loss: -4.4136\n",
      "Epoch 349/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1338 - val_loss: 2.8252\n",
      "Epoch 350/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9818 - val_loss: -3.5199\n",
      "Epoch 351/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9917 - val_loss: -4.3204\n",
      "Epoch 352/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1202 - val_loss: -2.9924\n",
      "Epoch 353/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.2062 - val_loss: -3.5662\n",
      "Epoch 354/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9765 - val_loss: -4.3372\n",
      "Epoch 355/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0199 - val_loss: -4.1363\n",
      "Epoch 356/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1633 - val_loss: -4.3831\n",
      "Epoch 357/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2020 - val_loss: -4.0905\n",
      "Epoch 358/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0920 - val_loss: -4.5481\n",
      "Epoch 359/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1677 - val_loss: -4.4218\n",
      "Epoch 360/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9084 - val_loss: -3.8564\n",
      "Epoch 361/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0514 - val_loss: -4.3449\n",
      "Epoch 362/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1369 - val_loss: -3.8188\n",
      "Epoch 363/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1145 - val_loss: -3.8374\n",
      "Epoch 364/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0842 - val_loss: -2.5878\n",
      "Epoch 365/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2363 - val_loss: -4.0450\n",
      "Epoch 366/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0979 - val_loss: -3.9436\n",
      "Epoch 367/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1703 - val_loss: -3.9204\n",
      "Epoch 368/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1252 - val_loss: -4.1245\n",
      "Epoch 369/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0413 - val_loss: -4.6504\n",
      "Epoch 370/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1733 - val_loss: -4.0776\n",
      "Epoch 371/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0687 - val_loss: -4.0656\n",
      "Epoch 372/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2130 - val_loss: -2.7883\n",
      "Epoch 373/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2272 - val_loss: -4.2707\n",
      "Epoch 374/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1087 - val_loss: -3.1670\n",
      "Epoch 375/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1720 - val_loss: -4.3540\n",
      "Epoch 376/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2237 - val_loss: -3.8778\n",
      "Epoch 377/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1634 - val_loss: -4.4531\n",
      "Epoch 378/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1922 - val_loss: -3.4677\n",
      "Epoch 379/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2404 - val_loss: -4.1769\n",
      "Epoch 380/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2431 - val_loss: -4.2123\n",
      "Epoch 381/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1225 - val_loss: -2.1142\n",
      "Epoch 382/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1223 - val_loss: -2.9682\n",
      "Epoch 383/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1962 - val_loss: -4.5520\n",
      "Epoch 384/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1203 - val_loss: -3.6502\n",
      "Epoch 385/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1938 - val_loss: -3.9603\n",
      "Epoch 386/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1833"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -5.1833 - val_loss: -4.6714\n",
      "Epoch 387/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2305 - val_loss: -1.9535\n",
      "Epoch 388/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1213 - val_loss: -4.0953\n",
      "Epoch 389/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.2383 - val_loss: -3.4752\n",
      "Epoch 390/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1863 - val_loss: -2.8231\n",
      "Epoch 391/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1737 - val_loss: -4.4738\n",
      "Epoch 392/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1806 - val_loss: -4.3798\n",
      "Epoch 393/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8573 - val_loss: -2.3291\n",
      "Epoch 394/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2228 - val_loss: -4.2158\n",
      "Epoch 395/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.8183 - val_loss: -3.6343\n",
      "Epoch 396/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9427 - val_loss: -4.1860\n",
      "Epoch 397/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.0220 - val_loss: -4.4206\n",
      "Epoch 398/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0525 - val_loss: -3.2819\n",
      "Epoch 399/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9226 - val_loss: -2.6763\n",
      "Epoch 400/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1381 - val_loss: -3.7947\n",
      "Epoch 401/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0806 - val_loss: -3.4474\n",
      "Epoch 402/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.1304 - val_loss: -3.8589\n",
      "Epoch 403/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9768 - val_loss: -3.5649\n",
      "Epoch 404/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.0919 - val_loss: -4.5459\n",
      "Epoch 405/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1033 - val_loss: -3.6957\n",
      "Epoch 406/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0897 - val_loss: -3.1048\n",
      "Epoch 407/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2287 - val_loss: -4.4244\n",
      "Epoch 408/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.1471 - val_loss: -4.6561\n",
      "Epoch 409/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0733 - val_loss: -4.0018\n",
      "Epoch 410/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0929 - val_loss: -4.3883\n",
      "Epoch 411/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1822 - val_loss: -3.6132\n",
      "Epoch 412/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1871 - val_loss: -4.3310\n",
      "Epoch 413/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2019 - val_loss: -3.7269\n",
      "Epoch 414/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1355 - val_loss: -0.4052\n",
      "Epoch 415/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1500 - val_loss: -4.3309\n",
      "Epoch 416/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2281 - val_loss: -4.5542\n",
      "Epoch 417/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2395 - val_loss: -4.6231\n",
      "Epoch 418/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2431 - val_loss: -4.5338\n",
      "Epoch 419/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2095 - val_loss: -3.6344\n",
      "Epoch 420/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0768 - val_loss: -4.3614\n",
      "Epoch 421/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2555 - val_loss: -3.9511\n",
      "Epoch 422/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.0308 - val_loss: -3.9045\n",
      "Epoch 423/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1271 - val_loss: -3.6970\n",
      "Epoch 424/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8738 - val_loss: -4.3909\n",
      "Epoch 425/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1938 - val_loss: -4.1519\n",
      "Epoch 426/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1441 - val_loss: -4.0769\n",
      "Epoch 427/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0484 - val_loss: -3.9012\n",
      "Epoch 428/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1706 - val_loss: -3.8382\n",
      "Epoch 429/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1838 - val_loss: -4.1232\n",
      "Epoch 430/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2050"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -5.2050 - val_loss: -4.7148\n",
      "Epoch 431/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2447 - val_loss: -3.7543\n",
      "Epoch 432/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1915 - val_loss: -4.4337\n",
      "Epoch 433/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2745 - val_loss: -4.4093\n",
      "Epoch 434/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0580 - val_loss: -4.4800\n",
      "Epoch 435/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2435 - val_loss: -4.2411\n",
      "Epoch 436/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2712 - val_loss: -4.3935\n",
      "Epoch 437/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0463 - val_loss: -3.8542\n",
      "Epoch 438/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.9400 - val_loss: -4.3432\n",
      "Epoch 439/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1723 - val_loss: -3.9362\n",
      "Epoch 440/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2024 - val_loss: -4.4094\n",
      "Epoch 441/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2799 - val_loss: -3.7881\n",
      "Epoch 442/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2711 - val_loss: -4.5550\n",
      "Epoch 443/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1810 - val_loss: -1.2209\n",
      "Epoch 444/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2305 - val_loss: -4.3733\n",
      "Epoch 445/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1980 - val_loss: -4.4314\n",
      "Epoch 446/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1842 - val_loss: -2.6526\n",
      "Epoch 447/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2268 - val_loss: -4.1892\n",
      "Epoch 448/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1640 - val_loss: -4.1550\n",
      "Epoch 449/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3028 - val_loss: -3.4947\n",
      "Epoch 450/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2119 - val_loss: -4.1586\n",
      "Epoch 451/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3130 - val_loss: -3.5486\n",
      "Epoch 452/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.2526 - val_loss: -4.0910\n",
      "Epoch 453/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1190 - val_loss: -4.4634\n",
      "Epoch 454/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2679 - val_loss: -3.5839\n",
      "Epoch 455/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2091 - val_loss: -4.1753\n",
      "Epoch 456/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2908 - val_loss: -4.5624\n",
      "Epoch 457/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2210 - val_loss: -4.0180\n",
      "Epoch 458/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2801 - val_loss: -4.0211\n",
      "Epoch 459/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3067 - val_loss: -3.0854\n",
      "Epoch 460/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1811 - val_loss: -4.1244\n",
      "Epoch 461/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0498 - val_loss: -4.5119\n",
      "Epoch 462/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2947 - val_loss: -4.2142\n",
      "Epoch 463/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.1453 - val_loss: -2.9462\n",
      "Epoch 464/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1689 - val_loss: -3.4670\n",
      "Epoch 465/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2521 - val_loss: -3.3280\n",
      "Epoch 466/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2362 - val_loss: -4.5195\n",
      "Epoch 467/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9625 - val_loss: -4.1794\n",
      "Epoch 468/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1301 - val_loss: -3.8410\n",
      "Epoch 469/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2066 - val_loss: -4.4740\n",
      "Epoch 470/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3003 - val_loss: -4.4243\n",
      "Epoch 471/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1985 - val_loss: -4.5830\n",
      "Epoch 472/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0806 - val_loss: -4.1048\n",
      "Epoch 473/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2167 - val_loss: -4.0491\n",
      "Epoch 474/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9588 - val_loss: -3.0615\n",
      "Epoch 475/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2295 - val_loss: -2.8460\n",
      "Epoch 476/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2152 - val_loss: -4.4881\n",
      "Epoch 477/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2195 - val_loss: -4.3029\n",
      "Epoch 478/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0442 - val_loss: -4.1417\n",
      "Epoch 479/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2541 - val_loss: -4.2426\n",
      "Epoch 480/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1567 - val_loss: -4.2180\n",
      "Epoch 481/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2814 - val_loss: -3.4195\n",
      "Epoch 482/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2650 - val_loss: -4.1214\n",
      "Epoch 483/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2255 - val_loss: -1.3667\n",
      "Epoch 484/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1697 - val_loss: -4.3636\n",
      "Epoch 485/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0704 - val_loss: -4.0246\n",
      "Epoch 486/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1852 - val_loss: -3.8685\n",
      "Epoch 487/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1804 - val_loss: -3.9758\n",
      "Epoch 488/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2837 - val_loss: -1.4998\n",
      "Epoch 489/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0864 - val_loss: -4.2698\n",
      "Epoch 490/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2296 - val_loss: -3.6031\n",
      "Epoch 491/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2564 - val_loss: -4.2294\n",
      "Epoch 492/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9148 - val_loss: -3.6764\n",
      "Epoch 493/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0726 - val_loss: -4.0756\n",
      "Epoch 494/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1418"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 246ms/step - loss: -5.1418 - val_loss: -4.7246\n",
      "Epoch 495/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2104 - val_loss: -4.0824\n",
      "Epoch 496/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0456 - val_loss: -4.3985\n",
      "Epoch 497/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0121 - val_loss: -4.1805\n",
      "Epoch 498/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2837 - val_loss: -4.5092\n",
      "Epoch 499/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2614 - val_loss: -4.0386\n",
      "Epoch 500/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2476 - val_loss: -3.3350\n",
      "Epoch 501/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1821 - val_loss: -3.9281\n",
      "Epoch 502/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3039 - val_loss: -2.3002\n",
      "Epoch 503/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2131 - val_loss: -4.6026\n",
      "Epoch 504/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2617 - val_loss: -4.4202\n",
      "Epoch 505/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2274 - val_loss: -4.3206\n",
      "Epoch 506/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2204 - val_loss: -3.1821\n",
      "Epoch 507/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2058 - val_loss: -4.3739\n",
      "Epoch 508/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1088 - val_loss: -4.2116\n",
      "Epoch 509/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2389 - val_loss: -4.2982\n",
      "Epoch 510/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0812 - val_loss: -4.2182\n",
      "Epoch 511/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2269 - val_loss: -4.1308\n",
      "Epoch 512/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3210 - val_loss: -4.4707\n",
      "Epoch 513/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1969 - val_loss: -3.5399\n",
      "Epoch 514/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2830 - val_loss: 0.1065\n",
      "Epoch 515/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7739 - val_loss: -4.4723\n",
      "Epoch 516/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2080 - val_loss: -2.6063\n",
      "Epoch 517/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3051 - val_loss: -4.4019\n",
      "Epoch 518/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.2475 - val_loss: -4.0411\n",
      "Epoch 519/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2399 - val_loss: -2.9335\n",
      "Epoch 520/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2793 - val_loss: -4.6485\n",
      "Epoch 521/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9613 - val_loss: -3.6346\n",
      "Epoch 522/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9440 - val_loss: -4.1509\n",
      "Epoch 523/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2699"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -5.2699 - val_loss: -4.7505\n",
      "Epoch 524/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0772 - val_loss: -3.5355\n",
      "Epoch 525/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2810 - val_loss: -3.8189\n",
      "Epoch 526/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.2170 - val_loss: -4.4731\n",
      "Epoch 527/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2502 - val_loss: -3.6358\n",
      "Epoch 528/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8172 - val_loss: -4.6555\n",
      "Epoch 529/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.1808 - val_loss: -4.3368\n",
      "Epoch 530/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0686 - val_loss: -4.1026\n",
      "Epoch 531/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2500 - val_loss: -0.1710\n",
      "Epoch 532/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0808 - val_loss: -4.2910\n",
      "Epoch 533/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2371 - val_loss: -2.5624\n",
      "Epoch 534/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2501 - val_loss: -3.5392\n",
      "Epoch 535/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1228 - val_loss: -4.4414\n",
      "Epoch 536/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1779 - val_loss: -4.2796\n",
      "Epoch 537/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.1937 - val_loss: -4.5880\n",
      "Epoch 538/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2821 - val_loss: -4.1420\n",
      "Epoch 539/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3095 - val_loss: -4.1140\n",
      "Epoch 540/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3060 - val_loss: -3.9060\n",
      "Epoch 541/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0945 - val_loss: -4.4054\n",
      "Epoch 542/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2776 - val_loss: -4.2006\n",
      "Epoch 543/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3235 - val_loss: -0.9211\n",
      "Epoch 544/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2049 - val_loss: -4.4593\n",
      "Epoch 545/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1998 - val_loss: -4.2709\n",
      "Epoch 546/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2992 - val_loss: -4.1079\n",
      "Epoch 547/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1005 - val_loss: -4.7073\n",
      "Epoch 548/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2821 - val_loss: -3.0874\n",
      "Epoch 549/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3507 - val_loss: -4.1798\n",
      "Epoch 550/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2536 - val_loss: -4.1828\n",
      "Epoch 551/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2434 - val_loss: -4.4598\n",
      "Epoch 552/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3400 - val_loss: -4.5133\n",
      "Epoch 553/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2032 - val_loss: -2.8146\n",
      "Epoch 554/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2793 - val_loss: -4.3446\n",
      "Epoch 555/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3692 - val_loss: -4.3917\n",
      "Epoch 556/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1994 - val_loss: -2.8007\n",
      "Epoch 557/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2505 - val_loss: -4.3707\n",
      "Epoch 558/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1828 - val_loss: -4.1358\n",
      "Epoch 559/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3399 - val_loss: -3.4184\n",
      "Epoch 560/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3133 - val_loss: -1.2637\n",
      "Epoch 561/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2538 - val_loss: -3.5992\n",
      "Epoch 562/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2822 - val_loss: -2.8852\n",
      "Epoch 563/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1488 - val_loss: -4.2281\n",
      "Epoch 564/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2844 - val_loss: -2.7837\n",
      "Epoch 565/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.0919 - val_loss: -4.4464\n",
      "Epoch 566/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1970 - val_loss: -3.5363\n",
      "Epoch 567/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1902 - val_loss: -3.9546\n",
      "Epoch 568/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3087 - val_loss: -4.0780\n",
      "Epoch 569/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1745 - val_loss: -4.0646\n",
      "Epoch 570/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3964 - val_loss: -3.1281\n",
      "Epoch 571/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1559 - val_loss: -4.0870\n",
      "Epoch 572/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3502 - val_loss: 4.1797\n",
      "Epoch 573/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0638 - val_loss: -4.4718\n",
      "Epoch 574/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1133 - val_loss: -3.7691\n",
      "Epoch 575/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2809 - val_loss: -4.6227\n",
      "Epoch 576/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1449 - val_loss: -3.5939\n",
      "Epoch 577/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2723 - val_loss: -4.1351\n",
      "Epoch 578/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2736 - val_loss: -4.3060\n",
      "Epoch 579/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2682 - val_loss: -4.4052\n",
      "Epoch 580/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3010 - val_loss: -4.3939\n",
      "Epoch 581/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3004 - val_loss: -4.2626\n",
      "Epoch 582/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2125 - val_loss: -3.7132\n",
      "Epoch 583/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2511 - val_loss: -4.4144\n",
      "Epoch 584/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2646 - val_loss: -3.8672\n",
      "Epoch 585/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1020 - val_loss: -4.1516\n",
      "Epoch 586/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2895 - val_loss: -4.2299\n",
      "Epoch 587/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3498 - val_loss: -4.3578\n",
      "Epoch 588/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3087 - val_loss: -3.2741\n",
      "Epoch 589/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2574 - val_loss: -3.9571\n",
      "Epoch 590/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1240 - val_loss: -4.2881\n",
      "Epoch 591/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3495 - val_loss: -2.1870\n",
      "Epoch 592/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3161 - val_loss: -4.2503\n",
      "Epoch 593/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2327 - val_loss: -4.6158\n",
      "Epoch 594/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3568 - val_loss: -4.3024\n",
      "Epoch 595/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.2912 - val_loss: -3.7362\n",
      "Epoch 596/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2239 - val_loss: -4.4838\n",
      "Epoch 597/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8304 - val_loss: -4.3466\n",
      "Epoch 598/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0975 - val_loss: -4.2698\n",
      "Epoch 599/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1491 - val_loss: -2.0707\n",
      "Epoch 600/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1080 - val_loss: -3.8603\n",
      "Epoch 601/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3175 - val_loss: -2.5799\n",
      "Epoch 602/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2228 - val_loss: -4.5335\n",
      "Epoch 603/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2332 - val_loss: -3.6961\n",
      "Epoch 604/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3225 - val_loss: -3.7162\n",
      "Epoch 605/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3161 - val_loss: -4.3722\n",
      "Epoch 606/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3346 - val_loss: -3.9611\n",
      "Epoch 607/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2777 - val_loss: -4.3817\n",
      "Epoch 608/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3135 - val_loss: -4.3931\n",
      "Epoch 609/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3388 - val_loss: -1.7190\n",
      "Epoch 610/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1665 - val_loss: -4.4297\n",
      "Epoch 611/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3676 - val_loss: -3.9430\n",
      "Epoch 612/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2586 - val_loss: -1.7818\n",
      "Epoch 613/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2970 - val_loss: -4.6408\n",
      "Epoch 614/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9684 - val_loss: -3.8976\n",
      "Epoch 615/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2354 - val_loss: -4.2016\n",
      "Epoch 616/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3122 - val_loss: -4.5180\n",
      "Epoch 617/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3912 - val_loss: -4.0552\n",
      "Epoch 618/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3285 - val_loss: -3.2287\n",
      "Epoch 619/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2602 - val_loss: -2.3329\n",
      "Epoch 620/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1779 - val_loss: -4.1690\n",
      "Epoch 621/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1582 - val_loss: -4.5581\n",
      "Epoch 622/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1722 - val_loss: -4.1661\n",
      "Epoch 623/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3218 - val_loss: -4.6227\n",
      "Epoch 624/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0563 - val_loss: -4.3672\n",
      "Epoch 625/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2356 - val_loss: -4.2759\n",
      "Epoch 626/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3408 - val_loss: -2.8832\n",
      "Epoch 627/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2728 - val_loss: -4.4092\n",
      "Epoch 628/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.1631 - val_loss: -3.7994\n",
      "Epoch 629/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3791 - val_loss: -3.4166\n",
      "Epoch 630/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2831 - val_loss: -3.4432\n",
      "Epoch 631/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2759 - val_loss: -3.5307\n",
      "Epoch 632/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2804 - val_loss: -3.7942\n",
      "Epoch 633/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1525 - val_loss: -3.1593\n",
      "Epoch 634/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3026 - val_loss: -3.9972\n",
      "Epoch 635/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3472 - val_loss: -3.5525\n",
      "Epoch 636/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3795 - val_loss: -4.0486\n",
      "Epoch 637/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3527 - val_loss: -3.9741\n",
      "Epoch 638/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3388 - val_loss: -1.1783\n",
      "Epoch 639/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.9883 - val_loss: -4.3242\n",
      "Epoch 640/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1308 - val_loss: -4.2647\n",
      "Epoch 641/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3555 - val_loss: -4.4828\n",
      "Epoch 642/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3113 - val_loss: -2.7416\n",
      "Epoch 643/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1829 - val_loss: -4.2129\n",
      "Epoch 644/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3376 - val_loss: -2.8368\n",
      "Epoch 645/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0584 - val_loss: -4.5130\n",
      "Epoch 646/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3506 - val_loss: -4.2115\n",
      "Epoch 647/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3101 - val_loss: -3.9135\n",
      "Epoch 648/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2388 - val_loss: -3.8243\n",
      "Epoch 649/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3180 - val_loss: -3.6757\n",
      "Epoch 650/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2794"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -5.2794 - val_loss: -4.8550\n",
      "Epoch 651/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3428 - val_loss: -4.3881\n",
      "Epoch 652/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3317 - val_loss: -4.4138\n",
      "Epoch 653/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2194 - val_loss: -3.5517\n",
      "Epoch 654/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.0878 - val_loss: -4.0534\n",
      "Epoch 655/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1848 - val_loss: -4.0275\n",
      "Epoch 656/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9335 - val_loss: -4.5003\n",
      "Epoch 657/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1696 - val_loss: -4.6659\n",
      "Epoch 658/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0351 - val_loss: -4.0147\n",
      "Epoch 659/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2164 - val_loss: -4.1454\n",
      "Epoch 660/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1960 - val_loss: -4.1540\n",
      "Epoch 661/2000\n",
      "118/118 [==============================] - 21s 179ms/step - loss: -5.2038 - val_loss: -4.3365\n",
      "Epoch 662/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2770 - val_loss: -4.4646\n",
      "Epoch 663/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9732 - val_loss: -4.2637\n",
      "Epoch 664/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3341 - val_loss: -4.1886\n",
      "Epoch 665/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3743 - val_loss: -4.4774\n",
      "Epoch 666/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3133 - val_loss: -4.2697\n",
      "Epoch 667/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3371 - val_loss: -4.0527\n",
      "Epoch 668/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2724 - val_loss: -0.2906\n",
      "Epoch 669/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2770 - val_loss: -4.0597\n",
      "Epoch 670/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2494 - val_loss: -4.6702\n",
      "Epoch 671/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3018 - val_loss: -4.4010\n",
      "Epoch 672/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3379 - val_loss: -4.0098\n",
      "Epoch 673/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3436 - val_loss: -4.3250\n",
      "Epoch 674/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2625 - val_loss: -4.4510\n",
      "Epoch 675/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2356 - val_loss: -4.1996\n",
      "Epoch 676/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3130 - val_loss: -4.6924\n",
      "Epoch 677/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2522 - val_loss: -4.2136\n",
      "Epoch 678/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3049 - val_loss: -4.4955\n",
      "Epoch 679/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4045 - val_loss: -4.1881\n",
      "Epoch 680/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3117 - val_loss: -4.4928\n",
      "Epoch 681/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3030 - val_loss: -3.3446\n",
      "Epoch 682/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3780 - val_loss: -3.4089\n",
      "Epoch 683/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3301 - val_loss: -4.2500\n",
      "Epoch 684/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3510 - val_loss: -4.4883\n",
      "Epoch 685/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3289 - val_loss: -2.0706\n",
      "Epoch 686/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2645 - val_loss: -3.8317\n",
      "Epoch 687/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3182 - val_loss: -4.3428\n",
      "Epoch 688/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3948 - val_loss: 1.0327\n",
      "Epoch 689/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3272 - val_loss: -2.9448\n",
      "Epoch 690/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2665 - val_loss: -3.9894\n",
      "Epoch 691/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2178 - val_loss: -4.4443\n",
      "Epoch 692/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2401 - val_loss: -4.2302\n",
      "Epoch 693/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3196 - val_loss: -4.6024\n",
      "Epoch 694/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3068 - val_loss: -2.3312\n",
      "Epoch 695/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3342 - val_loss: -3.2422\n",
      "Epoch 696/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8099 - val_loss: -3.3544\n",
      "Epoch 697/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5846 - val_loss: -2.7614\n",
      "Epoch 698/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9407 - val_loss: -4.4307\n",
      "Epoch 699/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1579 - val_loss: -4.7688\n",
      "Epoch 700/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1679 - val_loss: -4.5654\n",
      "Epoch 701/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2199 - val_loss: -3.6993\n",
      "Epoch 702/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1883 - val_loss: -4.6689\n",
      "Epoch 703/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2482 - val_loss: -4.3194\n",
      "Epoch 704/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2878 - val_loss: -4.4164\n",
      "Epoch 705/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2790 - val_loss: -4.4726\n",
      "Epoch 706/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1120 - val_loss: -4.5742\n",
      "Epoch 707/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3418 - val_loss: -4.6382\n",
      "Epoch 708/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2594 - val_loss: -4.6972\n",
      "Epoch 709/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2492 - val_loss: -3.5727\n",
      "Epoch 710/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3939 - val_loss: -4.1512\n",
      "Epoch 711/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2034 - val_loss: -4.6582\n",
      "Epoch 712/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3473 - val_loss: -3.8318\n",
      "Epoch 713/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3347 - val_loss: -4.4017\n",
      "Epoch 714/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2582 - val_loss: -4.4415\n",
      "Epoch 715/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3694 - val_loss: -3.5409\n",
      "Epoch 716/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2832 - val_loss: -3.1746\n",
      "Epoch 717/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3769 - val_loss: -1.1216\n",
      "Epoch 718/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2565 - val_loss: -4.3451\n",
      "Epoch 719/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7316 - val_loss: -3.9174\n",
      "Epoch 720/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.1850 - val_loss: -4.3335\n",
      "Epoch 721/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2504 - val_loss: -3.6992\n",
      "Epoch 722/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1071 - val_loss: -4.5424\n",
      "Epoch 723/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3625 - val_loss: 19.5846\n",
      "Epoch 724/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7257 - val_loss: -4.2397\n",
      "Epoch 725/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2928 - val_loss: -3.8505\n",
      "Epoch 726/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2514 - val_loss: -4.2121\n",
      "Epoch 727/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3301 - val_loss: -1.3355\n",
      "Epoch 728/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3186 - val_loss: -4.1714\n",
      "Epoch 729/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2823 - val_loss: -3.5326\n",
      "Epoch 730/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2661 - val_loss: -4.5896\n",
      "Epoch 731/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3067 - val_loss: -4.2288\n",
      "Epoch 732/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3745 - val_loss: -4.1305\n",
      "Epoch 733/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2886 - val_loss: -4.2522\n",
      "Epoch 734/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3904 - val_loss: -4.5384\n",
      "Epoch 735/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2035 - val_loss: -4.5831\n",
      "Epoch 736/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3842 - val_loss: -3.0457\n",
      "Epoch 737/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2995 - val_loss: -4.2086\n",
      "Epoch 738/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3480 - val_loss: -3.4480\n",
      "Epoch 739/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2473 - val_loss: -4.4810\n",
      "Epoch 740/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3750 - val_loss: -2.4177\n",
      "Epoch 741/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2477 - val_loss: -3.5178\n",
      "Epoch 742/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2542 - val_loss: -4.6080\n",
      "Epoch 743/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3193 - val_loss: -4.1916\n",
      "Epoch 744/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2440 - val_loss: -4.3708\n",
      "Epoch 745/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3715 - val_loss: -3.1510\n",
      "Epoch 746/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3348 - val_loss: -4.6014\n",
      "Epoch 747/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4032 - val_loss: -4.3358\n",
      "Epoch 748/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2238 - val_loss: -3.5849\n",
      "Epoch 749/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1808 - val_loss: -4.6398\n",
      "Epoch 750/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2469 - val_loss: -4.6454\n",
      "Epoch 751/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3366 - val_loss: -3.6529\n",
      "Epoch 752/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3764 - val_loss: -3.7696\n",
      "Epoch 753/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3794 - val_loss: -2.5205\n",
      "Epoch 754/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2325 - val_loss: -3.9121\n",
      "Epoch 755/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3406 - val_loss: -4.0568\n",
      "Epoch 756/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3815 - val_loss: -4.0520\n",
      "Epoch 757/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3479 - val_loss: -3.8823\n",
      "Epoch 758/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0204 - val_loss: -4.5225\n",
      "Epoch 759/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1276 - val_loss: -4.1902\n",
      "Epoch 760/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0288 - val_loss: -4.3591\n",
      "Epoch 761/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.2557 - val_loss: 2.5100\n",
      "Epoch 762/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7865 - val_loss: -3.6814\n",
      "Epoch 763/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3014 - val_loss: -4.3684\n",
      "Epoch 764/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1845 - val_loss: -4.4017\n",
      "Epoch 765/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2792 - val_loss: -4.5050\n",
      "Epoch 766/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3077 - val_loss: -3.0727\n",
      "Epoch 767/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3199 - val_loss: -4.5496\n",
      "Epoch 768/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9557 - val_loss: -4.5301\n",
      "Epoch 769/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2466 - val_loss: -4.5919\n",
      "Epoch 770/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1110 - val_loss: -4.6168\n",
      "Epoch 771/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2281 - val_loss: -4.5688\n",
      "Epoch 772/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4020 - val_loss: -4.1145\n",
      "Epoch 773/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1227 - val_loss: -4.6008\n",
      "Epoch 774/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3555 - val_loss: -4.2630\n",
      "Epoch 775/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3702 - val_loss: -4.2203\n",
      "Epoch 776/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3317 - val_loss: -4.7382\n",
      "Epoch 777/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.2994 - val_loss: -4.7658\n",
      "Epoch 778/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3257 - val_loss: -3.2302\n",
      "Epoch 779/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2779 - val_loss: -4.3069\n",
      "Epoch 780/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3230 - val_loss: -4.8300\n",
      "Epoch 781/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2504 - val_loss: -4.4236\n",
      "Epoch 782/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4480 - val_loss: -3.7795\n",
      "Epoch 783/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.2586 - val_loss: -3.8595\n",
      "Epoch 784/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3585 - val_loss: -4.7689\n",
      "Epoch 785/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.4260"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 237ms/step - loss: -5.4260 - val_loss: -4.8815\n",
      "Epoch 786/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3485 - val_loss: -4.4772\n",
      "Epoch 787/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2876 - val_loss: -4.2225\n",
      "Epoch 788/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.3651 - val_loss: -1.5318\n",
      "Epoch 789/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4224 - val_loss: -3.9392\n",
      "Epoch 790/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3740 - val_loss: -2.9105\n",
      "Epoch 791/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1014 - val_loss: -3.9339\n",
      "Epoch 792/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2783 - val_loss: -3.9245\n",
      "Epoch 793/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7827 - val_loss: -4.3343\n",
      "Epoch 794/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2142 - val_loss: -3.9942\n",
      "Epoch 795/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2532 - val_loss: -3.4615\n",
      "Epoch 796/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3000 - val_loss: -4.4909\n",
      "Epoch 797/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2394 - val_loss: -4.7618\n",
      "Epoch 798/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2918 - val_loss: -4.0045\n",
      "Epoch 799/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1754 - val_loss: -4.4196\n",
      "Epoch 800/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3870 - val_loss: -4.0614\n",
      "Epoch 801/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1576 - val_loss: -3.7849\n",
      "Epoch 802/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4091 - val_loss: -4.1284\n",
      "Epoch 803/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3560 - val_loss: -4.2645\n",
      "Epoch 804/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4252 - val_loss: -4.0678\n",
      "Epoch 805/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0465 - val_loss: -4.5254\n",
      "Epoch 806/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3650 - val_loss: -4.3356\n",
      "Epoch 807/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2858 - val_loss: -4.5351\n",
      "Epoch 808/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3058 - val_loss: -4.5880\n",
      "Epoch 809/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3549 - val_loss: -4.5296\n",
      "Epoch 810/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2695 - val_loss: -2.0746\n",
      "Epoch 811/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2874 - val_loss: -4.4162\n",
      "Epoch 812/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3961 - val_loss: -4.1217\n",
      "Epoch 813/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2667 - val_loss: -4.3568\n",
      "Epoch 814/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3396 - val_loss: -4.4532\n",
      "Epoch 815/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3931 - val_loss: -3.8853\n",
      "Epoch 816/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2746 - val_loss: -4.2427\n",
      "Epoch 817/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3128 - val_loss: -4.2242\n",
      "Epoch 818/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2691 - val_loss: -4.2183\n",
      "Epoch 819/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3439 - val_loss: -4.6213\n",
      "Epoch 820/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2889 - val_loss: -4.2189\n",
      "Epoch 821/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3503 - val_loss: -3.6495\n",
      "Epoch 822/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3465 - val_loss: -3.9930\n",
      "Epoch 823/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2733 - val_loss: -4.4226\n",
      "Epoch 824/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2316 - val_loss: -4.3146\n",
      "Epoch 825/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4392 - val_loss: -4.2736\n",
      "Epoch 826/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2426 - val_loss: -4.5035\n",
      "Epoch 827/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3324 - val_loss: -4.7779\n",
      "Epoch 828/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3689 - val_loss: -3.0765\n",
      "Epoch 829/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1946 - val_loss: -2.7309\n",
      "Epoch 830/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3637 - val_loss: -3.9594\n",
      "Epoch 831/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4018 - val_loss: -4.3325\n",
      "Epoch 832/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2888 - val_loss: -4.5610\n",
      "Epoch 833/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4378 - val_loss: -4.1362\n",
      "Epoch 834/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3008 - val_loss: -4.5128\n",
      "Epoch 835/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2550 - val_loss: -4.6994\n",
      "Epoch 836/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3948 - val_loss: -4.0695\n",
      "Epoch 837/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3127 - val_loss: -4.0981\n",
      "Epoch 838/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3366 - val_loss: -4.5763\n",
      "Epoch 839/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4546 - val_loss: -3.7821\n",
      "Epoch 840/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3435 - val_loss: -3.9006\n",
      "Epoch 841/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3411 - val_loss: -3.3640\n",
      "Epoch 842/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4137 - val_loss: -4.3200\n",
      "Epoch 843/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.3895 - val_loss: -4.7880\n",
      "Epoch 844/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3820 - val_loss: -1.8008\n",
      "Epoch 845/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9066 - val_loss: -4.4070\n",
      "Epoch 846/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3324 - val_loss: -3.8143\n",
      "Epoch 847/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2443 - val_loss: -3.1335\n",
      "Epoch 848/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3090 - val_loss: -2.6736\n",
      "Epoch 849/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3458 - val_loss: -4.1856\n",
      "Epoch 850/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2817 - val_loss: -2.8511\n",
      "Epoch 851/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4200 - val_loss: -3.7718\n",
      "Epoch 852/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1853 - val_loss: -4.1198\n",
      "Epoch 853/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4152 - val_loss: 0.3486\n",
      "Epoch 854/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.2807 - val_loss: -4.1851\n",
      "Epoch 855/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3559 - val_loss: -4.3136\n",
      "Epoch 856/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3546 - val_loss: -4.3738\n",
      "Epoch 857/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4419 - val_loss: -4.3944\n",
      "Epoch 858/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1714 - val_loss: -4.2013\n",
      "Epoch 859/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2565 - val_loss: -3.8649\n",
      "Epoch 860/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3691 - val_loss: -4.6157\n",
      "Epoch 861/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3482 - val_loss: -4.3547\n",
      "Epoch 862/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1984 - val_loss: -4.0729\n",
      "Epoch 863/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3734 - val_loss: -3.6252\n",
      "Epoch 864/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3303 - val_loss: -3.9375\n",
      "Epoch 865/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2727 - val_loss: -4.2122\n",
      "Epoch 866/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4056 - val_loss: -4.5168\n",
      "Epoch 867/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3961 - val_loss: -0.2170\n",
      "Epoch 868/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3886 - val_loss: -3.9349\n",
      "Epoch 869/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3787"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 256ms/step - loss: -5.3787 - val_loss: -4.8818\n",
      "Epoch 870/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3713 - val_loss: -3.9037\n",
      "Epoch 871/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3862 - val_loss: -3.6920\n",
      "Epoch 872/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4063 - val_loss: -4.5233\n",
      "Epoch 873/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3328 - val_loss: -4.5554\n",
      "Epoch 874/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7294 - val_loss: -3.8477\n",
      "Epoch 875/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1829 - val_loss: -3.8351\n",
      "Epoch 876/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2336 - val_loss: -4.5651\n",
      "Epoch 877/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3326 - val_loss: -3.5045\n",
      "Epoch 878/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1526 - val_loss: -4.2620\n",
      "Epoch 879/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3263 - val_loss: -1.3172\n",
      "Epoch 880/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2687 - val_loss: -4.4141\n",
      "Epoch 881/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3803 - val_loss: -4.0365\n",
      "Epoch 882/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3725 - val_loss: -4.1027\n",
      "Epoch 883/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3454 - val_loss: -2.2565\n",
      "Epoch 884/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3592 - val_loss: -4.4958\n",
      "Epoch 885/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3755 - val_loss: -4.0694\n",
      "Epoch 886/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3027 - val_loss: -4.4512\n",
      "Epoch 887/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3436 - val_loss: -4.4887\n",
      "Epoch 888/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4348 - val_loss: -4.5687\n",
      "Epoch 889/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3102 - val_loss: -3.4326\n",
      "Epoch 890/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1766 - val_loss: -4.6373\n",
      "Epoch 891/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3511 - val_loss: -3.6211\n",
      "Epoch 892/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2614 - val_loss: -4.3049\n",
      "Epoch 893/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3708 - val_loss: -4.4896\n",
      "Epoch 894/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3824 - val_loss: -4.4305\n",
      "Epoch 895/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3318"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -5.3318 - val_loss: -4.9536\n",
      "Epoch 896/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3563 - val_loss: -4.4745\n",
      "Epoch 897/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2414 - val_loss: -3.3565\n",
      "Epoch 898/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3917 - val_loss: -2.9311\n",
      "Epoch 899/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3319 - val_loss: -4.2995\n",
      "Epoch 900/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4797 - val_loss: -4.5795\n",
      "Epoch 901/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3867 - val_loss: -3.9929\n",
      "Epoch 902/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4580 - val_loss: -4.7811\n",
      "Epoch 903/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3348 - val_loss: -4.4461\n",
      "Epoch 904/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3344 - val_loss: -3.3629\n",
      "Epoch 905/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3980 - val_loss: -4.7419\n",
      "Epoch 906/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3764 - val_loss: -1.0905\n",
      "Epoch 907/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2447 - val_loss: -4.5978\n",
      "Epoch 908/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4065 - val_loss: -4.5241\n",
      "Epoch 909/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4770 - val_loss: -1.6768\n",
      "Epoch 910/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2541 - val_loss: -4.1657\n",
      "Epoch 911/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3866 - val_loss: -4.8643\n",
      "Epoch 912/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4509 - val_loss: -3.1000\n",
      "Epoch 913/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2858 - val_loss: -4.6112\n",
      "Epoch 914/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8379 - val_loss: -3.9180\n",
      "Epoch 915/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3434 - val_loss: -4.3201\n",
      "Epoch 916/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2796 - val_loss: -3.5662\n",
      "Epoch 917/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3317 - val_loss: -4.0046\n",
      "Epoch 918/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4400 - val_loss: -3.3535\n",
      "Epoch 919/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3629 - val_loss: -3.9488\n",
      "Epoch 920/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2479 - val_loss: -3.0069\n",
      "Epoch 921/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3558 - val_loss: -4.6775\n",
      "Epoch 922/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4052 - val_loss: -4.4429\n",
      "Epoch 923/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2676 - val_loss: -4.2013\n",
      "Epoch 924/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4526 - val_loss: -4.3967\n",
      "Epoch 925/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4491 - val_loss: -4.5132\n",
      "Epoch 926/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3519 - val_loss: -4.3711\n",
      "Epoch 927/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3711 - val_loss: -4.3308\n",
      "Epoch 928/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4453 - val_loss: -2.7587\n",
      "Epoch 929/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4610 - val_loss: -4.5991\n",
      "Epoch 930/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4750 - val_loss: -4.0886\n",
      "Epoch 931/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2446 - val_loss: -4.6102\n",
      "Epoch 932/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3391 - val_loss: -4.3538\n",
      "Epoch 933/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2916 - val_loss: -4.3618\n",
      "Epoch 934/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4020 - val_loss: -4.4185\n",
      "Epoch 935/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4279 - val_loss: -2.4319\n",
      "Epoch 936/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3687 - val_loss: -3.9199\n",
      "Epoch 937/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1529 - val_loss: -4.5753\n",
      "Epoch 938/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3694 - val_loss: -4.2573\n",
      "Epoch 939/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4288 - val_loss: -2.1015\n",
      "Epoch 940/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4286 - val_loss: -4.2167\n",
      "Epoch 941/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4143 - val_loss: -4.1260\n",
      "Epoch 942/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3943 - val_loss: -4.6931\n",
      "Epoch 943/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3824 - val_loss: -4.8295\n",
      "Epoch 944/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4158 - val_loss: -2.4547\n",
      "Epoch 945/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4049 - val_loss: -4.0372\n",
      "Epoch 946/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4854 - val_loss: -4.6092\n",
      "Epoch 947/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3965 - val_loss: -3.9601\n",
      "Epoch 948/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3953 - val_loss: -4.3713\n",
      "Epoch 949/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3660 - val_loss: -3.9231\n",
      "Epoch 950/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3951 - val_loss: -3.5271\n",
      "Epoch 951/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2556 - val_loss: -4.7873\n",
      "Epoch 952/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3824 - val_loss: -3.9034\n",
      "Epoch 953/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3713 - val_loss: -4.5681\n",
      "Epoch 954/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3412 - val_loss: -4.6152\n",
      "Epoch 955/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3124 - val_loss: -4.6521\n",
      "Epoch 956/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4467 - val_loss: -3.6002\n",
      "Epoch 957/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3413 - val_loss: -4.5211\n",
      "Epoch 958/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4748 - val_loss: -3.7650\n",
      "Epoch 959/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4965 - val_loss: -4.5589\n",
      "Epoch 960/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2962 - val_loss: -4.5715\n",
      "Epoch 961/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2839 - val_loss: -4.4908\n",
      "Epoch 962/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4662 - val_loss: 1.2773\n",
      "Epoch 963/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3003 - val_loss: -3.6330\n",
      "Epoch 964/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4048 - val_loss: -4.6177\n",
      "Epoch 965/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4094 - val_loss: -4.5394\n",
      "Epoch 966/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4377 - val_loss: -4.6317\n",
      "Epoch 967/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2377 - val_loss: -3.7379\n",
      "Epoch 968/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9392 - val_loss: -4.2655\n",
      "Epoch 969/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2307 - val_loss: -4.5508\n",
      "Epoch 970/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3012 - val_loss: -4.6224\n",
      "Epoch 971/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3126 - val_loss: -4.5784\n",
      "Epoch 972/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4592 - val_loss: -4.3100\n",
      "Epoch 973/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4261 - val_loss: 0.3119\n",
      "Epoch 974/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2987 - val_loss: -4.3786\n",
      "Epoch 975/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3549 - val_loss: -4.0741\n",
      "Epoch 976/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3566 - val_loss: -3.9105\n",
      "Epoch 977/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4048 - val_loss: -3.6545\n",
      "Epoch 978/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4232 - val_loss: -4.7136\n",
      "Epoch 979/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3469 - val_loss: -3.7742\n",
      "Epoch 980/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4214 - val_loss: -3.5394\n",
      "Epoch 981/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4618 - val_loss: -2.4822\n",
      "Epoch 982/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3925 - val_loss: -3.8475\n",
      "Epoch 983/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4317 - val_loss: -4.6267\n",
      "Epoch 984/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4931 - val_loss: -4.0125\n",
      "Epoch 985/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3521 - val_loss: -4.7223\n",
      "Epoch 986/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4548 - val_loss: -3.8543\n",
      "Epoch 987/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2300 - val_loss: -4.2143\n",
      "Epoch 988/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5102 - val_loss: -4.6212\n",
      "Epoch 989/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4242 - val_loss: -4.5931\n",
      "Epoch 990/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2936 - val_loss: -4.4471\n",
      "Epoch 991/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4972 - val_loss: -4.6004\n",
      "Epoch 992/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3140 - val_loss: -4.7532\n",
      "Epoch 993/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4340 - val_loss: -4.4288\n",
      "Epoch 994/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4598 - val_loss: -3.8861\n",
      "Epoch 995/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4123 - val_loss: -4.4938\n",
      "Epoch 996/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4186 - val_loss: -4.3827\n",
      "Epoch 997/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2145 - val_loss: -3.1205\n",
      "Epoch 998/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4070 - val_loss: -4.3328\n",
      "Epoch 999/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3696 - val_loss: -4.0488\n",
      "Epoch 1000/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3318 - val_loss: -3.6403\n",
      "Epoch 1001/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8632 - val_loss: -4.7347\n",
      "Epoch 1002/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3315 - val_loss: -4.2530\n",
      "Epoch 1003/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3361 - val_loss: -3.8235\n",
      "Epoch 1004/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2941 - val_loss: -3.6406\n",
      "Epoch 1005/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3549 - val_loss: -4.3597\n",
      "Epoch 1006/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3674 - val_loss: -4.6341\n",
      "Epoch 1007/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3353 - val_loss: -4.0614\n",
      "Epoch 1008/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3497 - val_loss: -4.4091\n",
      "Epoch 1009/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3565 - val_loss: -4.3141\n",
      "Epoch 1010/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3644 - val_loss: -4.2179\n",
      "Epoch 1011/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4693 - val_loss: 2.7383\n",
      "Epoch 1012/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4109 - val_loss: -3.4555\n",
      "Epoch 1013/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3772 - val_loss: -4.5342\n",
      "Epoch 1014/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3656 - val_loss: -4.5115\n",
      "Epoch 1015/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4608 - val_loss: -4.4725\n",
      "Epoch 1016/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4406 - val_loss: -3.5496\n",
      "Epoch 1017/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3902 - val_loss: -4.5088\n",
      "Epoch 1018/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4335 - val_loss: -4.5312\n",
      "Epoch 1019/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2626 - val_loss: -4.4546\n",
      "Epoch 1020/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4827 - val_loss: -3.9479\n",
      "Epoch 1021/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4457 - val_loss: -3.8514\n",
      "Epoch 1022/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4614 - val_loss: -4.6784\n",
      "Epoch 1023/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4050 - val_loss: -3.5693\n",
      "Epoch 1024/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3995 - val_loss: -4.4302\n",
      "Epoch 1025/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4294 - val_loss: -0.2538\n",
      "Epoch 1026/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4234 - val_loss: -4.1484\n",
      "Epoch 1027/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4529 - val_loss: -4.8971\n",
      "Epoch 1028/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3938 - val_loss: -3.3557\n",
      "Epoch 1029/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4553 - val_loss: -1.5602\n",
      "Epoch 1030/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4389 - val_loss: -2.2530\n",
      "Epoch 1031/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8384 - val_loss: -4.3380\n",
      "Epoch 1032/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3196 - val_loss: -1.7871\n",
      "Epoch 1033/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3281 - val_loss: -4.6821\n",
      "Epoch 1034/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2338 - val_loss: -3.1612\n",
      "Epoch 1035/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4194 - val_loss: -3.8032\n",
      "Epoch 1036/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3438 - val_loss: -4.1195\n",
      "Epoch 1037/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3723 - val_loss: -4.5905\n",
      "Epoch 1038/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2566 - val_loss: -4.1583\n",
      "Epoch 1039/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3906 - val_loss: -2.6323\n",
      "Epoch 1040/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4074 - val_loss: -4.0002\n",
      "Epoch 1041/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.5014 - val_loss: -4.7859\n",
      "Epoch 1042/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5075 - val_loss: -4.1406\n",
      "Epoch 1043/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.4006 - val_loss: -2.2455\n",
      "Epoch 1044/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -2.7392 - val_loss: -2.6187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1045/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.0464 - val_loss: -2.7893\n",
      "Epoch 1046/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.2248 - val_loss: -3.1823\n",
      "Epoch 1047/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -3.3625 - val_loss: -3.2561\n",
      "Epoch 1048/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.4977 - val_loss: -3.2661\n",
      "Epoch 1049/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.5749 - val_loss: -3.3462\n",
      "Epoch 1050/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -3.6402 - val_loss: -3.3427\n",
      "Epoch 1051/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.7450 - val_loss: -3.4585\n",
      "Epoch 1052/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.8243 - val_loss: -3.4335\n",
      "Epoch 1053/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9035 - val_loss: -3.6456\n",
      "Epoch 1054/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9272 - val_loss: -3.5832\n",
      "Epoch 1055/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9321 - val_loss: -3.5630\n",
      "Epoch 1056/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0249 - val_loss: -3.6600\n",
      "Epoch 1057/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0838 - val_loss: -3.6878\n",
      "Epoch 1058/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1053 - val_loss: -3.6705\n",
      "Epoch 1059/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1480 - val_loss: -3.9681\n",
      "Epoch 1060/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1861 - val_loss: -3.9598\n",
      "Epoch 1061/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.1884 - val_loss: -3.7500\n",
      "Epoch 1062/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2310 - val_loss: -3.8226\n",
      "Epoch 1063/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.7286 - val_loss: -3.7321\n",
      "Epoch 1064/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.1814 - val_loss: -3.4709\n",
      "Epoch 1065/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7687 - val_loss: -4.0481\n",
      "Epoch 1066/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9185 - val_loss: -2.0694\n",
      "Epoch 1067/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0359 - val_loss: -3.7898\n",
      "Epoch 1068/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1000 - val_loss: -3.8263\n",
      "Epoch 1069/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9786 - val_loss: -3.2102\n",
      "Epoch 1070/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2351 - val_loss: -3.9362\n",
      "Epoch 1071/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2231 - val_loss: -3.8865\n",
      "Epoch 1072/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2300 - val_loss: -4.0484\n",
      "Epoch 1073/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1795 - val_loss: -3.9527\n",
      "Epoch 1074/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2247 - val_loss: -3.9486\n",
      "Epoch 1075/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2948 - val_loss: -3.7532\n",
      "Epoch 1076/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3113 - val_loss: -3.9412\n",
      "Epoch 1077/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2433 - val_loss: -4.1164\n",
      "Epoch 1078/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3025 - val_loss: -4.0022\n",
      "Epoch 1079/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2908 - val_loss: -4.2673\n",
      "Epoch 1080/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3212 - val_loss: -4.1625\n",
      "Epoch 1081/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2822 - val_loss: -3.9311\n",
      "Epoch 1082/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3600 - val_loss: -3.9563\n",
      "Epoch 1083/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3704 - val_loss: -3.8931\n",
      "Epoch 1084/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3390 - val_loss: -3.0959\n",
      "Epoch 1085/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -4.2824 - val_loss: -4.1469\n",
      "Epoch 1086/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.7339 - val_loss: -4.0060\n",
      "Epoch 1087/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3157 - val_loss: -3.6300\n",
      "Epoch 1088/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2304 - val_loss: -3.9262\n",
      "Epoch 1089/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2983 - val_loss: -3.8337\n",
      "Epoch 1090/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2278 - val_loss: -3.5161\n",
      "Epoch 1091/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1967 - val_loss: -2.8876\n",
      "Epoch 1092/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3440 - val_loss: -3.9180\n",
      "Epoch 1093/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3624 - val_loss: -4.0991\n",
      "Epoch 1094/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3969 - val_loss: -4.2541\n",
      "Epoch 1095/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3554 - val_loss: -4.2016\n",
      "Epoch 1096/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4201 - val_loss: -3.9950\n",
      "Epoch 1097/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.3411 - val_loss: -3.2049\n",
      "Epoch 1098/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4896 - val_loss: -4.4026\n",
      "Epoch 1099/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4946 - val_loss: -3.8130\n",
      "Epoch 1100/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4268 - val_loss: -4.1767\n",
      "Epoch 1101/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4498 - val_loss: -4.3095\n",
      "Epoch 1102/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3945 - val_loss: -3.9148\n",
      "Epoch 1103/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4075 - val_loss: -4.4661\n",
      "Epoch 1104/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2980 - val_loss: -3.3232\n",
      "Epoch 1105/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.5507 - val_loss: -4.2005\n",
      "Epoch 1106/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5355 - val_loss: -4.3021\n",
      "Epoch 1107/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.5967 - val_loss: -2.6827\n",
      "Epoch 1108/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5003 - val_loss: -4.2820\n",
      "Epoch 1109/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4224 - val_loss: -4.4762\n",
      "Epoch 1110/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.7227 - val_loss: -4.4151\n",
      "Epoch 1111/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3034 - val_loss: -4.5399\n",
      "Epoch 1112/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4910 - val_loss: -3.8574\n",
      "Epoch 1113/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5507 - val_loss: -4.4096\n",
      "Epoch 1114/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5793 - val_loss: -4.6427\n",
      "Epoch 1115/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7422 - val_loss: -3.5057\n",
      "Epoch 1116/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6947 - val_loss: -4.0995\n",
      "Epoch 1117/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7250 - val_loss: -4.5731\n",
      "Epoch 1118/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7903 - val_loss: -4.6030\n",
      "Epoch 1119/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7867 - val_loss: -4.1450\n",
      "Epoch 1120/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8144 - val_loss: -4.6039\n",
      "Epoch 1121/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8373 - val_loss: -4.6712\n",
      "Epoch 1122/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8836 - val_loss: -4.5566\n",
      "Epoch 1123/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8765 - val_loss: -4.6034\n",
      "Epoch 1124/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7748 - val_loss: -4.3919\n",
      "Epoch 1125/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9196 - val_loss: -4.6961\n",
      "Epoch 1126/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9678 - val_loss: -4.1670\n",
      "Epoch 1127/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9235 - val_loss: -4.5368\n",
      "Epoch 1128/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0308 - val_loss: -3.5878\n",
      "Epoch 1129/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0174 - val_loss: -3.9469\n",
      "Epoch 1130/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0484 - val_loss: -3.1960\n",
      "Epoch 1131/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7908 - val_loss: -3.9860\n",
      "Epoch 1132/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -4.9739 - val_loss: -4.2976\n",
      "Epoch 1133/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0532 - val_loss: -4.4442\n",
      "Epoch 1134/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9206 - val_loss: -4.1455\n",
      "Epoch 1135/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7531 - val_loss: -4.2692\n",
      "Epoch 1136/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0563 - val_loss: -4.2961\n",
      "Epoch 1137/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0246 - val_loss: -4.4940\n",
      "Epoch 1138/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.0687 - val_loss: -3.2531\n",
      "Epoch 1139/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0308 - val_loss: -1.6075\n",
      "Epoch 1140/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9964 - val_loss: -3.2910\n",
      "Epoch 1141/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0766 - val_loss: -3.8967\n",
      "Epoch 1142/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0914 - val_loss: -4.3732\n",
      "Epoch 1143/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0483 - val_loss: -4.5657\n",
      "Epoch 1144/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1611 - val_loss: -4.3410\n",
      "Epoch 1145/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0884 - val_loss: -4.6062\n",
      "Epoch 1146/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.0807 - val_loss: -4.3489\n",
      "Epoch 1147/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1576 - val_loss: -3.6136\n",
      "Epoch 1148/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4269 - val_loss: -4.0985\n",
      "Epoch 1149/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0471 - val_loss: -4.5226\n",
      "Epoch 1150/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1644 - val_loss: 2.6508\n",
      "Epoch 1151/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9431 - val_loss: -3.3897\n",
      "Epoch 1152/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1692 - val_loss: -3.7062\n",
      "Epoch 1153/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1214 - val_loss: -4.1063\n",
      "Epoch 1154/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1633 - val_loss: -2.7490\n",
      "Epoch 1155/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1303 - val_loss: -3.2343\n",
      "Epoch 1156/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1468 - val_loss: -3.9291\n",
      "Epoch 1157/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1498 - val_loss: -2.2160\n",
      "Epoch 1158/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9593 - val_loss: -4.2066\n",
      "Epoch 1159/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2275 - val_loss: -3.9704\n",
      "Epoch 1160/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1674 - val_loss: -3.8044\n",
      "Epoch 1161/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2325 - val_loss: -4.0459\n",
      "Epoch 1162/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0917 - val_loss: -4.2279\n",
      "Epoch 1163/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2366 - val_loss: -4.4868\n",
      "Epoch 1164/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1375 - val_loss: -4.4447\n",
      "Epoch 1165/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2604 - val_loss: -4.3868\n",
      "Epoch 1166/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2734 - val_loss: -4.1550\n",
      "Epoch 1167/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9904 - val_loss: -3.9988\n",
      "Epoch 1168/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0641 - val_loss: -4.2193\n",
      "Epoch 1169/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1794 - val_loss: -4.7627\n",
      "Epoch 1170/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2579 - val_loss: -3.2628\n",
      "Epoch 1171/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1444 - val_loss: -4.1351\n",
      "Epoch 1172/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2696 - val_loss: -4.1105\n",
      "Epoch 1173/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6716 - val_loss: -3.9825\n",
      "Epoch 1174/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1786 - val_loss: -4.4577\n",
      "Epoch 1175/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1936 - val_loss: -4.6081\n",
      "Epoch 1176/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1820 - val_loss: -2.2129\n",
      "Epoch 1177/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2935 - val_loss: 1.1082\n",
      "Epoch 1178/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1556 - val_loss: -2.5768\n",
      "Epoch 1179/2000\n",
      "118/118 [==============================] - 21s 179ms/step - loss: -5.1978 - val_loss: -4.3616\n",
      "Epoch 1180/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7680 - val_loss: -4.3996\n",
      "Epoch 1181/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1687 - val_loss: -4.4308\n",
      "Epoch 1182/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1276 - val_loss: -4.2587\n",
      "Epoch 1183/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3379 - val_loss: -4.5594\n",
      "Epoch 1184/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2310 - val_loss: -4.6398\n",
      "Epoch 1185/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2052 - val_loss: -3.9595\n",
      "Epoch 1186/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2585 - val_loss: -4.1819\n",
      "Epoch 1187/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2929 - val_loss: -4.6318\n",
      "Epoch 1188/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2603 - val_loss: -4.5181\n",
      "Epoch 1189/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2741 - val_loss: -1.9694\n",
      "Epoch 1190/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.0191 - val_loss: -4.3201\n",
      "Epoch 1191/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0310 - val_loss: -4.3040\n",
      "Epoch 1192/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3227 - val_loss: -4.2128\n",
      "Epoch 1193/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 21s 181ms/step - loss: -5.1611 - val_loss: -4.8782\n",
      "Epoch 1194/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2936 - val_loss: -3.8080\n",
      "Epoch 1195/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2839 - val_loss: -3.8383\n",
      "Epoch 1196/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2338 - val_loss: -4.7411\n",
      "Epoch 1197/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2914 - val_loss: -3.5368\n",
      "Epoch 1198/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3409 - val_loss: -1.3636\n",
      "Epoch 1199/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3054 - val_loss: -0.8033\n",
      "Epoch 1200/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1960 - val_loss: -4.1589\n",
      "Epoch 1201/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3447 - val_loss: -4.6020\n",
      "Epoch 1202/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2656 - val_loss: -4.1914\n",
      "Epoch 1203/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2972 - val_loss: -4.0092\n",
      "Epoch 1204/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3704 - val_loss: -3.3291\n",
      "Epoch 1205/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7903 - val_loss: -4.5904\n",
      "Epoch 1206/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0993 - val_loss: -4.5162\n",
      "Epoch 1207/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2505 - val_loss: -4.2340\n",
      "Epoch 1208/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2649 - val_loss: -4.3894\n",
      "Epoch 1209/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0949 - val_loss: -4.4657\n",
      "Epoch 1210/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3754 - val_loss: -3.9176\n",
      "Epoch 1211/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2844 - val_loss: -4.5779\n",
      "Epoch 1212/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4033 - val_loss: -3.5890\n",
      "Epoch 1213/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2015 - val_loss: -4.1915\n",
      "Epoch 1214/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3485 - val_loss: -4.7130\n",
      "Epoch 1215/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2480 - val_loss: -4.5082\n",
      "Epoch 1216/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1230 - val_loss: -4.1850\n",
      "Epoch 1217/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2359 - val_loss: -3.7182\n",
      "Epoch 1218/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2303 - val_loss: -2.9868\n",
      "Epoch 1219/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3463 - val_loss: -4.4398\n",
      "Epoch 1220/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4097 - val_loss: -3.4064\n",
      "Epoch 1221/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3369 - val_loss: -4.5380\n",
      "Epoch 1222/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3328 - val_loss: -4.7940\n",
      "Epoch 1223/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3659 - val_loss: -4.2197\n",
      "Epoch 1224/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3372 - val_loss: -4.6940\n",
      "Epoch 1225/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2350 - val_loss: -3.4781\n",
      "Epoch 1226/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3885 - val_loss: -3.9069\n",
      "Epoch 1227/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2742 - val_loss: -4.6250\n",
      "Epoch 1228/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3531 - val_loss: -0.6046\n",
      "Epoch 1229/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3667 - val_loss: -3.4786\n",
      "Epoch 1230/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0559 - val_loss: -4.5063\n",
      "Epoch 1231/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1228 - val_loss: -4.1119\n",
      "Epoch 1232/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2055 - val_loss: -4.3748\n",
      "Epoch 1233/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3241 - val_loss: -4.2904\n",
      "Epoch 1234/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3319 - val_loss: -4.7614\n",
      "Epoch 1235/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1062 - val_loss: -3.4634\n",
      "Epoch 1236/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2625 - val_loss: 1.9965\n",
      "Epoch 1237/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1856 - val_loss: -4.1146\n",
      "Epoch 1238/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3275 - val_loss: -3.6434\n",
      "Epoch 1239/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3916 - val_loss: -3.6579\n",
      "Epoch 1240/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3761 - val_loss: -4.4357\n",
      "Epoch 1241/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8461 - val_loss: -4.6233\n",
      "Epoch 1242/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1480 - val_loss: -4.5229\n",
      "Epoch 1243/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3380 - val_loss: -4.1706\n",
      "Epoch 1244/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2327 - val_loss: -4.4224\n",
      "Epoch 1245/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3327 - val_loss: -4.3364\n",
      "Epoch 1246/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2982 - val_loss: -4.0658\n",
      "Epoch 1247/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4043 - val_loss: 0.9351\n",
      "Epoch 1248/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2993 - val_loss: -4.3367\n",
      "Epoch 1249/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3558 - val_loss: -4.5454\n",
      "Epoch 1250/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4432 - val_loss: -3.8036\n",
      "Epoch 1251/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3000 - val_loss: -4.3306\n",
      "Epoch 1252/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3995 - val_loss: -4.3941\n",
      "Epoch 1253/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3152 - val_loss: -4.1176\n",
      "Epoch 1254/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3963 - val_loss: -3.0284\n",
      "Epoch 1255/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3666 - val_loss: -3.5787\n",
      "Epoch 1256/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4915 - val_loss: -4.5535\n",
      "Epoch 1257/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1923 - val_loss: -3.9552\n",
      "Epoch 1258/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2893 - val_loss: -4.5278\n",
      "Epoch 1259/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3392 - val_loss: -4.1196\n",
      "Epoch 1260/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3632 - val_loss: -4.7789\n",
      "Epoch 1261/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3173 - val_loss: -4.0681\n",
      "Epoch 1262/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4011 - val_loss: -4.2085\n",
      "Epoch 1263/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3737 - val_loss: -2.2305\n",
      "Epoch 1264/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3616 - val_loss: -4.8066\n",
      "Epoch 1265/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3195 - val_loss: -4.2564\n",
      "Epoch 1266/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3191 - val_loss: -4.5520\n",
      "Epoch 1267/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3476 - val_loss: -2.5497\n",
      "Epoch 1268/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.4511 - val_loss: -4.2705\n",
      "Epoch 1269/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4413 - val_loss: -4.5043\n",
      "Epoch 1270/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3229 - val_loss: -4.5150\n",
      "Epoch 1271/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3667 - val_loss: -3.9994\n",
      "Epoch 1272/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3955 - val_loss: -2.6711\n",
      "Epoch 1273/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4286 - val_loss: -4.4052\n",
      "Epoch 1274/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4304 - val_loss: -3.4879\n",
      "Epoch 1275/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4554 - val_loss: -4.4448\n",
      "Epoch 1276/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4016 - val_loss: -4.4085\n",
      "Epoch 1277/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3702 - val_loss: -3.9624\n",
      "Epoch 1278/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4592 - val_loss: -4.5620\n",
      "Epoch 1279/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4341 - val_loss: -4.0705\n",
      "Epoch 1280/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3425 - val_loss: -4.0569\n",
      "Epoch 1281/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2814 - val_loss: -4.7033\n",
      "Epoch 1282/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3586 - val_loss: -4.4196\n",
      "Epoch 1283/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3636 - val_loss: -2.7067\n",
      "Epoch 1284/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4237 - val_loss: -4.3236\n",
      "Epoch 1285/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4561 - val_loss: -4.8741\n",
      "Epoch 1286/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4339 - val_loss: -4.3008\n",
      "Epoch 1287/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4926 - val_loss: -1.3801\n",
      "Epoch 1288/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3513 - val_loss: -4.5515\n",
      "Epoch 1289/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4746 - val_loss: -4.4450\n",
      "Epoch 1290/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1476 - val_loss: -4.3746\n",
      "Epoch 1291/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3596 - val_loss: -1.8272\n",
      "Epoch 1292/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4006 - val_loss: -4.8046\n",
      "Epoch 1293/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4330 - val_loss: 2.2966\n",
      "Epoch 1294/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2736 - val_loss: -4.3362\n",
      "Epoch 1295/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3227 - val_loss: -4.3718\n",
      "Epoch 1296/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4499 - val_loss: -4.5208\n",
      "Epoch 1297/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1688 - val_loss: -3.8333\n",
      "Epoch 1298/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4866 - val_loss: -4.2867\n",
      "Epoch 1299/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4022 - val_loss: -4.0479\n",
      "Epoch 1300/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3641 - val_loss: -2.5864\n",
      "Epoch 1301/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4136 - val_loss: -4.1768\n",
      "Epoch 1302/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1388 - val_loss: -3.7131\n",
      "Epoch 1303/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3901 - val_loss: -3.2610\n",
      "Epoch 1304/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4173 - val_loss: -2.8526\n",
      "Epoch 1305/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2458 - val_loss: -4.0293\n",
      "Epoch 1306/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4864 - val_loss: -4.0118\n",
      "Epoch 1307/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4697 - val_loss: -4.1889\n",
      "Epoch 1308/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4963 - val_loss: -3.1927\n",
      "Epoch 1309/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3770 - val_loss: -4.6660\n",
      "Epoch 1310/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4695 - val_loss: -3.9707\n",
      "Epoch 1311/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3042 - val_loss: -4.3346\n",
      "Epoch 1312/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2967 - val_loss: -3.3536\n",
      "Epoch 1313/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4620 - val_loss: -4.4966\n",
      "Epoch 1314/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4282 - val_loss: -4.9005\n",
      "Epoch 1315/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4015 - val_loss: -3.5074\n",
      "Epoch 1316/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4808 - val_loss: -1.2561\n",
      "Epoch 1317/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3690 - val_loss: -4.6237\n",
      "Epoch 1318/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4502 - val_loss: -4.3058\n",
      "Epoch 1319/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5152 - val_loss: -4.4007\n",
      "Epoch 1320/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4496 - val_loss: -4.2222\n",
      "Epoch 1321/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4848 - val_loss: -4.1844\n",
      "Epoch 1322/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4891 - val_loss: -4.6962\n",
      "Epoch 1323/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4304 - val_loss: -4.7043\n",
      "Epoch 1324/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4426 - val_loss: -4.0156\n",
      "Epoch 1325/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3902 - val_loss: -4.5364\n",
      "Epoch 1326/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4469 - val_loss: -3.8690\n",
      "Epoch 1327/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4522 - val_loss: -4.8900\n",
      "Epoch 1328/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4434 - val_loss: -4.9264\n",
      "Epoch 1329/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4716 - val_loss: -4.1195\n",
      "Epoch 1330/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4086 - val_loss: -4.5585\n",
      "Epoch 1331/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5184 - val_loss: -2.0672\n",
      "Epoch 1332/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4041 - val_loss: -4.3224\n",
      "Epoch 1333/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4560 - val_loss: -3.8851\n",
      "Epoch 1334/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5138 - val_loss: -4.1857\n",
      "Epoch 1335/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3658 - val_loss: -4.5739\n",
      "Epoch 1336/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3937 - val_loss: -4.5358\n",
      "Epoch 1337/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5044 - val_loss: -4.6060\n",
      "Epoch 1338/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5294 - val_loss: -3.7184\n",
      "Epoch 1339/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4600 - val_loss: -3.6605\n",
      "Epoch 1340/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4061 - val_loss: 2.1822\n",
      "Epoch 1341/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3959 - val_loss: -3.9971\n",
      "Epoch 1342/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3914 - val_loss: -4.2584\n",
      "Epoch 1343/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5148 - val_loss: -3.6440\n",
      "Epoch 1344/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4797 - val_loss: -4.3523\n",
      "Epoch 1345/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4218 - val_loss: -4.6455\n",
      "Epoch 1346/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5306 - val_loss: -4.2983\n",
      "Epoch 1347/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5065 - val_loss: -3.5365\n",
      "Epoch 1348/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4127 - val_loss: -4.0969\n",
      "Epoch 1349/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5163 - val_loss: -4.5477\n",
      "Epoch 1350/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4270 - val_loss: -4.5507\n",
      "Epoch 1351/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4055 - val_loss: -4.6066\n",
      "Epoch 1352/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4641 - val_loss: -4.3017\n",
      "Epoch 1353/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4799 - val_loss: -4.2972\n",
      "Epoch 1354/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5321 - val_loss: -4.4140\n",
      "Epoch 1355/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2104 - val_loss: -4.2923\n",
      "Epoch 1356/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2078 - val_loss: -3.9939\n",
      "Epoch 1357/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2544 - val_loss: -4.4908\n",
      "Epoch 1358/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3716 - val_loss: -4.5183\n",
      "Epoch 1359/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3963 - val_loss: -3.4364\n",
      "Epoch 1360/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0609 - val_loss: -4.6393\n",
      "Epoch 1361/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3819 - val_loss: -1.9941\n",
      "Epoch 1362/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4253 - val_loss: -3.0863\n",
      "Epoch 1363/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4581 - val_loss: -3.7593\n",
      "Epoch 1364/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4079 - val_loss: -4.1505\n",
      "Epoch 1365/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5210 - val_loss: -4.3370\n",
      "Epoch 1366/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4969 - val_loss: -4.7067\n",
      "Epoch 1367/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4388 - val_loss: -4.4045\n",
      "Epoch 1368/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4514 - val_loss: -3.6996\n",
      "Epoch 1369/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5176 - val_loss: -2.2623\n",
      "Epoch 1370/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5229 - val_loss: -3.5002\n",
      "Epoch 1371/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5016 - val_loss: -4.8643\n",
      "Epoch 1372/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4232 - val_loss: -4.8035\n",
      "Epoch 1373/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4696 - val_loss: -4.0138\n",
      "Epoch 1374/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4901 - val_loss: -4.1629\n",
      "Epoch 1375/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2859 - val_loss: -4.5336\n",
      "Epoch 1376/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5292 - val_loss: -3.7431\n",
      "Epoch 1377/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5051 - val_loss: -4.0789\n",
      "Epoch 1378/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5195 - val_loss: -4.2062\n",
      "Epoch 1379/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4334 - val_loss: -4.4917\n",
      "Epoch 1380/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4719 - val_loss: -4.6416\n",
      "Epoch 1381/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4955 - val_loss: -4.2891\n",
      "Epoch 1382/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4947 - val_loss: -4.7412\n",
      "Epoch 1383/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4272 - val_loss: -4.5840\n",
      "Epoch 1384/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5637 - val_loss: -3.9779\n",
      "Epoch 1385/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4410 - val_loss: -4.4895\n",
      "Epoch 1386/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7797 - val_loss: -4.7070\n",
      "Epoch 1387/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4928 - val_loss: -4.0375\n",
      "Epoch 1388/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4440 - val_loss: -3.6564\n",
      "Epoch 1389/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5063 - val_loss: -4.4342\n",
      "Epoch 1390/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4774 - val_loss: -4.5041\n",
      "Epoch 1391/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4588 - val_loss: -4.1525\n",
      "Epoch 1392/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2685 - val_loss: -4.3599\n",
      "Epoch 1393/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5508 - val_loss: -1.7601\n",
      "Epoch 1394/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5042 - val_loss: -4.0666\n",
      "Epoch 1395/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5089 - val_loss: -4.8950\n",
      "Epoch 1396/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5527 - val_loss: -3.6623\n",
      "Epoch 1397/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.3838 - val_loss: -3.9441\n",
      "Epoch 1398/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4856 - val_loss: -3.5465\n",
      "Epoch 1399/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4952 - val_loss: -4.4354\n",
      "Epoch 1400/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4670 - val_loss: -4.5476\n",
      "Epoch 1401/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4902 - val_loss: -4.4806\n",
      "Epoch 1402/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5658 - val_loss: -4.1266\n",
      "Epoch 1403/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3722 - val_loss: -3.6363\n",
      "Epoch 1404/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4474 - val_loss: -4.2465\n",
      "Epoch 1405/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1158 - val_loss: -3.9412\n",
      "Epoch 1406/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1754 - val_loss: -3.6474\n",
      "Epoch 1407/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4306 - val_loss: -4.6975\n",
      "Epoch 1408/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4172 - val_loss: -2.8453\n",
      "Epoch 1409/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4119 - val_loss: -3.8214\n",
      "Epoch 1410/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3436 - val_loss: -4.5070\n",
      "Epoch 1411/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4584 - val_loss: -4.3725\n",
      "Epoch 1412/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4194 - val_loss: -4.8854\n",
      "Epoch 1413/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4100 - val_loss: -4.7034\n",
      "Epoch 1414/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0312 - val_loss: -4.4413\n",
      "Epoch 1415/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4245 - val_loss: -4.1705\n",
      "Epoch 1416/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3955"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_100_layer_call_fn, lstm_cell_100_layer_call_and_return_conditional_losses, lstm_cell_101_layer_call_fn, lstm_cell_101_layer_call_and_return_conditional_losses, lstm_cell_102_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -5.3955 - val_loss: -5.0038\n",
      "Epoch 1417/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4551 - val_loss: -3.5155\n",
      "Epoch 1418/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3934 - val_loss: -4.6810\n",
      "Epoch 1419/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3430 - val_loss: -4.4859\n",
      "Epoch 1420/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4268 - val_loss: -4.3272\n",
      "Epoch 1421/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4928 - val_loss: -4.7869\n",
      "Epoch 1422/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5059 - val_loss: -3.6342\n",
      "Epoch 1423/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4618 - val_loss: -4.6341\n",
      "Epoch 1424/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5175 - val_loss: -4.6507\n",
      "Epoch 1425/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5215 - val_loss: -1.9908\n",
      "Epoch 1426/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4096 - val_loss: -4.5199\n",
      "Epoch 1427/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4936 - val_loss: -4.4158\n",
      "Epoch 1428/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3842 - val_loss: -4.4413\n",
      "Epoch 1429/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4348 - val_loss: -4.7315\n",
      "Epoch 1430/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4603 - val_loss: -4.1606\n",
      "Epoch 1431/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4762 - val_loss: -3.1889\n",
      "Epoch 1432/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9295 - val_loss: -4.0171\n",
      "Epoch 1433/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3732 - val_loss: -4.5700\n",
      "Epoch 1434/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3430 - val_loss: -3.7281\n",
      "Epoch 1435/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4973 - val_loss: -3.6897\n",
      "Epoch 1436/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4440 - val_loss: -4.6714\n",
      "Epoch 1437/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5331 - val_loss: -3.7569\n",
      "Epoch 1438/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.4206 - val_loss: -3.6347\n",
      "Epoch 1439/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5206 - val_loss: -4.5536\n",
      "Epoch 1440/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5420 - val_loss: -4.3423\n",
      "Epoch 1441/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3149 - val_loss: -3.7333\n",
      "Epoch 1442/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5594 - val_loss: -4.7547\n",
      "Epoch 1443/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5286 - val_loss: -4.5866\n",
      "Epoch 1444/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4174 - val_loss: -4.2504\n",
      "Epoch 1445/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5561 - val_loss: -4.5600\n",
      "Epoch 1446/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4846 - val_loss: -2.8437\n",
      "Epoch 1447/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5098 - val_loss: -2.1801\n",
      "Epoch 1448/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.1651 - val_loss: -3.4103\n",
      "Epoch 1449/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8367 - val_loss: -4.5371\n",
      "Epoch 1450/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8848 - val_loss: -4.2616\n",
      "Epoch 1451/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1594 - val_loss: -4.6592\n",
      "Epoch 1452/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0721 - val_loss: -4.5725\n",
      "Epoch 1453/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3212 - val_loss: -0.3416\n",
      "Epoch 1454/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2634 - val_loss: -4.5669\n",
      "Epoch 1455/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2754 - val_loss: -4.4079\n",
      "Epoch 1456/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3538 - val_loss: -4.4739\n",
      "Epoch 1457/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3943 - val_loss: -4.4700\n",
      "Epoch 1458/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3827 - val_loss: -4.4688\n",
      "Epoch 1459/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3749 - val_loss: -4.4948\n",
      "Epoch 1460/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4140 - val_loss: -3.7296\n",
      "Epoch 1461/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4211 - val_loss: -4.1655\n",
      "Epoch 1462/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4273 - val_loss: -4.1273\n",
      "Epoch 1463/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2348 - val_loss: -4.6755\n",
      "Epoch 1464/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4869 - val_loss: -3.4619\n",
      "Epoch 1465/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4065 - val_loss: -3.9747\n",
      "Epoch 1466/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5212 - val_loss: -3.4886\n",
      "Epoch 1467/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4513 - val_loss: -4.2655\n",
      "Epoch 1468/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5052 - val_loss: -2.3964\n",
      "Epoch 1469/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3957 - val_loss: -3.8797\n",
      "Epoch 1470/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3826 - val_loss: -2.9300\n",
      "Epoch 1471/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4342 - val_loss: -3.3954\n",
      "Epoch 1472/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4459 - val_loss: -4.3767\n",
      "Epoch 1473/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4437 - val_loss: -4.1117\n",
      "Epoch 1474/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3958 - val_loss: -4.1666\n",
      "Epoch 1475/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4241 - val_loss: -4.4889\n",
      "Epoch 1476/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5672 - val_loss: -4.5519\n",
      "Epoch 1477/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.2789 - val_loss: -3.7977\n",
      "Epoch 1478/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9360 - val_loss: -4.6141\n",
      "Epoch 1479/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2148 - val_loss: -4.7402\n",
      "Epoch 1480/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2772 - val_loss: -4.6780\n",
      "Epoch 1481/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4152 - val_loss: -4.6732\n",
      "Epoch 1482/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3406 - val_loss: -4.3536\n",
      "Epoch 1483/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2087 - val_loss: -4.2578\n",
      "Epoch 1484/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4074 - val_loss: -4.2699\n",
      "Epoch 1485/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4172 - val_loss: -3.8820\n",
      "Epoch 1486/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2263 - val_loss: -4.7500\n",
      "Epoch 1487/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3084 - val_loss: -4.8046\n",
      "Epoch 1488/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3514 - val_loss: -4.6361\n",
      "Epoch 1489/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3440 - val_loss: -4.1543\n",
      "Epoch 1490/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3455 - val_loss: -4.3206\n",
      "Epoch 1491/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4586 - val_loss: -4.6789\n",
      "Epoch 1492/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4602 - val_loss: -3.0850\n",
      "Epoch 1493/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2279 - val_loss: -4.2259\n",
      "Epoch 1494/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4208 - val_loss: -2.2195\n",
      "Epoch 1495/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5049 - val_loss: -3.3228\n",
      "Epoch 1496/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3257 - val_loss: -4.6721\n",
      "Epoch 1497/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4484 - val_loss: -4.4470\n",
      "Epoch 1498/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5159 - val_loss: -4.4132\n",
      "Epoch 1499/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3457 - val_loss: -4.4773\n",
      "Epoch 1500/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4626 - val_loss: -2.3991\n",
      "Epoch 1501/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5004 - val_loss: -4.4024\n",
      "Epoch 1502/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4684 - val_loss: -4.3302\n",
      "Epoch 1503/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4712 - val_loss: -1.9631\n",
      "Epoch 1504/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3756 - val_loss: -3.8353\n",
      "Epoch 1505/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3835 - val_loss: -2.7424\n",
      "Epoch 1506/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5024 - val_loss: -3.9747\n",
      "Epoch 1507/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4965 - val_loss: -3.8652\n",
      "Epoch 1508/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5151 - val_loss: -4.2727\n",
      "Epoch 1509/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5422 - val_loss: -4.1834\n",
      "Epoch 1510/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4653 - val_loss: -3.5042\n",
      "Epoch 1511/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4340 - val_loss: -4.4144\n",
      "Epoch 1512/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4571 - val_loss: -4.5125\n",
      "Epoch 1513/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5361 - val_loss: -3.7122\n",
      "Epoch 1514/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4876 - val_loss: -4.7303\n",
      "Epoch 1515/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5237 - val_loss: -3.6995\n",
      "Epoch 1516/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5532 - val_loss: -3.7370\n",
      "Epoch 1517/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4928 - val_loss: -4.4300\n",
      "Epoch 1518/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.5052 - val_loss: -4.1758\n",
      "Epoch 1519/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3822 - val_loss: -4.3896\n",
      "Epoch 1520/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4880 - val_loss: -4.3478\n",
      "Epoch 1521/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4932 - val_loss: -3.2733\n",
      "Epoch 1522/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5632 - val_loss: -3.9215\n",
      "Epoch 1523/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3679 - val_loss: -4.0304\n",
      "Epoch 1524/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3498 - val_loss: -4.5940\n",
      "Epoch 1525/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5732 - val_loss: -3.5850\n",
      "Epoch 1526/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5552 - val_loss: -4.2899\n",
      "Epoch 1527/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4883 - val_loss: -3.9540\n",
      "Epoch 1528/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5313 - val_loss: -4.5971\n",
      "Epoch 1529/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.5820 - val_loss: 5.9336\n",
      "Epoch 1530/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1103 - val_loss: -3.5590\n",
      "Epoch 1531/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5557 - val_loss: -3.9528\n",
      "Epoch 1532/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4560 - val_loss: -3.6941\n",
      "Epoch 1533/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5379 - val_loss: -2.4049\n",
      "Epoch 1534/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5654 - val_loss: -3.9947\n",
      "Epoch 1535/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4341 - val_loss: -4.4736\n",
      "Epoch 1536/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4855 - val_loss: -4.6675\n",
      "Epoch 1537/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5168 - val_loss: -4.1080\n",
      "Epoch 1538/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6018 - val_loss: -4.5486\n",
      "Epoch 1539/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4854 - val_loss: -4.4689\n",
      "Epoch 1540/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.4602 - val_loss: -4.5456\n",
      "Epoch 1541/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5341 - val_loss: -4.6911\n",
      "Epoch 1542/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5255 - val_loss: -3.5784\n",
      "Epoch 1543/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4071 - val_loss: -4.3977\n",
      "Epoch 1544/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4281 - val_loss: -4.2403\n",
      "Epoch 1545/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5047 - val_loss: -4.7962\n",
      "Epoch 1546/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4704 - val_loss: -4.0291\n",
      "Epoch 1547/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5035 - val_loss: -4.1358\n",
      "Epoch 1548/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4680 - val_loss: -4.7947\n",
      "Epoch 1549/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5496 - val_loss: -3.0376\n",
      "Epoch 1550/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5063 - val_loss: -3.8568\n",
      "Epoch 1551/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.2513 - val_loss: -3.5619\n",
      "Epoch 1552/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4122 - val_loss: -2.5908\n",
      "Epoch 1553/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4797 - val_loss: -3.8985\n",
      "Epoch 1554/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4828 - val_loss: -3.1928\n",
      "Epoch 1555/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5575 - val_loss: -4.0377\n",
      "Epoch 1556/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4564 - val_loss: -4.5219\n",
      "Epoch 1557/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4393 - val_loss: -4.0798\n",
      "Epoch 1558/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5168 - val_loss: -2.8748\n",
      "Epoch 1559/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5134 - val_loss: -4.5338\n",
      "Epoch 1560/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4783 - val_loss: -4.0516\n",
      "Epoch 1561/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3739 - val_loss: -3.9655\n",
      "Epoch 1562/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6477 - val_loss: -4.2091\n",
      "Epoch 1563/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0681 - val_loss: -4.1677\n",
      "Epoch 1564/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3341 - val_loss: -4.2335\n",
      "Epoch 1565/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3990 - val_loss: -3.8895\n",
      "Epoch 1566/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2233 - val_loss: -4.6926\n",
      "Epoch 1567/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4576 - val_loss: -4.2956\n",
      "Epoch 1568/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3530 - val_loss: -4.7227\n",
      "Epoch 1569/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3275 - val_loss: -4.3528\n",
      "Epoch 1570/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4589 - val_loss: -4.1719\n",
      "Epoch 1571/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4943 - val_loss: -4.1174\n",
      "Epoch 1572/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3924 - val_loss: -4.0437\n",
      "Epoch 1573/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4972 - val_loss: -4.1234\n",
      "Epoch 1574/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4889 - val_loss: -4.4791\n",
      "Epoch 1575/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5581 - val_loss: -4.6079\n",
      "Epoch 1576/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4926 - val_loss: -4.9155\n",
      "Epoch 1577/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5187 - val_loss: -3.6011\n",
      "Epoch 1578/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5182 - val_loss: -4.1349\n",
      "Epoch 1579/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5200 - val_loss: -3.2308\n",
      "Epoch 1580/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4498 - val_loss: -3.6041\n",
      "Epoch 1581/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5584 - val_loss: -2.7669\n",
      "Epoch 1582/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5101 - val_loss: -3.7962\n",
      "Epoch 1583/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5551 - val_loss: -4.5848\n",
      "Epoch 1584/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5508 - val_loss: -2.9090\n",
      "Epoch 1585/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5433 - val_loss: -4.1275\n",
      "Epoch 1586/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4092 - val_loss: -3.9393\n",
      "Epoch 1587/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5460 - val_loss: -3.8943\n",
      "Epoch 1588/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4910 - val_loss: -4.7633\n",
      "Epoch 1589/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4671 - val_loss: -3.9346\n",
      "Epoch 1590/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5736 - val_loss: -3.9272\n",
      "Epoch 1591/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5850 - val_loss: -4.0293\n",
      "Epoch 1592/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5641 - val_loss: -4.2856\n",
      "Epoch 1593/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5267 - val_loss: -4.2926\n",
      "Epoch 1594/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5628 - val_loss: -4.0713\n",
      "Epoch 1595/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4451 - val_loss: -3.4322\n",
      "Epoch 1596/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4150 - val_loss: -4.7080\n",
      "Epoch 1597/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5150 - val_loss: -4.4421\n",
      "Epoch 1598/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5893 - val_loss: -4.2430\n",
      "Epoch 1599/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5681 - val_loss: -2.7936\n",
      "Epoch 1600/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5447 - val_loss: -3.4954\n",
      "Epoch 1601/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5455 - val_loss: -4.4105\n",
      "Epoch 1602/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4351 - val_loss: -4.0054\n",
      "Epoch 1603/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6247 - val_loss: -4.1477\n",
      "Epoch 1604/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5200 - val_loss: -3.6683\n",
      "Epoch 1605/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5202 - val_loss: -4.4019\n",
      "Epoch 1606/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6176 - val_loss: -3.2640\n",
      "Epoch 1607/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4854 - val_loss: -4.8384\n",
      "Epoch 1608/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5185 - val_loss: -4.1040\n",
      "Epoch 1609/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5416 - val_loss: -3.6443\n",
      "Epoch 1610/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5711 - val_loss: -4.4050\n",
      "Epoch 1611/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4213 - val_loss: -4.6488\n",
      "Epoch 1612/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5714 - val_loss: -4.6949\n",
      "Epoch 1613/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5197 - val_loss: -4.1614\n",
      "Epoch 1614/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.5898 - val_loss: -3.4554\n",
      "Epoch 1615/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5571 - val_loss: -4.1768\n",
      "Epoch 1616/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.5363 - val_loss: -4.5721\n",
      "Epoch 1617/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.5471 - val_loss: -4.6895\n",
      "Epoch 1618/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5034 - val_loss: -4.0885\n",
      "Epoch 1619/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5966 - val_loss: -1.5989\n",
      "Epoch 1620/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1947 - val_loss: -4.6057\n",
      "Epoch 1621/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4206 - val_loss: -3.5321\n",
      "Epoch 1622/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8645 - val_loss: -4.2517\n",
      "Epoch 1623/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4024 - val_loss: -4.1566\n",
      "Epoch 1624/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3564 - val_loss: -4.4340\n",
      "Epoch 1625/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5119 - val_loss: -4.1884\n",
      "Epoch 1626/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4419 - val_loss: -3.6196\n",
      "Epoch 1627/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4868 - val_loss: -3.5165\n",
      "Epoch 1628/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5086 - val_loss: -4.3133\n",
      "Epoch 1629/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3087 - val_loss: -4.1873\n",
      "Epoch 1630/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5403 - val_loss: -4.5103\n",
      "Epoch 1631/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5153 - val_loss: -4.8188\n",
      "Epoch 1632/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4735 - val_loss: -4.6785\n",
      "Epoch 1633/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5883 - val_loss: -3.8212\n",
      "Epoch 1634/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4932 - val_loss: -4.8525\n",
      "Epoch 1635/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5336 - val_loss: -4.7411\n",
      "Epoch 1636/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4611 - val_loss: -3.7871\n",
      "Epoch 1637/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5485 - val_loss: -4.5063\n",
      "Epoch 1638/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.5634 - val_loss: -2.6262\n",
      "Epoch 1639/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.6105 - val_loss: -3.7270\n",
      "Epoch 1640/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4618 - val_loss: -4.2615\n",
      "Epoch 1641/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5912 - val_loss: -2.3184\n",
      "Epoch 1642/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.4260 - val_loss: -4.4483\n",
      "Epoch 1643/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5959 - val_loss: -4.4703\n",
      "Epoch 1644/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4997 - val_loss: -4.4093\n",
      "Epoch 1645/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5011 - val_loss: -4.4363\n",
      "Epoch 1646/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9636 - val_loss: -4.4891\n",
      "Epoch 1647/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4553 - val_loss: -4.6414\n",
      "Epoch 1648/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5056 - val_loss: -4.5021\n",
      "Epoch 1649/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5651 - val_loss: -3.6291\n",
      "Epoch 1650/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5168 - val_loss: -4.4465\n",
      "Epoch 1651/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4539 - val_loss: -4.4382\n",
      "Epoch 1652/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6148 - val_loss: -3.3641\n",
      "Epoch 1653/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4964 - val_loss: -4.3538\n",
      "Epoch 1654/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4888 - val_loss: -4.5495\n",
      "Epoch 1655/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6049 - val_loss: -3.0905\n",
      "Epoch 1656/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5392 - val_loss: -3.9201\n",
      "Epoch 1657/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5666 - val_loss: -3.3903\n",
      "Epoch 1658/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5332 - val_loss: -2.9539\n",
      "Epoch 1659/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4772 - val_loss: -4.6513\n",
      "Epoch 1660/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5480 - val_loss: -4.3520\n",
      "Epoch 1661/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.5971 - val_loss: -4.1038\n",
      "Epoch 1662/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5341 - val_loss: -4.1577\n",
      "Epoch 1663/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5180 - val_loss: -2.9591\n",
      "Epoch 1664/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5812 - val_loss: -3.7482\n",
      "Epoch 1665/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2626 - val_loss: -4.7311\n",
      "Epoch 1666/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5746 - val_loss: -4.2092\n",
      "Epoch 1667/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5654 - val_loss: -4.0309\n",
      "Epoch 1668/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5280 - val_loss: -3.2597\n",
      "Epoch 1669/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5957 - val_loss: -3.8899\n",
      "Epoch 1670/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4032 - val_loss: -4.5334\n",
      "Epoch 1671/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5267 - val_loss: -4.8214\n",
      "Epoch 1672/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.5268 - val_loss: -3.8482\n",
      "Epoch 1673/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6452 - val_loss: -3.1359\n",
      "Epoch 1674/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5378 - val_loss: -4.3643\n",
      "Epoch 1675/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.5525 - val_loss: -4.2110\n",
      "Epoch 1676/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4576 - val_loss: -3.8658\n",
      "Epoch 1677/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5798 - val_loss: -4.3449\n",
      "Epoch 1678/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5921 - val_loss: -4.0326\n",
      "Epoch 1679/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5433 - val_loss: -1.3174\n",
      "Epoch 1680/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5783 - val_loss: -4.5978\n",
      "Epoch 1681/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6158 - val_loss: -4.1098\n",
      "Epoch 1682/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5364 - val_loss: -1.4209\n",
      "Epoch 1683/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.5950 - val_loss: -2.8918\n",
      "Epoch 1684/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5514 - val_loss: -4.4866\n",
      "Epoch 1685/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5371 - val_loss: -3.2928\n",
      "Epoch 1686/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4053 - val_loss: -4.2042\n",
      "Epoch 1687/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6362 - val_loss: -3.9183\n",
      "Epoch 1688/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5942 - val_loss: -4.1426\n",
      "Epoch 1689/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.5833 - val_loss: -2.1129\n",
      "Epoch 1690/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4492 - val_loss: -4.5887\n",
      "Epoch 1691/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5679 - val_loss: -3.8765\n",
      "Epoch 1692/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6030 - val_loss: -3.5469\n",
      "Epoch 1693/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5322 - val_loss: -4.0769\n",
      "Epoch 1694/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6111 - val_loss: -4.0111\n",
      "Epoch 1695/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6327 - val_loss: -2.3617\n",
      "Epoch 1696/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6243 - val_loss: -3.7680\n",
      "Epoch 1697/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4972 - val_loss: -3.1899\n",
      "Epoch 1698/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6277 - val_loss: -3.8936\n",
      "Epoch 1699/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2076 - val_loss: -3.5409\n",
      "Epoch 1700/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.6329 - val_loss: -4.0011\n",
      "Epoch 1701/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6107 - val_loss: -3.1595\n",
      "Epoch 1702/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6524 - val_loss: -4.6795\n",
      "Epoch 1703/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2469 - val_loss: -4.8920\n",
      "Epoch 1704/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5845 - val_loss: -4.6890\n",
      "Epoch 1705/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.6025 - val_loss: -4.4757\n",
      "Epoch 1706/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6426 - val_loss: -3.7250\n",
      "Epoch 1707/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6186 - val_loss: -4.5765\n",
      "Epoch 1708/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6098 - val_loss: -4.0481\n",
      "Epoch 1709/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5595 - val_loss: -3.9909\n",
      "Epoch 1710/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5483 - val_loss: -4.7777\n",
      "Epoch 1711/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5595 - val_loss: -3.6030\n",
      "Epoch 1712/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6645 - val_loss: -4.2404\n",
      "Epoch 1713/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5054 - val_loss: -3.8451\n",
      "Epoch 1714/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6451 - val_loss: -4.2269\n",
      "Epoch 1715/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4713 - val_loss: -4.4185\n",
      "Epoch 1716/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5268 - val_loss: -3.3392\n",
      "Epoch 1717/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6246 - val_loss: -2.4954\n",
      "Epoch 1718/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6819 - val_loss: -3.8913\n",
      "Epoch 1719/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.6361 - val_loss: -4.4615\n",
      "Epoch 1720/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8310 - val_loss: -4.1235\n",
      "Epoch 1721/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8729 - val_loss: -4.1471\n",
      "Epoch 1722/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3094 - val_loss: -1.0760\n",
      "Epoch 1723/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3319 - val_loss: -3.3136\n",
      "Epoch 1724/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4469 - val_loss: -4.1069\n",
      "Epoch 1725/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4023 - val_loss: -4.4559\n",
      "Epoch 1726/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3008 - val_loss: -4.0100\n",
      "Epoch 1727/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1494 - val_loss: -3.7034\n",
      "Epoch 1728/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3016 - val_loss: -2.6317\n",
      "Epoch 1729/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2801 - val_loss: -3.6473\n",
      "Epoch 1730/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.3578 - val_loss: -4.4209\n",
      "Epoch 1731/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3807 - val_loss: -4.3435\n",
      "Epoch 1732/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4507 - val_loss: -4.2897\n",
      "Epoch 1733/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4861 - val_loss: -3.0560\n",
      "Epoch 1734/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4537 - val_loss: -4.5660\n",
      "Epoch 1735/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5491 - val_loss: -2.9701\n",
      "Epoch 1736/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4741 - val_loss: -3.7528\n",
      "Epoch 1737/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5032 - val_loss: -4.6051\n",
      "Epoch 1738/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4465 - val_loss: -3.8922\n",
      "Epoch 1739/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5083 - val_loss: -3.7752\n",
      "Epoch 1740/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5544 - val_loss: -3.2909\n",
      "Epoch 1741/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4342 - val_loss: -4.5093\n",
      "Epoch 1742/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6033 - val_loss: -3.9959\n",
      "Epoch 1743/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5804 - val_loss: -4.1708\n",
      "Epoch 1744/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3403 - val_loss: -4.3869\n",
      "Epoch 1745/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6051 - val_loss: -3.8069\n",
      "Epoch 1746/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6246 - val_loss: -2.9598\n",
      "Epoch 1747/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5197 - val_loss: -3.8245\n",
      "Epoch 1748/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1637 - val_loss: -4.2646\n",
      "Epoch 1749/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4458 - val_loss: 0.1010\n",
      "Epoch 1750/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3649 - val_loss: -4.3396\n",
      "Epoch 1751/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5662 - val_loss: -4.2937\n",
      "Epoch 1752/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5561 - val_loss: -4.4614\n",
      "Epoch 1753/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4671 - val_loss: -4.7892\n",
      "Epoch 1754/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.4618 - val_loss: -4.3867\n",
      "Epoch 1755/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4012 - val_loss: -4.3600\n",
      "Epoch 1756/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5664 - val_loss: -4.3849\n",
      "Epoch 1757/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5938 - val_loss: -3.9706\n",
      "Epoch 1758/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5737 - val_loss: -4.1601\n",
      "Epoch 1759/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5486 - val_loss: -3.4117\n",
      "Epoch 1760/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3844 - val_loss: -3.8728\n",
      "Epoch 1761/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.4979 - val_loss: 0.0166\n",
      "Epoch 1762/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5550 - val_loss: -3.6807\n",
      "Epoch 1763/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.5842 - val_loss: -3.9728\n",
      "Epoch 1764/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6139 - val_loss: -3.9615\n",
      "Epoch 1765/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5903 - val_loss: -3.2022\n",
      "Epoch 1766/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6190 - val_loss: -3.8272\n",
      "Epoch 1767/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5782 - val_loss: -4.3924\n",
      "Epoch 1768/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4817 - val_loss: -4.2861\n",
      "Epoch 1769/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4458 - val_loss: -4.3795\n",
      "Epoch 1770/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6633 - val_loss: -4.4447\n",
      "Epoch 1771/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4749 - val_loss: -4.5151\n",
      "Epoch 1772/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4676 - val_loss: -2.1189\n",
      "Epoch 1773/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5924 - val_loss: -4.1390\n",
      "Epoch 1774/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6616 - val_loss: -2.2731\n",
      "Epoch 1775/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6105 - val_loss: -3.9147\n",
      "Epoch 1776/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.6165 - val_loss: -4.2903\n",
      "Epoch 1777/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5448 - val_loss: -4.4579\n",
      "Epoch 1778/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6489 - val_loss: -4.5629\n",
      "Epoch 1779/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5860 - val_loss: -3.2305\n",
      "Epoch 1780/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4261 - val_loss: -4.2326\n",
      "Epoch 1781/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6035 - val_loss: -4.2919\n",
      "Epoch 1782/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5295 - val_loss: -3.7554\n",
      "Epoch 1783/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6787 - val_loss: -3.1948\n",
      "Epoch 1784/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2781 - val_loss: -4.5223\n",
      "Epoch 1785/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.6182 - val_loss: -4.2590\n",
      "Epoch 1786/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.6145 - val_loss: -4.3372\n",
      "Epoch 1787/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5548 - val_loss: -3.8020\n",
      "Epoch 1788/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5451 - val_loss: -4.0849\n",
      "Epoch 1789/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.6395 - val_loss: -4.5930\n",
      "Epoch 1790/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5289 - val_loss: -4.3519\n",
      "Epoch 1791/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4186 - val_loss: -4.5730\n",
      "Epoch 1792/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5285 - val_loss: -4.6433\n",
      "Epoch 1793/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.6208 - val_loss: -4.1139\n",
      "Epoch 1794/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6172 - val_loss: -4.1858\n",
      "Epoch 1795/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5606 - val_loss: -4.6920\n",
      "Epoch 1796/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3945 - val_loss: -4.3777\n",
      "Epoch 1797/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5548 - val_loss: -4.2775\n",
      "Epoch 1798/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5806 - val_loss: -3.1456\n",
      "Epoch 1799/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5835 - val_loss: -4.3541\n",
      "Epoch 1800/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6163 - val_loss: -4.6278\n",
      "Epoch 1801/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.6282 - val_loss: -2.8248\n",
      "Epoch 1802/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5374 - val_loss: -3.5017\n",
      "Epoch 1803/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5947 - val_loss: -4.5151\n",
      "Epoch 1804/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5935 - val_loss: -4.3980\n",
      "Epoch 1805/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6594 - val_loss: -3.4740\n",
      "Epoch 1806/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5587 - val_loss: -4.2874\n",
      "Epoch 1807/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.6405 - val_loss: -3.9058\n",
      "Epoch 1808/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6658 - val_loss: -0.0566\n",
      "Epoch 1809/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3766 - val_loss: -3.7227\n",
      "Epoch 1810/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.6640 - val_loss: -4.5775\n",
      "Epoch 1811/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5911 - val_loss: -4.3543\n",
      "Epoch 1812/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6320 - val_loss: -4.3176\n",
      "Epoch 1813/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5240 - val_loss: -3.9760\n",
      "Epoch 1814/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6391 - val_loss: -3.6027\n",
      "Epoch 1815/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6412 - val_loss: -3.5303\n",
      "Epoch 1816/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5109 - val_loss: -1.5348\n",
      "Epoch 1817/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6422 - val_loss: -4.0320\n",
      "Epoch 1818/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.6918 - val_loss: -4.0010\n",
      "Epoch 1819/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.6329 - val_loss: -3.3908\n",
      "Epoch 1820/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6583 - val_loss: -3.9474\n",
      "Epoch 1821/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.5903 - val_loss: -3.1751\n",
      "Epoch 1822/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5343 - val_loss: -4.4372\n",
      "Epoch 1823/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6619 - val_loss: -4.0339\n",
      "Epoch 1824/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2498 - val_loss: -4.6124\n",
      "Epoch 1825/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4145 - val_loss: -3.9225\n",
      "Epoch 1826/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5079 - val_loss: -4.3176\n",
      "Epoch 1827/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4632 - val_loss: -3.3814\n",
      "Epoch 1828/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5566 - val_loss: -4.2635\n",
      "Epoch 1829/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5527 - val_loss: -3.6495\n",
      "Epoch 1830/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4741 - val_loss: -4.0109\n",
      "Epoch 1831/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6020 - val_loss: -4.0729\n",
      "Epoch 1832/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3619 - val_loss: -4.4947\n",
      "Epoch 1833/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5371 - val_loss: -2.5329\n",
      "Epoch 1834/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4841 - val_loss: -3.3433\n",
      "Epoch 1835/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5582 - val_loss: -2.6177\n",
      "Epoch 1836/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6317 - val_loss: -4.1212\n",
      "Epoch 1837/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4668 - val_loss: -3.1529\n",
      "Epoch 1838/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.6243 - val_loss: -2.4972\n",
      "Epoch 1839/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6318 - val_loss: -2.7331\n",
      "Epoch 1840/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4632 - val_loss: -3.9295\n",
      "Epoch 1841/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.5994 - val_loss: -4.0468\n",
      "Epoch 1842/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3933 - val_loss: -4.2344\n",
      "Epoch 1843/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5578 - val_loss: -3.4062\n",
      "Epoch 1844/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4674 - val_loss: -3.8915\n",
      "Epoch 1845/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6332 - val_loss: -4.0100\n",
      "Epoch 1846/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6971 - val_loss: -3.1061\n",
      "Epoch 1847/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4854 - val_loss: -4.3681\n",
      "Epoch 1848/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6333 - val_loss: -2.7690\n",
      "Epoch 1849/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5448 - val_loss: -3.6784\n",
      "Epoch 1850/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6286 - val_loss: -3.3777\n",
      "Epoch 1851/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.6410 - val_loss: -3.7645\n",
      "Epoch 1852/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5024 - val_loss: -4.1282\n",
      "Epoch 1853/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6131 - val_loss: -1.2081\n",
      "Epoch 1854/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1837 - val_loss: -3.9026\n",
      "Epoch 1855/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6617 - val_loss: -2.9163\n",
      "Epoch 1856/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1960 - val_loss: -3.5593\n",
      "Epoch 1857/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2541 - val_loss: -4.3295\n",
      "Epoch 1858/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2893 - val_loss: -3.6256\n",
      "Epoch 1859/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3821 - val_loss: -4.5533\n",
      "Epoch 1860/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3527 - val_loss: -1.0832\n",
      "Epoch 1861/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1759 - val_loss: -4.6716\n",
      "Epoch 1862/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3908 - val_loss: -3.3636\n",
      "Epoch 1863/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4925 - val_loss: 3.5381\n",
      "Epoch 1864/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4335 - val_loss: -3.9685\n",
      "Epoch 1865/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3978 - val_loss: -4.3273\n",
      "Epoch 1866/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5055 - val_loss: -3.6168\n",
      "Epoch 1867/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4223 - val_loss: -4.0356\n",
      "Epoch 1868/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5507 - val_loss: -3.8231\n",
      "Epoch 1869/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4100 - val_loss: -4.3721\n",
      "Epoch 1870/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5682 - val_loss: -3.0152\n",
      "Epoch 1871/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5366 - val_loss: -3.8426\n",
      "Epoch 1872/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4988 - val_loss: -3.8532\n",
      "Epoch 1873/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3931 - val_loss: -4.1964\n",
      "Epoch 1874/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.5784 - val_loss: -3.7751\n",
      "Epoch 1875/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0134 - val_loss: -4.4700\n",
      "Epoch 1876/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3272 - val_loss: -4.1141\n",
      "Epoch 1877/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4781 - val_loss: -3.1870\n",
      "Epoch 1878/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5309 - val_loss: -3.3289\n",
      "Epoch 1879/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5371 - val_loss: -4.0862\n",
      "Epoch 1880/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5924 - val_loss: -0.4306\n",
      "Epoch 1881/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5695 - val_loss: -4.1395\n",
      "Epoch 1882/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5231 - val_loss: -4.0186\n",
      "Epoch 1883/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5311 - val_loss: -3.3629\n",
      "Epoch 1884/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5629 - val_loss: -4.2126\n",
      "Epoch 1885/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4395 - val_loss: -3.7909\n",
      "Epoch 1886/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5417 - val_loss: -4.1085\n",
      "Epoch 1887/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.6228 - val_loss: -4.0873\n",
      "Epoch 1888/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6265 - val_loss: -3.4837\n",
      "Epoch 1889/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6703 - val_loss: -3.2192\n",
      "Epoch 1890/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.6751 - val_loss: -3.8839\n",
      "Epoch 1891/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6150 - val_loss: -3.9680\n",
      "Epoch 1892/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5447 - val_loss: -3.1797\n",
      "Epoch 1893/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2859 - val_loss: -3.4624\n",
      "Epoch 1894/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5945 - val_loss: -4.1304\n",
      "Epoch 1895/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5972 - val_loss: -2.8045\n",
      "Epoch 1896/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6672 - val_loss: -1.8933\n",
      "Epoch 1897/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5763 - val_loss: -4.0872\n",
      "Epoch 1898/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4971 - val_loss: -4.2241\n",
      "Epoch 1899/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5702 - val_loss: -4.0794\n",
      "Epoch 1900/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5763 - val_loss: -4.3660\n",
      "Epoch 1901/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6524 - val_loss: -4.2824\n",
      "Epoch 1902/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6025 - val_loss: -4.6627\n",
      "Epoch 1903/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6191 - val_loss: -4.3011\n",
      "Epoch 1904/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6251 - val_loss: -4.1492\n",
      "Epoch 1905/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5614 - val_loss: -4.3199\n",
      "Epoch 1906/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6570 - val_loss: -4.3667\n",
      "Epoch 1907/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6038 - val_loss: -3.4659\n",
      "Epoch 1908/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.6282 - val_loss: -4.2812\n",
      "Epoch 1909/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.6877 - val_loss: -4.0636\n",
      "Epoch 1910/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.6088 - val_loss: -1.6682\n",
      "Epoch 1911/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5861 - val_loss: -3.6252\n",
      "Epoch 1912/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6219 - val_loss: -4.2034\n",
      "Epoch 1913/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6666 - val_loss: -3.8077\n",
      "Epoch 1914/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6692 - val_loss: -4.3585\n",
      "Epoch 1915/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6480 - val_loss: -3.9322\n",
      "Epoch 1916/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5567 - val_loss: -3.9772\n",
      "Epoch 1917/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6632 - val_loss: -4.4373\n",
      "Epoch 1918/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5916 - val_loss: -4.2494\n",
      "Epoch 1919/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6069 - val_loss: -4.2913\n",
      "Epoch 1920/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.7227 - val_loss: -4.3974\n",
      "Epoch 1921/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6629 - val_loss: -3.4112\n",
      "Epoch 1922/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5681 - val_loss: -4.4813\n",
      "Epoch 1923/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.6907 - val_loss: -4.2146\n",
      "Epoch 1924/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6572 - val_loss: -3.9969\n",
      "Epoch 1925/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5493 - val_loss: -3.9302\n",
      "Epoch 1926/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3135 - val_loss: -4.0186\n",
      "Epoch 1927/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5760 - val_loss: -3.6887\n",
      "Epoch 1928/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6553 - val_loss: -2.9096\n",
      "Epoch 1929/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6419 - val_loss: -4.2473\n",
      "Epoch 1930/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6407 - val_loss: -2.1801\n",
      "Epoch 1931/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.5916 - val_loss: -3.1711\n",
      "Epoch 1932/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6169 - val_loss: -3.7808\n",
      "Epoch 1933/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6376 - val_loss: -3.9783\n",
      "Epoch 1934/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.6202 - val_loss: -3.6868\n",
      "Epoch 1935/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6302 - val_loss: -4.3035\n",
      "Epoch 1936/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6817 - val_loss: -4.3169\n",
      "Epoch 1937/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.7065 - val_loss: -3.0581\n",
      "Epoch 1938/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4769 - val_loss: -4.0741\n",
      "Epoch 1939/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6608 - val_loss: -4.2787\n",
      "Epoch 1940/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6398 - val_loss: -4.2606\n",
      "Epoch 1941/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6698 - val_loss: -3.8828\n",
      "Epoch 1942/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.6641 - val_loss: -3.0815\n",
      "Epoch 1943/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2870 - val_loss: -3.9826\n",
      "Epoch 1944/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5362 - val_loss: -2.7495\n",
      "Epoch 1945/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6115 - val_loss: -3.5374\n",
      "Epoch 1946/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.6458 - val_loss: -3.6382\n",
      "Epoch 1947/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.7105 - val_loss: -3.8711\n",
      "Epoch 1948/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5605 - val_loss: -4.2439\n",
      "Epoch 1949/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.7125 - val_loss: -2.6906\n",
      "Epoch 1950/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6476 - val_loss: 6.3507\n",
      "Epoch 1951/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3476 - val_loss: -3.9788\n",
      "Epoch 1952/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6169 - val_loss: -3.4913\n",
      "Epoch 1953/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.7051 - val_loss: -3.9837\n",
      "Epoch 1954/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6981 - val_loss: -2.4002\n",
      "Epoch 1955/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6669 - val_loss: -4.0209\n",
      "Epoch 1956/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2768 - val_loss: -3.9263\n",
      "Epoch 1957/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5867 - val_loss: -4.0823\n",
      "Epoch 1958/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.7037 - val_loss: -3.7387\n",
      "Epoch 1959/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.6815 - val_loss: -3.6058\n",
      "Epoch 1960/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4530 - val_loss: -4.1528\n",
      "Epoch 1961/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.7454 - val_loss: -3.6849\n",
      "Epoch 1962/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.6339 - val_loss: -4.0953\n",
      "Epoch 1963/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.7017 - val_loss: -3.7272\n",
      "Epoch 1964/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.7182 - val_loss: -2.1116\n",
      "Epoch 1965/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6474 - val_loss: -4.0569\n",
      "Epoch 1966/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6744 - val_loss: -3.7394\n",
      "Epoch 1967/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.6178 - val_loss: -4.1416\n",
      "Epoch 1968/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.6461 - val_loss: -2.5737\n",
      "Epoch 1969/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5797 - val_loss: -3.9055\n",
      "Epoch 1970/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.7094 - val_loss: -3.1156\n",
      "Epoch 1971/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6087 - val_loss: 6.8176\n",
      "Epoch 1972/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5777 - val_loss: -3.2190\n",
      "Epoch 1973/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6868 - val_loss: -3.8426\n",
      "Epoch 1974/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.7162 - val_loss: -3.1315\n",
      "Epoch 1975/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.6592 - val_loss: -3.5342\n",
      "Epoch 1976/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.7273 - val_loss: -3.6566\n",
      "Epoch 1977/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2770 - val_loss: -3.9904\n",
      "Epoch 1978/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.6674 - val_loss: -3.5154\n",
      "Epoch 1979/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5767 - val_loss: -3.3303\n",
      "Epoch 1980/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4969 - val_loss: -4.3564\n",
      "Epoch 1981/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4973 - val_loss: -3.7497\n",
      "Epoch 1982/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6324 - val_loss: -3.4494\n",
      "Epoch 1983/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4064 - val_loss: -3.6545\n",
      "Epoch 1984/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6996 - val_loss: -3.5693\n",
      "Epoch 1985/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.7184 - val_loss: -3.6660\n",
      "Epoch 1986/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.6451 - val_loss: -3.8905\n",
      "Epoch 1987/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6289 - val_loss: -3.7736\n",
      "Epoch 1988/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4428 - val_loss: -3.9659\n",
      "Epoch 1989/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5514 - val_loss: -3.9469\n",
      "Epoch 1990/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6411 - val_loss: -4.1477\n",
      "Epoch 1991/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5984 - val_loss: -3.9102\n",
      "Epoch 1992/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.7034 - val_loss: -4.2597\n",
      "Epoch 1993/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.7129 - val_loss: -3.7121\n",
      "Epoch 1994/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6663 - val_loss: -3.8470\n",
      "Epoch 1995/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.7311 - val_loss: -3.7919\n",
      "Epoch 1996/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5725 - val_loss: -3.9713\n",
      "Epoch 1997/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6126 - val_loss: -2.8188\n",
      "Epoch 1998/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.7409 - val_loss: -2.4807\n",
      "Epoch 1999/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.7233 - val_loss: -3.7163\n",
      "Epoch 2000/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6238 - val_loss: -4.3252\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 7s 44ms/step - loss: -5.1714\n",
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 8s 46ms/step - loss: -5.2377\n",
      "WARNING:tensorflow:Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8296"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 32s 246ms/step - loss: -0.8296 - val_loss: -0.0199\n",
      "Epoch 2/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -1.2299 - val_loss: 0.4383\n",
      "Epoch 3/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -1.2443 - val_loss: 0.5671\n",
      "Epoch 4/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2490 - val_loss: 0.4942\n",
      "Epoch 5/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -1.2517 - val_loss: 0.5545\n",
      "Epoch 6/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -1.2538 - val_loss: 0.5924\n",
      "Epoch 7/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -1.2548 - val_loss: 0.3881\n",
      "Epoch 8/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.2666"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 246ms/step - loss: -1.2666 - val_loss: -0.2173\n",
      "Epoch 9/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2655 - val_loss: 0.5491\n",
      "Epoch 10/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -1.2633 - val_loss: 0.5238\n",
      "Epoch 11/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -1.2583 - val_loss: 0.2495\n",
      "Epoch 12/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -1.2609 - val_loss: 0.5307\n",
      "Epoch 13/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.4564"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -1.4564 - val_loss: -0.6082\n",
      "Epoch 14/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.3096"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -2.3096 - val_loss: -2.1790\n",
      "Epoch 15/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.0639"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 251ms/step - loss: -3.0639 - val_loss: -2.3378\n",
      "Epoch 16/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.4636 - val_loss: -1.7272\n",
      "Epoch 17/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.6501"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -3.6501 - val_loss: -2.7256\n",
      "Epoch 18/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.7579"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -3.7579 - val_loss: -3.4058\n",
      "Epoch 19/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.8575"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -3.8575 - val_loss: -3.5069\n",
      "Epoch 20/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9829 - val_loss: -2.8683\n",
      "Epoch 21/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.0285 - val_loss: -3.4887\n",
      "Epoch 22/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.0092"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -4.0092 - val_loss: -3.5924\n",
      "Epoch 23/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0727 - val_loss: -3.2662\n",
      "Epoch 24/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.1689"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 244ms/step - loss: -4.1689 - val_loss: -3.8541\n",
      "Epoch 25/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0457 - val_loss: -2.1706\n",
      "Epoch 26/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -3.8200 - val_loss: -3.7143\n",
      "Epoch 27/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1191 - val_loss: -3.5303\n",
      "Epoch 28/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2793 - val_loss: -3.4161\n",
      "Epoch 29/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.2350"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -4.2350 - val_loss: -3.8781\n",
      "Epoch 30/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2669 - val_loss: -1.8003\n",
      "Epoch 31/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3302 - val_loss: -3.7022\n",
      "Epoch 32/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0533 - val_loss: -3.6862\n",
      "Epoch 33/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1194 - val_loss: -3.6535\n",
      "Epoch 34/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3276 - val_loss: -3.7371\n",
      "Epoch 35/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.3434"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.3434 - val_loss: -3.9180\n",
      "Epoch 36/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3670 - val_loss: -1.0319\n",
      "Epoch 37/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3458 - val_loss: -3.6333\n",
      "Epoch 38/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3049 - val_loss: -3.1027\n",
      "Epoch 39/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3171 - val_loss: -3.5141\n",
      "Epoch 40/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4275 - val_loss: -3.7218\n",
      "Epoch 41/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.0099 - val_loss: -2.6611\n",
      "Epoch 42/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1297 - val_loss: -3.5584\n",
      "Epoch 43/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2412 - val_loss: -3.4183\n",
      "Epoch 44/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3980 - val_loss: -3.8591\n",
      "Epoch 45/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3949 - val_loss: -3.1895\n",
      "Epoch 46/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.3786"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -4.3786 - val_loss: -3.9209\n",
      "Epoch 47/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4792 - val_loss: -3.5566\n",
      "Epoch 48/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4562 - val_loss: -2.1711\n",
      "Epoch 49/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.5008 - val_loss: -4.0243\n",
      "Epoch 50/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3127 - val_loss: -3.6162\n",
      "Epoch 51/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4649 - val_loss: -3.5858\n",
      "Epoch 52/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4923"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 245ms/step - loss: -4.4923 - val_loss: -4.0552\n",
      "Epoch 53/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3533 - val_loss: -3.0858\n",
      "Epoch 54/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.8782 - val_loss: -3.7341\n",
      "Epoch 55/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2701 - val_loss: -3.4451\n",
      "Epoch 56/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4501 - val_loss: -4.0329\n",
      "Epoch 57/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4981 - val_loss: -3.3756\n",
      "Epoch 58/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4986 - val_loss: -3.7743\n",
      "Epoch 59/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4768 - val_loss: -3.9914\n",
      "Epoch 60/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5089"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 246ms/step - loss: -4.5089 - val_loss: -4.0692\n",
      "Epoch 61/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5251 - val_loss: -3.9476\n",
      "Epoch 62/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -4.5449 - val_loss: -4.1073\n",
      "Epoch 63/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5571 - val_loss: -3.7240\n",
      "Epoch 64/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4574 - val_loss: -3.0047\n",
      "Epoch 65/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.4141 - val_loss: -3.0414\n",
      "Epoch 66/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5294 - val_loss: -4.0465\n",
      "Epoch 67/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5619 - val_loss: -3.9584\n",
      "Epoch 68/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5869"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -4.5869 - val_loss: -4.1133\n",
      "Epoch 69/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.6911 - val_loss: -3.4879\n",
      "Epoch 70/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5041 - val_loss: -3.5439\n",
      "Epoch 71/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.5304 - val_loss: 1.9947\n",
      "Epoch 72/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4134 - val_loss: -3.7052\n",
      "Epoch 73/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5177 - val_loss: -3.7563\n",
      "Epoch 74/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5073 - val_loss: -3.6139\n",
      "Epoch 75/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6159 - val_loss: -4.1132\n",
      "Epoch 76/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4178 - val_loss: -3.5200\n",
      "Epoch 77/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5740 - val_loss: -3.8410\n",
      "Epoch 78/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6266 - val_loss: -3.6763\n",
      "Epoch 79/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.3007 - val_loss: -3.1255\n",
      "Epoch 80/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.1383 - val_loss: -2.6141\n",
      "Epoch 81/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0357 - val_loss: -2.9537\n",
      "Epoch 82/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2371 - val_loss: -3.1050\n",
      "Epoch 83/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2175 - val_loss: -3.6957\n",
      "Epoch 84/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -0.6489 - val_loss: -1.3891\n",
      "Epoch 85/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -2.8089 - val_loss: -2.1154\n",
      "Epoch 86/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.5183 - val_loss: -2.6974\n",
      "Epoch 87/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8025 - val_loss: -2.8474\n",
      "Epoch 88/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9234 - val_loss: -3.2030\n",
      "Epoch 89/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0163 - val_loss: -3.2625\n",
      "Epoch 90/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.1555 - val_loss: -3.3014\n",
      "Epoch 91/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1065 - val_loss: -3.1020\n",
      "Epoch 92/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2550 - val_loss: -3.5279\n",
      "Epoch 93/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0738 - val_loss: -1.6730\n",
      "Epoch 94/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3247 - val_loss: -3.6326\n",
      "Epoch 95/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3044 - val_loss: -3.3472\n",
      "Epoch 96/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3336 - val_loss: -3.5782\n",
      "Epoch 97/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1693 - val_loss: -3.2311\n",
      "Epoch 98/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0948 - val_loss: -3.3832\n",
      "Epoch 99/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4511 - val_loss: -3.5215\n",
      "Epoch 100/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4497 - val_loss: -3.9790\n",
      "Epoch 101/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1778 - val_loss: -2.7208\n",
      "Epoch 102/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3532 - val_loss: -3.5018\n",
      "Epoch 103/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.3131 - val_loss: -2.7603\n",
      "Epoch 104/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5171 - val_loss: -3.9617\n",
      "Epoch 105/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4978 - val_loss: -3.9532\n",
      "Epoch 106/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5727 - val_loss: -4.0398\n",
      "Epoch 107/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5884 - val_loss: -3.9342\n",
      "Epoch 108/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4463 - val_loss: -3.8595\n",
      "Epoch 109/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4991 - val_loss: -2.6703\n",
      "Epoch 110/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5839 - val_loss: -3.9436\n",
      "Epoch 111/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5832 - val_loss: -3.3867\n",
      "Epoch 112/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5646 - val_loss: -0.7511\n",
      "Epoch 113/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6148 - val_loss: -4.1028\n",
      "Epoch 114/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.3899 - val_loss: -3.9756\n",
      "Epoch 115/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5104 - val_loss: -3.6407\n",
      "Epoch 116/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3796 - val_loss: -3.8588\n",
      "Epoch 117/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5826 - val_loss: -3.5950\n",
      "Epoch 118/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6736"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 244ms/step - loss: -4.6736 - val_loss: -4.2707\n",
      "Epoch 119/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4915 - val_loss: -3.9277\n",
      "Epoch 120/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5891 - val_loss: -3.4166\n",
      "Epoch 121/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.8209 - val_loss: -3.7962\n",
      "Epoch 122/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2983 - val_loss: -3.6499\n",
      "Epoch 123/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4283 - val_loss: -3.7320\n",
      "Epoch 124/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6050 - val_loss: -4.0571\n",
      "Epoch 125/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6473 - val_loss: -3.4776\n",
      "Epoch 126/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7521 - val_loss: -4.0359\n",
      "Epoch 127/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.8670 - val_loss: -3.6512\n",
      "Epoch 128/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5361 - val_loss: -4.0052\n",
      "Epoch 129/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5511 - val_loss: -3.3222\n",
      "Epoch 130/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.6864 - val_loss: -4.2002\n",
      "Epoch 131/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6237 - val_loss: -4.2478\n",
      "Epoch 132/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8599 - val_loss: -3.5518\n",
      "Epoch 133/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4511 - val_loss: -3.2633\n",
      "Epoch 134/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6180 - val_loss: -3.6600\n",
      "Epoch 135/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6576 - val_loss: -3.7214\n",
      "Epoch 136/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7268 - val_loss: -3.8056\n",
      "Epoch 137/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4825 - val_loss: -3.9265\n",
      "Epoch 138/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0469 - val_loss: -3.6930\n",
      "Epoch 139/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.8168 - val_loss: -3.8970\n",
      "Epoch 140/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3413 - val_loss: -3.5058\n",
      "Epoch 141/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6539 - val_loss: -3.5830\n",
      "Epoch 142/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9241 - val_loss: -2.1700\n",
      "Epoch 143/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.4491 - val_loss: -2.8891\n",
      "Epoch 144/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9074 - val_loss: -3.3288\n",
      "Epoch 145/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1420 - val_loss: -3.8340\n",
      "Epoch 146/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2895 - val_loss: -3.8639\n",
      "Epoch 147/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3193 - val_loss: -3.4484\n",
      "Epoch 148/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4050 - val_loss: -4.1692\n",
      "Epoch 149/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4188 - val_loss: -3.9502\n",
      "Epoch 150/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4386 - val_loss: -4.0750\n",
      "Epoch 151/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5036 - val_loss: -4.2519\n",
      "Epoch 152/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4317 - val_loss: -3.8368\n",
      "Epoch 153/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3674 - val_loss: -3.4480\n",
      "Epoch 154/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6184 - val_loss: -4.1713\n",
      "Epoch 155/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.4899 - val_loss: -4.1149\n",
      "Epoch 156/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5694 - val_loss: -3.7450\n",
      "Epoch 157/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1200 - val_loss: -4.0990\n",
      "Epoch 158/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6273 - val_loss: -4.1513\n",
      "Epoch 159/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5100 - val_loss: -4.0035\n",
      "Epoch 160/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5461 - val_loss: 0.0885\n",
      "Epoch 161/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6073 - val_loss: -3.0007\n",
      "Epoch 162/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6476"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.6476 - val_loss: -4.2789\n",
      "Epoch 163/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6020 - val_loss: -3.1968\n",
      "Epoch 164/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6211 - val_loss: -3.5218\n",
      "Epoch 165/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6678 - val_loss: -3.6081\n",
      "Epoch 166/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7465"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 246ms/step - loss: -4.7465 - val_loss: -4.3625\n",
      "Epoch 167/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4309 - val_loss: -3.0168\n",
      "Epoch 168/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.0960 - val_loss: -3.6183\n",
      "Epoch 169/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6787 - val_loss: -4.1767\n",
      "Epoch 170/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5722 - val_loss: -2.7343\n",
      "Epoch 171/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7018 - val_loss: -4.0716\n",
      "Epoch 172/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6736 - val_loss: -1.7758\n",
      "Epoch 173/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6046 - val_loss: -3.6812\n",
      "Epoch 174/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7336 - val_loss: -3.5801\n",
      "Epoch 175/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4533 - val_loss: -3.7121\n",
      "Epoch 176/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7398 - val_loss: -3.4302\n",
      "Epoch 177/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5912 - val_loss: -3.2624\n",
      "Epoch 178/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7154 - val_loss: -3.8858\n",
      "Epoch 179/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8036 - val_loss: -4.1788\n",
      "Epoch 180/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7230 - val_loss: -3.5693\n",
      "Epoch 181/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7924 - val_loss: -3.6401\n",
      "Epoch 182/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8239 - val_loss: -4.0319\n",
      "Epoch 183/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6690 - val_loss: -4.0728\n",
      "Epoch 184/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7510 - val_loss: -4.1786\n",
      "Epoch 185/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7088 - val_loss: -2.8356\n",
      "Epoch 186/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.6009 - val_loss: -2.7407\n",
      "Epoch 187/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1765 - val_loss: -4.0079\n",
      "Epoch 188/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5796 - val_loss: -3.7562\n",
      "Epoch 189/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6453 - val_loss: -4.1142\n",
      "Epoch 190/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6994 - val_loss: -4.0109\n",
      "Epoch 191/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6639 - val_loss: -3.9712\n",
      "Epoch 192/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7618 - val_loss: -4.0166\n",
      "Epoch 193/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7198 - val_loss: -1.8682\n",
      "Epoch 194/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.4971 - val_loss: -3.5944\n",
      "Epoch 195/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.6870 - val_loss: -3.6306\n",
      "Epoch 196/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1105 - val_loss: -3.4613\n",
      "Epoch 197/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4358 - val_loss: -3.9787\n",
      "Epoch 198/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5963 - val_loss: -3.5576\n",
      "Epoch 199/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6920 - val_loss: -4.2682\n",
      "Epoch 200/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7132 - val_loss: -3.9080\n",
      "Epoch 201/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7032 - val_loss: -4.1623\n",
      "Epoch 202/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4915 - val_loss: -4.0985\n",
      "Epoch 203/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7882 - val_loss: -4.1228\n",
      "Epoch 204/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7628 - val_loss: -3.1861\n",
      "Epoch 205/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7773 - val_loss: -3.3391\n",
      "Epoch 206/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3025 - val_loss: -2.6855\n",
      "Epoch 207/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.5855 - val_loss: -3.0065\n",
      "Epoch 208/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1709 - val_loss: -3.6806\n",
      "Epoch 209/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4145 - val_loss: -4.0569\n",
      "Epoch 210/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5431 - val_loss: -4.1049\n",
      "Epoch 211/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6488 - val_loss: -4.1878\n",
      "Epoch 212/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6155 - val_loss: -4.0008\n",
      "Epoch 213/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6251 - val_loss: -3.1930\n",
      "Epoch 214/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6604"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -4.6604 - val_loss: -4.4009\n",
      "Epoch 215/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4157 - val_loss: -4.1330\n",
      "Epoch 216/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6250 - val_loss: -3.9619\n",
      "Epoch 217/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4727 - val_loss: -4.1009\n",
      "Epoch 218/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7899 - val_loss: -4.1883\n",
      "Epoch 219/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7911 - val_loss: -4.1243\n",
      "Epoch 220/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4289 - val_loss: -3.7366\n",
      "Epoch 221/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7314 - val_loss: -4.1198\n",
      "Epoch 222/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7375 - val_loss: -4.1822\n",
      "Epoch 223/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7750 - val_loss: -4.1895\n",
      "Epoch 224/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7003 - val_loss: -3.5331\n",
      "Epoch 225/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6810 - val_loss: -3.9284\n",
      "Epoch 226/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8658"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 236ms/step - loss: -4.8658 - val_loss: -4.4153\n",
      "Epoch 227/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7171 - val_loss: -3.8088\n",
      "Epoch 228/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8306 - val_loss: -4.1805\n",
      "Epoch 229/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8556 - val_loss: -3.9099\n",
      "Epoch 230/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6894 - val_loss: -4.4080\n",
      "Epoch 231/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8307 - val_loss: -4.2378\n",
      "Epoch 232/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8093 - val_loss: -4.1713\n",
      "Epoch 233/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8609 - val_loss: -3.2408\n",
      "Epoch 234/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8103 - val_loss: -4.3667\n",
      "Epoch 235/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3918 - val_loss: -4.3058\n",
      "Epoch 236/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7397 - val_loss: -4.1778\n",
      "Epoch 237/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.5093 - val_loss: -4.1070\n",
      "Epoch 238/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0254 - val_loss: -2.7790\n",
      "Epoch 239/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2686 - val_loss: -4.2280\n",
      "Epoch 240/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4850 - val_loss: -3.9688\n",
      "Epoch 241/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6847 - val_loss: -3.8541\n",
      "Epoch 242/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7665 - val_loss: -3.9574\n",
      "Epoch 243/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7364 - val_loss: -4.0281\n",
      "Epoch 244/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8152 - val_loss: -4.0017\n",
      "Epoch 245/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6592 - val_loss: -4.1383\n",
      "Epoch 246/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.6039 - val_loss: -1.3710\n",
      "Epoch 247/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -2.4293 - val_loss: -2.6275\n",
      "Epoch 248/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -2.8003 - val_loss: -2.8837\n",
      "Epoch 249/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.1706 - val_loss: -3.5093\n",
      "Epoch 250/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -3.3610 - val_loss: -3.7730\n",
      "Epoch 251/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.4718 - val_loss: -3.9008\n",
      "Epoch 252/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.5252 - val_loss: -3.3673\n",
      "Epoch 253/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.6588 - val_loss: -3.0435\n",
      "Epoch 254/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.7427 - val_loss: -3.8639\n",
      "Epoch 255/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7645 - val_loss: -4.0218\n",
      "Epoch 256/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8809 - val_loss: -1.2994\n",
      "Epoch 257/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0136 - val_loss: -2.1974\n",
      "Epoch 258/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1382 - val_loss: -4.1719\n",
      "Epoch 259/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0892 - val_loss: -4.0824\n",
      "Epoch 260/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3313 - val_loss: -3.8237\n",
      "Epoch 261/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3372 - val_loss: -4.1385\n",
      "Epoch 262/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2927 - val_loss: -4.3050\n",
      "Epoch 263/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.4462 - val_loss: -4.0393\n",
      "Epoch 264/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.3062 - val_loss: -3.5622\n",
      "Epoch 265/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2394 - val_loss: -4.3301\n",
      "Epoch 266/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4962 - val_loss: -3.7332\n",
      "Epoch 267/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.8850 - val_loss: -3.5624\n",
      "Epoch 268/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4776 - val_loss: -3.8722\n",
      "Epoch 269/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4887 - val_loss: -3.7528\n",
      "Epoch 270/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3690 - val_loss: -3.3831\n",
      "Epoch 271/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7753 - val_loss: -3.6053\n",
      "Epoch 272/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -4.5168 - val_loss: -4.3215\n",
      "Epoch 273/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6778 - val_loss: -3.9555\n",
      "Epoch 274/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5529 - val_loss: -2.2530\n",
      "Epoch 275/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.8345 - val_loss: -3.4667\n",
      "Epoch 276/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4589 - val_loss: -3.7173\n",
      "Epoch 277/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5521 - val_loss: -3.0369\n",
      "Epoch 278/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.2591 - val_loss: -2.2518\n",
      "Epoch 279/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.8803 - val_loss: -2.8141\n",
      "Epoch 280/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2160 - val_loss: -3.0757\n",
      "Epoch 281/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4176 - val_loss: -3.3361\n",
      "Epoch 282/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5078 - val_loss: -3.5930\n",
      "Epoch 283/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.5271 - val_loss: -3.3788\n",
      "Epoch 284/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5587 - val_loss: -3.9678\n",
      "Epoch 285/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5613 - val_loss: -3.9319\n",
      "Epoch 286/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6643 - val_loss: -3.9432\n",
      "Epoch 287/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5822 - val_loss: -3.5506\n",
      "Epoch 288/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6830 - val_loss: -0.9855\n",
      "Epoch 289/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.6753 - val_loss: -3.2862\n",
      "Epoch 290/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.8248 - val_loss: -4.3334\n",
      "Epoch 291/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3819 - val_loss: -3.6517\n",
      "Epoch 292/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5782 - val_loss: -2.5983\n",
      "Epoch 293/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6488 - val_loss: -3.4408\n",
      "Epoch 294/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6681 - val_loss: -3.2286\n",
      "Epoch 295/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7209 - val_loss: -4.0703\n",
      "Epoch 296/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6940 - val_loss: -3.6892\n",
      "Epoch 297/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6681 - val_loss: -3.9489\n",
      "Epoch 298/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7151 - val_loss: -4.2113\n",
      "Epoch 299/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7468 - val_loss: -4.3201\n",
      "Epoch 300/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7335 - val_loss: -4.3729\n",
      "Epoch 301/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7330 - val_loss: -3.9358\n",
      "Epoch 302/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7137 - val_loss: -4.1484\n",
      "Epoch 303/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7792 - val_loss: -3.3782\n",
      "Epoch 304/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8182"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.8182 - val_loss: -4.4286\n",
      "Epoch 305/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9948 - val_loss: -3.9750\n",
      "Epoch 306/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1403 - val_loss: -4.1726\n",
      "Epoch 307/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6444 - val_loss: -4.1633\n",
      "Epoch 308/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.6986 - val_loss: -4.2176\n",
      "Epoch 309/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6579 - val_loss: -4.2138\n",
      "Epoch 310/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8132 - val_loss: -4.1049\n",
      "Epoch 311/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7943 - val_loss: -4.2916\n",
      "Epoch 312/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7275 - val_loss: -4.1712\n",
      "Epoch 313/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -2.7536 - val_loss: -2.6926\n",
      "Epoch 314/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.9004 - val_loss: -3.7396\n",
      "Epoch 315/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3963 - val_loss: -3.7492\n",
      "Epoch 316/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3894 - val_loss: -3.6744\n",
      "Epoch 317/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5894 - val_loss: -3.6885\n",
      "Epoch 318/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6198 - val_loss: -3.4343\n",
      "Epoch 319/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7013 - val_loss: -3.3153\n",
      "Epoch 320/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5623 - val_loss: -4.2069\n",
      "Epoch 321/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7423 - val_loss: -4.0123\n",
      "Epoch 322/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6389 - val_loss: -3.9880\n",
      "Epoch 323/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7323 - val_loss: -4.2249\n",
      "Epoch 324/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5836 - val_loss: -1.4231\n",
      "Epoch 325/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7732 - val_loss: -4.1123\n",
      "Epoch 326/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6876 - val_loss: -3.9896\n",
      "Epoch 327/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7713 - val_loss: -4.1560\n",
      "Epoch 328/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8098 - val_loss: -3.8555\n",
      "Epoch 329/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5952 - val_loss: -4.1223\n",
      "Epoch 330/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7747 - val_loss: -4.0921\n",
      "Epoch 331/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8559 - val_loss: -3.0606\n",
      "Epoch 332/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9050"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -4.9050 - val_loss: -4.4858\n",
      "Epoch 333/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6210 - val_loss: -3.8118\n",
      "Epoch 334/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8723 - val_loss: -4.1783\n",
      "Epoch 335/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8060 - val_loss: -3.5809\n",
      "Epoch 336/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8639 - val_loss: -4.1453\n",
      "Epoch 337/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8999 - val_loss: -4.2125\n",
      "Epoch 338/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3671 - val_loss: -4.2741\n",
      "Epoch 339/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8380 - val_loss: -4.3098\n",
      "Epoch 340/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8427 - val_loss: -2.0081\n",
      "Epoch 341/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8813 - val_loss: -2.6305\n",
      "Epoch 342/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8153 - val_loss: -4.0498\n",
      "Epoch 343/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7664 - val_loss: -4.0679\n",
      "Epoch 344/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9438 - val_loss: -4.3218\n",
      "Epoch 345/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7580 - val_loss: -4.0053\n",
      "Epoch 346/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9613 - val_loss: -4.3634\n",
      "Epoch 347/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8671 - val_loss: -4.1497\n",
      "Epoch 348/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9265 - val_loss: -4.1318\n",
      "Epoch 349/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8733 - val_loss: -2.7299\n",
      "Epoch 350/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9265 - val_loss: -3.5703\n",
      "Epoch 351/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8401 - val_loss: -4.1181\n",
      "Epoch 352/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8699 - val_loss: -4.2049\n",
      "Epoch 353/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9442 - val_loss: -3.5825\n",
      "Epoch 354/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7718 - val_loss: -4.1739\n",
      "Epoch 355/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9033 - val_loss: -4.2722\n",
      "Epoch 356/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9293 - val_loss: -4.0543\n",
      "Epoch 357/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8780 - val_loss: -4.1315\n",
      "Epoch 358/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8818 - val_loss: -4.3889\n",
      "Epoch 359/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0079"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 242ms/step - loss: -5.0079 - val_loss: -4.5139\n",
      "Epoch 360/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7924 - val_loss: -4.0895\n",
      "Epoch 361/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.0017 - val_loss: -2.2759\n",
      "Epoch 362/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.3682 - val_loss: -2.7135\n",
      "Epoch 363/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7223 - val_loss: -3.0647\n",
      "Epoch 364/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9844 - val_loss: -2.5206\n",
      "Epoch 365/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.0837 - val_loss: -3.0751\n",
      "Epoch 366/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.1819 - val_loss: -3.2489\n",
      "Epoch 367/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1591 - val_loss: -3.6173\n",
      "Epoch 368/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2818 - val_loss: -3.4297\n",
      "Epoch 369/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3416 - val_loss: -3.4038\n",
      "Epoch 370/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2600 - val_loss: -3.3073\n",
      "Epoch 371/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4137 - val_loss: -3.6118\n",
      "Epoch 372/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4709 - val_loss: -3.3097\n",
      "Epoch 373/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4519 - val_loss: -3.6972\n",
      "Epoch 374/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.4744 - val_loss: -3.2174\n",
      "Epoch 375/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1682 - val_loss: -3.7145\n",
      "Epoch 376/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2524 - val_loss: -3.9695\n",
      "Epoch 377/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4534 - val_loss: -3.9095\n",
      "Epoch 378/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5214 - val_loss: -3.2115\n",
      "Epoch 379/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.5689 - val_loss: -3.8721\n",
      "Epoch 380/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5192 - val_loss: -2.9722\n",
      "Epoch 381/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6040 - val_loss: -4.0338\n",
      "Epoch 382/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6263 - val_loss: -3.8714\n",
      "Epoch 383/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4683 - val_loss: -3.8877\n",
      "Epoch 384/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6326 - val_loss: -3.9977\n",
      "Epoch 385/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5736 - val_loss: -3.6849\n",
      "Epoch 386/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6090 - val_loss: -4.0864\n",
      "Epoch 387/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5848 - val_loss: -3.9959\n",
      "Epoch 388/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6846 - val_loss: -3.7956\n",
      "Epoch 389/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6589 - val_loss: -4.1185\n",
      "Epoch 390/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6604 - val_loss: -4.0544\n",
      "Epoch 391/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6768 - val_loss: -2.7146\n",
      "Epoch 392/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6460 - val_loss: -4.1444\n",
      "Epoch 393/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6652 - val_loss: -3.9486\n",
      "Epoch 394/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7488 - val_loss: -2.5472\n",
      "Epoch 395/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6195 - val_loss: -2.5952\n",
      "Epoch 396/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6698 - val_loss: -4.1078\n",
      "Epoch 397/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7552 - val_loss: -4.0894\n",
      "Epoch 398/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7523 - val_loss: -3.9132\n",
      "Epoch 399/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0486 - val_loss: -3.8411\n",
      "Epoch 400/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6379 - val_loss: -3.9659\n",
      "Epoch 401/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6289 - val_loss: -3.9479\n",
      "Epoch 402/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6575 - val_loss: -3.9992\n",
      "Epoch 403/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7141 - val_loss: -3.9421\n",
      "Epoch 404/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8004 - val_loss: -4.2626\n",
      "Epoch 405/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7903 - val_loss: -3.9408\n",
      "Epoch 406/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7342 - val_loss: -3.5754\n",
      "Epoch 407/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8315 - val_loss: -4.1744\n",
      "Epoch 408/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8364 - val_loss: -4.3254\n",
      "Epoch 409/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7986 - val_loss: -4.2100\n",
      "Epoch 410/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8275 - val_loss: -4.1550\n",
      "Epoch 411/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8775 - val_loss: -3.6983\n",
      "Epoch 412/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8844 - val_loss: -4.1647\n",
      "Epoch 413/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8808 - val_loss: -4.2112\n",
      "Epoch 414/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8807 - val_loss: -2.2527\n",
      "Epoch 415/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8154 - val_loss: -4.0711\n",
      "Epoch 416/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8479 - val_loss: -3.7504\n",
      "Epoch 417/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8905 - val_loss: -4.1873\n",
      "Epoch 418/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9407 - val_loss: -4.1385\n",
      "Epoch 419/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9241 - val_loss: -4.3254\n",
      "Epoch 420/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8240 - val_loss: -4.3043\n",
      "Epoch 421/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9903 - val_loss: -4.0142\n",
      "Epoch 422/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9187 - val_loss: -4.4437\n",
      "Epoch 423/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8835 - val_loss: -2.4017\n",
      "Epoch 424/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7256 - val_loss: -4.4855\n",
      "Epoch 425/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9264 - val_loss: -3.1221\n",
      "Epoch 426/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8795 - val_loss: -4.4154\n",
      "Epoch 427/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9309 - val_loss: -4.3025\n",
      "Epoch 428/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9643 - val_loss: -4.0805\n",
      "Epoch 429/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9515 - val_loss: -4.2037\n",
      "Epoch 430/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7974 - val_loss: -4.1016\n",
      "Epoch 431/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9887 - val_loss: -3.6398\n",
      "Epoch 432/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9131 - val_loss: -4.1402\n",
      "Epoch 433/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9911 - val_loss: -4.4109\n",
      "Epoch 434/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9086 - val_loss: -4.2130\n",
      "Epoch 435/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9378 - val_loss: -4.2765\n",
      "Epoch 436/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0081 - val_loss: -4.3757\n",
      "Epoch 437/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0109 - val_loss: -4.4834\n",
      "Epoch 438/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8891 - val_loss: -4.2662\n",
      "Epoch 439/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0438 - val_loss: -4.3015\n",
      "Epoch 440/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9962 - val_loss: -4.4546\n",
      "Epoch 441/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9840 - val_loss: -3.4608\n",
      "Epoch 442/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0008 - val_loss: -4.2295\n",
      "Epoch 443/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0038 - val_loss: -4.2495\n",
      "Epoch 444/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8547 - val_loss: -3.3585\n",
      "Epoch 445/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8945 - val_loss: -4.2928\n",
      "Epoch 446/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9087 - val_loss: -4.1700\n",
      "Epoch 447/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0280 - val_loss: -1.1647\n",
      "Epoch 448/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8472 - val_loss: -4.4639\n",
      "Epoch 449/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9718 - val_loss: -4.3624\n",
      "Epoch 450/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0177 - val_loss: -4.3487\n",
      "Epoch 451/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9978 - val_loss: -4.4596\n",
      "Epoch 452/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9888"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -4.9888 - val_loss: -4.6617\n",
      "Epoch 453/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.0183 - val_loss: -4.5658\n",
      "Epoch 454/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0293 - val_loss: -4.2691\n",
      "Epoch 455/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9365 - val_loss: -4.4232\n",
      "Epoch 456/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0838 - val_loss: -4.6256\n",
      "Epoch 457/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0747 - val_loss: -2.3282\n",
      "Epoch 458/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9951 - val_loss: -4.1934\n",
      "Epoch 459/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0566 - val_loss: -4.4369\n",
      "Epoch 460/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0631 - val_loss: -3.9336\n",
      "Epoch 461/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0027 - val_loss: -4.3071\n",
      "Epoch 462/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0590 - val_loss: -4.3396\n",
      "Epoch 463/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0652 - val_loss: -3.1905\n",
      "Epoch 464/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9522 - val_loss: -3.7246\n",
      "Epoch 465/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0744 - val_loss: -4.1265\n",
      "Epoch 466/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0329 - val_loss: -4.5404\n",
      "Epoch 467/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0356 - val_loss: -4.5492\n",
      "Epoch 468/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0771 - val_loss: -4.2352\n",
      "Epoch 469/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.0812 - val_loss: -3.4069\n",
      "Epoch 470/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9770 - val_loss: -4.2148\n",
      "Epoch 471/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1046 - val_loss: -4.3693\n",
      "Epoch 472/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9311 - val_loss: -4.2226\n",
      "Epoch 473/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0972 - val_loss: -4.2895\n",
      "Epoch 474/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0748 - val_loss: -4.3753\n",
      "Epoch 475/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0590 - val_loss: -0.9403\n",
      "Epoch 476/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0853 - val_loss: -4.5271\n",
      "Epoch 477/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9903 - val_loss: -4.5055\n",
      "Epoch 478/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0631 - val_loss: -4.3644\n",
      "Epoch 479/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8200 - val_loss: -4.5892\n",
      "Epoch 480/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1072 - val_loss: -4.5701\n",
      "Epoch 481/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0743 - val_loss: -3.9647\n",
      "Epoch 482/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9922 - val_loss: -3.8664\n",
      "Epoch 483/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1105 - val_loss: -4.4100\n",
      "Epoch 484/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1503 - val_loss: -2.9502\n",
      "Epoch 485/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7494 - val_loss: -4.5254\n",
      "Epoch 486/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9529 - val_loss: -2.7952\n",
      "Epoch 487/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0223 - val_loss: -4.1705\n",
      "Epoch 488/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0688 - val_loss: -3.5011\n",
      "Epoch 489/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9251 - val_loss: -4.6233\n",
      "Epoch 490/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0607 - val_loss: -4.4337\n",
      "Epoch 491/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0767 - val_loss: -4.5029\n",
      "Epoch 492/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0699 - val_loss: -3.8807\n",
      "Epoch 493/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0367 - val_loss: -4.6532\n",
      "Epoch 494/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1214 - val_loss: -4.1631\n",
      "Epoch 495/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0596 - val_loss: -4.4898\n",
      "Epoch 496/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1274 - val_loss: -3.6901\n",
      "Epoch 497/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9649 - val_loss: -4.2200\n",
      "Epoch 498/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1556 - val_loss: -3.6140\n",
      "Epoch 499/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.0937 - val_loss: -4.4659\n",
      "Epoch 500/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0893 - val_loss: -4.2267\n",
      "Epoch 501/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1543 - val_loss: -4.3242\n",
      "Epoch 502/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1181 - val_loss: -3.9227\n",
      "Epoch 503/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0776 - val_loss: -4.6296\n",
      "Epoch 504/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1063 - val_loss: -3.9992\n",
      "Epoch 505/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1518 - val_loss: -0.8197\n",
      "Epoch 506/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0058 - val_loss: -4.1175\n",
      "Epoch 507/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0686 - val_loss: -4.5803\n",
      "Epoch 508/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0447 - val_loss: -4.4927\n",
      "Epoch 509/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0649 - val_loss: -3.6448\n",
      "Epoch 510/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1470 - val_loss: -3.4187\n",
      "Epoch 511/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: 8.2860 - val_loss: -2.1094\n",
      "Epoch 512/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.4597 - val_loss: -2.9450\n",
      "Epoch 513/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0164 - val_loss: -3.1944\n",
      "Epoch 514/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.9329 - val_loss: -3.3493\n",
      "Epoch 515/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.5442 - val_loss: -3.3513\n",
      "Epoch 516/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.7471 - val_loss: -3.2320\n",
      "Epoch 517/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.1684 - val_loss: -3.5669\n",
      "Epoch 518/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5484 - val_loss: -3.6142\n",
      "Epoch 519/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6016 - val_loss: -3.3287\n",
      "Epoch 520/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6438 - val_loss: -3.9132\n",
      "Epoch 521/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6934 - val_loss: -4.0418\n",
      "Epoch 522/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6950 - val_loss: -3.9483\n",
      "Epoch 523/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7553 - val_loss: -3.6805\n",
      "Epoch 524/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7297 - val_loss: -4.0120\n",
      "Epoch 525/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7706 - val_loss: -3.9984\n",
      "Epoch 526/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1031 - val_loss: -3.6259\n",
      "Epoch 527/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5323 - val_loss: -3.9376\n",
      "Epoch 528/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7066 - val_loss: -4.0214\n",
      "Epoch 529/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6838 - val_loss: -3.9485\n",
      "Epoch 530/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7782 - val_loss: -3.9711\n",
      "Epoch 531/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8083 - val_loss: -4.0121\n",
      "Epoch 532/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.7843 - val_loss: -3.8797\n",
      "Epoch 533/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5679 - val_loss: -3.8707\n",
      "Epoch 534/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8430 - val_loss: -4.1908\n",
      "Epoch 535/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8839 - val_loss: -4.1882\n",
      "Epoch 536/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8760 - val_loss: -4.0746\n",
      "Epoch 537/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8924 - val_loss: -4.1043\n",
      "Epoch 538/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8482 - val_loss: -3.6559\n",
      "Epoch 539/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8712 - val_loss: -4.2225\n",
      "Epoch 540/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9211 - val_loss: -4.1826\n",
      "Epoch 541/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9078 - val_loss: -4.2083\n",
      "Epoch 542/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8763 - val_loss: -4.1209\n",
      "Epoch 543/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8543 - val_loss: -4.2268\n",
      "Epoch 544/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9563 - val_loss: -3.5189\n",
      "Epoch 545/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9345 - val_loss: -4.2465\n",
      "Epoch 546/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9780 - val_loss: -2.7759\n",
      "Epoch 547/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9259 - val_loss: -4.2190\n",
      "Epoch 548/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9742 - val_loss: -1.7835\n",
      "Epoch 549/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9264 - val_loss: -4.2415\n",
      "Epoch 550/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0167 - val_loss: -4.2818\n",
      "Epoch 551/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8744 - val_loss: -4.2218\n",
      "Epoch 552/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9142 - val_loss: -4.2099\n",
      "Epoch 553/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9132 - val_loss: -4.0821\n",
      "Epoch 554/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8215 - val_loss: -4.1327\n",
      "Epoch 555/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0205 - val_loss: -2.0785\n",
      "Epoch 556/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4119 - val_loss: -3.2006\n",
      "Epoch 557/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8868 - val_loss: -4.3234\n",
      "Epoch 558/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9286 - val_loss: -4.1787\n",
      "Epoch 559/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9376 - val_loss: -3.9375\n",
      "Epoch 560/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9789 - val_loss: -1.6296\n",
      "Epoch 561/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9813 - val_loss: -3.2916\n",
      "Epoch 562/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9663 - val_loss: -2.9404\n",
      "Epoch 563/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9139 - val_loss: -3.7714\n",
      "Epoch 564/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7586 - val_loss: -4.1354\n",
      "Epoch 565/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8661 - val_loss: -3.9709\n",
      "Epoch 566/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9092 - val_loss: -4.0097\n",
      "Epoch 567/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9239 - val_loss: -3.9872\n",
      "Epoch 568/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9291 - val_loss: -3.8676\n",
      "Epoch 569/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8718 - val_loss: -4.2906\n",
      "Epoch 570/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0116 - val_loss: -3.9178\n",
      "Epoch 571/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9659 - val_loss: -4.4510\n",
      "Epoch 572/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9706 - val_loss: -4.0198\n",
      "Epoch 573/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9173 - val_loss: -4.3477\n",
      "Epoch 574/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5763 - val_loss: -3.9089\n",
      "Epoch 575/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8256 - val_loss: -4.2595\n",
      "Epoch 576/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9056 - val_loss: -4.1753\n",
      "Epoch 577/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0134 - val_loss: -2.3417\n",
      "Epoch 578/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9549 - val_loss: -4.1774\n",
      "Epoch 579/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9511 - val_loss: -3.0923\n",
      "Epoch 580/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9477 - val_loss: -3.9770\n",
      "Epoch 581/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0733 - val_loss: -3.2720\n",
      "Epoch 582/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9360 - val_loss: -3.6334\n",
      "Epoch 583/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9872 - val_loss: -4.4175\n",
      "Epoch 584/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0845 - val_loss: -3.4880\n",
      "Epoch 585/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9926 - val_loss: -3.5523\n",
      "Epoch 586/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9816 - val_loss: -4.1715\n",
      "Epoch 587/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0203 - val_loss: -3.5927\n",
      "Epoch 588/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0604 - val_loss: -4.0432\n",
      "Epoch 589/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0051 - val_loss: -3.8046\n",
      "Epoch 590/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9902 - val_loss: -3.4603\n",
      "Epoch 591/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0176 - val_loss: -3.5507\n",
      "Epoch 592/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0790 - val_loss: -3.9596\n",
      "Epoch 593/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8373 - val_loss: -4.3866\n",
      "Epoch 594/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0680 - val_loss: -3.7834\n",
      "Epoch 595/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0763 - val_loss: -4.1831\n",
      "Epoch 596/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0145 - val_loss: -4.2654\n",
      "Epoch 597/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0636 - val_loss: -2.0496\n",
      "Epoch 598/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1198 - val_loss: -4.2103\n",
      "Epoch 599/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0286 - val_loss: -4.4296\n",
      "Epoch 600/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0239 - val_loss: -4.1272\n",
      "Epoch 601/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0947 - val_loss: -4.0583\n",
      "Epoch 602/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 186ms/step - loss: -3.8517 - val_loss: -4.1629\n",
      "Epoch 603/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6347 - val_loss: -4.3258\n",
      "Epoch 604/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8702 - val_loss: -3.7050\n",
      "Epoch 605/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9453 - val_loss: -4.0340\n",
      "Epoch 606/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0225 - val_loss: -4.3412\n",
      "Epoch 607/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8432 - val_loss: -4.0468\n",
      "Epoch 608/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9266 - val_loss: -4.5072\n",
      "Epoch 609/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.0562 - val_loss: -4.2548\n",
      "Epoch 610/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0268 - val_loss: -4.1074\n",
      "Epoch 611/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9414 - val_loss: -3.6945\n",
      "Epoch 612/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0333 - val_loss: -4.1013\n",
      "Epoch 613/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0024 - val_loss: -3.8625\n",
      "Epoch 614/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0847 - val_loss: -1.8702\n",
      "Epoch 615/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0640 - val_loss: -4.0538\n",
      "Epoch 616/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0623 - val_loss: -4.3017\n",
      "Epoch 617/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0877 - val_loss: -4.2823\n",
      "Epoch 618/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0404 - val_loss: -4.0080\n",
      "Epoch 619/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1013 - val_loss: 0.7799\n",
      "Epoch 620/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9520 - val_loss: -4.1886\n",
      "Epoch 621/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0768 - val_loss: -4.2120\n",
      "Epoch 622/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0490 - val_loss: -4.3396\n",
      "Epoch 623/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0869 - val_loss: -3.1321\n",
      "Epoch 624/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4206 - val_loss: -3.1316\n",
      "Epoch 625/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2911 - val_loss: -4.2393\n",
      "Epoch 626/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.6071 - val_loss: -4.2808\n",
      "Epoch 627/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6912 - val_loss: -4.2745\n",
      "Epoch 628/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7338 - val_loss: -3.2268\n",
      "Epoch 629/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7593 - val_loss: -4.0710\n",
      "Epoch 630/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8976 - val_loss: -3.8686\n",
      "Epoch 631/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8746 - val_loss: -4.1877\n",
      "Epoch 632/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9214 - val_loss: -3.9685\n",
      "Epoch 633/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9546 - val_loss: -3.8813\n",
      "Epoch 634/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9564 - val_loss: -4.1141\n",
      "Epoch 635/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9415 - val_loss: -3.6440\n",
      "Epoch 636/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0178 - val_loss: -4.3270\n",
      "Epoch 637/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0159 - val_loss: -4.0695\n",
      "Epoch 638/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0346 - val_loss: -4.5371\n",
      "Epoch 639/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0615 - val_loss: -4.1839\n",
      "Epoch 640/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6989 - val_loss: -4.1620\n",
      "Epoch 641/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1078 - val_loss: -4.1547\n",
      "Epoch 642/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9824 - val_loss: -4.0966\n",
      "Epoch 643/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0830 - val_loss: -3.8773\n",
      "Epoch 644/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9876 - val_loss: -4.5817\n",
      "Epoch 645/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0538 - val_loss: -3.5649\n",
      "Epoch 646/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1123 - val_loss: -3.9494\n",
      "Epoch 647/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0586 - val_loss: -3.9917\n",
      "Epoch 648/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0874 - val_loss: -4.1565\n",
      "Epoch 649/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0950 - val_loss: -3.5461\n",
      "Epoch 650/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9705 - val_loss: -4.6301\n",
      "Epoch 651/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1525 - val_loss: -4.4586\n",
      "Epoch 652/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0593 - val_loss: -4.4801\n",
      "Epoch 653/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9954 - val_loss: -4.3228\n",
      "Epoch 654/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0120 - val_loss: -4.5352\n",
      "Epoch 655/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1509 - val_loss: -3.9947\n",
      "Epoch 656/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1088 - val_loss: -4.2960\n",
      "Epoch 657/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0999"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -5.0999 - val_loss: -4.6874\n",
      "Epoch 658/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0382 - val_loss: -3.1735\n",
      "Epoch 659/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0568 - val_loss: -3.8537\n",
      "Epoch 660/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1159 - val_loss: -4.4168\n",
      "Epoch 661/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0727 - val_loss: -4.1964\n",
      "Epoch 662/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0573 - val_loss: -4.1779\n",
      "Epoch 663/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1227 - val_loss: -4.3870\n",
      "Epoch 664/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0975 - val_loss: -4.3641\n",
      "Epoch 665/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1367 - val_loss: -3.7793\n",
      "Epoch 666/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0408 - val_loss: -4.5549\n",
      "Epoch 667/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1605 - val_loss: -4.5906\n",
      "Epoch 668/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1846 - val_loss: -3.9930\n",
      "Epoch 669/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9471 - val_loss: -3.2576\n",
      "Epoch 670/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7264 - val_loss: -4.4000\n",
      "Epoch 671/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0371 - val_loss: -4.4083\n",
      "Epoch 672/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0465 - val_loss: -4.1413\n",
      "Epoch 673/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1048"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -5.1048 - val_loss: -4.7002\n",
      "Epoch 674/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1753 - val_loss: -3.7298\n",
      "Epoch 675/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9839 - val_loss: -3.5678\n",
      "Epoch 676/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0687 - val_loss: -4.1532\n",
      "Epoch 677/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1516 - val_loss: -1.7540\n",
      "Epoch 678/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0234 - val_loss: -4.2230\n",
      "Epoch 679/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1564 - val_loss: -4.0989\n",
      "Epoch 680/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1259 - val_loss: -4.3952\n",
      "Epoch 681/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1892 - val_loss: -3.5783\n",
      "Epoch 682/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1804 - val_loss: -3.7794\n",
      "Epoch 683/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1304 - val_loss: -4.1534\n",
      "Epoch 684/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1601"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -5.1601 - val_loss: -4.7405\n",
      "Epoch 685/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1791 - val_loss: -4.3407\n",
      "Epoch 686/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1062 - val_loss: -4.5218\n",
      "Epoch 687/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1077 - val_loss: -3.9339\n",
      "Epoch 688/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0697 - val_loss: -2.4389\n",
      "Epoch 689/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0840 - val_loss: -4.3465\n",
      "Epoch 690/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2213 - val_loss: -4.5735\n",
      "Epoch 691/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1850 - val_loss: -4.3930\n",
      "Epoch 692/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1162 - val_loss: -4.5265\n",
      "Epoch 693/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1052 - val_loss: -4.3744\n",
      "Epoch 694/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2772 - val_loss: -1.9928\n",
      "Epoch 695/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1285 - val_loss: -4.4443\n",
      "Epoch 696/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2384 - val_loss: -3.5518\n",
      "Epoch 697/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0792 - val_loss: -2.7179\n",
      "Epoch 698/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1826 - val_loss: -4.0228\n",
      "Epoch 699/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1031 - val_loss: -4.5343\n",
      "Epoch 700/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2284 - val_loss: -4.4028\n",
      "Epoch 701/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1793 - val_loss: -3.8127\n",
      "Epoch 702/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0322 - val_loss: -4.5671\n",
      "Epoch 703/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1865 - val_loss: -4.2619\n",
      "Epoch 704/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2180 - val_loss: -4.4754\n",
      "Epoch 705/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2880 - val_loss: -3.9699\n",
      "Epoch 706/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0480 - val_loss: -3.9903\n",
      "Epoch 707/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2168 - val_loss: -4.6528\n",
      "Epoch 708/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1858 - val_loss: -4.7276\n",
      "Epoch 709/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1917 - val_loss: -1.8746\n",
      "Epoch 710/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2381 - val_loss: -4.5006\n",
      "Epoch 711/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0684 - val_loss: -4.0247\n",
      "Epoch 712/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2314 - val_loss: -3.7871\n",
      "Epoch 713/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7052 - val_loss: -3.3579\n",
      "Epoch 714/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1792 - val_loss: -4.3858\n",
      "Epoch 715/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2138 - val_loss: -3.8280\n",
      "Epoch 716/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0989 - val_loss: -4.4822\n",
      "Epoch 717/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1665 - val_loss: -1.1050\n",
      "Epoch 718/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0856 - val_loss: -2.9130\n",
      "Epoch 719/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0761 - val_loss: -4.3358\n",
      "Epoch 720/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1487 - val_loss: -3.6608\n",
      "Epoch 721/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1438 - val_loss: -4.3844\n",
      "Epoch 722/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2204 - val_loss: -2.6864\n",
      "Epoch 723/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1554 - val_loss: -3.8535\n",
      "Epoch 724/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2703 - val_loss: -3.8190\n",
      "Epoch 725/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0927 - val_loss: -4.2142\n",
      "Epoch 726/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2281 - val_loss: -4.4224\n",
      "Epoch 727/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2428 - val_loss: -4.4131\n",
      "Epoch 728/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2651 - val_loss: -3.9313\n",
      "Epoch 729/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2061 - val_loss: -3.7339\n",
      "Epoch 730/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1777 - val_loss: -4.2912\n",
      "Epoch 731/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2234 - val_loss: -4.4328\n",
      "Epoch 732/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2075 - val_loss: -4.0221\n",
      "Epoch 733/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2629 - val_loss: -4.0362\n",
      "Epoch 734/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1048 - val_loss: -3.5909\n",
      "Epoch 735/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2163 - val_loss: -4.0960\n",
      "Epoch 736/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1414 - val_loss: -4.1051\n",
      "Epoch 737/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3196 - val_loss: -3.1166\n",
      "Epoch 738/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1027 - val_loss: -4.1618\n",
      "Epoch 739/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1051 - val_loss: -4.3438\n",
      "Epoch 740/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2776 - val_loss: -4.6551\n",
      "Epoch 741/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1206 - val_loss: -4.7171\n",
      "Epoch 742/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1328 - val_loss: -4.4828\n",
      "Epoch 743/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2696 - val_loss: -4.4902\n",
      "Epoch 744/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0036 - val_loss: -4.5026\n",
      "Epoch 745/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1912 - val_loss: -4.3231\n",
      "Epoch 746/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1610 - val_loss: -4.4430\n",
      "Epoch 747/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2646 - val_loss: -3.9860\n",
      "Epoch 748/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1302 - val_loss: -3.8082\n",
      "Epoch 749/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1351 - val_loss: -2.2608\n",
      "Epoch 750/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2183 - val_loss: -4.4519\n",
      "Epoch 751/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2384 - val_loss: -4.6394\n",
      "Epoch 752/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2120 - val_loss: -3.5758\n",
      "Epoch 753/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2098 - val_loss: -3.3127\n",
      "Epoch 754/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0836 - val_loss: -4.0577\n",
      "Epoch 755/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2243 - val_loss: -3.8785\n",
      "Epoch 756/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2610 - val_loss: -3.1879\n",
      "Epoch 757/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2658 - val_loss: -4.2223\n",
      "Epoch 758/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2332 - val_loss: -4.5444\n",
      "Epoch 759/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2598 - val_loss: -3.9955\n",
      "Epoch 760/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2642 - val_loss: -3.9594\n",
      "Epoch 761/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3228 - val_loss: -4.1631\n",
      "Epoch 762/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1281 - val_loss: -4.4604\n",
      "Epoch 763/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3010 - val_loss: -3.9779\n",
      "Epoch 764/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3036 - val_loss: -4.4763\n",
      "Epoch 765/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1992 - val_loss: -1.0392\n",
      "Epoch 766/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1939 - val_loss: -4.0624\n",
      "Epoch 767/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2688 - val_loss: -4.2778\n",
      "Epoch 768/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2236 - val_loss: -3.8239\n",
      "Epoch 769/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2841 - val_loss: -4.7328\n",
      "Epoch 770/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2669 - val_loss: -3.4544\n",
      "Epoch 771/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2120 - val_loss: -4.4898\n",
      "Epoch 772/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3171 - val_loss: -4.6490\n",
      "Epoch 773/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2473 - val_loss: -4.2464\n",
      "Epoch 774/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2987 - val_loss: -3.8188\n",
      "Epoch 775/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2622 - val_loss: -4.1989\n",
      "Epoch 776/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2255 - val_loss: -4.4358\n",
      "Epoch 777/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3013 - val_loss: -4.4882\n",
      "Epoch 778/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2396 - val_loss: -3.4019\n",
      "Epoch 779/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0539 - val_loss: -4.4728\n",
      "Epoch 780/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2139 - val_loss: -4.4230\n",
      "Epoch 781/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2967 - val_loss: -4.1483\n",
      "Epoch 782/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3616 - val_loss: -4.2505\n",
      "Epoch 783/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1729 - val_loss: -3.2797\n",
      "Epoch 784/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3173"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -5.3173 - val_loss: -4.8205\n",
      "Epoch 785/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1560 - val_loss: -3.5461\n",
      "Epoch 786/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2287 - val_loss: -3.9379\n",
      "Epoch 787/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3291 - val_loss: -3.2275\n",
      "Epoch 788/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6412 - val_loss: -3.6857\n",
      "Epoch 789/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8944 - val_loss: -4.0138\n",
      "Epoch 790/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0379 - val_loss: -4.5451\n",
      "Epoch 791/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0827 - val_loss: -3.8285\n",
      "Epoch 792/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.2044 - val_loss: -4.4986\n",
      "Epoch 793/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1049 - val_loss: -3.5116\n",
      "Epoch 794/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1818 - val_loss: -4.3754\n",
      "Epoch 795/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2014 - val_loss: -3.2247\n",
      "Epoch 796/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1868 - val_loss: -4.1734\n",
      "Epoch 797/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1614 - val_loss: -4.4536\n",
      "Epoch 798/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2706 - val_loss: -3.7006\n",
      "Epoch 799/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0736 - val_loss: -4.3275\n",
      "Epoch 800/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2418 - val_loss: -4.0634\n",
      "Epoch 801/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0793 - val_loss: -3.4486\n",
      "Epoch 802/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1549 - val_loss: -4.2345\n",
      "Epoch 803/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3100 - val_loss: -4.2790\n",
      "Epoch 804/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2846 - val_loss: -3.9269\n",
      "Epoch 805/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2272 - val_loss: -4.4129\n",
      "Epoch 806/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3004 - val_loss: -4.4202\n",
      "Epoch 807/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2185 - val_loss: -4.3992\n",
      "Epoch 808/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2538 - val_loss: -4.3073\n",
      "Epoch 809/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2127 - val_loss: -4.7451\n",
      "Epoch 810/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3026 - val_loss: -1.8042\n",
      "Epoch 811/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2566 - val_loss: -3.1789\n",
      "Epoch 812/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3164 - val_loss: -1.3551\n",
      "Epoch 813/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2946 - val_loss: -4.7299\n",
      "Epoch 814/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2656 - val_loss: -4.4368\n",
      "Epoch 815/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2604 - val_loss: -4.3558\n",
      "Epoch 816/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3167 - val_loss: -3.1612\n",
      "Epoch 817/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1693 - val_loss: -3.7864\n",
      "Epoch 818/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0495 - val_loss: -4.2658\n",
      "Epoch 819/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8957 - val_loss: -4.3314\n",
      "Epoch 820/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1697 - val_loss: -3.4343\n",
      "Epoch 821/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2904 - val_loss: -3.3970\n",
      "Epoch 822/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1605 - val_loss: -4.2837\n",
      "Epoch 823/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2504 - val_loss: -3.9859\n",
      "Epoch 824/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3015 - val_loss: -4.4499\n",
      "Epoch 825/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2288 - val_loss: -4.5451\n",
      "Epoch 826/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2828 - val_loss: -4.4795\n",
      "Epoch 827/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3110 - val_loss: -4.2627\n",
      "Epoch 828/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2935 - val_loss: -4.3424\n",
      "Epoch 829/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1714 - val_loss: -2.5743\n",
      "Epoch 830/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3133 - val_loss: -4.2926\n",
      "Epoch 831/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2781 - val_loss: -4.1881\n",
      "Epoch 832/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3696 - val_loss: -3.8684\n",
      "Epoch 833/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1646 - val_loss: -3.0879\n",
      "Epoch 834/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0874 - val_loss: -4.2370\n",
      "Epoch 835/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7735 - val_loss: -4.0472\n",
      "Epoch 836/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.0741 - val_loss: -4.2644\n",
      "Epoch 837/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1863 - val_loss: -4.1157\n",
      "Epoch 838/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2973 - val_loss: -4.4559\n",
      "Epoch 839/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2277 - val_loss: -4.3132\n",
      "Epoch 840/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2912 - val_loss: -4.4854\n",
      "Epoch 841/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1578 - val_loss: -4.3100\n",
      "Epoch 842/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2505 - val_loss: -3.8675\n",
      "Epoch 843/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2320 - val_loss: -3.8509\n",
      "Epoch 844/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2878 - val_loss: -2.8515\n",
      "Epoch 845/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7811 - val_loss: -4.2429\n",
      "Epoch 846/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6537 - val_loss: -2.8764\n",
      "Epoch 847/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.9655 - val_loss: -3.5082\n",
      "Epoch 848/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0985 - val_loss: -3.9973\n",
      "Epoch 849/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1803 - val_loss: -3.7825\n",
      "Epoch 850/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2474 - val_loss: -3.5748\n",
      "Epoch 851/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2102 - val_loss: -3.0305\n",
      "Epoch 852/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1935 - val_loss: -4.3577\n",
      "Epoch 853/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2638 - val_loss: -3.8592\n",
      "Epoch 854/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2488 - val_loss: -4.0999\n",
      "Epoch 855/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2419 - val_loss: -3.6546\n",
      "Epoch 856/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3772 - val_loss: -3.9051\n",
      "Epoch 857/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2444 - val_loss: -3.3751\n",
      "Epoch 858/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2350 - val_loss: -3.4086\n",
      "Epoch 859/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2552 - val_loss: -4.2089\n",
      "Epoch 860/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2686 - val_loss: -3.5362\n",
      "Epoch 861/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2124 - val_loss: -4.3693\n",
      "Epoch 862/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2732 - val_loss: -3.2850\n",
      "Epoch 863/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2956 - val_loss: -4.2552\n",
      "Epoch 864/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3771 - val_loss: -3.9497\n",
      "Epoch 865/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2741 - val_loss: -3.6061\n",
      "Epoch 866/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2554 - val_loss: -4.3276\n",
      "Epoch 867/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1660 - val_loss: -3.8763\n",
      "Epoch 868/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2588 - val_loss: -4.5637\n",
      "Epoch 869/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -3.9579 - val_loss: -4.5940\n",
      "Epoch 870/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.0462 - val_loss: -3.0882\n",
      "Epoch 871/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2843 - val_loss: -4.4846\n",
      "Epoch 872/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5808 - val_loss: -3.8331\n",
      "Epoch 873/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9666 - val_loss: -4.3136\n",
      "Epoch 874/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0720 - val_loss: -3.5710\n",
      "Epoch 875/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1371 - val_loss: -4.5963\n",
      "Epoch 876/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1639 - val_loss: -4.7000\n",
      "Epoch 877/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2094 - val_loss: -4.4158\n",
      "Epoch 878/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1823 - val_loss: -4.1466\n",
      "Epoch 879/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2410 - val_loss: -3.3571\n",
      "Epoch 880/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1667 - val_loss: -4.1321\n",
      "Epoch 881/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0864 - val_loss: -4.2365\n",
      "Epoch 882/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2871 - val_loss: -3.7724\n",
      "Epoch 883/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2301 - val_loss: -3.7481\n",
      "Epoch 884/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2759 - val_loss: -3.4945\n",
      "Epoch 885/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8716 - val_loss: -4.4173\n",
      "Epoch 886/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0887 - val_loss: -4.0518\n",
      "Epoch 887/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1842 - val_loss: -3.9235\n",
      "Epoch 888/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0298 - val_loss: -4.2373\n",
      "Epoch 889/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3038 - val_loss: -3.9894\n",
      "Epoch 890/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1442 - val_loss: -3.1354\n",
      "Epoch 891/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2856 - val_loss: -2.0895\n",
      "Epoch 892/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2819 - val_loss: -4.5641\n",
      "Epoch 893/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2799 - val_loss: -3.5829\n",
      "Epoch 894/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2176 - val_loss: -4.0040\n",
      "Epoch 895/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2355 - val_loss: -4.5346\n",
      "Epoch 896/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3237 - val_loss: -4.4633\n",
      "Epoch 897/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2909 - val_loss: -4.3897\n",
      "Epoch 898/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2063 - val_loss: -4.5048\n",
      "Epoch 899/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2509 - val_loss: -4.7611\n",
      "Epoch 900/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2750 - val_loss: -4.0790\n",
      "Epoch 901/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2249 - val_loss: -4.3922\n",
      "Epoch 902/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2225 - val_loss: -4.3733\n",
      "Epoch 903/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2327 - val_loss: -4.3751\n",
      "Epoch 904/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1842 - val_loss: -4.6695\n",
      "Epoch 905/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3077 - val_loss: -4.6223\n",
      "Epoch 906/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3471 - val_loss: -3.3758\n",
      "Epoch 907/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3084 - val_loss: -4.3049\n",
      "Epoch 908/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2769 - val_loss: -4.3097\n",
      "Epoch 909/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3098 - val_loss: -4.0505\n",
      "Epoch 910/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3097 - val_loss: -3.8142\n",
      "Epoch 911/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2359 - val_loss: -4.3615\n",
      "Epoch 912/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3851 - val_loss: -4.6458\n",
      "Epoch 913/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2679 - val_loss: -4.1204\n",
      "Epoch 914/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2759 - val_loss: -3.3940\n",
      "Epoch 915/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3733 - val_loss: -3.7892\n",
      "Epoch 916/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2468 - val_loss: -4.1077\n",
      "Epoch 917/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3509 - val_loss: -3.6408\n",
      "Epoch 918/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2704 - val_loss: -4.0228\n",
      "Epoch 919/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3587 - val_loss: -4.0841\n",
      "Epoch 920/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3166 - val_loss: -4.3424\n",
      "Epoch 921/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2489 - val_loss: -4.2759\n",
      "Epoch 922/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2907 - val_loss: -3.8670\n",
      "Epoch 923/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3453 - val_loss: -4.1996\n",
      "Epoch 924/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2199 - val_loss: -4.3970\n",
      "Epoch 925/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3159 - val_loss: -3.8787\n",
      "Epoch 926/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3527 - val_loss: -4.4008\n",
      "Epoch 927/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2415 - val_loss: -3.7342\n",
      "Epoch 928/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2965 - val_loss: -4.2349\n",
      "Epoch 929/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2945 - val_loss: -4.3480\n",
      "Epoch 930/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4135 - val_loss: -4.0952\n",
      "Epoch 931/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3075 - val_loss: -4.4163\n",
      "Epoch 932/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3863 - val_loss: -4.1701\n",
      "Epoch 933/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2876 - val_loss: -4.3426\n",
      "Epoch 934/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1835 - val_loss: -4.5545\n",
      "Epoch 935/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3312 - val_loss: -2.5711\n",
      "Epoch 936/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3094 - val_loss: -3.5676\n",
      "Epoch 937/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3029 - val_loss: -4.3647\n",
      "Epoch 938/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3502 - val_loss: -3.2711\n",
      "Epoch 939/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3065 - val_loss: -4.2561\n",
      "Epoch 940/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3649 - val_loss: -4.1636\n",
      "Epoch 941/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4160 - val_loss: -3.1993\n",
      "Epoch 942/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2756 - val_loss: -4.3370\n",
      "Epoch 943/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2873 - val_loss: -4.0025\n",
      "Epoch 944/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3669 - val_loss: 0.5629\n",
      "Epoch 945/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0135 - val_loss: -1.9802\n",
      "Epoch 946/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3577 - val_loss: -4.4493\n",
      "Epoch 947/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3068 - val_loss: -3.3132\n",
      "Epoch 948/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3866 - val_loss: -3.4183\n",
      "Epoch 949/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1999 - val_loss: -3.4790\n",
      "Epoch 950/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4305 - val_loss: -3.5303\n",
      "Epoch 951/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2855 - val_loss: -4.1750\n",
      "Epoch 952/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3841 - val_loss: -4.1206\n",
      "Epoch 953/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3198 - val_loss: -3.6849\n",
      "Epoch 954/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2739 - val_loss: -4.2625\n",
      "Epoch 955/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3203 - val_loss: -4.0859\n",
      "Epoch 956/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2644 - val_loss: -3.2686\n",
      "Epoch 957/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.2952 - val_loss: -4.5284\n",
      "Epoch 958/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3792 - val_loss: -4.2122\n",
      "Epoch 959/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3623 - val_loss: -4.3646\n",
      "Epoch 960/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3206 - val_loss: -4.4626\n",
      "Epoch 961/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3576 - val_loss: -4.5658\n",
      "Epoch 962/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3512 - val_loss: -4.4883\n",
      "Epoch 963/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3673 - val_loss: -4.0458\n",
      "Epoch 964/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1611 - val_loss: -4.0342\n",
      "Epoch 965/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2675 - val_loss: -4.4752\n",
      "Epoch 966/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3350 - val_loss: -4.3516\n",
      "Epoch 967/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3721 - val_loss: -2.5181\n",
      "Epoch 968/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3669 - val_loss: -4.1264\n",
      "Epoch 969/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3964 - val_loss: -3.2872\n",
      "Epoch 970/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3617 - val_loss: -4.2617\n",
      "Epoch 971/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3211 - val_loss: -4.2553\n",
      "Epoch 972/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4125 - val_loss: -3.9145\n",
      "Epoch 973/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3193 - val_loss: -4.3213\n",
      "Epoch 974/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1463 - val_loss: -4.4993\n",
      "Epoch 975/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2078 - val_loss: -4.1941\n",
      "Epoch 976/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3381 - val_loss: -3.7775\n",
      "Epoch 977/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3538 - val_loss: -0.9497\n",
      "Epoch 978/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2354 - val_loss: -4.5843\n",
      "Epoch 979/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2468 - val_loss: -3.9714\n",
      "Epoch 980/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3759 - val_loss: -3.5009\n",
      "Epoch 981/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3311 - val_loss: -3.6183\n",
      "Epoch 982/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3682 - val_loss: -3.9223\n",
      "Epoch 983/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3532 - val_loss: -3.7884\n",
      "Epoch 984/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3649 - val_loss: -4.1943\n",
      "Epoch 985/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2292 - val_loss: -4.1807\n",
      "Epoch 986/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2446 - val_loss: -3.5755\n",
      "Epoch 987/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3738 - val_loss: -4.2714\n",
      "Epoch 988/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4490 - val_loss: -4.0876\n",
      "Epoch 989/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2945 - val_loss: -3.7416\n",
      "Epoch 990/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4248 - val_loss: -3.7080\n",
      "Epoch 991/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2956 - val_loss: -3.0194\n",
      "Epoch 992/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3189 - val_loss: -4.3849\n",
      "Epoch 993/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3202 - val_loss: -4.2156\n",
      "Epoch 994/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4326 - val_loss: -3.6244\n",
      "Epoch 995/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2135 - val_loss: -3.6419\n",
      "Epoch 996/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4496 - val_loss: -3.5074\n",
      "Epoch 997/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3879 - val_loss: -1.6037\n",
      "Epoch 998/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3930 - val_loss: -3.3178\n",
      "Epoch 999/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2791 - val_loss: -4.3748\n",
      "Epoch 1000/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3982 - val_loss: -4.0865\n",
      "Epoch 1001/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2506 - val_loss: -4.2652\n",
      "Epoch 1002/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3901 - val_loss: -3.5356\n",
      "Epoch 1003/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3025 - val_loss: -4.2148\n",
      "Epoch 1004/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4348 - val_loss: -3.8195\n",
      "Epoch 1005/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3163 - val_loss: -2.8391\n",
      "Epoch 1006/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2484 - val_loss: -4.3861\n",
      "Epoch 1007/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3649 - val_loss: -4.0385\n",
      "Epoch 1008/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3739 - val_loss: -4.0788\n",
      "Epoch 1009/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4209 - val_loss: 8.8554\n",
      "Epoch 1010/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6471 - val_loss: -3.8632\n",
      "Epoch 1011/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1855 - val_loss: -3.6473\n",
      "Epoch 1012/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2445 - val_loss: -4.3220\n",
      "Epoch 1013/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2133 - val_loss: -4.2062\n",
      "Epoch 1014/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1913 - val_loss: -3.8794\n",
      "Epoch 1015/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2161 - val_loss: -3.7097\n",
      "Epoch 1016/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3397 - val_loss: -2.4338\n",
      "Epoch 1017/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1814 - val_loss: -4.3192\n",
      "Epoch 1018/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3657 - val_loss: -4.2588\n",
      "Epoch 1019/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2854 - val_loss: -3.3619\n",
      "Epoch 1020/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3683 - val_loss: -3.5397\n",
      "Epoch 1021/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3061 - val_loss: -4.3946\n",
      "Epoch 1022/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3408 - val_loss: -3.6255\n",
      "Epoch 1023/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2559 - val_loss: -4.2624\n",
      "Epoch 1024/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2661 - val_loss: -4.2674\n",
      "Epoch 1025/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4074 - val_loss: -1.8502\n",
      "Epoch 1026/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3028 - val_loss: -4.4878\n",
      "Epoch 1027/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2820 - val_loss: -4.3841\n",
      "Epoch 1028/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4303 - val_loss: -4.0338\n",
      "Epoch 1029/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3861 - val_loss: 2.1652\n",
      "Epoch 1030/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0836 - val_loss: -4.1255\n",
      "Epoch 1031/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2717 - val_loss: -4.3227\n",
      "Epoch 1032/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4135 - val_loss: -3.3551\n",
      "Epoch 1033/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3709 - val_loss: -3.6394\n",
      "Epoch 1034/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3441 - val_loss: -4.3874\n",
      "Epoch 1035/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4260 - val_loss: -3.4828\n",
      "Epoch 1036/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3833 - val_loss: -3.6298\n",
      "Epoch 1037/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4011 - val_loss: -4.5509\n",
      "Epoch 1038/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3121 - val_loss: -4.2946\n",
      "Epoch 1039/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3574 - val_loss: -3.7219\n",
      "Epoch 1040/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3562 - val_loss: -4.3488\n",
      "Epoch 1041/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4175 - val_loss: -4.2960\n",
      "Epoch 1042/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4216 - val_loss: -4.1402\n",
      "Epoch 1043/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4139 - val_loss: -3.1662\n",
      "Epoch 1044/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2340 - val_loss: -4.2401\n",
      "Epoch 1045/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3536 - val_loss: -4.3254\n",
      "Epoch 1046/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4494 - val_loss: -3.9481\n",
      "Epoch 1047/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3197 - val_loss: -3.5010\n",
      "Epoch 1048/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4411 - val_loss: -4.3684\n",
      "Epoch 1049/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3937 - val_loss: -3.2786\n",
      "Epoch 1050/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2982 - val_loss: -3.8951\n",
      "Epoch 1051/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4105 - val_loss: -3.3100\n",
      "Epoch 1052/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2977 - val_loss: -3.4330\n",
      "Epoch 1053/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3832 - val_loss: -3.4556\n",
      "Epoch 1054/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3667 - val_loss: -3.9643\n",
      "Epoch 1055/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3719 - val_loss: -3.3719\n",
      "Epoch 1056/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4160 - val_loss: -1.5243\n",
      "Epoch 1057/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3479 - val_loss: -3.7150\n",
      "Epoch 1058/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3881 - val_loss: -4.0419\n",
      "Epoch 1059/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3511 - val_loss: -4.1577\n",
      "Epoch 1060/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3594 - val_loss: -4.2217\n",
      "Epoch 1061/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4298 - val_loss: -4.1427\n",
      "Epoch 1062/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5072 - val_loss: -4.1764\n",
      "Epoch 1063/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3027 - val_loss: -2.6426\n",
      "Epoch 1064/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3321 - val_loss: -3.9755\n",
      "Epoch 1065/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4058 - val_loss: -3.3354\n",
      "Epoch 1066/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4078 - val_loss: -4.1342\n",
      "Epoch 1067/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4163 - val_loss: -2.9622\n",
      "Epoch 1068/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3979 - val_loss: -4.1565\n",
      "Epoch 1069/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4274 - val_loss: 0.3072\n",
      "Epoch 1070/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2621 - val_loss: -4.5314\n",
      "Epoch 1071/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4674 - val_loss: -3.2485\n",
      "Epoch 1072/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3923 - val_loss: -4.4991\n",
      "Epoch 1073/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4580 - val_loss: -3.7599\n",
      "Epoch 1074/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2371 - val_loss: -3.9532\n",
      "Epoch 1075/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4274 - val_loss: -2.7920\n",
      "Epoch 1076/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4370 - val_loss: -3.6335\n",
      "Epoch 1077/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4632 - val_loss: -4.5556\n",
      "Epoch 1078/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3622 - val_loss: -1.7884\n",
      "Epoch 1079/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3593 - val_loss: -4.2972\n",
      "Epoch 1080/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4339 - val_loss: -4.0644\n",
      "Epoch 1081/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3426 - val_loss: -3.7854\n",
      "Epoch 1082/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3817 - val_loss: -4.2024\n",
      "Epoch 1083/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3926 - val_loss: -4.5856\n",
      "Epoch 1084/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4261 - val_loss: -1.5197\n",
      "Epoch 1085/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3840 - val_loss: -4.0349\n",
      "Epoch 1086/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3950 - val_loss: -3.6197\n",
      "Epoch 1087/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3603 - val_loss: -4.7323\n",
      "Epoch 1088/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3638 - val_loss: -4.5202\n",
      "Epoch 1089/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4236 - val_loss: -3.2929\n",
      "Epoch 1090/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4008 - val_loss: -3.6964\n",
      "Epoch 1091/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4191 - val_loss: -4.1756\n",
      "Epoch 1092/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4065 - val_loss: -4.0211\n",
      "Epoch 1093/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4052 - val_loss: -4.6366\n",
      "Epoch 1094/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3338 - val_loss: -3.6376\n",
      "Epoch 1095/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3168 - val_loss: -4.1780\n",
      "Epoch 1096/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3791 - val_loss: -1.9533\n",
      "Epoch 1097/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2873 - val_loss: -4.5292\n",
      "Epoch 1098/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3210 - val_loss: -4.4126\n",
      "Epoch 1099/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4863 - val_loss: -4.2128\n",
      "Epoch 1100/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4378 - val_loss: -2.7504\n",
      "Epoch 1101/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0439 - val_loss: -4.4697\n",
      "Epoch 1102/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2321 - val_loss: -3.1644\n",
      "Epoch 1103/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2055 - val_loss: -4.4268\n",
      "Epoch 1104/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4540 - val_loss: -3.7669\n",
      "Epoch 1105/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3335 - val_loss: -4.1180\n",
      "Epoch 1106/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3925 - val_loss: -4.6638\n",
      "Epoch 1107/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3543 - val_loss: -4.1164\n",
      "Epoch 1108/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3735 - val_loss: -3.9101\n",
      "Epoch 1109/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3991 - val_loss: -0.4671\n",
      "Epoch 1110/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4606 - val_loss: -3.6064\n",
      "Epoch 1111/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3114 - val_loss: -4.0433\n",
      "Epoch 1112/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3909 - val_loss: -4.1727\n",
      "Epoch 1113/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4766 - val_loss: -4.5108\n",
      "Epoch 1114/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3648 - val_loss: -3.6017\n",
      "Epoch 1115/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4103 - val_loss: -3.6720\n",
      "Epoch 1116/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4313 - val_loss: -3.7247\n",
      "Epoch 1117/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3704 - val_loss: -4.1609\n",
      "Epoch 1118/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4292 - val_loss: -3.3087\n",
      "Epoch 1119/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4752 - val_loss: -4.1393\n",
      "Epoch 1120/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4522 - val_loss: 1.1169\n",
      "Epoch 1121/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4193 - val_loss: -3.3579\n",
      "Epoch 1122/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.4575 - val_loss: -4.0805\n",
      "Epoch 1123/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4364 - val_loss: -0.5837\n",
      "Epoch 1124/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2761 - val_loss: -2.0795\n",
      "Epoch 1125/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3185 - val_loss: -2.4084\n",
      "Epoch 1126/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3908 - val_loss: -4.0331\n",
      "Epoch 1127/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4024 - val_loss: -4.4712\n",
      "Epoch 1128/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4343 - val_loss: -2.3983\n",
      "Epoch 1129/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3825 - val_loss: -3.1352\n",
      "Epoch 1130/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4558 - val_loss: -4.2699\n",
      "Epoch 1131/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3190 - val_loss: -3.7634\n",
      "Epoch 1132/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4091 - val_loss: -3.7812\n",
      "Epoch 1133/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4174 - val_loss: -4.2434\n",
      "Epoch 1134/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3758 - val_loss: -4.4790\n",
      "Epoch 1135/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3624 - val_loss: -3.8251\n",
      "Epoch 1136/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4389 - val_loss: -4.3662\n",
      "Epoch 1137/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3808 - val_loss: -4.6535\n",
      "Epoch 1138/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3959 - val_loss: -3.0735\n",
      "Epoch 1139/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3977 - val_loss: -2.5326\n",
      "Epoch 1140/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4081 - val_loss: -4.4093\n",
      "Epoch 1141/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4633 - val_loss: -3.7983\n",
      "Epoch 1142/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3289 - val_loss: -3.6689\n",
      "Epoch 1143/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4195 - val_loss: -4.3740\n",
      "Epoch 1144/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4104 - val_loss: -4.8131\n",
      "Epoch 1145/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3419 - val_loss: -4.6418\n",
      "Epoch 1146/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4581 - val_loss: -4.4048\n",
      "Epoch 1147/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3955 - val_loss: -4.0649\n",
      "Epoch 1148/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5082 - val_loss: -3.4024\n",
      "Epoch 1149/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4714 - val_loss: -4.4277\n",
      "Epoch 1150/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4101 - val_loss: -1.9930\n",
      "Epoch 1151/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4328 - val_loss: -3.3273\n",
      "Epoch 1152/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2386 - val_loss: -4.3761\n",
      "Epoch 1153/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4244 - val_loss: -3.9251\n",
      "Epoch 1154/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4533 - val_loss: -4.5472\n",
      "Epoch 1155/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3847 - val_loss: -1.9677\n",
      "Epoch 1156/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3841 - val_loss: -3.7199\n",
      "Epoch 1157/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4710 - val_loss: -1.3603\n",
      "Epoch 1158/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1772 - val_loss: -1.6563\n",
      "Epoch 1159/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2740 - val_loss: -4.8064\n",
      "Epoch 1160/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2662 - val_loss: -4.5266\n",
      "Epoch 1161/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3953 - val_loss: -4.0372\n",
      "Epoch 1162/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3309 - val_loss: -4.5445\n",
      "Epoch 1163/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4112 - val_loss: -4.1816\n",
      "Epoch 1164/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4315 - val_loss: -4.6482\n",
      "Epoch 1165/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4533 - val_loss: 0.0816\n",
      "Epoch 1166/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2951 - val_loss: -3.5621\n",
      "Epoch 1167/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4245 - val_loss: -3.8990\n",
      "Epoch 1168/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3451 - val_loss: -4.1370\n",
      "Epoch 1169/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2527 - val_loss: -4.5375\n",
      "Epoch 1170/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4036 - val_loss: -4.5192\n",
      "Epoch 1171/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3960 - val_loss: -4.3411\n",
      "Epoch 1172/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4603 - val_loss: 0.5923\n",
      "Epoch 1173/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2914 - val_loss: -3.7891\n",
      "Epoch 1174/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3655 - val_loss: -4.2191\n",
      "Epoch 1175/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4219 - val_loss: -3.8437\n",
      "Epoch 1176/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4969 - val_loss: -0.9113\n",
      "Epoch 1177/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3827 - val_loss: -4.2539\n",
      "Epoch 1178/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4019 - val_loss: -3.3286\n",
      "Epoch 1179/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1759 - val_loss: -4.3320\n",
      "Epoch 1180/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4259 - val_loss: -3.6611\n",
      "Epoch 1181/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2890 - val_loss: -4.6541\n",
      "Epoch 1182/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3293 - val_loss: -4.6678\n",
      "Epoch 1183/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4614 - val_loss: -3.5863\n",
      "Epoch 1184/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4345 - val_loss: -2.5652\n",
      "Epoch 1185/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3256 - val_loss: -4.3369\n",
      "Epoch 1186/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4308 - val_loss: -3.8818\n",
      "Epoch 1187/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3904 - val_loss: -4.1701\n",
      "Epoch 1188/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4154 - val_loss: -4.1783\n",
      "Epoch 1189/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4157 - val_loss: -3.4247\n",
      "Epoch 1190/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3420 - val_loss: -3.6408\n",
      "Epoch 1191/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4141 - val_loss: -4.0432\n",
      "Epoch 1192/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4388 - val_loss: -4.2873\n",
      "Epoch 1193/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3093 - val_loss: -4.2798\n",
      "Epoch 1194/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4021 - val_loss: -3.9844\n",
      "Epoch 1195/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4746 - val_loss: -3.8671\n",
      "Epoch 1196/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4712 - val_loss: -2.6821\n",
      "Epoch 1197/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3202 - val_loss: -3.7670\n",
      "Epoch 1198/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4856 - val_loss: -1.1980\n",
      "Epoch 1199/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3614 - val_loss: -3.0655\n",
      "Epoch 1200/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4858 - val_loss: -3.4903\n",
      "Epoch 1201/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5125 - val_loss: -4.1219\n",
      "Epoch 1202/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3501 - val_loss: -3.3726\n",
      "Epoch 1203/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4125 - val_loss: -4.0680\n",
      "Epoch 1204/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5231 - val_loss: -3.1090\n",
      "Epoch 1205/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4575 - val_loss: -4.0904\n",
      "Epoch 1206/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3592 - val_loss: -4.4076\n",
      "Epoch 1207/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3380 - val_loss: -4.7392\n",
      "Epoch 1208/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4831 - val_loss: -4.0378\n",
      "Epoch 1209/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2830 - val_loss: -3.8024\n",
      "Epoch 1210/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2059 - val_loss: -3.6233\n",
      "Epoch 1211/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4386 - val_loss: -3.2623\n",
      "Epoch 1212/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1775 - val_loss: -4.4233\n",
      "Epoch 1213/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3603 - val_loss: -4.0008\n",
      "Epoch 1214/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4021 - val_loss: -3.7557\n",
      "Epoch 1215/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4680 - val_loss: -4.2340\n",
      "Epoch 1216/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4566 - val_loss: -4.2169\n",
      "Epoch 1217/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2622 - val_loss: -3.8196\n",
      "Epoch 1218/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4946 - val_loss: -4.3730\n",
      "Epoch 1219/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3691 - val_loss: -4.2432\n",
      "Epoch 1220/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4974 - val_loss: -1.5995\n",
      "Epoch 1221/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3558 - val_loss: -4.4692\n",
      "Epoch 1222/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4095 - val_loss: -4.2694\n",
      "Epoch 1223/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4726 - val_loss: -4.0837\n",
      "Epoch 1224/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4523 - val_loss: -3.9617\n",
      "Epoch 1225/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3451 - val_loss: -3.2495\n",
      "Epoch 1226/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4572 - val_loss: -4.6772\n",
      "Epoch 1227/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4756 - val_loss: -3.8757\n",
      "Epoch 1228/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4407 - val_loss: -4.0410\n",
      "Epoch 1229/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4262 - val_loss: -2.1502\n",
      "Epoch 1230/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4186 - val_loss: -4.4311\n",
      "Epoch 1231/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4329 - val_loss: -3.6838\n",
      "Epoch 1232/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4413 - val_loss: -2.3650\n",
      "Epoch 1233/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4225 - val_loss: -4.3222\n",
      "Epoch 1234/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4902 - val_loss: -4.2776\n",
      "Epoch 1235/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4054 - val_loss: 3.3911\n",
      "Epoch 1236/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2450 - val_loss: -4.4687\n",
      "Epoch 1237/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4613 - val_loss: -4.5103\n",
      "Epoch 1238/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4247 - val_loss: -3.1566\n",
      "Epoch 1239/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4948 - val_loss: -3.7216\n",
      "Epoch 1240/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4274 - val_loss: -4.2678\n",
      "Epoch 1241/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4741 - val_loss: -4.5432\n",
      "Epoch 1242/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4304 - val_loss: -4.5366\n",
      "Epoch 1243/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4917 - val_loss: -3.4237\n",
      "Epoch 1244/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4024 - val_loss: -3.7060\n",
      "Epoch 1245/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4295 - val_loss: -3.8279\n",
      "Epoch 1246/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4594 - val_loss: -4.6355\n",
      "Epoch 1247/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8996 - val_loss: -3.6395\n",
      "Epoch 1248/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3384 - val_loss: -4.1628\n",
      "Epoch 1249/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3634 - val_loss: -3.6271\n",
      "Epoch 1250/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3874 - val_loss: -3.0105\n",
      "Epoch 1251/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4595 - val_loss: -4.3322\n",
      "Epoch 1252/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2687 - val_loss: -4.4493\n",
      "Epoch 1253/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4055 - val_loss: -4.0696\n",
      "Epoch 1254/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4137 - val_loss: -3.8549\n",
      "Epoch 1255/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4414 - val_loss: -4.1789\n",
      "Epoch 1256/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8975 - val_loss: -3.0792\n",
      "Epoch 1257/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8570 - val_loss: -3.5698\n",
      "Epoch 1258/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1730 - val_loss: -4.2884\n",
      "Epoch 1259/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2367 - val_loss: -3.9259\n",
      "Epoch 1260/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2028 - val_loss: -4.2906\n",
      "Epoch 1261/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2741 - val_loss: -2.6914\n",
      "Epoch 1262/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3063 - val_loss: -4.2315\n",
      "Epoch 1263/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3191 - val_loss: -0.4617\n",
      "Epoch 1264/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4007 - val_loss: -3.3225\n",
      "Epoch 1265/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3159 - val_loss: -4.1652\n",
      "Epoch 1266/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4204 - val_loss: -4.4434\n",
      "Epoch 1267/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4009 - val_loss: -3.6154\n",
      "Epoch 1268/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4624 - val_loss: -4.5972\n",
      "Epoch 1269/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3431 - val_loss: -3.5297\n",
      "Epoch 1270/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4447 - val_loss: -4.6120\n",
      "Epoch 1271/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4999 - val_loss: -2.9012\n",
      "Epoch 1272/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4069 - val_loss: -3.4385\n",
      "Epoch 1273/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1937 - val_loss: -3.7286\n",
      "Epoch 1274/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4206 - val_loss: -4.1343\n",
      "Epoch 1275/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3126 - val_loss: -4.1074\n",
      "Epoch 1276/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5131 - val_loss: -2.8431\n",
      "Epoch 1277/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4783 - val_loss: -2.3242\n",
      "Epoch 1278/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3032 - val_loss: -4.3913\n",
      "Epoch 1279/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3184 - val_loss: -3.9665\n",
      "Epoch 1280/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4779 - val_loss: -4.0092\n",
      "Epoch 1281/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3393 - val_loss: -4.5400\n",
      "Epoch 1282/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3744 - val_loss: -3.6419\n",
      "Epoch 1283/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4706 - val_loss: -3.2070\n",
      "Epoch 1284/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4241 - val_loss: 1.4152\n",
      "Epoch 1285/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3379 - val_loss: -3.3362\n",
      "Epoch 1286/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5087 - val_loss: -3.5549\n",
      "Epoch 1287/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4665 - val_loss: -2.6492\n",
      "Epoch 1288/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4487 - val_loss: -4.2657\n",
      "Epoch 1289/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4078 - val_loss: -4.0634\n",
      "Epoch 1290/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3753 - val_loss: -4.6765\n",
      "Epoch 1291/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4206 - val_loss: -3.8907\n",
      "Epoch 1292/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4702 - val_loss: -4.4693\n",
      "Epoch 1293/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5138 - val_loss: -4.3348\n",
      "Epoch 1294/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4546 - val_loss: -3.9606\n",
      "Epoch 1295/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4218 - val_loss: -4.1056\n",
      "Epoch 1296/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4453 - val_loss: -4.1986\n",
      "Epoch 1297/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3783 - val_loss: -4.7710\n",
      "Epoch 1298/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5003 - val_loss: -4.2408\n",
      "Epoch 1299/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4688 - val_loss: -4.0301\n",
      "Epoch 1300/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3403 - val_loss: -4.2772\n",
      "Epoch 1301/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4599 - val_loss: -3.7120\n",
      "Epoch 1302/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2567 - val_loss: -3.7315\n",
      "Epoch 1303/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4725 - val_loss: -4.7402\n",
      "Epoch 1304/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4559 - val_loss: -4.7476\n",
      "Epoch 1305/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4500 - val_loss: -3.5788\n",
      "Epoch 1306/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4593 - val_loss: -3.6278\n",
      "Epoch 1307/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4857 - val_loss: -1.9235\n",
      "Epoch 1308/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4886 - val_loss: -3.9069\n",
      "Epoch 1309/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3148 - val_loss: -2.4796\n",
      "Epoch 1310/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4992 - val_loss: -4.0191\n",
      "Epoch 1311/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4954 - val_loss: -3.8407\n",
      "Epoch 1312/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4180 - val_loss: -3.9610\n",
      "Epoch 1313/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5210 - val_loss: -4.7958\n",
      "Epoch 1314/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4392 - val_loss: -4.5124\n",
      "Epoch 1315/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4027 - val_loss: -4.0462\n",
      "Epoch 1316/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3982"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -5.3982 - val_loss: -4.8238\n",
      "Epoch 1317/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5083 - val_loss: -4.3761\n",
      "Epoch 1318/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4472 - val_loss: -4.2218\n",
      "Epoch 1319/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4264 - val_loss: -4.0331\n",
      "Epoch 1320/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4440 - val_loss: -3.1024\n",
      "Epoch 1321/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4729 - val_loss: -4.4293\n",
      "Epoch 1322/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5053 - val_loss: -4.3176\n",
      "Epoch 1323/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4558 - val_loss: -4.4672\n",
      "Epoch 1324/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4924 - val_loss: -3.6621\n",
      "Epoch 1325/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4623 - val_loss: -4.1466\n",
      "Epoch 1326/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4806 - val_loss: -2.6407\n",
      "Epoch 1327/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4257 - val_loss: -3.6999\n",
      "Epoch 1328/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5048 - val_loss: -4.4949\n",
      "Epoch 1329/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3355 - val_loss: -3.3995\n",
      "Epoch 1330/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4138 - val_loss: 0.6257\n",
      "Epoch 1331/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4502 - val_loss: -3.5554\n",
      "Epoch 1332/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4983 - val_loss: -4.6559\n",
      "Epoch 1333/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4841 - val_loss: -3.6284\n",
      "Epoch 1334/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4960 - val_loss: -4.4136\n",
      "Epoch 1335/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4508 - val_loss: -4.5389\n",
      "Epoch 1336/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0669 - val_loss: -3.6265\n",
      "Epoch 1337/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1611 - val_loss: -4.4448\n",
      "Epoch 1338/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4288 - val_loss: -3.4863\n",
      "Epoch 1339/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4509 - val_loss: -3.4832\n",
      "Epoch 1340/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4382 - val_loss: -4.0871\n",
      "Epoch 1341/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2450 - val_loss: -4.0097\n",
      "Epoch 1342/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4379 - val_loss: -4.5528\n",
      "Epoch 1343/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4062 - val_loss: -4.3966\n",
      "Epoch 1344/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4264 - val_loss: -4.4074\n",
      "Epoch 1345/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4280 - val_loss: -4.8004\n",
      "Epoch 1346/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4666 - val_loss: -4.3746\n",
      "Epoch 1347/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3894 - val_loss: -3.7746\n",
      "Epoch 1348/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5098 - val_loss: -3.4096\n",
      "Epoch 1349/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4021 - val_loss: -4.0847\n",
      "Epoch 1350/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5419 - val_loss: -4.7405\n",
      "Epoch 1351/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3467 - val_loss: -4.6976\n",
      "Epoch 1352/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4668 - val_loss: -4.3139\n",
      "Epoch 1353/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2837 - val_loss: -4.6904\n",
      "Epoch 1354/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3585 - val_loss: -2.4754\n",
      "Epoch 1355/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.4343"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -5.4343 - val_loss: -4.8488\n",
      "Epoch 1356/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2977 - val_loss: -4.4282\n",
      "Epoch 1357/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4390 - val_loss: -4.6092\n",
      "Epoch 1358/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5323 - val_loss: -4.1360\n",
      "Epoch 1359/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4767 - val_loss: -4.0066\n",
      "Epoch 1360/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3997 - val_loss: -3.5448\n",
      "Epoch 1361/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5184 - val_loss: -3.0106\n",
      "Epoch 1362/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2748 - val_loss: -3.4547\n",
      "Epoch 1363/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4470 - val_loss: -0.4040\n",
      "Epoch 1364/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3872"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -5.3872 - val_loss: -4.9107\n",
      "Epoch 1365/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4808 - val_loss: -2.7403\n",
      "Epoch 1366/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4892 - val_loss: -4.0541\n",
      "Epoch 1367/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5263 - val_loss: -3.8518\n",
      "Epoch 1368/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3747 - val_loss: -3.6808\n",
      "Epoch 1369/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3863 - val_loss: -3.8222\n",
      "Epoch 1370/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4761 - val_loss: -3.7266\n",
      "Epoch 1371/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.5241 - val_loss: -4.4775\n",
      "Epoch 1372/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4189 - val_loss: -3.8097\n",
      "Epoch 1373/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5091 - val_loss: 1.7223\n",
      "Epoch 1374/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4526 - val_loss: -1.8595\n",
      "Epoch 1375/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3737 - val_loss: -3.8046\n",
      "Epoch 1376/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4475 - val_loss: -4.6380\n",
      "Epoch 1377/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5156 - val_loss: -4.3875\n",
      "Epoch 1378/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5392 - val_loss: -4.4421\n",
      "Epoch 1379/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3655 - val_loss: -1.7821\n",
      "Epoch 1380/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4548 - val_loss: -3.6666\n",
      "Epoch 1381/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3964 - val_loss: -4.0984\n",
      "Epoch 1382/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4190 - val_loss: -4.5497\n",
      "Epoch 1383/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3137 - val_loss: -3.8296\n",
      "Epoch 1384/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5306 - val_loss: -4.1026\n",
      "Epoch 1385/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4612 - val_loss: -3.4837\n",
      "Epoch 1386/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4828 - val_loss: -4.0347\n",
      "Epoch 1387/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5507 - val_loss: -3.5199\n",
      "Epoch 1388/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4878 - val_loss: -4.8453\n",
      "Epoch 1389/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4576 - val_loss: -4.4780\n",
      "Epoch 1390/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4387 - val_loss: -2.7757\n",
      "Epoch 1391/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4890 - val_loss: -4.7199\n",
      "Epoch 1392/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2800 - val_loss: -4.5729\n",
      "Epoch 1393/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.5054 - val_loss: -3.6134\n",
      "Epoch 1394/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4330 - val_loss: -4.3617\n",
      "Epoch 1395/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2455 - val_loss: -3.7589\n",
      "Epoch 1396/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3886 - val_loss: -4.0203\n",
      "Epoch 1397/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4781 - val_loss: -3.0951\n",
      "Epoch 1398/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4653 - val_loss: -3.4214\n",
      "Epoch 1399/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4654 - val_loss: -4.2774\n",
      "Epoch 1400/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5057 - val_loss: -4.5104\n",
      "Epoch 1401/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5292 - val_loss: -4.4319\n",
      "Epoch 1402/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4908 - val_loss: -3.6679\n",
      "Epoch 1403/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4310 - val_loss: -0.1888\n",
      "Epoch 1404/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.5048 - val_loss: -4.2498\n",
      "Epoch 1405/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5261 - val_loss: -4.2763\n",
      "Epoch 1406/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5035 - val_loss: -0.7464\n",
      "Epoch 1407/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3743 - val_loss: -4.8633\n",
      "Epoch 1408/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5338 - val_loss: -2.0880\n",
      "Epoch 1409/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4610 - val_loss: -3.8792\n",
      "Epoch 1410/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4258 - val_loss: -4.8905\n",
      "Epoch 1411/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4600 - val_loss: -4.2612\n",
      "Epoch 1412/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4893 - val_loss: -4.7967\n",
      "Epoch 1413/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4926 - val_loss: -3.6479\n",
      "Epoch 1414/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3694 - val_loss: -2.2442\n",
      "Epoch 1415/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3769 - val_loss: -3.9503\n",
      "Epoch 1416/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4819 - val_loss: -4.6709\n",
      "Epoch 1417/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5066 - val_loss: -1.4143\n",
      "Epoch 1418/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4480 - val_loss: -4.5670\n",
      "Epoch 1419/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4887 - val_loss: -4.4024\n",
      "Epoch 1420/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3545 - val_loss: -4.0154\n",
      "Epoch 1421/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4617 - val_loss: -4.1475\n",
      "Epoch 1422/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5193 - val_loss: -4.3924\n",
      "Epoch 1423/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5146 - val_loss: -3.9322\n",
      "Epoch 1424/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4698 - val_loss: -0.5734\n",
      "Epoch 1425/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3884 - val_loss: -2.5359\n",
      "Epoch 1426/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3745 - val_loss: -4.3918\n",
      "Epoch 1427/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4217 - val_loss: -4.1624\n",
      "Epoch 1428/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5263 - val_loss: -4.0207\n",
      "Epoch 1429/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4225 - val_loss: -4.2854\n",
      "Epoch 1430/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4826 - val_loss: -4.4080\n",
      "Epoch 1431/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5557 - val_loss: -3.5857\n",
      "Epoch 1432/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3250 - val_loss: -4.4333\n",
      "Epoch 1433/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5600 - val_loss: -3.1440\n",
      "Epoch 1434/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3851 - val_loss: -3.9237\n",
      "Epoch 1435/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5641 - val_loss: -3.6428\n",
      "Epoch 1436/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5150 - val_loss: -4.1016\n",
      "Epoch 1437/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4212 - val_loss: -4.3718\n",
      "Epoch 1438/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4895 - val_loss: -4.2666\n",
      "Epoch 1439/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4196 - val_loss: -4.6232\n",
      "Epoch 1440/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4556 - val_loss: -4.1592\n",
      "Epoch 1441/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1149 - val_loss: -3.8235\n",
      "Epoch 1442/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3732 - val_loss: -4.4769\n",
      "Epoch 1443/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4105 - val_loss: -4.5332\n",
      "Epoch 1444/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3583 - val_loss: -4.3093\n",
      "Epoch 1445/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4210 - val_loss: -4.1444\n",
      "Epoch 1446/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4449 - val_loss: -4.1877\n",
      "Epoch 1447/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4645 - val_loss: -3.8148\n",
      "Epoch 1448/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4794 - val_loss: -4.0454\n",
      "Epoch 1449/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4245 - val_loss: -4.6099\n",
      "Epoch 1450/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4643 - val_loss: -3.3417\n",
      "Epoch 1451/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5189 - val_loss: -3.7874\n",
      "Epoch 1452/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4075 - val_loss: -4.3853\n",
      "Epoch 1453/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5148 - val_loss: 1.6078\n",
      "Epoch 1454/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4318 - val_loss: -4.5646\n",
      "Epoch 1455/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4745 - val_loss: -4.1187\n",
      "Epoch 1456/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5055 - val_loss: -3.6760\n",
      "Epoch 1457/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4420 - val_loss: -4.3638\n",
      "Epoch 1458/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4504 - val_loss: -4.8898\n",
      "Epoch 1459/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4934 - val_loss: -4.5957\n",
      "Epoch 1460/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5326 - val_loss: -4.4127\n",
      "Epoch 1461/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5012 - val_loss: -4.4974\n",
      "Epoch 1462/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5198 - val_loss: -4.4756\n",
      "Epoch 1463/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3570 - val_loss: -4.5378\n",
      "Epoch 1464/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4753 - val_loss: -4.0116\n",
      "Epoch 1465/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4659 - val_loss: -4.0237\n",
      "Epoch 1466/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5005 - val_loss: -3.0452\n",
      "Epoch 1467/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5259 - val_loss: -1.3786\n",
      "Epoch 1468/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5026 - val_loss: -2.4250\n",
      "Epoch 1469/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5127 - val_loss: -4.4605\n",
      "Epoch 1470/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4019 - val_loss: -3.0525\n",
      "Epoch 1471/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3814 - val_loss: -4.6426\n",
      "Epoch 1472/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4767 - val_loss: -3.3443\n",
      "Epoch 1473/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4487 - val_loss: -4.6145\n",
      "Epoch 1474/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4905 - val_loss: -4.2864\n",
      "Epoch 1475/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4752 - val_loss: -3.9031\n",
      "Epoch 1476/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5483 - val_loss: -3.7861\n",
      "Epoch 1477/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1295 - val_loss: -3.5515\n",
      "Epoch 1478/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2939 - val_loss: -4.1877\n",
      "Epoch 1479/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2987 - val_loss: -1.6806\n",
      "Epoch 1480/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2532 - val_loss: -4.4893\n",
      "Epoch 1481/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4085 - val_loss: -4.6111\n",
      "Epoch 1482/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2552 - val_loss: -4.4444\n",
      "Epoch 1483/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4562 - val_loss: -3.1601\n",
      "Epoch 1484/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3673 - val_loss: -4.5126\n",
      "Epoch 1485/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4454 - val_loss: -4.3315\n",
      "Epoch 1486/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3234 - val_loss: -4.2804\n",
      "Epoch 1487/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4568 - val_loss: -4.5757\n",
      "Epoch 1488/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4183 - val_loss: -4.6246\n",
      "Epoch 1489/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3191 - val_loss: -4.2865\n",
      "Epoch 1490/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1899 - val_loss: -4.5711\n",
      "Epoch 1491/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.4275"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -5.4275 - val_loss: -4.9561\n",
      "Epoch 1492/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4806 - val_loss: -4.0069\n",
      "Epoch 1493/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3685 - val_loss: -4.7234\n",
      "Epoch 1494/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4536 - val_loss: -3.8381\n",
      "Epoch 1495/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4461 - val_loss: -4.0373\n",
      "Epoch 1496/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5043 - val_loss: 2.4105\n",
      "Epoch 1497/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8333 - val_loss: -4.1284\n",
      "Epoch 1498/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3025 - val_loss: -4.3597\n",
      "Epoch 1499/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4082 - val_loss: -3.9728\n",
      "Epoch 1500/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3682 - val_loss: -4.4415\n",
      "Epoch 1501/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4491 - val_loss: -4.5925\n",
      "Epoch 1502/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4922 - val_loss: -3.5276\n",
      "Epoch 1503/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4771 - val_loss: -4.3107\n",
      "Epoch 1504/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4279 - val_loss: -3.1301\n",
      "Epoch 1505/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4742 - val_loss: -4.4409\n",
      "Epoch 1506/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3676 - val_loss: -4.5767\n",
      "Epoch 1507/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4534 - val_loss: -3.4848\n",
      "Epoch 1508/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4191 - val_loss: -4.4924\n",
      "Epoch 1509/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5118 - val_loss: -4.2286\n",
      "Epoch 1510/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3609 - val_loss: -3.6320\n",
      "Epoch 1511/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4402 - val_loss: -4.4430\n",
      "Epoch 1512/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4717 - val_loss: -4.7288\n",
      "Epoch 1513/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4818 - val_loss: -2.3474\n",
      "Epoch 1514/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4365 - val_loss: -4.3019\n",
      "Epoch 1515/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4783 - val_loss: -2.9951\n",
      "Epoch 1516/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3556 - val_loss: -3.4655\n",
      "Epoch 1517/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3774 - val_loss: -4.0914\n",
      "Epoch 1518/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3194 - val_loss: -4.4156\n",
      "Epoch 1519/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5106 - val_loss: -4.5377\n",
      "Epoch 1520/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4504 - val_loss: -4.2128\n",
      "Epoch 1521/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4743 - val_loss: -0.5752\n",
      "Epoch 1522/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4804 - val_loss: -4.5873\n",
      "Epoch 1523/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4647 - val_loss: -2.9841\n",
      "Epoch 1524/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5016 - val_loss: -2.5800\n",
      "Epoch 1525/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4714 - val_loss: -3.3142\n",
      "Epoch 1526/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4860 - val_loss: -4.0997\n",
      "Epoch 1527/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.5217 - val_loss: -4.0190\n",
      "Epoch 1528/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4272 - val_loss: -4.7738\n",
      "Epoch 1529/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5490 - val_loss: -4.5349\n",
      "Epoch 1530/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.5065 - val_loss: -4.5252\n",
      "Epoch 1531/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5036 - val_loss: -4.1152\n",
      "Epoch 1532/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4726 - val_loss: -3.1805\n",
      "Epoch 1533/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5043 - val_loss: -4.1338\n",
      "Epoch 1534/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4788 - val_loss: -4.1052\n",
      "Epoch 1535/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5264 - val_loss: -4.2465\n",
      "Epoch 1536/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5399 - val_loss: -4.1158\n",
      "Epoch 1537/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4546 - val_loss: -3.2329\n",
      "Epoch 1538/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4086 - val_loss: -4.9050\n",
      "Epoch 1539/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5785 - val_loss: -4.6157\n",
      "Epoch 1540/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4571 - val_loss: -4.3696\n",
      "Epoch 1541/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5598 - val_loss: -3.7169\n",
      "Epoch 1542/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4499 - val_loss: -3.3220\n",
      "Epoch 1543/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5043 - val_loss: -4.5785\n",
      "Epoch 1544/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4377 - val_loss: -4.3955\n",
      "Epoch 1545/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4387 - val_loss: -3.1007\n",
      "Epoch 1546/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4255 - val_loss: -4.6746\n",
      "Epoch 1547/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4830 - val_loss: -3.2865\n",
      "Epoch 1548/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5282 - val_loss: -4.5425\n",
      "Epoch 1549/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5269 - val_loss: -4.0640\n",
      "Epoch 1550/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5501 - val_loss: -3.8059\n",
      "Epoch 1551/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4752 - val_loss: -4.4072\n",
      "Epoch 1552/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3848 - val_loss: -4.7978\n",
      "Epoch 1553/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5092 - val_loss: -2.5645\n",
      "Epoch 1554/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5162 - val_loss: -4.5409\n",
      "Epoch 1555/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5406 - val_loss: -3.4384\n",
      "Epoch 1556/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4487 - val_loss: -4.1638\n",
      "Epoch 1557/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4855 - val_loss: -4.8402\n",
      "Epoch 1558/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4682 - val_loss: -3.1852\n",
      "Epoch 1559/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5062 - val_loss: -4.1958\n",
      "Epoch 1560/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5357 - val_loss: -4.0490\n",
      "Epoch 1561/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5182 - val_loss: -3.2902\n",
      "Epoch 1562/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5545 - val_loss: -3.7799\n",
      "Epoch 1563/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4703 - val_loss: -3.0957\n",
      "Epoch 1564/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2988 - val_loss: -4.5076\n",
      "Epoch 1565/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3739 - val_loss: -2.8510\n",
      "Epoch 1566/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4808 - val_loss: -4.4294\n",
      "Epoch 1567/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4875 - val_loss: -4.7377\n",
      "Epoch 1568/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4075 - val_loss: -4.1043\n",
      "Epoch 1569/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4999 - val_loss: -4.1932\n",
      "Epoch 1570/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4057 - val_loss: -4.1560\n",
      "Epoch 1571/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5653 - val_loss: -4.3296\n",
      "Epoch 1572/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4533 - val_loss: -3.4513\n",
      "Epoch 1573/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5416 - val_loss: -4.3576\n",
      "Epoch 1574/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.5000 - val_loss: -1.5553\n",
      "Epoch 1575/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4427 - val_loss: -3.5197\n",
      "Epoch 1576/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4619 - val_loss: -4.4459\n",
      "Epoch 1577/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5312 - val_loss: -3.1621\n",
      "Epoch 1578/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4549 - val_loss: -4.0562\n",
      "Epoch 1579/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5509 - val_loss: -2.6149\n",
      "Epoch 1580/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4628 - val_loss: -3.8640\n",
      "Epoch 1581/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5614 - val_loss: -3.8060\n",
      "Epoch 1582/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5115 - val_loss: -2.7597\n",
      "Epoch 1583/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5015 - val_loss: -4.3776\n",
      "Epoch 1584/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5521 - val_loss: -4.7028\n",
      "Epoch 1585/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2595 - val_loss: -4.3395\n",
      "Epoch 1586/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5400 - val_loss: -4.0553\n",
      "Epoch 1587/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5783 - val_loss: -2.3108\n",
      "Epoch 1588/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4860 - val_loss: -4.5022\n",
      "Epoch 1589/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4421 - val_loss: -4.6097\n",
      "Epoch 1590/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4930 - val_loss: -4.3168\n",
      "Epoch 1591/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5276 - val_loss: -3.9357\n",
      "Epoch 1592/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4637 - val_loss: -4.7173\n",
      "Epoch 1593/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5250 - val_loss: -3.6246\n",
      "Epoch 1594/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5037 - val_loss: -4.2592\n",
      "Epoch 1595/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4545 - val_loss: -3.9355\n",
      "Epoch 1596/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4185 - val_loss: -1.9852\n",
      "Epoch 1597/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5083 - val_loss: -2.1287\n",
      "Epoch 1598/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5479 - val_loss: -4.3336\n",
      "Epoch 1599/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2206 - val_loss: -1.5909\n",
      "Epoch 1600/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2738 - val_loss: -2.7004\n",
      "Epoch 1601/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3730 - val_loss: -4.7871\n",
      "Epoch 1602/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4209 - val_loss: -2.3047\n",
      "Epoch 1603/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3964 - val_loss: -2.8713\n",
      "Epoch 1604/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5038 - val_loss: -3.6749\n",
      "Epoch 1605/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5173 - val_loss: -2.2640\n",
      "Epoch 1606/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4297 - val_loss: -4.5576\n",
      "Epoch 1607/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5073 - val_loss: -4.7094\n",
      "Epoch 1608/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4834 - val_loss: -2.5002\n",
      "Epoch 1609/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4782 - val_loss: -4.1393\n",
      "Epoch 1610/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4874 - val_loss: -4.1261\n",
      "Epoch 1611/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5023 - val_loss: -1.7811\n",
      "Epoch 1612/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5392 - val_loss: -4.4584\n",
      "Epoch 1613/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4531 - val_loss: -4.5458\n",
      "Epoch 1614/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5649 - val_loss: -3.5247\n",
      "Epoch 1615/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5028 - val_loss: -4.3917\n",
      "Epoch 1616/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4963 - val_loss: -4.7553\n",
      "Epoch 1617/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4396 - val_loss: -4.7693\n",
      "Epoch 1618/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5816 - val_loss: -3.5903\n",
      "Epoch 1619/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5617 - val_loss: -3.7865\n",
      "Epoch 1620/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4065 - val_loss: -3.2593\n",
      "Epoch 1621/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5147 - val_loss: -4.1126\n",
      "Epoch 1622/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5298 - val_loss: -4.0912\n",
      "Epoch 1623/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3708 - val_loss: -4.0443\n",
      "Epoch 1624/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5412 - val_loss: -4.0757\n",
      "Epoch 1625/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5738 - val_loss: -3.9328\n",
      "Epoch 1626/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4264 - val_loss: -0.1417\n",
      "Epoch 1627/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4748 - val_loss: -4.3701\n",
      "Epoch 1628/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5659 - val_loss: -4.0009\n",
      "Epoch 1629/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5180 - val_loss: -4.3774\n",
      "Epoch 1630/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4633 - val_loss: -4.4262\n",
      "Epoch 1631/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5892 - val_loss: -3.9156\n",
      "Epoch 1632/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4083 - val_loss: -4.1729\n",
      "Epoch 1633/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5162 - val_loss: -3.8080\n",
      "Epoch 1634/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5607 - val_loss: -4.0036\n",
      "Epoch 1635/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5459 - val_loss: 2.4319\n",
      "Epoch 1636/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4321 - val_loss: -3.1240\n",
      "Epoch 1637/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4908 - val_loss: -4.4399\n",
      "Epoch 1638/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5291 - val_loss: -4.5365\n",
      "Epoch 1639/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5793 - val_loss: -3.8869\n",
      "Epoch 1640/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5167 - val_loss: -4.5039\n",
      "Epoch 1641/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5690 - val_loss: -2.7758\n",
      "Epoch 1642/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4686 - val_loss: -4.2495\n",
      "Epoch 1643/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5152 - val_loss: -4.2589\n",
      "Epoch 1644/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0889 - val_loss: -4.1401\n",
      "Epoch 1645/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3876 - val_loss: -4.2526\n",
      "Epoch 1646/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3783 - val_loss: -3.4054\n",
      "Epoch 1647/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4745 - val_loss: -4.9275\n",
      "Epoch 1648/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4871 - val_loss: -4.0933\n",
      "Epoch 1649/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5685 - val_loss: -4.4957\n",
      "Epoch 1650/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3597 - val_loss: -3.9876\n",
      "Epoch 1651/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4836 - val_loss: -4.4479\n",
      "Epoch 1652/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2779 - val_loss: -3.7089\n",
      "Epoch 1653/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3990 - val_loss: -3.9283\n",
      "Epoch 1654/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4462 - val_loss: -4.3361\n",
      "Epoch 1655/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4169 - val_loss: -2.7887\n",
      "Epoch 1656/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4883 - val_loss: -4.6901\n",
      "Epoch 1657/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5212 - val_loss: -4.1118\n",
      "Epoch 1658/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5231 - val_loss: -3.8452\n",
      "Epoch 1659/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5257 - val_loss: -4.1922\n",
      "Epoch 1660/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5083 - val_loss: -4.0206\n",
      "Epoch 1661/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5072 - val_loss: -4.0272\n",
      "Epoch 1662/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4329 - val_loss: -4.3139\n",
      "Epoch 1663/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4793 - val_loss: -4.3672\n",
      "Epoch 1664/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5394 - val_loss: -4.3998\n",
      "Epoch 1665/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5309 - val_loss: -3.8774\n",
      "Epoch 1666/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5607 - val_loss: -2.2737\n",
      "Epoch 1667/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4181 - val_loss: -3.1464\n",
      "Epoch 1668/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5793 - val_loss: -3.6529\n",
      "Epoch 1669/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5588 - val_loss: -4.9419\n",
      "Epoch 1670/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5327 - val_loss: -4.5645\n",
      "Epoch 1671/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4482 - val_loss: -4.3284\n",
      "Epoch 1672/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5240 - val_loss: -4.3240\n",
      "Epoch 1673/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4652 - val_loss: -4.3823\n",
      "Epoch 1674/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5492 - val_loss: -3.9381\n",
      "Epoch 1675/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5630 - val_loss: -2.9004\n",
      "Epoch 1676/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3976 - val_loss: -3.1223\n",
      "Epoch 1677/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5468 - val_loss: -3.5807\n",
      "Epoch 1678/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5859 - val_loss: -4.2232\n",
      "Epoch 1679/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5681 - val_loss: -3.5013\n",
      "Epoch 1680/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5216 - val_loss: -4.4456\n",
      "Epoch 1681/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.5619 - val_loss: -3.9768\n",
      "Epoch 1682/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3712 - val_loss: -3.5594\n",
      "Epoch 1683/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5353 - val_loss: -3.4413\n",
      "Epoch 1684/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5092 - val_loss: -4.8026\n",
      "Epoch 1685/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5457 - val_loss: -2.7084\n",
      "Epoch 1686/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4733 - val_loss: -4.3724\n",
      "Epoch 1687/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5088 - val_loss: -3.7017\n",
      "Epoch 1688/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5457 - val_loss: -4.4896\n",
      "Epoch 1689/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4659 - val_loss: -4.1061\n",
      "Epoch 1690/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5696 - val_loss: -3.5610\n",
      "Epoch 1691/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5506 - val_loss: -4.3076\n",
      "Epoch 1692/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.5633 - val_loss: -4.4419\n",
      "Epoch 1693/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5574 - val_loss: -3.9086\n",
      "Epoch 1694/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5182 - val_loss: -3.1107\n",
      "Epoch 1695/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5174 - val_loss: -4.3637\n",
      "Epoch 1696/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5819 - val_loss: -3.5389\n",
      "Epoch 1697/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5185 - val_loss: -4.2989\n",
      "Epoch 1698/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4930 - val_loss: -3.4802\n",
      "Epoch 1699/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5056 - val_loss: -4.7898\n",
      "Epoch 1700/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4165 - val_loss: -4.1910\n",
      "Epoch 1701/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5051 - val_loss: -4.6062\n",
      "Epoch 1702/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5869 - val_loss: -3.8266\n",
      "Epoch 1703/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5249 - val_loss: -4.3217\n",
      "Epoch 1704/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2946 - val_loss: -4.1548\n",
      "Epoch 1705/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3959 - val_loss: -4.5544\n",
      "Epoch 1706/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5218 - val_loss: -3.5910\n",
      "Epoch 1707/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4957 - val_loss: -2.8035\n",
      "Epoch 1708/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5663 - val_loss: -4.0258\n",
      "Epoch 1709/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4982 - val_loss: -3.5058\n",
      "Epoch 1710/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4740 - val_loss: -4.3827\n",
      "Epoch 1711/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5282 - val_loss: -3.7841\n",
      "Epoch 1712/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4635 - val_loss: -3.9051\n",
      "Epoch 1713/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6043 - val_loss: -3.5396\n",
      "Epoch 1714/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.5746 - val_loss: -4.3648\n",
      "Epoch 1715/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4679 - val_loss: -3.9350\n",
      "Epoch 1716/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4856 - val_loss: -3.8003\n",
      "Epoch 1717/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6010 - val_loss: -2.8382\n",
      "Epoch 1718/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5848 - val_loss: -4.5001\n",
      "Epoch 1719/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4264 - val_loss: -4.5545\n",
      "Epoch 1720/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5074 - val_loss: -3.9280\n",
      "Epoch 1721/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5686 - val_loss: -4.4911\n",
      "Epoch 1722/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5771 - val_loss: -2.7389\n",
      "Epoch 1723/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5653 - val_loss: -3.5193\n",
      "Epoch 1724/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3474 - val_loss: -3.5829\n",
      "Epoch 1725/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0537 - val_loss: -4.4214\n",
      "Epoch 1726/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1451 - val_loss: -2.6702\n",
      "Epoch 1727/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2810 - val_loss: -4.0559\n",
      "Epoch 1728/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3131 - val_loss: -4.2836\n",
      "Epoch 1729/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4175 - val_loss: -4.1955\n",
      "Epoch 1730/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3671 - val_loss: -3.4863\n",
      "Epoch 1731/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1155 - val_loss: -4.8279\n",
      "Epoch 1732/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2400 - val_loss: -4.0437\n",
      "Epoch 1733/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3158 - val_loss: -4.7225\n",
      "Epoch 1734/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4294 - val_loss: -4.3990\n",
      "Epoch 1735/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4439 - val_loss: -4.2857\n",
      "Epoch 1736/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4838 - val_loss: -3.6471\n",
      "Epoch 1737/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3542 - val_loss: -4.3532\n",
      "Epoch 1738/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4891 - val_loss: -4.2283\n",
      "Epoch 1739/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3790 - val_loss: -4.3493\n",
      "Epoch 1740/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3794 - val_loss: -4.4846\n",
      "Epoch 1741/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4859 - val_loss: -0.8481\n",
      "Epoch 1742/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3013 - val_loss: -4.1218\n",
      "Epoch 1743/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5122 - val_loss: -4.5504\n",
      "Epoch 1744/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4982 - val_loss: -4.8079\n",
      "Epoch 1745/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5128 - val_loss: -4.5481\n",
      "Epoch 1746/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3463 - val_loss: -4.2957\n",
      "Epoch 1747/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2496 - val_loss: -4.5949\n",
      "Epoch 1748/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3728 - val_loss: -2.3328\n",
      "Epoch 1749/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4319 - val_loss: -3.6360\n",
      "Epoch 1750/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3817 - val_loss: -4.2781\n",
      "Epoch 1751/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3749 - val_loss: -4.2579\n",
      "Epoch 1752/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4813 - val_loss: -4.2667\n",
      "Epoch 1753/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5370 - val_loss: -4.0998\n",
      "Epoch 1754/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3229 - val_loss: -3.6161\n",
      "Epoch 1755/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4875 - val_loss: -4.0266\n",
      "Epoch 1756/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5133 - val_loss: -4.3684\n",
      "Epoch 1757/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4684 - val_loss: -4.2847\n",
      "Epoch 1758/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4846 - val_loss: -4.6216\n",
      "Epoch 1759/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5358 - val_loss: -3.1931\n",
      "Epoch 1760/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5403 - val_loss: -4.1754\n",
      "Epoch 1761/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3438 - val_loss: -4.1308\n",
      "Epoch 1762/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5543 - val_loss: -3.0648\n",
      "Epoch 1763/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4165 - val_loss: -2.6414\n",
      "Epoch 1764/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5592 - val_loss: -3.9946\n",
      "Epoch 1765/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3584 - val_loss: -3.6998\n",
      "Epoch 1766/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5608 - val_loss: -2.6765\n",
      "Epoch 1767/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5037 - val_loss: -2.3302\n",
      "Epoch 1768/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5372 - val_loss: -4.0179\n",
      "Epoch 1769/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4215 - val_loss: -4.2417\n",
      "Epoch 1770/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5800 - val_loss: -4.4613\n",
      "Epoch 1771/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5501 - val_loss: -4.2668\n",
      "Epoch 1772/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5492 - val_loss: -2.4390\n",
      "Epoch 1773/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4786 - val_loss: -4.2084\n",
      "Epoch 1774/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5042 - val_loss: -3.8656\n",
      "Epoch 1775/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5788 - val_loss: -4.0890\n",
      "Epoch 1776/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5715 - val_loss: -4.6691\n",
      "Epoch 1777/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4621 - val_loss: -4.9117\n",
      "Epoch 1778/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5414 - val_loss: -3.8599\n",
      "Epoch 1779/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5416 - val_loss: -3.0251\n",
      "Epoch 1780/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5584 - val_loss: -4.1070\n",
      "Epoch 1781/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5641 - val_loss: -4.6015\n",
      "Epoch 1782/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5411 - val_loss: -4.2619\n",
      "Epoch 1783/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5825 - val_loss: -3.7839\n",
      "Epoch 1784/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3171 - val_loss: -4.6010\n",
      "Epoch 1785/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5089 - val_loss: -4.6508\n",
      "Epoch 1786/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6058 - val_loss: -2.9952\n",
      "Epoch 1787/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4979 - val_loss: -4.5266\n",
      "Epoch 1788/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5595 - val_loss: -2.9853\n",
      "Epoch 1789/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2085 - val_loss: -3.8913\n",
      "Epoch 1790/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4209 - val_loss: -3.6921\n",
      "Epoch 1791/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5028 - val_loss: -4.2830\n",
      "Epoch 1792/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4871 - val_loss: -4.6603\n",
      "Epoch 1793/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5364 - val_loss: -3.3059\n",
      "Epoch 1794/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4680 - val_loss: -3.8888\n",
      "Epoch 1795/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5429 - val_loss: -4.4454\n",
      "Epoch 1796/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4687 - val_loss: -4.5300\n",
      "Epoch 1797/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5183 - val_loss: -4.0052\n",
      "Epoch 1798/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5184 - val_loss: -3.6849\n",
      "Epoch 1799/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3627 - val_loss: -4.4734\n",
      "Epoch 1800/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5343 - val_loss: -4.4270\n",
      "Epoch 1801/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5895 - val_loss: -3.9321\n",
      "Epoch 1802/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4841 - val_loss: -2.8972\n",
      "Epoch 1803/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6067 - val_loss: -1.9198\n",
      "Epoch 1804/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5192 - val_loss: -4.4925\n",
      "Epoch 1805/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5266 - val_loss: -3.4543\n",
      "Epoch 1806/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5325 - val_loss: -4.2693\n",
      "Epoch 1807/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4927 - val_loss: -4.7878\n",
      "Epoch 1808/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5145 - val_loss: -4.1346\n",
      "Epoch 1809/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5376 - val_loss: -4.3945\n",
      "Epoch 1810/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.5495 - val_loss: -4.3912\n",
      "Epoch 1811/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5631 - val_loss: -2.4347\n",
      "Epoch 1812/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5439 - val_loss: -4.2923\n",
      "Epoch 1813/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5215 - val_loss: -3.6975\n",
      "Epoch 1814/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5545 - val_loss: -4.0042\n",
      "Epoch 1815/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5972 - val_loss: -3.4416\n",
      "Epoch 1816/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5329 - val_loss: -1.0969\n",
      "Epoch 1817/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5719 - val_loss: -3.6501\n",
      "Epoch 1818/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5644 - val_loss: -4.4397\n",
      "Epoch 1819/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5789 - val_loss: -4.4327\n",
      "Epoch 1820/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5419 - val_loss: -3.8905\n",
      "Epoch 1821/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.5048 - val_loss: -3.2208\n",
      "Epoch 1822/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2318 - val_loss: -4.1487\n",
      "Epoch 1823/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4180 - val_loss: -3.0280\n",
      "Epoch 1824/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4473 - val_loss: -4.9470\n",
      "Epoch 1825/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4556 - val_loss: -4.5050\n",
      "Epoch 1826/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5559 - val_loss: -3.2781\n",
      "Epoch 1827/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5134 - val_loss: -3.2226\n",
      "Epoch 1828/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5069 - val_loss: -4.9116\n",
      "Epoch 1829/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4807 - val_loss: -3.2191\n",
      "Epoch 1830/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5206 - val_loss: -2.1475\n",
      "Epoch 1831/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5218 - val_loss: -4.1547\n",
      "Epoch 1832/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4556 - val_loss: -3.1579\n",
      "Epoch 1833/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4633 - val_loss: -4.2193\n",
      "Epoch 1834/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5613 - val_loss: -3.8495\n",
      "Epoch 1835/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4791 - val_loss: -4.5379\n",
      "Epoch 1836/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6121 - val_loss: -3.3946\n",
      "Epoch 1837/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3999 - val_loss: 4.7833\n",
      "Epoch 1838/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4361 - val_loss: -1.9542\n",
      "Epoch 1839/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5576 - val_loss: -4.8580\n",
      "Epoch 1840/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4650 - val_loss: -4.2199\n",
      "Epoch 1841/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4566 - val_loss: -3.3962\n",
      "Epoch 1842/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6023 - val_loss: -2.7600\n",
      "Epoch 1843/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3375 - val_loss: -3.9183\n",
      "Epoch 1844/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5882 - val_loss: -4.3684\n",
      "Epoch 1845/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5647 - val_loss: -4.3100\n",
      "Epoch 1846/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5460 - val_loss: -4.1332\n",
      "Epoch 1847/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6094 - val_loss: -4.3119\n",
      "Epoch 1848/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5946 - val_loss: -4.5946\n",
      "Epoch 1849/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5826 - val_loss: -2.8302\n",
      "Epoch 1850/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4950 - val_loss: -2.1868\n",
      "Epoch 1851/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1196 - val_loss: -4.6090\n",
      "Epoch 1852/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3030 - val_loss: -3.9002\n",
      "Epoch 1853/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4560 - val_loss: -3.1001\n",
      "Epoch 1854/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4754 - val_loss: -3.6728\n",
      "Epoch 1855/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4154 - val_loss: -4.6438\n",
      "Epoch 1856/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5291 - val_loss: -3.9758\n",
      "Epoch 1857/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4691 - val_loss: -4.3392\n",
      "Epoch 1858/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4952 - val_loss: -4.5770\n",
      "Epoch 1859/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4017 - val_loss: -4.5357\n",
      "Epoch 1860/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5805 - val_loss: -2.7311\n",
      "Epoch 1861/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5566 - val_loss: -3.5353\n",
      "Epoch 1862/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5393 - val_loss: -4.4763\n",
      "Epoch 1863/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2302 - val_loss: -4.4286\n",
      "Epoch 1864/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2432 - val_loss: -4.3749\n",
      "Epoch 1865/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3537 - val_loss: -4.4767\n",
      "Epoch 1866/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4220 - val_loss: -4.2826\n",
      "Epoch 1867/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4576 - val_loss: -4.3800\n",
      "Epoch 1868/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4810 - val_loss: -3.8261\n",
      "Epoch 1869/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4040 - val_loss: -4.4680\n",
      "Epoch 1870/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5319 - val_loss: -3.3184\n",
      "Epoch 1871/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5337 - val_loss: -3.6451\n",
      "Epoch 1872/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5247 - val_loss: -2.5782\n",
      "Epoch 1873/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5330 - val_loss: -3.7331\n",
      "Epoch 1874/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5539 - val_loss: -4.2040\n",
      "Epoch 1875/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5171 - val_loss: -2.8690\n",
      "Epoch 1876/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4222 - val_loss: -4.0797\n",
      "Epoch 1877/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5646 - val_loss: -4.2343\n",
      "Epoch 1878/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5313 - val_loss: -3.7716\n",
      "Epoch 1879/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4738 - val_loss: -4.5633\n",
      "Epoch 1880/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4475 - val_loss: -3.5134\n",
      "Epoch 1881/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4296 - val_loss: -4.2942\n",
      "Epoch 1882/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5135 - val_loss: -3.7070\n",
      "Epoch 1883/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5913 - val_loss: -1.9944\n",
      "Epoch 1884/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3764 - val_loss: -3.7221\n",
      "Epoch 1885/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5315 - val_loss: -3.7432\n",
      "Epoch 1886/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5331 - val_loss: -4.2040\n",
      "Epoch 1887/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5631 - val_loss: -4.1177\n",
      "Epoch 1888/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5657 - val_loss: -3.8895\n",
      "Epoch 1889/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4714 - val_loss: -3.9268\n",
      "Epoch 1890/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5676 - val_loss: -4.6053\n",
      "Epoch 1891/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5647 - val_loss: -3.9238\n",
      "Epoch 1892/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4653 - val_loss: -4.1648\n",
      "Epoch 1893/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5715 - val_loss: -4.2367\n",
      "Epoch 1894/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5987 - val_loss: -2.9041\n",
      "Epoch 1895/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5487 - val_loss: -4.7877\n",
      "Epoch 1896/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4638 - val_loss: -2.4786\n",
      "Epoch 1897/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5812 - val_loss: -4.0179\n",
      "Epoch 1898/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4608 - val_loss: -1.8810\n",
      "Epoch 1899/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5492 - val_loss: -3.9009\n",
      "Epoch 1900/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6206 - val_loss: -4.5335\n",
      "Epoch 1901/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6262 - val_loss: -4.7470\n",
      "Epoch 1902/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2327 - val_loss: -4.4951\n",
      "Epoch 1903/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3780 - val_loss: -4.7967\n",
      "Epoch 1904/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4900 - val_loss: -4.3078\n",
      "Epoch 1905/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5155 - val_loss: -2.9958\n",
      "Epoch 1906/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4911 - val_loss: -3.5812\n",
      "Epoch 1907/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5211 - val_loss: -4.2429\n",
      "Epoch 1908/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5561 - val_loss: -4.3538\n",
      "Epoch 1909/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5893 - val_loss: -3.4660\n",
      "Epoch 1910/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2546 - val_loss: -3.6097\n",
      "Epoch 1911/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5756 - val_loss: -4.4802\n",
      "Epoch 1912/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5475 - val_loss: -3.5871\n",
      "Epoch 1913/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5348 - val_loss: -3.7079\n",
      "Epoch 1914/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5896 - val_loss: -2.9367\n",
      "Epoch 1915/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5634 - val_loss: -4.4565\n",
      "Epoch 1916/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5885 - val_loss: -3.4579\n",
      "Epoch 1917/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4979 - val_loss: -4.2640\n",
      "Epoch 1918/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5684 - val_loss: -4.3955\n",
      "Epoch 1919/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5031 - val_loss: -3.7713\n",
      "Epoch 1920/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5772 - val_loss: -4.4936\n",
      "Epoch 1921/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5831 - val_loss: -4.0853\n",
      "Epoch 1922/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5550 - val_loss: -2.5787\n",
      "Epoch 1923/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5445 - val_loss: -3.5704\n",
      "Epoch 1924/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5978 - val_loss: -4.6158\n",
      "Epoch 1925/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5822 - val_loss: -3.4649\n",
      "Epoch 1926/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5245 - val_loss: -3.5325\n",
      "Epoch 1927/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2824"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_110_layer_call_fn, lstm_cell_110_layer_call_and_return_conditional_losses, lstm_cell_111_layer_call_fn, lstm_cell_111_layer_call_and_return_conditional_losses, lstm_cell_112_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -5.2824 - val_loss: -5.0641\n",
      "Epoch 1928/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3717 - val_loss: -4.1433\n",
      "Epoch 1929/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4418 - val_loss: -4.1560\n",
      "Epoch 1930/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5018 - val_loss: -1.1716\n",
      "Epoch 1931/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4843 - val_loss: -0.6700\n",
      "Epoch 1932/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5215 - val_loss: -3.9082\n",
      "Epoch 1933/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4173 - val_loss: -4.3436\n",
      "Epoch 1934/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5608 - val_loss: -4.7874\n",
      "Epoch 1935/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4823 - val_loss: -4.6636\n",
      "Epoch 1936/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5314 - val_loss: -4.7570\n",
      "Epoch 1937/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5238 - val_loss: -2.7808\n",
      "Epoch 1938/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4674 - val_loss: -4.5183\n",
      "Epoch 1939/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3939 - val_loss: -4.4736\n",
      "Epoch 1940/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4841 - val_loss: -3.9136\n",
      "Epoch 1941/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5808 - val_loss: -3.8233\n",
      "Epoch 1942/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5930 - val_loss: -2.8611\n",
      "Epoch 1943/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4040 - val_loss: -4.2726\n",
      "Epoch 1944/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.5756 - val_loss: -3.8493\n",
      "Epoch 1945/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4920 - val_loss: -2.4783\n",
      "Epoch 1946/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4779 - val_loss: -4.3475\n",
      "Epoch 1947/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5750 - val_loss: -4.1595\n",
      "Epoch 1948/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5560 - val_loss: -4.1378\n",
      "Epoch 1949/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5949 - val_loss: -3.3662\n",
      "Epoch 1950/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5791 - val_loss: -1.7663\n",
      "Epoch 1951/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.5158 - val_loss: -3.1412\n",
      "Epoch 1952/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5848 - val_loss: -4.3606\n",
      "Epoch 1953/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5352 - val_loss: -4.3672\n",
      "Epoch 1954/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4639 - val_loss: -3.8939\n",
      "Epoch 1955/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5357 - val_loss: -4.5010\n",
      "Epoch 1956/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4931 - val_loss: -4.2622\n",
      "Epoch 1957/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.6194 - val_loss: -4.7295\n",
      "Epoch 1958/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6097 - val_loss: -4.0750\n",
      "Epoch 1959/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4993 - val_loss: -4.1699\n",
      "Epoch 1960/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5087 - val_loss: -4.7004\n",
      "Epoch 1961/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5950 - val_loss: -4.2194\n",
      "Epoch 1962/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5041 - val_loss: -4.5437\n",
      "Epoch 1963/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5541 - val_loss: -2.4208\n",
      "Epoch 1964/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1898 - val_loss: -4.4649\n",
      "Epoch 1965/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1719 - val_loss: -4.2139\n",
      "Epoch 1966/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2134 - val_loss: -4.3342\n",
      "Epoch 1967/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3615 - val_loss: -4.7863\n",
      "Epoch 1968/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.2949 - val_loss: -0.8689\n",
      "Epoch 1969/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.7507 - val_loss: -2.0399\n",
      "Epoch 1970/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3746 - val_loss: -2.7094\n",
      "Epoch 1971/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5923 - val_loss: -3.2471\n",
      "Epoch 1972/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7254 - val_loss: -3.5085\n",
      "Epoch 1973/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8022 - val_loss: -3.6711\n",
      "Epoch 1974/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8709 - val_loss: -3.4787\n",
      "Epoch 1975/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8968 - val_loss: -3.7469\n",
      "Epoch 1976/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9442 - val_loss: -3.8193\n",
      "Epoch 1977/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9622 - val_loss: -3.8643\n",
      "Epoch 1978/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0010 - val_loss: -3.8087\n",
      "Epoch 1979/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0166 - val_loss: -3.8469\n",
      "Epoch 1980/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0244 - val_loss: -3.5316\n",
      "Epoch 1981/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0191 - val_loss: -3.9367\n",
      "Epoch 1982/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0879 - val_loss: -3.9442\n",
      "Epoch 1983/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0982 - val_loss: -4.0502\n",
      "Epoch 1984/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1001 - val_loss: -3.8680\n",
      "Epoch 1985/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1139 - val_loss: -4.1534\n",
      "Epoch 1986/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1194 - val_loss: -4.0890\n",
      "Epoch 1987/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1183 - val_loss: -3.6607\n",
      "Epoch 1988/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1210 - val_loss: -3.9296\n",
      "Epoch 1989/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1591 - val_loss: -4.2161\n",
      "Epoch 1990/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1770 - val_loss: -3.8737\n",
      "Epoch 1991/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1835 - val_loss: -4.0163\n",
      "Epoch 1992/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1905 - val_loss: -3.8736\n",
      "Epoch 1993/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1392 - val_loss: -3.8173\n",
      "Epoch 1994/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2082 - val_loss: -3.9802\n",
      "Epoch 1995/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1956 - val_loss: -4.0595\n",
      "Epoch 1996/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2297 - val_loss: -3.6623\n",
      "Epoch 1997/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2434 - val_loss: -3.8897\n",
      "Epoch 1998/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2016 - val_loss: -3.2538\n",
      "Epoch 1999/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2353 - val_loss: -4.4442\n",
      "Epoch 2000/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2056 - val_loss: -4.0656\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 7s 44ms/step - loss: -4.9657\n",
      "WARNING:tensorflow:Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 9s 44ms/step - loss: -5.3059\n",
      "WARNING:tensorflow:Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.9935"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 33s 250ms/step - loss: -0.9935 - val_loss: -0.8825\n",
      "Epoch 2/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -1.2428 - val_loss: 0.4508\n",
      "Epoch 3/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.2474 - val_loss: 0.4559\n",
      "Epoch 4/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -1.2601 - val_loss: 0.2745\n",
      "Epoch 5/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.4990 - val_loss: -0.4022\n",
      "Epoch 6/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.3239"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -2.3239 - val_loss: -1.0391\n",
      "Epoch 7/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.8857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 246ms/step - loss: -2.8857 - val_loss: -1.5700\n",
      "Epoch 8/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.2648"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 246ms/step - loss: -3.2648 - val_loss: -2.6680\n",
      "Epoch 9/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.4539"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -3.4539 - val_loss: -3.0539\n",
      "Epoch 10/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7572 - val_loss: -1.2060\n",
      "Epoch 11/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.7574 - val_loss: -2.9630\n",
      "Epoch 12/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9044 - val_loss: -3.0201\n",
      "Epoch 13/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.0555"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -4.0555 - val_loss: -3.4860\n",
      "Epoch 14/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.1062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -4.1062 - val_loss: -3.7796\n",
      "Epoch 15/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0064 - val_loss: -3.5942\n",
      "Epoch 16/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1351 - val_loss: -3.5998\n",
      "Epoch 17/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2010 - val_loss: -3.4166\n",
      "Epoch 18/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.2914"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 246ms/step - loss: -4.2914 - val_loss: -3.9311\n",
      "Epoch 19/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2561 - val_loss: -3.0444\n",
      "Epoch 20/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3204 - val_loss: -3.8722\n",
      "Epoch 21/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3755 - val_loss: -0.8702\n",
      "Epoch 22/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4109 - val_loss: 0.0625\n",
      "Epoch 23/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3253 - val_loss: -3.1704\n",
      "Epoch 24/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4317"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 243ms/step - loss: -4.4317 - val_loss: -3.9815\n",
      "Epoch 25/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.3840 - val_loss: -3.8176\n",
      "Epoch 26/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3787 - val_loss: -3.9789\n",
      "Epoch 27/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2720 - val_loss: -3.5633\n",
      "Epoch 28/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5294 - val_loss: -1.7222\n",
      "Epoch 29/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1497 - val_loss: -1.6645\n",
      "Epoch 30/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3376 - val_loss: -2.9053\n",
      "Epoch 31/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3382 - val_loss: -3.9730\n",
      "Epoch 32/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4208 - val_loss: -2.8771\n",
      "Epoch 33/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.5083 - val_loss: -3.9872\n",
      "Epoch 34/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5790"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -4.5790 - val_loss: -4.1679\n",
      "Epoch 35/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3550 - val_loss: -4.0482\n",
      "Epoch 36/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5729 - val_loss: -3.5633\n",
      "Epoch 37/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5394 - val_loss: -4.0659\n",
      "Epoch 38/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5274 - val_loss: -3.3357\n",
      "Epoch 39/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6022"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -4.6022 - val_loss: -4.2247\n",
      "Epoch 40/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5844 - val_loss: -3.8189\n",
      "Epoch 41/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6149 - val_loss: -4.0130\n",
      "Epoch 42/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5296 - val_loss: -4.0791\n",
      "Epoch 43/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6315 - val_loss: -2.5451\n",
      "Epoch 44/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6048 - val_loss: -3.2233\n",
      "Epoch 45/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6375 - val_loss: -4.1315\n",
      "Epoch 46/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5700 - val_loss: -2.4980\n",
      "Epoch 47/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6505"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.6505 - val_loss: -4.2698\n",
      "Epoch 48/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6886 - val_loss: -4.1459\n",
      "Epoch 49/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6235 - val_loss: -2.9004\n",
      "Epoch 50/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5960 - val_loss: -4.2641\n",
      "Epoch 51/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6625 - val_loss: -2.8882\n",
      "Epoch 52/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5984 - val_loss: -4.0624\n",
      "Epoch 53/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6950 - val_loss: -3.5597\n",
      "Epoch 54/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5860 - val_loss: -3.7127\n",
      "Epoch 55/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6359 - val_loss: -3.4756\n",
      "Epoch 56/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7379 - val_loss: -3.1019\n",
      "Epoch 57/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6755 - val_loss: -4.2013\n",
      "Epoch 58/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6538 - val_loss: -4.0966\n",
      "Epoch 59/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.1271 - val_loss: -3.4471\n",
      "Epoch 60/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.7503 - val_loss: -3.4687\n",
      "Epoch 61/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2661 - val_loss: -3.9660\n",
      "Epoch 62/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4204 - val_loss: -3.9496\n",
      "Epoch 63/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3380 - val_loss: -1.7436\n",
      "Epoch 64/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3723 - val_loss: -3.7197\n",
      "Epoch 65/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5555"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 242ms/step - loss: -4.5555 - val_loss: -4.3029\n",
      "Epoch 66/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4996 - val_loss: -3.6490\n",
      "Epoch 67/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6219 - val_loss: -3.9910\n",
      "Epoch 68/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5604 - val_loss: -3.9342\n",
      "Epoch 69/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5538 - val_loss: -2.9340\n",
      "Epoch 70/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1964 - val_loss: -3.1512\n",
      "Epoch 71/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5445 - val_loss: -3.3004\n",
      "Epoch 72/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5382 - val_loss: -2.9845\n",
      "Epoch 73/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6309 - val_loss: -3.9606\n",
      "Epoch 74/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7434 - val_loss: -2.5247\n",
      "Epoch 75/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6922 - val_loss: -4.2831\n",
      "Epoch 76/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7107 - val_loss: -3.7173\n",
      "Epoch 77/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6752 - val_loss: -3.9987\n",
      "Epoch 78/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5646 - val_loss: -3.6799\n",
      "Epoch 79/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1665 - val_loss: -4.1419\n",
      "Epoch 80/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6990 - val_loss: -3.4041\n",
      "Epoch 81/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7145 - val_loss: -2.7193\n",
      "Epoch 82/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7559 - val_loss: -3.8060\n",
      "Epoch 83/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7338 - val_loss: -3.5600\n",
      "Epoch 84/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7675 - val_loss: -4.0774\n",
      "Epoch 85/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8190"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -4.8190 - val_loss: -4.3664\n",
      "Epoch 86/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7518 - val_loss: -2.9862\n",
      "Epoch 87/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7498 - val_loss: -2.9589\n",
      "Epoch 88/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8433 - val_loss: -3.3886\n",
      "Epoch 89/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2878 - val_loss: -4.0483\n",
      "Epoch 90/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5980 - val_loss: -4.1094\n",
      "Epoch 91/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7619 - val_loss: -4.2585\n",
      "Epoch 92/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7246 - val_loss: -3.9030\n",
      "Epoch 93/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8006 - val_loss: -3.6190\n",
      "Epoch 94/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8327 - val_loss: -4.2826\n",
      "Epoch 95/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6949 - val_loss: -3.8673\n",
      "Epoch 96/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8643"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -4.8643 - val_loss: -4.3995\n",
      "Epoch 97/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7492 - val_loss: -4.1563\n",
      "Epoch 98/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7433 - val_loss: -3.9004\n",
      "Epoch 99/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8795 - val_loss: -3.3981\n",
      "Epoch 100/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4473 - val_loss: -3.8873\n",
      "Epoch 101/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7084 - val_loss: -4.1652\n",
      "Epoch 102/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7042 - val_loss: -3.8361\n",
      "Epoch 103/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8131 - val_loss: -4.0480\n",
      "Epoch 104/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7930 - val_loss: -4.3828\n",
      "Epoch 105/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8760 - val_loss: -4.0625\n",
      "Epoch 106/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3694 - val_loss: -4.2645\n",
      "Epoch 107/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7724 - val_loss: -3.9042\n",
      "Epoch 108/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7654 - val_loss: -4.3017\n",
      "Epoch 109/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7861 - val_loss: -4.2524\n",
      "Epoch 110/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8685 - val_loss: -4.2322\n",
      "Epoch 111/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7792 - val_loss: -3.8186\n",
      "Epoch 112/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7311 - val_loss: -3.4159\n",
      "Epoch 113/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.5607 - val_loss: -4.0990\n",
      "Epoch 114/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3363 - val_loss: -2.9588\n",
      "Epoch 115/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6279 - val_loss: -4.0390\n",
      "Epoch 116/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7343 - val_loss: -4.2809\n",
      "Epoch 117/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6709 - val_loss: -4.0350\n",
      "Epoch 118/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7810 - val_loss: -4.2192\n",
      "Epoch 119/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8317 - val_loss: -3.3421\n",
      "Epoch 120/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7861 - val_loss: -4.0904\n",
      "Epoch 121/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6871 - val_loss: -4.2751\n",
      "Epoch 122/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8202 - val_loss: -4.1289\n",
      "Epoch 123/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8857 - val_loss: -4.2223\n",
      "Epoch 124/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8133 - val_loss: -4.2196\n",
      "Epoch 125/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7852 - val_loss: -4.2200\n",
      "Epoch 126/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8155 - val_loss: -4.2883\n",
      "Epoch 127/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0000 - val_loss: -3.8353\n",
      "Epoch 128/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5063 - val_loss: -3.9709\n",
      "Epoch 129/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6946 - val_loss: -3.2699\n",
      "Epoch 130/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7915 - val_loss: -4.2410\n",
      "Epoch 131/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7422 - val_loss: -2.6294\n",
      "Epoch 132/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8325 - val_loss: -3.2977\n",
      "Epoch 133/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8530 - val_loss: -3.2708\n",
      "Epoch 134/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5583 - val_loss: -3.6029\n",
      "Epoch 135/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7381 - val_loss: -4.2929\n",
      "Epoch 136/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8672 - val_loss: -4.3601\n",
      "Epoch 137/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7530 - val_loss: -3.8890\n",
      "Epoch 138/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6737 - val_loss: -4.1889\n",
      "Epoch 139/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9112 - val_loss: -4.3613\n",
      "Epoch 140/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8293 - val_loss: -4.3322\n",
      "Epoch 141/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9276 - val_loss: -4.3289\n",
      "Epoch 142/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8905 - val_loss: -4.0690\n",
      "Epoch 143/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8115 - val_loss: -3.4338\n",
      "Epoch 144/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8654 - val_loss: -3.9076\n",
      "Epoch 145/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8504 - val_loss: -3.3349\n",
      "Epoch 146/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9335 - val_loss: -4.0502\n",
      "Epoch 147/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8795 - val_loss: -4.2473\n",
      "Epoch 148/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7627 - val_loss: -3.6897\n",
      "Epoch 149/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9460 - val_loss: -3.2788\n",
      "Epoch 150/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9700 - val_loss: -3.6822\n",
      "Epoch 151/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7537 - val_loss: -3.9240\n",
      "Epoch 152/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8963 - val_loss: -2.6839\n",
      "Epoch 153/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9182 - val_loss: -4.3870\n",
      "Epoch 154/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1583 - val_loss: -2.4595\n",
      "Epoch 155/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.7839 - val_loss: -3.5777\n",
      "Epoch 156/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2659 - val_loss: -4.0784\n",
      "Epoch 157/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3482 - val_loss: -3.9111\n",
      "Epoch 158/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5223 - val_loss: -4.0638\n",
      "Epoch 159/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5833 - val_loss: -3.0343\n",
      "Epoch 160/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5747 - val_loss: -4.1515\n",
      "Epoch 161/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5735 - val_loss: -3.8212\n",
      "Epoch 162/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6571 - val_loss: -3.9958\n",
      "Epoch 163/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6463 - val_loss: -4.3056\n",
      "Epoch 164/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6525 - val_loss: -4.1900\n",
      "Epoch 165/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7519 - val_loss: -2.7364\n",
      "Epoch 166/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8121 - val_loss: -4.3312\n",
      "Epoch 167/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7892 - val_loss: -2.3265\n",
      "Epoch 168/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5388 - val_loss: -3.1328\n",
      "Epoch 169/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5937"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -4.5937 - val_loss: -4.4077\n",
      "Epoch 170/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7936 - val_loss: -3.1627\n",
      "Epoch 171/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8230 - val_loss: -4.3704\n",
      "Epoch 172/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5855 - val_loss: -4.2238\n",
      "Epoch 173/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8046 - val_loss: -3.9587\n",
      "Epoch 174/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9001 - val_loss: -4.1287\n",
      "Epoch 175/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8284 - val_loss: -2.5946\n",
      "Epoch 176/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6499 - val_loss: -4.3378\n",
      "Epoch 177/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8758 - val_loss: -3.2870\n",
      "Epoch 178/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8846 - val_loss: -4.1703\n",
      "Epoch 179/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8550 - val_loss: -4.2933\n",
      "Epoch 180/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8290 - val_loss: -4.1226\n",
      "Epoch 181/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9043 - val_loss: -3.1469\n",
      "Epoch 182/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8449 - val_loss: -4.3319\n",
      "Epoch 183/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6873 - val_loss: -3.6472\n",
      "Epoch 184/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9007 - val_loss: -4.1036\n",
      "Epoch 185/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8864 - val_loss: -4.0481\n",
      "Epoch 186/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8864 - val_loss: -4.2289\n",
      "Epoch 187/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9170 - val_loss: 0.0047\n",
      "Epoch 188/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9013"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -4.9013 - val_loss: -4.4300\n",
      "Epoch 189/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5804 - val_loss: -3.6352\n",
      "Epoch 190/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8538 - val_loss: -4.4103\n",
      "Epoch 191/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9014 - val_loss: -3.6920\n",
      "Epoch 192/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8691 - val_loss: -3.9488\n",
      "Epoch 193/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9818 - val_loss: -2.1029\n",
      "Epoch 194/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8854 - val_loss: -4.3098\n",
      "Epoch 195/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8398 - val_loss: -4.1926\n",
      "Epoch 196/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7520 - val_loss: -3.6711\n",
      "Epoch 197/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7841 - val_loss: -4.1567\n",
      "Epoch 198/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0448 - val_loss: -1.9966\n",
      "Epoch 199/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7854 - val_loss: -4.0577\n",
      "Epoch 200/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9614 - val_loss: -4.3120\n",
      "Epoch 201/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9283 - val_loss: -4.1733\n",
      "Epoch 202/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9621 - val_loss: -4.3324\n",
      "Epoch 203/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9136 - val_loss: -3.9815\n",
      "Epoch 204/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9499 - val_loss: -3.8600\n",
      "Epoch 205/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5571 - val_loss: -4.3536\n",
      "Epoch 206/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9367 - val_loss: -3.7567\n",
      "Epoch 207/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9740 - val_loss: -4.0597\n",
      "Epoch 208/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9843 - val_loss: -4.3248\n",
      "Epoch 209/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8599 - val_loss: -4.1241\n",
      "Epoch 210/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9840 - val_loss: -3.2039\n",
      "Epoch 211/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0065 - val_loss: -4.2726\n",
      "Epoch 212/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7684 - val_loss: -4.3016\n",
      "Epoch 213/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9809 - val_loss: -2.3987\n",
      "Epoch 214/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9184 - val_loss: -4.3507\n",
      "Epoch 215/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0528"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -5.0528 - val_loss: -4.4311\n",
      "Epoch 216/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9997 - val_loss: -4.3488\n",
      "Epoch 217/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9987 - val_loss: -3.3172\n",
      "Epoch 218/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7826 - val_loss: -4.3646\n",
      "Epoch 219/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0092 - val_loss: -2.6471\n",
      "Epoch 220/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7915 - val_loss: -4.2164\n",
      "Epoch 221/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9425 - val_loss: -4.2486\n",
      "Epoch 222/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9127 - val_loss: -4.2147\n",
      "Epoch 223/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0373 - val_loss: -3.5066\n",
      "Epoch 224/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9923"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -4.9923 - val_loss: -4.4351\n",
      "Epoch 225/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8926 - val_loss: -4.2184\n",
      "Epoch 226/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0632 - val_loss: -3.9909\n",
      "Epoch 227/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0222 - val_loss: -2.4968\n",
      "Epoch 228/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.2712 - val_loss: -3.8176\n",
      "Epoch 229/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9181 - val_loss: -3.6942\n",
      "Epoch 230/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9248 - val_loss: -4.0106\n",
      "Epoch 231/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7444 - val_loss: -3.7714\n",
      "Epoch 232/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8531 - val_loss: -4.2376\n",
      "Epoch 233/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0146 - val_loss: -4.1767\n",
      "Epoch 234/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8166 - val_loss: -4.3457\n",
      "Epoch 235/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0047"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -5.0047 - val_loss: -4.6412\n",
      "Epoch 236/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6984 - val_loss: -3.4453\n",
      "Epoch 237/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9625 - val_loss: -4.3044\n",
      "Epoch 238/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9549 - val_loss: -4.4472\n",
      "Epoch 239/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0559 - val_loss: -4.3501\n",
      "Epoch 240/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0177 - val_loss: -4.3772\n",
      "Epoch 241/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9971 - val_loss: -1.9740\n",
      "Epoch 242/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0103 - val_loss: -4.2792\n",
      "Epoch 243/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0348 - val_loss: -3.8201\n",
      "Epoch 244/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0281 - val_loss: -4.5842\n",
      "Epoch 245/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0621 - val_loss: -3.9213\n",
      "Epoch 246/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0528 - val_loss: -4.3851\n",
      "Epoch 247/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9477 - val_loss: -4.2928\n",
      "Epoch 248/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0447 - val_loss: -3.7468\n",
      "Epoch 249/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0579 - val_loss: -3.6683\n",
      "Epoch 250/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0494 - val_loss: -4.2811\n",
      "Epoch 251/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9957 - val_loss: -4.4144\n",
      "Epoch 252/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0349 - val_loss: -3.5071\n",
      "Epoch 253/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9959 - val_loss: -4.4402\n",
      "Epoch 254/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0179 - val_loss: -4.1958\n",
      "Epoch 255/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0051 - val_loss: -4.4015\n",
      "Epoch 256/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0367 - val_loss: -4.5938\n",
      "Epoch 257/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1015 - val_loss: -4.4089\n",
      "Epoch 258/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0513 - val_loss: -4.4463\n",
      "Epoch 259/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0743 - val_loss: -3.9136\n",
      "Epoch 260/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0093 - val_loss: -4.4728\n",
      "Epoch 261/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1218 - val_loss: -4.5185\n",
      "Epoch 262/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0681 - val_loss: -4.6022\n",
      "Epoch 263/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0160 - val_loss: -1.3572\n",
      "Epoch 264/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0871 - val_loss: -4.3160\n",
      "Epoch 265/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9909 - val_loss: -4.3099\n",
      "Epoch 266/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1742 - val_loss: -3.1573\n",
      "Epoch 267/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0103 - val_loss: -4.1110\n",
      "Epoch 268/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8612 - val_loss: -3.9996\n",
      "Epoch 269/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8431 - val_loss: -4.1983\n",
      "Epoch 270/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6942 - val_loss: -4.1875\n",
      "Epoch 271/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0092 - val_loss: -4.1607\n",
      "Epoch 272/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0464 - val_loss: -4.5233\n",
      "Epoch 273/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0429 - val_loss: -4.1606\n",
      "Epoch 274/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0539 - val_loss: -4.2989\n",
      "Epoch 275/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0584 - val_loss: -4.1299\n",
      "Epoch 276/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0694 - val_loss: -3.5718\n",
      "Epoch 277/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0532 - val_loss: -3.9798\n",
      "Epoch 278/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0974 - val_loss: -4.4395\n",
      "Epoch 279/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1375 - val_loss: -3.0812\n",
      "Epoch 280/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0961 - val_loss: -4.5215\n",
      "Epoch 281/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1102 - val_loss: -4.3852\n",
      "Epoch 282/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1248 - val_loss: -4.5262\n",
      "Epoch 283/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0307 - val_loss: -4.5090\n",
      "Epoch 284/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0841 - val_loss: -2.5644\n",
      "Epoch 285/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9219 - val_loss: -3.8400\n",
      "Epoch 286/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2077 - val_loss: -4.2097\n",
      "Epoch 287/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9949 - val_loss: -3.7856\n",
      "Epoch 288/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2066 - val_loss: -4.3022\n",
      "Epoch 289/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0004 - val_loss: -4.1844\n",
      "Epoch 290/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.8024 - val_loss: -3.0642\n",
      "Epoch 291/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.4507 - val_loss: -3.0058\n",
      "Epoch 292/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1772 - val_loss: -2.9518\n",
      "Epoch 293/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5404 - val_loss: -4.2106\n",
      "Epoch 294/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7434 - val_loss: -3.7162\n",
      "Epoch 295/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6115 - val_loss: -4.0920\n",
      "Epoch 296/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6705 - val_loss: -3.8205\n",
      "Epoch 297/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8445 - val_loss: -4.2841\n",
      "Epoch 298/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5365 - val_loss: -4.2377\n",
      "Epoch 299/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7179 - val_loss: -4.2878\n",
      "Epoch 300/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8195 - val_loss: -4.4062\n",
      "Epoch 301/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8936 - val_loss: -3.9241\n",
      "Epoch 302/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8746 - val_loss: -4.3317\n",
      "Epoch 303/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9171 - val_loss: -4.4885\n",
      "Epoch 304/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8706 - val_loss: -4.0572\n",
      "Epoch 305/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0312 - val_loss: -4.4358\n",
      "Epoch 306/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8756 - val_loss: -4.1503\n",
      "Epoch 307/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9598 - val_loss: -4.4377\n",
      "Epoch 308/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9450 - val_loss: -3.4743\n",
      "Epoch 309/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9276 - val_loss: -4.2117\n",
      "Epoch 310/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9703 - val_loss: -3.1480\n",
      "Epoch 311/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8935 - val_loss: -4.2983\n",
      "Epoch 312/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9499 - val_loss: -4.3986\n",
      "Epoch 313/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9651 - val_loss: -4.2506\n",
      "Epoch 314/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0630 - val_loss: -3.9968\n",
      "Epoch 315/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9566 - val_loss: -4.1384\n",
      "Epoch 316/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9562 - val_loss: -4.1688\n",
      "Epoch 317/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0083 - val_loss: -3.7648\n",
      "Epoch 318/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0308 - val_loss: -3.8686\n",
      "Epoch 319/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0048 - val_loss: -3.7729\n",
      "Epoch 320/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0370 - val_loss: -4.4486\n",
      "Epoch 321/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9763 - val_loss: -4.4346\n",
      "Epoch 322/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0678 - val_loss: -4.5095\n",
      "Epoch 323/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0612 - val_loss: -4.3490\n",
      "Epoch 324/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9457 - val_loss: -3.6428\n",
      "Epoch 325/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9868 - val_loss: -3.9961\n",
      "Epoch 326/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0922 - val_loss: -3.6073\n",
      "Epoch 327/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7731 - val_loss: -4.1072\n",
      "Epoch 328/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1058 - val_loss: -3.5859\n",
      "Epoch 329/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0578 - val_loss: -4.2203\n",
      "Epoch 330/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9696 - val_loss: -4.0346\n",
      "Epoch 331/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1287 - val_loss: -3.9646\n",
      "Epoch 332/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9515 - val_loss: -4.3392\n",
      "Epoch 333/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1788 - val_loss: -4.1003\n",
      "Epoch 334/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0733 - val_loss: -4.3213\n",
      "Epoch 335/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0904 - val_loss: -3.6428\n",
      "Epoch 336/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7743 - val_loss: -4.1241\n",
      "Epoch 337/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1041 - val_loss: -4.1097\n",
      "Epoch 338/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9906 - val_loss: -3.8328\n",
      "Epoch 339/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1212 - val_loss: -4.4827\n",
      "Epoch 340/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1110 - val_loss: -4.5153\n",
      "Epoch 341/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0385 - val_loss: -2.1513\n",
      "Epoch 342/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9990 - val_loss: -3.3375\n",
      "Epoch 343/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1177 - val_loss: -3.9988\n",
      "Epoch 344/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1582 - val_loss: -4.2123\n",
      "Epoch 345/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0454 - val_loss: -3.7951\n",
      "Epoch 346/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0448 - val_loss: -3.7721\n",
      "Epoch 347/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1151 - val_loss: -4.2941\n",
      "Epoch 348/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1663 - val_loss: -1.8048\n",
      "Epoch 349/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0696 - val_loss: 0.4466\n",
      "Epoch 350/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7344 - val_loss: -3.9087\n",
      "Epoch 351/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1063 - val_loss: -4.2989\n",
      "Epoch 352/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0942 - val_loss: -2.9590\n",
      "Epoch 353/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1737 - val_loss: -4.3105\n",
      "Epoch 354/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1239 - val_loss: -4.4317\n",
      "Epoch 355/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1752 - val_loss: -3.5529\n",
      "Epoch 356/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1862 - val_loss: -3.8949\n",
      "Epoch 357/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1239 - val_loss: -4.5516\n",
      "Epoch 358/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0459 - val_loss: -3.9831\n",
      "Epoch 359/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2099 - val_loss: -4.1682\n",
      "Epoch 360/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9982 - val_loss: -3.6478\n",
      "Epoch 361/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0989 - val_loss: -4.1008\n",
      "Epoch 362/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0498 - val_loss: -4.0106\n",
      "Epoch 363/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1941 - val_loss: -4.4314\n",
      "Epoch 364/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1104 - val_loss: -3.6297\n",
      "Epoch 365/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2409 - val_loss: -3.8137\n",
      "Epoch 366/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9958 - val_loss: -3.8625\n",
      "Epoch 367/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2183 - val_loss: -4.4208\n",
      "Epoch 368/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9388 - val_loss: -4.5973\n",
      "Epoch 369/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1251"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -5.1251 - val_loss: -4.6765\n",
      "Epoch 370/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2413 - val_loss: -4.3160\n",
      "Epoch 371/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0765 - val_loss: -4.4079\n",
      "Epoch 372/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.6631 - val_loss: -4.1874\n",
      "Epoch 373/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.3513 - val_loss: -4.3822\n",
      "Epoch 374/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9293 - val_loss: -2.0804\n",
      "Epoch 375/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0674 - val_loss: -4.5657\n",
      "Epoch 376/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9926 - val_loss: -4.4377\n",
      "Epoch 377/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1298 - val_loss: -3.9932\n",
      "Epoch 378/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1048 - val_loss: -2.8857\n",
      "Epoch 379/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9330 - val_loss: -4.3611\n",
      "Epoch 380/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0630 - val_loss: -4.2045\n",
      "Epoch 381/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0686 - val_loss: -4.3067\n",
      "Epoch 382/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0761 - val_loss: -4.2933\n",
      "Epoch 383/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1488 - val_loss: -4.3443\n",
      "Epoch 384/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1141 - val_loss: -3.6951\n",
      "Epoch 385/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0069 - val_loss: -4.2996\n",
      "Epoch 386/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1613 - val_loss: -3.7631\n",
      "Epoch 387/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1342 - val_loss: -4.6202\n",
      "Epoch 388/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1913 - val_loss: -4.3131\n",
      "Epoch 389/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0786 - val_loss: -4.2224\n",
      "Epoch 390/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0165 - val_loss: -3.9763\n",
      "Epoch 391/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0908 - val_loss: -4.3937\n",
      "Epoch 392/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2198 - val_loss: -4.4010\n",
      "Epoch 393/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1056 - val_loss: -4.3406\n",
      "Epoch 394/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2051 - val_loss: -3.6394\n",
      "Epoch 395/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1539 - val_loss: -4.1357\n",
      "Epoch 396/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1781 - val_loss: -3.1482\n",
      "Epoch 397/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2102 - val_loss: -3.4487\n",
      "Epoch 398/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1363 - val_loss: -4.0767\n",
      "Epoch 399/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0726 - val_loss: -1.1847\n",
      "Epoch 400/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1329 - val_loss: -3.4570\n",
      "Epoch 401/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1721 - val_loss: -3.6720\n",
      "Epoch 402/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1311 - val_loss: -4.1791\n",
      "Epoch 403/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2018 - val_loss: -4.3792\n",
      "Epoch 404/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2033 - val_loss: -4.5070\n",
      "Epoch 405/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1500 - val_loss: -4.1903\n",
      "Epoch 406/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2773 - val_loss: -4.5181\n",
      "Epoch 407/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1632 - val_loss: -4.4063\n",
      "Epoch 408/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2182"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -5.2182 - val_loss: -4.7852\n",
      "Epoch 409/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1946 - val_loss: -4.1921\n",
      "Epoch 410/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1054 - val_loss: -4.3195\n",
      "Epoch 411/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2876 - val_loss: -3.2525\n",
      "Epoch 412/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2566 - val_loss: -4.3405\n",
      "Epoch 413/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1589 - val_loss: -4.4504\n",
      "Epoch 414/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1307 - val_loss: -3.1101\n",
      "Epoch 415/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0900 - val_loss: -4.6808\n",
      "Epoch 416/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1061 - val_loss: -3.2349\n",
      "Epoch 417/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2659 - val_loss: -4.4313\n",
      "Epoch 418/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2359 - val_loss: -4.6151\n",
      "Epoch 419/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2075 - val_loss: -3.3643\n",
      "Epoch 420/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1988 - val_loss: -4.1035\n",
      "Epoch 421/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9651 - val_loss: -3.7572\n",
      "Epoch 422/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1610 - val_loss: -4.5778\n",
      "Epoch 423/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1864 - val_loss: -1.6699\n",
      "Epoch 424/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2003 - val_loss: -4.0499\n",
      "Epoch 425/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2202 - val_loss: -4.3525\n",
      "Epoch 426/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2466 - val_loss: -4.2124\n",
      "Epoch 427/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1617 - val_loss: -3.3730\n",
      "Epoch 428/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2840 - val_loss: -4.2942\n",
      "Epoch 429/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2656 - val_loss: -4.4700\n",
      "Epoch 430/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1540 - val_loss: -4.5086\n",
      "Epoch 431/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2851 - val_loss: -0.3147\n",
      "Epoch 432/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1791 - val_loss: -2.3468\n",
      "Epoch 433/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2455 - val_loss: -3.6371\n",
      "Epoch 434/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1230 - val_loss: -3.7183\n",
      "Epoch 435/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9593 - val_loss: -4.2412\n",
      "Epoch 436/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -5.2051 - val_loss: -4.8340\n",
      "Epoch 437/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2092"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -5.2092 - val_loss: -4.8415\n",
      "Epoch 438/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7814 - val_loss: -4.4544\n",
      "Epoch 439/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1632 - val_loss: -3.4440\n",
      "Epoch 440/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1203 - val_loss: -3.9619\n",
      "Epoch 441/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2865 - val_loss: -3.7943\n",
      "Epoch 442/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1825 - val_loss: -4.3144\n",
      "Epoch 443/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9705 - val_loss: -4.2459\n",
      "Epoch 444/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2258 - val_loss: -2.8326\n",
      "Epoch 445/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1871 - val_loss: -4.2267\n",
      "Epoch 446/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9519 - val_loss: -4.3018\n",
      "Epoch 447/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1553 - val_loss: -4.6512\n",
      "Epoch 448/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2254 - val_loss: -3.5501\n",
      "Epoch 449/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2116 - val_loss: -4.1813\n",
      "Epoch 450/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1876 - val_loss: -3.5266\n",
      "Epoch 451/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1672 - val_loss: -3.7887\n",
      "Epoch 452/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3219 - val_loss: -3.5232\n",
      "Epoch 453/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2438 - val_loss: -3.8936\n",
      "Epoch 454/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2285 - val_loss: -4.1324\n",
      "Epoch 455/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2544 - val_loss: -4.2907\n",
      "Epoch 456/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2787 - val_loss: -4.6885\n",
      "Epoch 457/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1804 - val_loss: -4.4422\n",
      "Epoch 458/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2781 - val_loss: -3.7118\n",
      "Epoch 459/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9463 - val_loss: -3.2098\n",
      "Epoch 460/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1024 - val_loss: -3.8416\n",
      "Epoch 461/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2522 - val_loss: -3.8146\n",
      "Epoch 462/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1916 - val_loss: -4.2667\n",
      "Epoch 463/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2947 - val_loss: -3.5712\n",
      "Epoch 464/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0382 - val_loss: -4.2767\n",
      "Epoch 465/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2984 - val_loss: -3.9591\n",
      "Epoch 466/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2556 - val_loss: -4.4919\n",
      "Epoch 467/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3167 - val_loss: -2.2298\n",
      "Epoch 468/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2000 - val_loss: -4.1614\n",
      "Epoch 469/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1226 - val_loss: -4.6737\n",
      "Epoch 470/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2575 - val_loss: -4.3283\n",
      "Epoch 471/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0986 - val_loss: -4.2011\n",
      "Epoch 472/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2744 - val_loss: -3.5773\n",
      "Epoch 473/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1476 - val_loss: -4.3743\n",
      "Epoch 474/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2996 - val_loss: -3.4342\n",
      "Epoch 475/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2503 - val_loss: -1.7057\n",
      "Epoch 476/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2739 - val_loss: -4.3546\n",
      "Epoch 477/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2279 - val_loss: -4.5705\n",
      "Epoch 478/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2734 - val_loss: -4.5880\n",
      "Epoch 479/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3160 - val_loss: -4.7791\n",
      "Epoch 480/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1899 - val_loss: -4.0939\n",
      "Epoch 481/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3692 - val_loss: -2.8105\n",
      "Epoch 482/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2081 - val_loss: -4.6770\n",
      "Epoch 483/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3622 - val_loss: -3.8449\n",
      "Epoch 484/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3545 - val_loss: -3.5198\n",
      "Epoch 485/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0523 - val_loss: -4.0441\n",
      "Epoch 486/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2953 - val_loss: -4.5323\n",
      "Epoch 487/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3113 - val_loss: -4.3498\n",
      "Epoch 488/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2518 - val_loss: -1.6226\n",
      "Epoch 489/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3215 - val_loss: -3.8307\n",
      "Epoch 490/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2447 - val_loss: -4.2051\n",
      "Epoch 491/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2676 - val_loss: -4.2963\n",
      "Epoch 492/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2342 - val_loss: -4.0333\n",
      "Epoch 493/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2592"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -5.2592 - val_loss: -4.8535\n",
      "Epoch 494/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9680 - val_loss: -4.0738\n",
      "Epoch 495/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2662 - val_loss: -4.6372\n",
      "Epoch 496/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2667 - val_loss: -3.0213\n",
      "Epoch 497/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2377 - val_loss: -4.1075\n",
      "Epoch 498/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2907 - val_loss: -4.6415\n",
      "Epoch 499/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2204 - val_loss: -4.1796\n",
      "Epoch 500/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1984 - val_loss: -4.6513\n",
      "Epoch 501/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2809 - val_loss: -4.4752\n",
      "Epoch 502/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3358 - val_loss: -4.2657\n",
      "Epoch 503/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2673 - val_loss: -4.2621\n",
      "Epoch 504/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3246 - val_loss: -3.5188\n",
      "Epoch 505/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3819 - val_loss: -4.4044\n",
      "Epoch 506/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4498 - val_loss: -4.1612\n",
      "Epoch 507/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9830 - val_loss: -3.6139\n",
      "Epoch 508/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1477 - val_loss: -4.3436\n",
      "Epoch 509/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1045 - val_loss: -3.9554\n",
      "Epoch 510/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2140 - val_loss: -3.7642\n",
      "Epoch 511/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1025 - val_loss: -2.8759\n",
      "Epoch 512/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2410 - val_loss: -4.6768\n",
      "Epoch 513/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2109 - val_loss: -3.3100\n",
      "Epoch 514/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2881 - val_loss: -3.7114\n",
      "Epoch 515/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1325 - val_loss: -4.3952\n",
      "Epoch 516/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2135 - val_loss: -4.3094\n",
      "Epoch 517/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2219 - val_loss: -4.4537\n",
      "Epoch 518/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2644 - val_loss: -4.7480\n",
      "Epoch 519/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2222 - val_loss: -4.2668\n",
      "Epoch 520/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2923 - val_loss: -4.7396\n",
      "Epoch 521/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2898 - val_loss: -4.3300\n",
      "Epoch 522/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3657 - val_loss: -4.6049\n",
      "Epoch 523/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2869 - val_loss: -2.9664\n",
      "Epoch 524/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2330 - val_loss: -4.1022\n",
      "Epoch 525/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0895 - val_loss: -4.0494\n",
      "Epoch 526/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2904 - val_loss: -4.5819\n",
      "Epoch 527/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2787 - val_loss: -4.8490\n",
      "Epoch 528/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3201 - val_loss: -4.6578\n",
      "Epoch 529/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3319 - val_loss: -2.9726\n",
      "Epoch 530/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2076 - val_loss: -2.3299\n",
      "Epoch 531/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8420 - val_loss: -4.3582\n",
      "Epoch 532/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2808 - val_loss: -4.6852\n",
      "Epoch 533/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3543 - val_loss: -2.8756\n",
      "Epoch 534/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2278 - val_loss: -1.9213\n",
      "Epoch 535/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1365 - val_loss: -4.5567\n",
      "Epoch 536/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3200 - val_loss: -4.7112\n",
      "Epoch 537/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3296 - val_loss: -1.1638\n",
      "Epoch 538/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9897 - val_loss: -3.2835\n",
      "Epoch 539/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2861 - val_loss: -4.1937\n",
      "Epoch 540/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3905 - val_loss: -4.1506\n",
      "Epoch 541/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2434 - val_loss: -4.2251\n",
      "Epoch 542/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3334 - val_loss: -3.0001\n",
      "Epoch 543/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2403 - val_loss: -4.0811\n",
      "Epoch 544/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2667 - val_loss: -4.6884\n",
      "Epoch 545/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1990 - val_loss: -4.2201\n",
      "Epoch 546/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5898 - val_loss: -3.8022\n",
      "Epoch 547/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0127 - val_loss: -4.4032\n",
      "Epoch 548/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0442 - val_loss: -4.2622\n",
      "Epoch 549/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9546 - val_loss: -4.4619\n",
      "Epoch 550/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2342 - val_loss: -3.2981\n",
      "Epoch 551/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0635 - val_loss: -4.5604\n",
      "Epoch 552/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1873 - val_loss: -4.3463\n",
      "Epoch 553/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2423 - val_loss: -4.2066\n",
      "Epoch 554/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4095 - val_loss: -4.0003\n",
      "Epoch 555/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8863 - val_loss: -4.8276\n",
      "Epoch 556/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9814 - val_loss: -2.3550\n",
      "Epoch 557/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0278 - val_loss: -4.4449\n",
      "Epoch 558/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2050 - val_loss: -3.8868\n",
      "Epoch 559/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1620 - val_loss: -4.2785\n",
      "Epoch 560/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2636 - val_loss: 1.7131\n",
      "Epoch 561/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1218 - val_loss: -4.2952\n",
      "Epoch 562/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2555 - val_loss: -4.1559\n",
      "Epoch 563/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2582 - val_loss: -4.4478\n",
      "Epoch 564/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3028 - val_loss: -1.5910\n",
      "Epoch 565/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2533 - val_loss: -4.1670\n",
      "Epoch 566/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2441 - val_loss: -4.3598\n",
      "Epoch 567/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1086 - val_loss: -4.4381\n",
      "Epoch 568/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2272 - val_loss: -4.2717\n",
      "Epoch 569/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2487 - val_loss: -3.7532\n",
      "Epoch 570/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2794 - val_loss: -3.3389\n",
      "Epoch 571/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1250 - val_loss: -4.6568\n",
      "Epoch 572/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2634 - val_loss: -4.1291\n",
      "Epoch 573/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3000 - val_loss: -3.6525\n",
      "Epoch 574/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3049 - val_loss: -1.4222\n",
      "Epoch 575/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3093 - val_loss: -4.7309\n",
      "Epoch 576/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2270 - val_loss: -3.4155\n",
      "Epoch 577/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3018 - val_loss: -4.0781\n",
      "Epoch 578/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2624 - val_loss: -3.8604\n",
      "Epoch 579/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3547 - val_loss: -3.9123\n",
      "Epoch 580/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2161 - val_loss: -3.9838\n",
      "Epoch 581/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3105 - val_loss: -4.4489\n",
      "Epoch 582/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3362 - val_loss: -3.9870\n",
      "Epoch 583/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2682 - val_loss: -4.5474\n",
      "Epoch 584/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3901 - val_loss: -2.5403\n",
      "Epoch 585/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1490 - val_loss: -4.0973\n",
      "Epoch 586/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3461 - val_loss: -4.5730\n",
      "Epoch 587/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3322 - val_loss: -3.8536\n",
      "Epoch 588/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3291 - val_loss: -3.0619\n",
      "Epoch 589/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2885 - val_loss: -3.8004\n",
      "Epoch 590/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3142 - val_loss: -4.4836\n",
      "Epoch 591/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2437 - val_loss: -3.5454\n",
      "Epoch 592/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2806 - val_loss: -4.0590\n",
      "Epoch 593/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3213 - val_loss: -4.1869\n",
      "Epoch 594/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3347 - val_loss: -4.4711\n",
      "Epoch 595/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2674 - val_loss: -4.2968\n",
      "Epoch 596/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2524 - val_loss: -4.7052\n",
      "Epoch 597/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4173 - val_loss: -2.8739\n",
      "Epoch 598/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3545 - val_loss: -4.6084\n",
      "Epoch 599/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3089 - val_loss: -2.2538\n",
      "Epoch 600/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3472 - val_loss: -4.4768\n",
      "Epoch 601/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3171 - val_loss: -4.5451\n",
      "Epoch 602/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3689 - val_loss: -4.3895\n",
      "Epoch 603/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3882 - val_loss: -4.6498\n",
      "Epoch 604/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3709 - val_loss: -4.5697\n",
      "Epoch 605/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2861 - val_loss: -3.5038\n",
      "Epoch 606/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3353 - val_loss: -4.4422\n",
      "Epoch 607/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2889 - val_loss: -4.2592\n",
      "Epoch 608/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4261 - val_loss: -2.8847\n",
      "Epoch 609/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3136 - val_loss: -4.4901\n",
      "Epoch 610/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3355 - val_loss: -4.5731\n",
      "Epoch 611/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3746 - val_loss: -3.9385\n",
      "Epoch 612/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4169 - val_loss: -4.1577\n",
      "Epoch 613/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2877 - val_loss: -4.6189\n",
      "Epoch 614/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3799 - val_loss: -4.5245\n",
      "Epoch 615/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2202 - val_loss: -1.8670\n",
      "Epoch 616/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3579 - val_loss: -4.1610\n",
      "Epoch 617/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3864 - val_loss: -4.0903\n",
      "Epoch 618/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3681 - val_loss: -3.6232\n",
      "Epoch 619/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2125 - val_loss: -4.5927\n",
      "Epoch 620/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3405 - val_loss: -4.7947\n",
      "Epoch 621/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2948 - val_loss: -2.9800\n",
      "Epoch 622/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3328 - val_loss: -4.7066\n",
      "Epoch 623/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3607 - val_loss: -4.1586\n",
      "Epoch 624/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3481 - val_loss: -4.6275\n",
      "Epoch 625/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3534 - val_loss: -4.3638\n",
      "Epoch 626/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3284 - val_loss: -2.3014\n",
      "Epoch 627/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3578 - val_loss: -4.0570\n",
      "Epoch 628/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4103 - val_loss: -4.1949\n",
      "Epoch 629/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3120 - val_loss: -4.2523\n",
      "Epoch 630/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3791 - val_loss: -4.4819\n",
      "Epoch 631/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2735 - val_loss: -4.3309\n",
      "Epoch 632/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4324 - val_loss: -4.4981\n",
      "Epoch 633/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1490 - val_loss: -3.7340\n",
      "Epoch 634/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4202 - val_loss: -3.7947\n",
      "Epoch 635/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3387 - val_loss: -4.5323\n",
      "Epoch 636/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4269 - val_loss: -4.2527\n",
      "Epoch 637/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3816 - val_loss: -4.2257\n",
      "Epoch 638/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3297 - val_loss: -4.5385\n",
      "Epoch 639/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2836 - val_loss: -4.2186\n",
      "Epoch 640/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6467 - val_loss: -4.4755\n",
      "Epoch 641/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0113 - val_loss: -4.5084\n",
      "Epoch 642/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0598 - val_loss: -3.8089\n",
      "Epoch 643/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1919 - val_loss: -3.9607\n",
      "Epoch 644/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1138 - val_loss: -3.6627\n",
      "Epoch 645/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2361 - val_loss: -3.6963\n",
      "Epoch 646/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0031 - val_loss: -4.4259\n",
      "Epoch 647/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3102 - val_loss: -3.9014\n",
      "Epoch 648/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2980 - val_loss: -3.4723\n",
      "Epoch 649/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2475 - val_loss: -3.2871\n",
      "Epoch 650/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2838 - val_loss: -4.5117\n",
      "Epoch 651/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1815 - val_loss: -4.4885\n",
      "Epoch 652/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3856 - val_loss: -4.1886\n",
      "Epoch 653/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2824 - val_loss: -3.8968\n",
      "Epoch 654/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2000 - val_loss: -4.3730\n",
      "Epoch 655/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1783 - val_loss: -4.6407\n",
      "Epoch 656/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2621 - val_loss: -4.2039\n",
      "Epoch 657/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3613 - val_loss: -4.8338\n",
      "Epoch 658/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2928 - val_loss: -4.0631\n",
      "Epoch 659/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3027 - val_loss: -4.1269\n",
      "Epoch 660/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2204 - val_loss: -4.0652\n",
      "Epoch 661/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9118 - val_loss: -4.3607\n",
      "Epoch 662/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0811 - val_loss: -4.5455\n",
      "Epoch 663/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3532 - val_loss: -4.3839\n",
      "Epoch 664/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3313 - val_loss: -4.6141\n",
      "Epoch 665/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3657 - val_loss: -4.4437\n",
      "Epoch 666/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1505 - val_loss: -3.9788\n",
      "Epoch 667/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3203 - val_loss: -3.0734\n",
      "Epoch 668/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3794 - val_loss: -3.4631\n",
      "Epoch 669/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2664 - val_loss: -3.5940\n",
      "Epoch 670/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3847 - val_loss: -4.1128\n",
      "Epoch 671/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3083 - val_loss: -4.3451\n",
      "Epoch 672/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3579 - val_loss: -4.0730\n",
      "Epoch 673/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2281 - val_loss: -4.2455\n",
      "Epoch 674/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4113 - val_loss: -4.4577\n",
      "Epoch 675/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3452 - val_loss: -4.5311\n",
      "Epoch 676/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3080 - val_loss: -1.0939\n",
      "Epoch 677/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1235 - val_loss: -4.2409\n",
      "Epoch 678/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3286 - val_loss: -4.5415\n",
      "Epoch 679/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3298 - val_loss: -3.4985\n",
      "Epoch 680/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2087 - val_loss: -4.0330\n",
      "Epoch 681/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2592 - val_loss: -3.6042\n",
      "Epoch 682/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4699 - val_loss: -3.9167\n",
      "Epoch 683/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1638 - val_loss: -4.4693\n",
      "Epoch 684/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4032 - val_loss: -4.4329\n",
      "Epoch 685/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3519 - val_loss: -4.3100\n",
      "Epoch 686/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3821 - val_loss: -3.9101\n",
      "Epoch 687/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4075 - val_loss: -3.0081\n",
      "Epoch 688/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2315 - val_loss: -4.3928\n",
      "Epoch 689/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2859 - val_loss: -4.4783\n",
      "Epoch 690/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4078 - val_loss: -4.3170\n",
      "Epoch 691/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2716 - val_loss: -4.7286\n",
      "Epoch 692/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4396 - val_loss: -2.5809\n",
      "Epoch 693/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2221 - val_loss: -4.5857\n",
      "Epoch 694/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3361 - val_loss: -3.7603\n",
      "Epoch 695/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2845 - val_loss: -3.6651\n",
      "Epoch 696/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3964 - val_loss: -4.6655\n",
      "Epoch 697/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3594 - val_loss: -0.1881\n",
      "Epoch 698/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3330 - val_loss: -4.3816\n",
      "Epoch 699/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.8482 - val_loss: -4.1728\n",
      "Epoch 700/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4373 - val_loss: -4.1726\n",
      "Epoch 701/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8324 - val_loss: -1.4921\n",
      "Epoch 702/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9689 - val_loss: -4.3699\n",
      "Epoch 703/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0303 - val_loss: -4.5540\n",
      "Epoch 704/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1222 - val_loss: -4.0287\n",
      "Epoch 705/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1812 - val_loss: -1.8269\n",
      "Epoch 706/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2930 - val_loss: -3.8730\n",
      "Epoch 707/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1996 - val_loss: -4.4762\n",
      "Epoch 708/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2652 - val_loss: -4.0943\n",
      "Epoch 709/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2079 - val_loss: -3.7633\n",
      "Epoch 710/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3709 - val_loss: -4.5757\n",
      "Epoch 711/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2441 - val_loss: -4.6341\n",
      "Epoch 712/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2839 - val_loss: -4.1247\n",
      "Epoch 713/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3379 - val_loss: -4.1926\n",
      "Epoch 714/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0715 - val_loss: -3.5318\n",
      "Epoch 715/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9873 - val_loss: -4.2732\n",
      "Epoch 716/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5357 - val_loss: -3.6874\n",
      "Epoch 717/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7134 - val_loss: -3.7637\n",
      "Epoch 718/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8768 - val_loss: -4.5301\n",
      "Epoch 719/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9727 - val_loss: -4.2550\n",
      "Epoch 720/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9762 - val_loss: -4.2190\n",
      "Epoch 721/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1439 - val_loss: -4.3580\n",
      "Epoch 722/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2275 - val_loss: -4.3281\n",
      "Epoch 723/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1753 - val_loss: -4.7048\n",
      "Epoch 724/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2948 - val_loss: -3.6000\n",
      "Epoch 725/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2008 - val_loss: -4.3540\n",
      "Epoch 726/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2419 - val_loss: -4.1469\n",
      "Epoch 727/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2639 - val_loss: -4.6296\n",
      "Epoch 728/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1913 - val_loss: -4.5267\n",
      "Epoch 729/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2588 - val_loss: -4.0743\n",
      "Epoch 730/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2986 - val_loss: -3.8347\n",
      "Epoch 731/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3138 - val_loss: -3.6009\n",
      "Epoch 732/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2946 - val_loss: -4.1183\n",
      "Epoch 733/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3410 - val_loss: -4.0607\n",
      "Epoch 734/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3104 - val_loss: -2.9599\n",
      "Epoch 735/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3728 - val_loss: -4.3018\n",
      "Epoch 736/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2447 - val_loss: -4.2080\n",
      "Epoch 737/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3261 - val_loss: -4.6213\n",
      "Epoch 738/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3954 - val_loss: -3.8739\n",
      "Epoch 739/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9468 - val_loss: -3.7302\n",
      "Epoch 740/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4053 - val_loss: -1.0209\n",
      "Epoch 741/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2930 - val_loss: -3.9322\n",
      "Epoch 742/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3617 - val_loss: -3.3644\n",
      "Epoch 743/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3987 - val_loss: -4.6509\n",
      "Epoch 744/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9613 - val_loss: -4.2138\n",
      "Epoch 745/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1437 - val_loss: -4.5781\n",
      "Epoch 746/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2971 - val_loss: -4.5210\n",
      "Epoch 747/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0828 - val_loss: -4.3407\n",
      "Epoch 748/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2964 - val_loss: -4.5490\n",
      "Epoch 749/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3759 - val_loss: -4.4504\n",
      "Epoch 750/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3199 - val_loss: -4.5235\n",
      "Epoch 751/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4034 - val_loss: -4.3799\n",
      "Epoch 752/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3659 - val_loss: -4.5617\n",
      "Epoch 753/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3995 - val_loss: 1.4472\n",
      "Epoch 754/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6599 - val_loss: -3.7930\n",
      "Epoch 755/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3175 - val_loss: -4.5436\n",
      "Epoch 756/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3485 - val_loss: -3.7970\n",
      "Epoch 757/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1907 - val_loss: -4.7776\n",
      "Epoch 758/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2561 - val_loss: -2.6166\n",
      "Epoch 759/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2712 - val_loss: -4.4396\n",
      "Epoch 760/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3766 - val_loss: -4.0975\n",
      "Epoch 761/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2067 - val_loss: -3.5499\n",
      "Epoch 762/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4354 - val_loss: -4.1907\n",
      "Epoch 763/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3415 - val_loss: -4.6390\n",
      "Epoch 764/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3708 - val_loss: -4.7247\n",
      "Epoch 765/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3239 - val_loss: -3.4736\n",
      "Epoch 766/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4324 - val_loss: -4.5541\n",
      "Epoch 767/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6374 - val_loss: -4.4171\n",
      "Epoch 768/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0786 - val_loss: -4.5298\n",
      "Epoch 769/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2821 - val_loss: -4.5952\n",
      "Epoch 770/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3029 - val_loss: -4.4219\n",
      "Epoch 771/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2723 - val_loss: -3.8927\n",
      "Epoch 772/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4039 - val_loss: -4.1469\n",
      "Epoch 773/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2934 - val_loss: -4.5847\n",
      "Epoch 774/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3844 - val_loss: -4.0615\n",
      "Epoch 775/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3174 - val_loss: -4.2124\n",
      "Epoch 776/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3170 - val_loss: -4.5080\n",
      "Epoch 777/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4543 - val_loss: -4.7559\n",
      "Epoch 778/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3225 - val_loss: -4.0576\n",
      "Epoch 779/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0097 - val_loss: -4.6796\n",
      "Epoch 780/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3319 - val_loss: -3.9286\n",
      "Epoch 781/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2965 - val_loss: -4.6324\n",
      "Epoch 782/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3938 - val_loss: -4.5183\n",
      "Epoch 783/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1664 - val_loss: -3.5699\n",
      "Epoch 784/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.4554"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -5.4554 - val_loss: -4.8786\n",
      "Epoch 785/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3963 - val_loss: -4.3973\n",
      "Epoch 786/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3716 - val_loss: -4.5333\n",
      "Epoch 787/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3579 - val_loss: -3.5944\n",
      "Epoch 788/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3968 - val_loss: -2.7597\n",
      "Epoch 789/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3858 - val_loss: -4.6142\n",
      "Epoch 790/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4229 - val_loss: -4.3696\n",
      "Epoch 791/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4041 - val_loss: -4.1862\n",
      "Epoch 792/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4117 - val_loss: -4.0353\n",
      "Epoch 793/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1364 - val_loss: -4.3919\n",
      "Epoch 794/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2999 - val_loss: -4.8306\n",
      "Epoch 795/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3479 - val_loss: -3.4330\n",
      "Epoch 796/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3852 - val_loss: -4.6897\n",
      "Epoch 797/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3322 - val_loss: -4.5389\n",
      "Epoch 798/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3411 - val_loss: -4.3126\n",
      "Epoch 799/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2720 - val_loss: -4.3400\n",
      "Epoch 800/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4289 - val_loss: -1.7189\n",
      "Epoch 801/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3433 - val_loss: -4.5596\n",
      "Epoch 802/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4397 - val_loss: -3.7671\n",
      "Epoch 803/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4584 - val_loss: -1.6247\n",
      "Epoch 804/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2829 - val_loss: -4.8696\n",
      "Epoch 805/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2579 - val_loss: -4.5286\n",
      "Epoch 806/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2968 - val_loss: -2.8989\n",
      "Epoch 807/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4327 - val_loss: -4.6514\n",
      "Epoch 808/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3959 - val_loss: -4.3813\n",
      "Epoch 809/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4069 - val_loss: -4.3504\n",
      "Epoch 810/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4029 - val_loss: -2.4470\n",
      "Epoch 811/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4046 - val_loss: -4.2831\n",
      "Epoch 812/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4468 - val_loss: -4.2239\n",
      "Epoch 813/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4513 - val_loss: -2.4335\n",
      "Epoch 814/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4407 - val_loss: -3.3180\n",
      "Epoch 815/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3947 - val_loss: -4.2780\n",
      "Epoch 816/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3849 - val_loss: -4.2722\n",
      "Epoch 817/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3904 - val_loss: -4.5604\n",
      "Epoch 818/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3811 - val_loss: -4.2291\n",
      "Epoch 819/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4177 - val_loss: -4.7148\n",
      "Epoch 820/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3688 - val_loss: -4.1318\n",
      "Epoch 821/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4541 - val_loss: -3.9019\n",
      "Epoch 822/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2640 - val_loss: -4.2855\n",
      "Epoch 823/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4238 - val_loss: -3.8931\n",
      "Epoch 824/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4201 - val_loss: -4.1997\n",
      "Epoch 825/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3736 - val_loss: -4.4289\n",
      "Epoch 826/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5017 - val_loss: -4.3221\n",
      "Epoch 827/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4141 - val_loss: -4.4461\n",
      "Epoch 828/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4232 - val_loss: -3.9038\n",
      "Epoch 829/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2329 - val_loss: -3.3845\n",
      "Epoch 830/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3868 - val_loss: -3.6873\n",
      "Epoch 831/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4769 - val_loss: -2.2298\n",
      "Epoch 832/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2474 - val_loss: -4.4815\n",
      "Epoch 833/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4011 - val_loss: -4.5247\n",
      "Epoch 834/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4223 - val_loss: -4.2570\n",
      "Epoch 835/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4266 - val_loss: -4.0353\n",
      "Epoch 836/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3625 - val_loss: -4.2271\n",
      "Epoch 837/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4682 - val_loss: -4.4092\n",
      "Epoch 838/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3777 - val_loss: -4.4305\n",
      "Epoch 839/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3665 - val_loss: -2.9402\n",
      "Epoch 840/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2098 - val_loss: -4.2641\n",
      "Epoch 841/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4664 - val_loss: -4.5747\n",
      "Epoch 842/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4392 - val_loss: -4.5603\n",
      "Epoch 843/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4369 - val_loss: -3.9280\n",
      "Epoch 844/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3403 - val_loss: -2.9765\n",
      "Epoch 845/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9222 - val_loss: -3.7722\n",
      "Epoch 846/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6810 - val_loss: -3.9735\n",
      "Epoch 847/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1648 - val_loss: -4.6545\n",
      "Epoch 848/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2391 - val_loss: -3.7692\n",
      "Epoch 849/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3239 - val_loss: -4.0800\n",
      "Epoch 850/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2437 - val_loss: -4.5289\n",
      "Epoch 851/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3947 - val_loss: -3.9486\n",
      "Epoch 852/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3704 - val_loss: -3.3761\n",
      "Epoch 853/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3874 - val_loss: -4.4554\n",
      "Epoch 854/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3596 - val_loss: -4.6868\n",
      "Epoch 855/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1882 - val_loss: -4.3865\n",
      "Epoch 856/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4113 - val_loss: -4.7927\n",
      "Epoch 857/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3354 - val_loss: -3.8180\n",
      "Epoch 858/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4064 - val_loss: -2.8692\n",
      "Epoch 859/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3494 - val_loss: -3.7061\n",
      "Epoch 860/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4758 - val_loss: -4.5754\n",
      "Epoch 861/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3680 - val_loss: -4.6367\n",
      "Epoch 862/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3467 - val_loss: -4.7765\n",
      "Epoch 863/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3625 - val_loss: -3.5645\n",
      "Epoch 864/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4248 - val_loss: -4.4123\n",
      "Epoch 865/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3358 - val_loss: -3.8916\n",
      "Epoch 866/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2292 - val_loss: 1.1168\n",
      "Epoch 867/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4103 - val_loss: -0.5250\n",
      "Epoch 868/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3304 - val_loss: -4.6228\n",
      "Epoch 869/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3710 - val_loss: -4.6276\n",
      "Epoch 870/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3989 - val_loss: -4.3984\n",
      "Epoch 871/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3801 - val_loss: -3.6503\n",
      "Epoch 872/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3772 - val_loss: -4.0505\n",
      "Epoch 873/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4009 - val_loss: 0.3908\n",
      "Epoch 874/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4398 - val_loss: -4.3849\n",
      "Epoch 875/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3955 - val_loss: -3.5009\n",
      "Epoch 876/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4260 - val_loss: -3.7843\n",
      "Epoch 877/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4380 - val_loss: -3.8170\n",
      "Epoch 878/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2402 - val_loss: -4.7256\n",
      "Epoch 879/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4161 - val_loss: -4.7663\n",
      "Epoch 880/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4498 - val_loss: -4.6705\n",
      "Epoch 881/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5068 - val_loss: -0.4964\n",
      "Epoch 882/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3726 - val_loss: -4.5654\n",
      "Epoch 883/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4555 - val_loss: -3.8074\n",
      "Epoch 884/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3662 - val_loss: -4.1023\n",
      "Epoch 885/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3922 - val_loss: -4.4330\n",
      "Epoch 886/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4236 - val_loss: -4.5320\n",
      "Epoch 887/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3676 - val_loss: -3.9695\n",
      "Epoch 888/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4397 - val_loss: -4.6609\n",
      "Epoch 889/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3650 - val_loss: -4.6608\n",
      "Epoch 890/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3928 - val_loss: -2.7685\n",
      "Epoch 891/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4251 - val_loss: -4.0524\n",
      "Epoch 892/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3688 - val_loss: -4.3330\n",
      "Epoch 893/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3280 - val_loss: -4.0380\n",
      "Epoch 894/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4674 - val_loss: -3.3923\n",
      "Epoch 895/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4492 - val_loss: -4.4286\n",
      "Epoch 896/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4096 - val_loss: -4.7253\n",
      "Epoch 897/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3862 - val_loss: -4.4084\n",
      "Epoch 898/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5435 - val_loss: -4.4688\n",
      "Epoch 899/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5253 - val_loss: -3.9509\n",
      "Epoch 900/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4074 - val_loss: -4.0617\n",
      "Epoch 901/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3583 - val_loss: -4.5563\n",
      "Epoch 902/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4557 - val_loss: -4.6724\n",
      "Epoch 903/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4193 - val_loss: -4.4738\n",
      "Epoch 904/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4234 - val_loss: -3.5769\n",
      "Epoch 905/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4073 - val_loss: -4.1604\n",
      "Epoch 906/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5195 - val_loss: -3.3839\n",
      "Epoch 907/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1596 - val_loss: -4.5712\n",
      "Epoch 908/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8951 - val_loss: -4.4531\n",
      "Epoch 909/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3519 - val_loss: -4.4448\n",
      "Epoch 910/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3903 - val_loss: -4.1980\n",
      "Epoch 911/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4962 - val_loss: -4.1736\n",
      "Epoch 912/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3345 - val_loss: -4.4878\n",
      "Epoch 913/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3315 - val_loss: -4.3906\n",
      "Epoch 914/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3337 - val_loss: -4.0056\n",
      "Epoch 915/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4894 - val_loss: -4.3377\n",
      "Epoch 916/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4010 - val_loss: -3.4083\n",
      "Epoch 917/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9648 - val_loss: -4.1011\n",
      "Epoch 918/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1497 - val_loss: -2.4578\n",
      "Epoch 919/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2400 - val_loss: -4.2428\n",
      "Epoch 920/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2847 - val_loss: -4.4519\n",
      "Epoch 921/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2615 - val_loss: -4.6709\n",
      "Epoch 922/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4343 - val_loss: -4.7252\n",
      "Epoch 923/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3667 - val_loss: -2.6827\n",
      "Epoch 924/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2387 - val_loss: -4.6339\n",
      "Epoch 925/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2991 - val_loss: -3.7256\n",
      "Epoch 926/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1635 - val_loss: -4.0932\n",
      "Epoch 927/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4460 - val_loss: -3.9205\n",
      "Epoch 928/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4007 - val_loss: -4.2816\n",
      "Epoch 929/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4191 - val_loss: -3.9550\n",
      "Epoch 930/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5007 - val_loss: -4.0772\n",
      "Epoch 931/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.4570 - val_loss: -4.6679\n",
      "Epoch 932/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3308 - val_loss: -4.7355\n",
      "Epoch 933/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4291 - val_loss: -3.2336\n",
      "Epoch 934/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2881 - val_loss: -4.3988\n",
      "Epoch 935/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4668 - val_loss: -4.3240\n",
      "Epoch 936/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4040 - val_loss: -4.4246\n",
      "Epoch 937/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3408 - val_loss: -4.6232\n",
      "Epoch 938/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3673 - val_loss: -3.3961\n",
      "Epoch 939/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4849 - val_loss: -3.9994\n",
      "Epoch 940/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4121 - val_loss: -4.6908\n",
      "Epoch 941/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4485 - val_loss: -3.7168\n",
      "Epoch 942/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4564 - val_loss: -4.1075\n",
      "Epoch 943/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4695 - val_loss: -4.4458\n",
      "Epoch 944/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3589 - val_loss: -3.3868\n",
      "Epoch 945/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3818 - val_loss: -4.5924\n",
      "Epoch 946/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3252 - val_loss: -4.4742\n",
      "Epoch 947/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4606 - val_loss: -1.4241\n",
      "Epoch 948/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3817 - val_loss: -4.2144\n",
      "Epoch 949/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4069 - val_loss: -3.1823\n",
      "Epoch 950/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4681 - val_loss: -3.5169\n",
      "Epoch 951/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4770 - val_loss: -4.4642\n",
      "Epoch 952/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4003 - val_loss: -4.6266\n",
      "Epoch 953/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1492 - val_loss: -3.8124\n",
      "Epoch 954/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4210 - val_loss: -3.5279\n",
      "Epoch 955/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3945 - val_loss: -4.1016\n",
      "Epoch 956/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3079 - val_loss: -4.3890\n",
      "Epoch 957/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4822 - val_loss: -4.7053\n",
      "Epoch 958/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3980 - val_loss: -4.3986\n",
      "Epoch 959/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4331 - val_loss: -4.7290\n",
      "Epoch 960/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4395 - val_loss: -3.5549\n",
      "Epoch 961/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2955 - val_loss: -3.2802\n",
      "Epoch 962/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3977 - val_loss: -4.3076\n",
      "Epoch 963/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4812 - val_loss: -3.9002\n",
      "Epoch 964/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4448 - val_loss: -2.7556\n",
      "Epoch 965/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4624 - val_loss: -4.0956\n",
      "Epoch 966/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3310 - val_loss: -3.1857\n",
      "Epoch 967/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0149 - val_loss: -3.9698\n",
      "Epoch 968/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2968 - val_loss: -4.5367\n",
      "Epoch 969/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2933 - val_loss: -4.6941\n",
      "Epoch 970/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3881 - val_loss: -4.4463\n",
      "Epoch 971/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4273 - val_loss: -4.6370\n",
      "Epoch 972/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3400 - val_loss: -4.1330\n",
      "Epoch 973/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4668 - val_loss: -4.1924\n",
      "Epoch 974/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3843 - val_loss: -4.4864\n",
      "Epoch 975/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0382 - val_loss: -4.2001\n",
      "Epoch 976/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3228 - val_loss: -4.3664\n",
      "Epoch 977/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1351 - val_loss: -4.1647\n",
      "Epoch 978/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4510 - val_loss: -2.5833\n",
      "Epoch 979/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4501 - val_loss: -3.7405\n",
      "Epoch 980/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2236 - val_loss: -2.6368\n",
      "Epoch 981/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3845 - val_loss: -4.6045\n",
      "Epoch 982/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4954 - val_loss: -3.6617\n",
      "Epoch 983/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3373 - val_loss: -3.0588\n",
      "Epoch 984/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.4723"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 243ms/step - loss: -5.4723 - val_loss: -4.8818\n",
      "Epoch 985/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4698 - val_loss: -4.4814\n",
      "Epoch 986/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2420 - val_loss: -4.3724\n",
      "Epoch 987/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4291 - val_loss: -4.0049\n",
      "Epoch 988/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5041 - val_loss: -4.3799\n",
      "Epoch 989/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4632 - val_loss: -0.5276\n",
      "Epoch 990/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1877 - val_loss: -3.6920\n",
      "Epoch 991/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3310 - val_loss: -3.9021\n",
      "Epoch 992/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3687 - val_loss: -4.7288\n",
      "Epoch 993/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3845 - val_loss: -4.7886\n",
      "Epoch 994/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5036 - val_loss: -3.9713\n",
      "Epoch 995/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3195 - val_loss: -3.4719\n",
      "Epoch 996/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4640 - val_loss: -3.6216\n",
      "Epoch 997/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4570 - val_loss: -0.4446\n",
      "Epoch 998/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3246 - val_loss: -3.3031\n",
      "Epoch 999/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4730 - val_loss: -4.5867\n",
      "Epoch 1000/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5153 - val_loss: -2.1647\n",
      "Epoch 1001/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4538 - val_loss: -4.8344\n",
      "Epoch 1002/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4258 - val_loss: -2.9569\n",
      "Epoch 1003/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4730 - val_loss: -3.9603\n",
      "Epoch 1004/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4530 - val_loss: -2.3070\n",
      "Epoch 1005/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4771 - val_loss: -1.1852\n",
      "Epoch 1006/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3496 - val_loss: -4.4735\n",
      "Epoch 1007/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3693 - val_loss: -4.0734\n",
      "Epoch 1008/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5134 - val_loss: -4.3399\n",
      "Epoch 1009/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3499 - val_loss: -4.1178\n",
      "Epoch 1010/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5070 - val_loss: -3.3252\n",
      "Epoch 1011/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1639 - val_loss: -4.2720\n",
      "Epoch 1012/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1965 - val_loss: -3.9389\n",
      "Epoch 1013/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4774 - val_loss: -4.4426\n",
      "Epoch 1014/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4166 - val_loss: -4.1402\n",
      "Epoch 1015/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4323 - val_loss: -3.3449\n",
      "Epoch 1016/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3980 - val_loss: -3.4936\n",
      "Epoch 1017/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4275 - val_loss: -3.4734\n",
      "Epoch 1018/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4763 - val_loss: -3.9854\n",
      "Epoch 1019/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3796 - val_loss: -4.7530\n",
      "Epoch 1020/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5427 - val_loss: -2.5983\n",
      "Epoch 1021/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5376 - val_loss: -4.0377\n",
      "Epoch 1022/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4624 - val_loss: -3.5311\n",
      "Epoch 1023/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4186 - val_loss: -4.7881\n",
      "Epoch 1024/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3976 - val_loss: -4.1955\n",
      "Epoch 1025/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5143 - val_loss: -2.5500\n",
      "Epoch 1026/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4155 - val_loss: -4.7400\n",
      "Epoch 1027/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1947 - val_loss: -3.9671\n",
      "Epoch 1028/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4295 - val_loss: -2.6613\n",
      "Epoch 1029/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4722 - val_loss: 1.2249\n",
      "Epoch 1030/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2712 - val_loss: -3.1760\n",
      "Epoch 1031/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5321 - val_loss: -4.5926\n",
      "Epoch 1032/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5584 - val_loss: -2.8117\n",
      "Epoch 1033/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4918 - val_loss: -4.7580\n",
      "Epoch 1034/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3702 - val_loss: -4.3936\n",
      "Epoch 1035/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4591 - val_loss: -4.1380\n",
      "Epoch 1036/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3486 - val_loss: -4.1333\n",
      "Epoch 1037/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4672 - val_loss: -4.3015\n",
      "Epoch 1038/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5036 - val_loss: -4.2817\n",
      "Epoch 1039/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4953 - val_loss: -2.9348\n",
      "Epoch 1040/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3676 - val_loss: -4.5802\n",
      "Epoch 1041/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5341 - val_loss: -4.3305\n",
      "Epoch 1042/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3243 - val_loss: -4.6451\n",
      "Epoch 1043/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5281 - val_loss: -2.4096\n",
      "Epoch 1044/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4300 - val_loss: -4.2294\n",
      "Epoch 1045/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3266 - val_loss: -3.7466\n",
      "Epoch 1046/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3728 - val_loss: -4.7610\n",
      "Epoch 1047/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4872 - val_loss: -4.6797\n",
      "Epoch 1048/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4857 - val_loss: -4.0656\n",
      "Epoch 1049/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5306 - val_loss: -3.1327\n",
      "Epoch 1050/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3490 - val_loss: -4.6091\n",
      "Epoch 1051/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5263 - val_loss: -4.2358\n",
      "Epoch 1052/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4648 - val_loss: -4.0096\n",
      "Epoch 1053/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4887 - val_loss: -4.4412\n",
      "Epoch 1054/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5423 - val_loss: -2.4494\n",
      "Epoch 1055/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4117 - val_loss: -3.1878\n",
      "Epoch 1056/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4877 - val_loss: -2.8056\n",
      "Epoch 1057/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2817 - val_loss: -4.3068\n",
      "Epoch 1058/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5328 - val_loss: 1.1905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1059/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4177 - val_loss: -4.1758\n",
      "Epoch 1060/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4963 - val_loss: -4.8218\n",
      "Epoch 1061/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4363 - val_loss: -4.3410\n",
      "Epoch 1062/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5725 - val_loss: -3.4225\n",
      "Epoch 1063/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5637 - val_loss: -2.2151\n",
      "Epoch 1064/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4274 - val_loss: -3.9192\n",
      "Epoch 1065/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4082 - val_loss: -4.5075\n",
      "Epoch 1066/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4678 - val_loss: -3.5393\n",
      "Epoch 1067/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4945 - val_loss: -2.0707\n",
      "Epoch 1068/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4638 - val_loss: -4.4071\n",
      "Epoch 1069/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5015 - val_loss: -2.8020\n",
      "Epoch 1070/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4955 - val_loss: -4.6471\n",
      "Epoch 1071/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4698 - val_loss: -2.3436\n",
      "Epoch 1072/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4989 - val_loss: -3.5636\n",
      "Epoch 1073/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4667 - val_loss: -3.0861\n",
      "Epoch 1074/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4181 - val_loss: -4.4811\n",
      "Epoch 1075/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5402 - val_loss: -3.5504\n",
      "Epoch 1076/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4733 - val_loss: -3.7116\n",
      "Epoch 1077/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5581 - val_loss: -4.2828\n",
      "Epoch 1078/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4978 - val_loss: -1.2823\n",
      "Epoch 1079/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4656 - val_loss: -4.2275\n",
      "Epoch 1080/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3877 - val_loss: -4.4351\n",
      "Epoch 1081/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2107 - val_loss: -3.7995\n",
      "Epoch 1082/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3223 - val_loss: -3.5124\n",
      "Epoch 1083/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3825 - val_loss: -4.1577\n",
      "Epoch 1084/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4740 - val_loss: -3.4560\n",
      "Epoch 1085/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4488 - val_loss: -4.6044\n",
      "Epoch 1086/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4147 - val_loss: -4.0122\n",
      "Epoch 1087/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4747 - val_loss: -4.3543\n",
      "Epoch 1088/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3931 - val_loss: -4.5917\n",
      "Epoch 1089/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4210 - val_loss: -4.5621\n",
      "Epoch 1090/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5253 - val_loss: -3.2459\n",
      "Epoch 1091/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7755 - val_loss: -4.6859\n",
      "Epoch 1092/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3214 - val_loss: -1.0463\n",
      "Epoch 1093/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3941 - val_loss: -3.6390\n",
      "Epoch 1094/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3699 - val_loss: -2.2998\n",
      "Epoch 1095/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2808 - val_loss: -3.0296\n",
      "Epoch 1096/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3519 - val_loss: 1.0086\n",
      "Epoch 1097/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2659 - val_loss: -4.4018\n",
      "Epoch 1098/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3935 - val_loss: -3.6023\n",
      "Epoch 1099/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4483 - val_loss: -0.7519\n",
      "Epoch 1100/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3099 - val_loss: -4.1809\n",
      "Epoch 1101/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5325 - val_loss: -3.9034\n",
      "Epoch 1102/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4113 - val_loss: -4.5971\n",
      "Epoch 1103/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4943 - val_loss: -4.6849\n",
      "Epoch 1104/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4727 - val_loss: -3.3529\n",
      "Epoch 1105/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4702 - val_loss: -3.1361\n",
      "Epoch 1106/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3926 - val_loss: -4.4332\n",
      "Epoch 1107/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5160 - val_loss: -4.6199\n",
      "Epoch 1108/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3190 - val_loss: -4.4568\n",
      "Epoch 1109/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4581 - val_loss: -4.0972\n",
      "Epoch 1110/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4928 - val_loss: -4.0209\n",
      "Epoch 1111/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5784 - val_loss: -4.1884\n",
      "Epoch 1112/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4049 - val_loss: -4.0216\n",
      "Epoch 1113/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5127 - val_loss: -4.4357\n",
      "Epoch 1114/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4585 - val_loss: -4.6123\n",
      "Epoch 1115/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2409 - val_loss: -4.2758\n",
      "Epoch 1116/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4751 - val_loss: -4.4778\n",
      "Epoch 1117/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3546 - val_loss: -4.7048\n",
      "Epoch 1118/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0641 - val_loss: -3.8707\n",
      "Epoch 1119/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4596 - val_loss: -4.6663\n",
      "Epoch 1120/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4514 - val_loss: -4.5443\n",
      "Epoch 1121/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4604 - val_loss: -2.3500\n",
      "Epoch 1122/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3656 - val_loss: -4.7194\n",
      "Epoch 1123/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4248 - val_loss: -4.5729\n",
      "Epoch 1124/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3992 - val_loss: -3.4961\n",
      "Epoch 1125/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4861 - val_loss: -2.9356\n",
      "Epoch 1126/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5561 - val_loss: -3.9637\n",
      "Epoch 1127/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3899 - val_loss: -4.6942\n",
      "Epoch 1128/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4805 - val_loss: -2.2869\n",
      "Epoch 1129/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5337 - val_loss: -3.8278\n",
      "Epoch 1130/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5636 - val_loss: -4.4114\n",
      "Epoch 1131/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4919 - val_loss: -4.5337\n",
      "Epoch 1132/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3846 - val_loss: -4.4051\n",
      "Epoch 1133/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5116 - val_loss: -3.1757\n",
      "Epoch 1134/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7329 - val_loss: -4.4823\n",
      "Epoch 1135/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3474 - val_loss: -3.5284\n",
      "Epoch 1136/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4488 - val_loss: -3.5432\n",
      "Epoch 1137/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3936 - val_loss: -4.4359\n",
      "Epoch 1138/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4812 - val_loss: -3.5857\n",
      "Epoch 1139/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4533 - val_loss: -3.7887\n",
      "Epoch 1140/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3343 - val_loss: -4.4720\n",
      "Epoch 1141/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3505 - val_loss: -4.2688\n",
      "Epoch 1142/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4897 - val_loss: -4.1386\n",
      "Epoch 1143/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3659 - val_loss: -3.6736\n",
      "Epoch 1144/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4878 - val_loss: -3.9759\n",
      "Epoch 1145/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2998 - val_loss: -4.8650\n",
      "Epoch 1146/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5025 - val_loss: -4.3925\n",
      "Epoch 1147/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5163 - val_loss: -4.1036\n",
      "Epoch 1148/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4940 - val_loss: -3.7769\n",
      "Epoch 1149/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5602 - val_loss: -4.5198\n",
      "Epoch 1150/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5357 - val_loss: -2.8642\n",
      "Epoch 1151/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4343 - val_loss: -4.8456\n",
      "Epoch 1152/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1568 - val_loss: -4.3650\n",
      "Epoch 1153/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3608 - val_loss: -3.8526\n",
      "Epoch 1154/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3337 - val_loss: -4.6538\n",
      "Epoch 1155/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4302 - val_loss: -3.3522\n",
      "Epoch 1156/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3971"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -5.3971 - val_loss: -4.9828\n",
      "Epoch 1157/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4683 - val_loss: -2.6286\n",
      "Epoch 1158/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3101 - val_loss: -4.5021\n",
      "Epoch 1159/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4172 - val_loss: -4.8396\n",
      "Epoch 1160/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4781 - val_loss: -3.8451\n",
      "Epoch 1161/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5235 - val_loss: -3.7529\n",
      "Epoch 1162/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2848 - val_loss: -4.6064\n",
      "Epoch 1163/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5377 - val_loss: -4.0177\n",
      "Epoch 1164/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3104 - val_loss: -3.4171\n",
      "Epoch 1165/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4256 - val_loss: -4.4190\n",
      "Epoch 1166/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4646 - val_loss: -3.7820\n",
      "Epoch 1167/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4404 - val_loss: -2.7125\n",
      "Epoch 1168/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3519 - val_loss: -2.7989\n",
      "Epoch 1169/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4895 - val_loss: -3.6029\n",
      "Epoch 1170/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3520 - val_loss: -4.0945\n",
      "Epoch 1171/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4691 - val_loss: -4.2040\n",
      "Epoch 1172/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5623 - val_loss: -3.5260\n",
      "Epoch 1173/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5341 - val_loss: -4.0329\n",
      "Epoch 1174/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4863 - val_loss: -4.4575\n",
      "Epoch 1175/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4937 - val_loss: -4.2560\n",
      "Epoch 1176/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5422 - val_loss: -3.2126\n",
      "Epoch 1177/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3151 - val_loss: -4.0788\n",
      "Epoch 1178/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4745 - val_loss: -3.2338\n",
      "Epoch 1179/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5465 - val_loss: -4.3365\n",
      "Epoch 1180/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2717 - val_loss: -4.1948\n",
      "Epoch 1181/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4527 - val_loss: -4.3464\n",
      "Epoch 1182/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3554 - val_loss: -4.5176\n",
      "Epoch 1183/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5388 - val_loss: -2.8247\n",
      "Epoch 1184/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3965 - val_loss: -3.4544\n",
      "Epoch 1185/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4854 - val_loss: -4.4385\n",
      "Epoch 1186/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4722 - val_loss: -4.4643\n",
      "Epoch 1187/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3322 - val_loss: -3.4317\n",
      "Epoch 1188/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4879 - val_loss: -4.5972\n",
      "Epoch 1189/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4548 - val_loss: -4.1053\n",
      "Epoch 1190/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4737 - val_loss: -2.6452\n",
      "Epoch 1191/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3169 - val_loss: -4.4408\n",
      "Epoch 1192/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5134 - val_loss: -4.5445\n",
      "Epoch 1193/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4486 - val_loss: -4.7413\n",
      "Epoch 1194/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4857 - val_loss: -4.6792\n",
      "Epoch 1195/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4474 - val_loss: -4.3467\n",
      "Epoch 1196/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5525 - val_loss: -3.6544\n",
      "Epoch 1197/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3414 - val_loss: -2.9929\n",
      "Epoch 1198/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5370 - val_loss: -0.6659\n",
      "Epoch 1199/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3459 - val_loss: -4.1188\n",
      "Epoch 1200/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5670 - val_loss: -3.2769\n",
      "Epoch 1201/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4627 - val_loss: -3.2511\n",
      "Epoch 1202/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5413 - val_loss: -3.0542\n",
      "Epoch 1203/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5411 - val_loss: -3.9282\n",
      "Epoch 1204/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3632 - val_loss: -1.4340\n",
      "Epoch 1205/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5749 - val_loss: -3.3940\n",
      "Epoch 1206/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.5207 - val_loss: -3.0773\n",
      "Epoch 1207/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5222 - val_loss: -4.2860\n",
      "Epoch 1208/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4779 - val_loss: -3.8854\n",
      "Epoch 1209/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4338 - val_loss: -4.1624\n",
      "Epoch 1210/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4184 - val_loss: -3.6723\n",
      "Epoch 1211/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4802 - val_loss: -3.9376\n",
      "Epoch 1212/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4407 - val_loss: -4.6437\n",
      "Epoch 1213/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4572 - val_loss: -4.7878\n",
      "Epoch 1214/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5214 - val_loss: -3.8247\n",
      "Epoch 1215/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4878 - val_loss: -4.5630\n",
      "Epoch 1216/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5167 - val_loss: -3.9309\n",
      "Epoch 1217/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4177 - val_loss: -4.4140\n",
      "Epoch 1218/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5340 - val_loss: -4.1107\n",
      "Epoch 1219/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5335 - val_loss: -4.1806\n",
      "Epoch 1220/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5784 - val_loss: -2.1903\n",
      "Epoch 1221/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3434 - val_loss: -4.4763\n",
      "Epoch 1222/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3417 - val_loss: -4.3323\n",
      "Epoch 1223/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5026 - val_loss: -4.2018\n",
      "Epoch 1224/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4379 - val_loss: -4.7473\n",
      "Epoch 1225/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4335 - val_loss: -4.3655\n",
      "Epoch 1226/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5602 - val_loss: -3.8537\n",
      "Epoch 1227/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5540 - val_loss: -4.2111\n",
      "Epoch 1228/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5296 - val_loss: 0.2017\n",
      "Epoch 1229/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4815 - val_loss: -2.5485\n",
      "Epoch 1230/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4954 - val_loss: -4.6311\n",
      "Epoch 1231/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4824 - val_loss: -3.0437\n",
      "Epoch 1232/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3616 - val_loss: -4.0797\n",
      "Epoch 1233/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4602 - val_loss: -3.7110\n",
      "Epoch 1234/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5398 - val_loss: -4.3509\n",
      "Epoch 1235/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5174 - val_loss: 0.8721\n",
      "Epoch 1236/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5197 - val_loss: -4.1332\n",
      "Epoch 1237/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5446 - val_loss: -4.1744\n",
      "Epoch 1238/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5084 - val_loss: -4.0694\n",
      "Epoch 1239/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4522 - val_loss: -4.2403\n",
      "Epoch 1240/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4786 - val_loss: -4.7125\n",
      "Epoch 1241/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5676 - val_loss: -4.6544\n",
      "Epoch 1242/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4591 - val_loss: -4.3793\n",
      "Epoch 1243/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5603 - val_loss: -3.5944\n",
      "Epoch 1244/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2508 - val_loss: -4.4701\n",
      "Epoch 1245/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4866 - val_loss: -3.8441\n",
      "Epoch 1246/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3844 - val_loss: -4.4922\n",
      "Epoch 1247/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5014 - val_loss: -4.1601\n",
      "Epoch 1248/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5642 - val_loss: -4.1803\n",
      "Epoch 1249/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4938 - val_loss: -4.2274\n",
      "Epoch 1250/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5348 - val_loss: -4.1870\n",
      "Epoch 1251/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5507 - val_loss: -4.8194\n",
      "Epoch 1252/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5315 - val_loss: -3.2811\n",
      "Epoch 1253/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3896 - val_loss: -4.1215\n",
      "Epoch 1254/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5445 - val_loss: -2.5516\n",
      "Epoch 1255/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5073 - val_loss: -3.9682\n",
      "Epoch 1256/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4956 - val_loss: -4.4032\n",
      "Epoch 1257/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4789 - val_loss: -4.1456\n",
      "Epoch 1258/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4840 - val_loss: -3.5165\n",
      "Epoch 1259/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5831 - val_loss: -4.0448\n",
      "Epoch 1260/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4053 - val_loss: -4.4007\n",
      "Epoch 1261/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4685 - val_loss: -3.3850\n",
      "Epoch 1262/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5800 - val_loss: -4.1529\n",
      "Epoch 1263/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5381 - val_loss: 1.7871\n",
      "Epoch 1264/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5581 - val_loss: -3.7629\n",
      "Epoch 1265/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4617 - val_loss: -4.9326\n",
      "Epoch 1266/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5140 - val_loss: -4.1085\n",
      "Epoch 1267/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5326 - val_loss: -4.3545\n",
      "Epoch 1268/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5697 - val_loss: -2.0842\n",
      "Epoch 1269/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5867 - val_loss: -4.5078\n",
      "Epoch 1270/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4321 - val_loss: -4.4624\n",
      "Epoch 1271/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5497 - val_loss: -3.9159\n",
      "Epoch 1272/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5525 - val_loss: -3.1213\n",
      "Epoch 1273/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4498 - val_loss: -4.3352\n",
      "Epoch 1274/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5201 - val_loss: -3.5577\n",
      "Epoch 1275/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4642 - val_loss: -4.6001\n",
      "Epoch 1276/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5965 - val_loss: -4.1501\n",
      "Epoch 1277/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4639 - val_loss: -4.7102\n",
      "Epoch 1278/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4264 - val_loss: -4.5460\n",
      "Epoch 1279/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5529 - val_loss: -3.9282\n",
      "Epoch 1280/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5768 - val_loss: -2.9108\n",
      "Epoch 1281/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5590 - val_loss: -4.2366\n",
      "Epoch 1282/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5505 - val_loss: -3.6725\n",
      "Epoch 1283/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5950 - val_loss: -4.0748\n",
      "Epoch 1284/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3748 - val_loss: -3.6803\n",
      "Epoch 1285/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.2802 - val_loss: -4.3212\n",
      "Epoch 1286/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0331 - val_loss: -4.3808\n",
      "Epoch 1287/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2599 - val_loss: -3.6681\n",
      "Epoch 1288/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4450 - val_loss: -4.2148\n",
      "Epoch 1289/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2380 - val_loss: -4.5708\n",
      "Epoch 1290/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3530 - val_loss: -4.3594\n",
      "Epoch 1291/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5094 - val_loss: -2.8150\n",
      "Epoch 1292/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5064 - val_loss: -4.5919\n",
      "Epoch 1293/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5325 - val_loss: -3.8419\n",
      "Epoch 1294/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4640 - val_loss: -4.4143\n",
      "Epoch 1295/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4420 - val_loss: -4.1647\n",
      "Epoch 1296/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.5339 - val_loss: -2.7315\n",
      "Epoch 1297/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4631 - val_loss: -3.3968\n",
      "Epoch 1298/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5137 - val_loss: -4.5649\n",
      "Epoch 1299/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5384 - val_loss: -4.1499\n",
      "Epoch 1300/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5499 - val_loss: -0.3240\n",
      "Epoch 1301/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4835 - val_loss: -4.1958\n",
      "Epoch 1302/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5512 - val_loss: -4.3471\n",
      "Epoch 1303/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5513 - val_loss: -4.1587\n",
      "Epoch 1304/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5670 - val_loss: -3.5377\n",
      "Epoch 1305/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5557 - val_loss: -3.7503\n",
      "Epoch 1306/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4588 - val_loss: -3.4676\n",
      "Epoch 1307/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5994 - val_loss: -4.2801\n",
      "Epoch 1308/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2362 - val_loss: -4.4089\n",
      "Epoch 1309/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3017 - val_loss: -4.1938\n",
      "Epoch 1310/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4190 - val_loss: -4.3395\n",
      "Epoch 1311/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4936 - val_loss: -3.1902\n",
      "Epoch 1312/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1935 - val_loss: -4.4976\n",
      "Epoch 1313/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5686 - val_loss: -4.1709\n",
      "Epoch 1314/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5480 - val_loss: -4.0757\n",
      "Epoch 1315/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5159 - val_loss: -4.4212\n",
      "Epoch 1316/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5429 - val_loss: -3.2265\n",
      "Epoch 1317/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4493 - val_loss: -4.6956\n",
      "Epoch 1318/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5902 - val_loss: -3.8235\n",
      "Epoch 1319/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5395 - val_loss: -4.4817\n",
      "Epoch 1320/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4891 - val_loss: -3.7470\n",
      "Epoch 1321/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5922 - val_loss: -4.2291\n",
      "Epoch 1322/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5084 - val_loss: -4.5827\n",
      "Epoch 1323/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5950 - val_loss: -4.4816\n",
      "Epoch 1324/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5615 - val_loss: -4.4933\n",
      "Epoch 1325/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4871 - val_loss: -4.4460\n",
      "Epoch 1326/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4630 - val_loss: -3.8582\n",
      "Epoch 1327/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5469 - val_loss: -4.6896\n",
      "Epoch 1328/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3831 - val_loss: -3.8099\n",
      "Epoch 1329/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5202 - val_loss: -4.4168\n",
      "Epoch 1330/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4956 - val_loss: -4.4830\n",
      "Epoch 1331/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5246 - val_loss: -4.5097\n",
      "Epoch 1332/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4890 - val_loss: -0.9196\n",
      "Epoch 1333/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3684 - val_loss: -4.1388\n",
      "Epoch 1334/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5082 - val_loss: -2.3710\n",
      "Epoch 1335/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5445 - val_loss: -4.4088\n",
      "Epoch 1336/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4712 - val_loss: -3.0642\n",
      "Epoch 1337/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5076 - val_loss: -4.4906\n",
      "Epoch 1338/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6066 - val_loss: -3.1867\n",
      "Epoch 1339/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4095 - val_loss: -4.5723\n",
      "Epoch 1340/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5501 - val_loss: -0.5218\n",
      "Epoch 1341/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5417 - val_loss: -3.2180\n",
      "Epoch 1342/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4419 - val_loss: -0.4954\n",
      "Epoch 1343/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3535 - val_loss: -4.2663\n",
      "Epoch 1344/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5749 - val_loss: -4.4843\n",
      "Epoch 1345/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5791 - val_loss: -4.5522\n",
      "Epoch 1346/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6034 - val_loss: -4.3666\n",
      "Epoch 1347/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5958 - val_loss: -4.3208\n",
      "Epoch 1348/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6125 - val_loss: -4.2276\n",
      "Epoch 1349/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5567 - val_loss: -3.8420\n",
      "Epoch 1350/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4657 - val_loss: -4.2272\n",
      "Epoch 1351/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5233 - val_loss: -4.2694\n",
      "Epoch 1352/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5510 - val_loss: -1.4865\n",
      "Epoch 1353/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5708 - val_loss: -4.4858\n",
      "Epoch 1354/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5922 - val_loss: -3.5544\n",
      "Epoch 1355/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4914 - val_loss: -3.8191\n",
      "Epoch 1356/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5459 - val_loss: -3.3176\n",
      "Epoch 1357/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5274 - val_loss: -4.5255\n",
      "Epoch 1358/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5517 - val_loss: -4.0330\n",
      "Epoch 1359/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5752 - val_loss: -3.2893\n",
      "Epoch 1360/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4834 - val_loss: -4.0233\n",
      "Epoch 1361/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5635 - val_loss: -3.2504\n",
      "Epoch 1362/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6235 - val_loss: -1.1197\n",
      "Epoch 1363/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6430 - val_loss: -3.5399\n",
      "Epoch 1364/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4673 - val_loss: -2.5768\n",
      "Epoch 1365/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4492 - val_loss: -4.4359\n",
      "Epoch 1366/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1151 - val_loss: -4.6008\n",
      "Epoch 1367/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4021 - val_loss: -4.3074\n",
      "Epoch 1368/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5154 - val_loss: -3.8145\n",
      "Epoch 1369/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4929 - val_loss: -4.3278\n",
      "Epoch 1370/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4407 - val_loss: -3.9980\n",
      "Epoch 1371/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5454 - val_loss: -4.6678\n",
      "Epoch 1372/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5583 - val_loss: -4.4276\n",
      "Epoch 1373/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5233 - val_loss: -4.4636\n",
      "Epoch 1374/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5961 - val_loss: -3.7059\n",
      "Epoch 1375/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5523 - val_loss: -4.2715\n",
      "Epoch 1376/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4757 - val_loss: -4.4040\n",
      "Epoch 1377/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5878 - val_loss: -3.6174\n",
      "Epoch 1378/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5577 - val_loss: -3.9370\n",
      "Epoch 1379/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4552 - val_loss: -2.4728\n",
      "Epoch 1380/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5436 - val_loss: -4.3397\n",
      "Epoch 1381/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5763 - val_loss: -4.3527\n",
      "Epoch 1382/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6297 - val_loss: -4.5778\n",
      "Epoch 1383/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5401 - val_loss: -4.1326\n",
      "Epoch 1384/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4612 - val_loss: -3.6273\n",
      "Epoch 1385/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4567 - val_loss: -3.9807\n",
      "Epoch 1386/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4622 - val_loss: -4.5736\n",
      "Epoch 1387/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5084 - val_loss: -4.5665\n",
      "Epoch 1388/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5886 - val_loss: -2.6869\n",
      "Epoch 1389/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5997 - val_loss: -3.2898\n",
      "Epoch 1390/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5777 - val_loss: -3.1580\n",
      "Epoch 1391/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4914 - val_loss: -4.0432\n",
      "Epoch 1392/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5888 - val_loss: -3.5732\n",
      "Epoch 1393/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5504 - val_loss: -3.9224\n",
      "Epoch 1394/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6190 - val_loss: -3.6230\n",
      "Epoch 1395/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5369 - val_loss: -3.3217\n",
      "Epoch 1396/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.6033 - val_loss: -2.5361\n",
      "Epoch 1397/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4305 - val_loss: -4.5291\n",
      "Epoch 1398/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5437 - val_loss: -4.2971\n",
      "Epoch 1399/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4793 - val_loss: -4.3693\n",
      "Epoch 1400/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6082 - val_loss: -3.3696\n",
      "Epoch 1401/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5930 - val_loss: -4.4613\n",
      "Epoch 1402/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6524 - val_loss: -4.2227\n",
      "Epoch 1403/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7038 - val_loss: -3.0521\n",
      "Epoch 1404/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0737 - val_loss: -4.1248\n",
      "Epoch 1405/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3368 - val_loss: -4.0369\n",
      "Epoch 1406/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3520 - val_loss: -3.2230\n",
      "Epoch 1407/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4388 - val_loss: -4.5303\n",
      "Epoch 1408/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3807 - val_loss: -3.2091\n",
      "Epoch 1409/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4031 - val_loss: -3.8810\n",
      "Epoch 1410/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2994 - val_loss: -4.5880\n",
      "Epoch 1411/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4880 - val_loss: -4.3795\n",
      "Epoch 1412/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4253 - val_loss: -4.8397\n",
      "Epoch 1413/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4910 - val_loss: -3.7234\n",
      "Epoch 1414/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2634 - val_loss: -4.5648\n",
      "Epoch 1415/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4070 - val_loss: -2.3117\n",
      "Epoch 1416/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4998 - val_loss: -4.6313\n",
      "Epoch 1417/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4442 - val_loss: -4.0969\n",
      "Epoch 1418/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4928 - val_loss: -4.5237\n",
      "Epoch 1419/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4991 - val_loss: -4.0463\n",
      "Epoch 1420/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4538 - val_loss: -3.2407\n",
      "Epoch 1421/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5188 - val_loss: -3.9988\n",
      "Epoch 1422/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4723 - val_loss: -0.9163\n",
      "Epoch 1423/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5308 - val_loss: -4.2202\n",
      "Epoch 1424/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1973 - val_loss: -4.8499\n",
      "Epoch 1425/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3957 - val_loss: -3.8355\n",
      "Epoch 1426/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4667 - val_loss: -4.5813\n",
      "Epoch 1427/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5422 - val_loss: -3.6809\n",
      "Epoch 1428/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4912 - val_loss: -4.3103\n",
      "Epoch 1429/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5180 - val_loss: -4.5969\n",
      "Epoch 1430/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3990 - val_loss: -2.9697\n",
      "Epoch 1431/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5425 - val_loss: -3.9074\n",
      "Epoch 1432/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3807 - val_loss: -4.2628\n",
      "Epoch 1433/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5625 - val_loss: -4.2030\n",
      "Epoch 1434/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0080 - val_loss: -3.8318\n",
      "Epoch 1435/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1996 - val_loss: -3.9303\n",
      "Epoch 1436/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8330 - val_loss: -3.8992\n",
      "Epoch 1437/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0051 - val_loss: -4.3911\n",
      "Epoch 1438/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1254 - val_loss: -3.9015\n",
      "Epoch 1439/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1798 - val_loss: -4.5447\n",
      "Epoch 1440/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0144 - val_loss: -4.0249\n",
      "Epoch 1441/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2402 - val_loss: -4.0272\n",
      "Epoch 1442/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2444 - val_loss: -4.6956\n",
      "Epoch 1443/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3352 - val_loss: -4.5367\n",
      "Epoch 1444/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2903 - val_loss: -3.3017\n",
      "Epoch 1445/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1837 - val_loss: -4.7089\n",
      "Epoch 1446/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3410 - val_loss: -3.3081\n",
      "Epoch 1447/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3536 - val_loss: -4.1416\n",
      "Epoch 1448/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2825 - val_loss: -4.0134\n",
      "Epoch 1449/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3722 - val_loss: -2.9429\n",
      "Epoch 1450/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3104 - val_loss: -3.8243\n",
      "Epoch 1451/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3691 - val_loss: -4.1466\n",
      "Epoch 1452/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4437 - val_loss: -4.2363\n",
      "Epoch 1453/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4116 - val_loss: 5.2303\n",
      "Epoch 1454/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3086 - val_loss: -3.9341\n",
      "Epoch 1455/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4164 - val_loss: -4.5794\n",
      "Epoch 1456/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4189 - val_loss: -3.9591\n",
      "Epoch 1457/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4309 - val_loss: -3.9185\n",
      "Epoch 1458/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4150 - val_loss: -4.2861\n",
      "Epoch 1459/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2918 - val_loss: -4.6103\n",
      "Epoch 1460/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4610 - val_loss: -4.7679\n",
      "Epoch 1461/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2512 - val_loss: -4.3959\n",
      "Epoch 1462/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4510 - val_loss: -3.9912\n",
      "Epoch 1463/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4346 - val_loss: -3.4588\n",
      "Epoch 1464/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2273 - val_loss: -4.8481\n",
      "Epoch 1465/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4427 - val_loss: -3.7905\n",
      "Epoch 1466/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4542 - val_loss: -3.8492\n",
      "Epoch 1467/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4208 - val_loss: -4.0793\n",
      "Epoch 1468/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4936 - val_loss: -2.0722\n",
      "Epoch 1469/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3570 - val_loss: -3.0860\n",
      "Epoch 1470/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1804 - val_loss: -4.7345\n",
      "Epoch 1471/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4586 - val_loss: -4.4121\n",
      "Epoch 1472/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5070 - val_loss: -3.9750\n",
      "Epoch 1473/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4209 - val_loss: -3.7686\n",
      "Epoch 1474/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4428 - val_loss: -4.5234\n",
      "Epoch 1475/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4449 - val_loss: -4.3040\n",
      "Epoch 1476/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0196 - val_loss: -4.7726\n",
      "Epoch 1477/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3861 - val_loss: -4.2510\n",
      "Epoch 1478/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3628 - val_loss: -4.5826\n",
      "Epoch 1479/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4456 - val_loss: -4.9740\n",
      "Epoch 1480/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4217 - val_loss: -4.1923\n",
      "Epoch 1481/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4942 - val_loss: -4.7126\n",
      "Epoch 1482/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4709 - val_loss: -3.8739\n",
      "Epoch 1483/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4859 - val_loss: -2.1707\n",
      "Epoch 1484/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4189 - val_loss: -4.2547\n",
      "Epoch 1485/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.4616"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_120_layer_call_fn, lstm_cell_120_layer_call_and_return_conditional_losses, lstm_cell_121_layer_call_fn, lstm_cell_121_layer_call_and_return_conditional_losses, lstm_cell_122_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -5.4616 - val_loss: -5.0393\n",
      "Epoch 1486/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2788 - val_loss: -4.7126\n",
      "Epoch 1487/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4036 - val_loss: -4.2629\n",
      "Epoch 1488/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5177 - val_loss: -3.8807\n",
      "Epoch 1489/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4993 - val_loss: -4.5252\n",
      "Epoch 1490/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4700 - val_loss: -4.6881\n",
      "Epoch 1491/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4566 - val_loss: -4.1760\n",
      "Epoch 1492/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1489 - val_loss: -4.7731\n",
      "Epoch 1493/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0141 - val_loss: -4.2230\n",
      "Epoch 1494/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2593 - val_loss: -4.3909\n",
      "Epoch 1495/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4311 - val_loss: -1.8515\n",
      "Epoch 1496/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4161 - val_loss: -3.3880\n",
      "Epoch 1497/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9197 - val_loss: -4.6504\n",
      "Epoch 1498/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4574 - val_loss: -4.5679\n",
      "Epoch 1499/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5154 - val_loss: -3.3327\n",
      "Epoch 1500/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5171 - val_loss: -3.0407\n",
      "Epoch 1501/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4942 - val_loss: -4.3360\n",
      "Epoch 1502/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4513 - val_loss: -4.3967\n",
      "Epoch 1503/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3861 - val_loss: -4.2884\n",
      "Epoch 1504/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2191 - val_loss: 0.9198\n",
      "Epoch 1505/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0623 - val_loss: -4.2656\n",
      "Epoch 1506/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4514 - val_loss: -3.5366\n",
      "Epoch 1507/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4482 - val_loss: -4.1054\n",
      "Epoch 1508/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4956 - val_loss: -3.5224\n",
      "Epoch 1509/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4938 - val_loss: -4.1479\n",
      "Epoch 1510/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4998 - val_loss: -4.4374\n",
      "Epoch 1511/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5321 - val_loss: -4.5409\n",
      "Epoch 1512/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3362 - val_loss: -4.5988\n",
      "Epoch 1513/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5356 - val_loss: -2.1269\n",
      "Epoch 1514/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3864 - val_loss: -4.5886\n",
      "Epoch 1515/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4576 - val_loss: -3.1263\n",
      "Epoch 1516/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5342 - val_loss: -2.5900\n",
      "Epoch 1517/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4764 - val_loss: -3.5359\n",
      "Epoch 1518/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.5235 - val_loss: -4.5439\n",
      "Epoch 1519/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5037 - val_loss: -3.5829\n",
      "Epoch 1520/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4959 - val_loss: -4.2812\n",
      "Epoch 1521/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1401 - val_loss: -2.5107\n",
      "Epoch 1522/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.5163 - val_loss: -3.1513\n",
      "Epoch 1523/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2472 - val_loss: -3.8387\n",
      "Epoch 1524/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5538 - val_loss: -3.9668\n",
      "Epoch 1525/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6315 - val_loss: -3.7829\n",
      "Epoch 1526/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6599 - val_loss: -3.6128\n",
      "Epoch 1527/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8090 - val_loss: -3.9877\n",
      "Epoch 1528/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8963 - val_loss: -3.9264\n",
      "Epoch 1529/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9498 - val_loss: -3.4839\n",
      "Epoch 1530/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2856 - val_loss: -4.5585\n",
      "Epoch 1531/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6843 - val_loss: -3.8034\n",
      "Epoch 1532/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9208 - val_loss: -4.3257\n",
      "Epoch 1533/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0231 - val_loss: -4.0944\n",
      "Epoch 1534/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0500 - val_loss: -4.6331\n",
      "Epoch 1535/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1067 - val_loss: -1.3403\n",
      "Epoch 1536/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0160 - val_loss: -4.4953\n",
      "Epoch 1537/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1628 - val_loss: -3.9550\n",
      "Epoch 1538/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1986 - val_loss: -4.1844\n",
      "Epoch 1539/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0916 - val_loss: -4.5192\n",
      "Epoch 1540/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1816 - val_loss: -4.7089\n",
      "Epoch 1541/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1465 - val_loss: -4.8435\n",
      "Epoch 1542/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2862 - val_loss: -4.3634\n",
      "Epoch 1543/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1766 - val_loss: -4.1262\n",
      "Epoch 1544/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2925 - val_loss: -4.5860\n",
      "Epoch 1545/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2546 - val_loss: -4.3224\n",
      "Epoch 1546/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6843 - val_loss: -3.9956\n",
      "Epoch 1547/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8080 - val_loss: -2.9609\n",
      "Epoch 1548/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0809 - val_loss: -4.7203\n",
      "Epoch 1549/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1563 - val_loss: -3.8150\n",
      "Epoch 1550/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2525 - val_loss: -3.8441\n",
      "Epoch 1551/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.2699 - val_loss: -3.1008\n",
      "Epoch 1552/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1695 - val_loss: -4.7034\n",
      "Epoch 1553/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2979 - val_loss: -3.4982\n",
      "Epoch 1554/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2817 - val_loss: -4.3635\n",
      "Epoch 1555/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3096 - val_loss: -3.8476\n",
      "Epoch 1556/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3166 - val_loss: -4.4173\n",
      "Epoch 1557/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2809 - val_loss: -4.5224\n",
      "Epoch 1558/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3047 - val_loss: -4.3083\n",
      "Epoch 1559/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3499 - val_loss: -4.5009\n",
      "Epoch 1560/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2238 - val_loss: -4.3813\n",
      "Epoch 1561/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2720 - val_loss: -3.5276\n",
      "Epoch 1562/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3010 - val_loss: -4.4982\n",
      "Epoch 1563/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4009 - val_loss: -3.5326\n",
      "Epoch 1564/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3611 - val_loss: -4.0688\n",
      "Epoch 1565/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3516 - val_loss: -3.6314\n",
      "Epoch 1566/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3298 - val_loss: -4.8085\n",
      "Epoch 1567/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3774 - val_loss: -4.5214\n",
      "Epoch 1568/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4225 - val_loss: -3.5741\n",
      "Epoch 1569/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3293 - val_loss: -4.7596\n",
      "Epoch 1570/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4779 - val_loss: -3.7559\n",
      "Epoch 1571/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4296 - val_loss: -4.5377\n",
      "Epoch 1572/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4408 - val_loss: -4.2396\n",
      "Epoch 1573/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5070 - val_loss: -3.7761\n",
      "Epoch 1574/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2599 - val_loss: -3.0790\n",
      "Epoch 1575/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2975 - val_loss: -4.4131\n",
      "Epoch 1576/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3004 - val_loss: -4.6247\n",
      "Epoch 1577/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3362 - val_loss: -4.5552\n",
      "Epoch 1578/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4172 - val_loss: -3.9805\n",
      "Epoch 1579/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3283 - val_loss: -4.0143\n",
      "Epoch 1580/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3994 - val_loss: -4.9000\n",
      "Epoch 1581/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4313 - val_loss: -3.9754\n",
      "Epoch 1582/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4113 - val_loss: -4.6563\n",
      "Epoch 1583/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4625 - val_loss: -4.4748\n",
      "Epoch 1584/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3926 - val_loss: -2.5072\n",
      "Epoch 1585/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3529 - val_loss: -4.6782\n",
      "Epoch 1586/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4217 - val_loss: -0.1116\n",
      "Epoch 1587/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3551 - val_loss: -3.8786\n",
      "Epoch 1588/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5017 - val_loss: -3.9959\n",
      "Epoch 1589/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4454 - val_loss: -3.5025\n",
      "Epoch 1590/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4858 - val_loss: -3.9529\n",
      "Epoch 1591/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0209 - val_loss: -3.8393\n",
      "Epoch 1592/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0563 - val_loss: -4.4779\n",
      "Epoch 1593/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3411 - val_loss: -4.2278\n",
      "Epoch 1594/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4261 - val_loss: -4.7590\n",
      "Epoch 1595/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3854 - val_loss: -4.0970\n",
      "Epoch 1596/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4612 - val_loss: -3.7609\n",
      "Epoch 1597/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3953 - val_loss: -3.8812\n",
      "Epoch 1598/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4672 - val_loss: -4.3556\n",
      "Epoch 1599/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3559 - val_loss: -4.3174\n",
      "Epoch 1600/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4497 - val_loss: -2.9947\n",
      "Epoch 1601/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2597 - val_loss: -4.4129\n",
      "Epoch 1602/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4371 - val_loss: -3.1899\n",
      "Epoch 1603/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4697 - val_loss: -0.4892\n",
      "Epoch 1604/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4236 - val_loss: -4.1931\n",
      "Epoch 1605/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4328 - val_loss: -4.2391\n",
      "Epoch 1606/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4738 - val_loss: -3.6426\n",
      "Epoch 1607/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5300 - val_loss: -3.3226\n",
      "Epoch 1608/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4045 - val_loss: -1.2106\n",
      "Epoch 1609/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4161 - val_loss: -3.5689\n",
      "Epoch 1610/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5101 - val_loss: -4.0753\n",
      "Epoch 1611/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4791 - val_loss: -3.3708\n",
      "Epoch 1612/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5340 - val_loss: -2.8545\n",
      "Epoch 1613/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4922 - val_loss: -4.6805\n",
      "Epoch 1614/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5104 - val_loss: -4.5394\n",
      "Epoch 1615/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5211 - val_loss: -3.3249\n",
      "Epoch 1616/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3987 - val_loss: -4.5993\n",
      "Epoch 1617/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3641 - val_loss: -4.7204\n",
      "Epoch 1618/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5297 - val_loss: -2.3451\n",
      "Epoch 1619/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4469 - val_loss: -4.1924\n",
      "Epoch 1620/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4261 - val_loss: -4.0994\n",
      "Epoch 1621/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5220 - val_loss: -4.3291\n",
      "Epoch 1622/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2680 - val_loss: -2.4332\n",
      "Epoch 1623/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9871 - val_loss: -4.7230\n",
      "Epoch 1624/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3744 - val_loss: -4.0522\n",
      "Epoch 1625/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4564 - val_loss: -3.4122\n",
      "Epoch 1626/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3786 - val_loss: -4.2550\n",
      "Epoch 1627/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4103 - val_loss: -3.7384\n",
      "Epoch 1628/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1803 - val_loss: -2.6187\n",
      "Epoch 1629/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1367 - val_loss: -4.4077\n",
      "Epoch 1630/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3994 - val_loss: -4.7446\n",
      "Epoch 1631/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4194 - val_loss: -4.3488\n",
      "Epoch 1632/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4458 - val_loss: -3.7286\n",
      "Epoch 1633/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4741 - val_loss: -3.9971\n",
      "Epoch 1634/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1520 - val_loss: -3.2757\n",
      "Epoch 1635/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4500 - val_loss: -1.2807\n",
      "Epoch 1636/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2990 - val_loss: -3.9038\n",
      "Epoch 1637/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4972 - val_loss: -4.3807\n",
      "Epoch 1638/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3997 - val_loss: -3.4988\n",
      "Epoch 1639/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4870 - val_loss: -4.2443\n",
      "Epoch 1640/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4272 - val_loss: -4.6638\n",
      "Epoch 1641/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.5335 - val_loss: -2.1380\n",
      "Epoch 1642/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4681 - val_loss: -3.3432\n",
      "Epoch 1643/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5317 - val_loss: -4.3776\n",
      "Epoch 1644/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4829 - val_loss: -4.3670\n",
      "Epoch 1645/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5155 - val_loss: -4.3520\n",
      "Epoch 1646/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5146 - val_loss: -3.9501\n",
      "Epoch 1647/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4231 - val_loss: -3.9954\n",
      "Epoch 1648/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5466 - val_loss: -3.5196\n",
      "Epoch 1649/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5027 - val_loss: -4.3099\n",
      "Epoch 1650/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5127 - val_loss: -4.5091\n",
      "Epoch 1651/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3634 - val_loss: -4.7096\n",
      "Epoch 1652/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4889 - val_loss: -3.6046\n",
      "Epoch 1653/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3247 - val_loss: -4.4110\n",
      "Epoch 1654/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5507 - val_loss: -4.4492\n",
      "Epoch 1655/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5205 - val_loss: -4.4400\n",
      "Epoch 1656/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5636 - val_loss: -1.9920\n",
      "Epoch 1657/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5241 - val_loss: -3.9099\n",
      "Epoch 1658/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4063 - val_loss: -4.5739\n",
      "Epoch 1659/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5691 - val_loss: -4.2724\n",
      "Epoch 1660/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5627 - val_loss: -4.3652\n",
      "Epoch 1661/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3747 - val_loss: -4.2404\n",
      "Epoch 1662/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4907 - val_loss: -4.8420\n",
      "Epoch 1663/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5294 - val_loss: -4.3586\n",
      "Epoch 1664/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5369 - val_loss: -4.2982\n",
      "Epoch 1665/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5895 - val_loss: -4.2352\n",
      "Epoch 1666/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5117 - val_loss: -2.8449\n",
      "Epoch 1667/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3875 - val_loss: -4.0500\n",
      "Epoch 1668/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4913 - val_loss: -4.5956\n",
      "Epoch 1669/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3193 - val_loss: -3.3327\n",
      "Epoch 1670/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4194 - val_loss: -4.3230\n",
      "Epoch 1671/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4682 - val_loss: -4.6030\n",
      "Epoch 1672/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4790 - val_loss: -4.0887\n",
      "Epoch 1673/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5003 - val_loss: -3.4689\n",
      "Epoch 1674/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2978 - val_loss: -3.6445\n",
      "Epoch 1675/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4211 - val_loss: -4.0745\n",
      "Epoch 1676/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4745 - val_loss: -3.1113\n",
      "Epoch 1677/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5657 - val_loss: -3.9025\n",
      "Epoch 1678/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4248 - val_loss: -2.6609\n",
      "Epoch 1679/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5553 - val_loss: -4.0266\n",
      "Epoch 1680/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5791 - val_loss: -4.0105\n",
      "Epoch 1681/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5427 - val_loss: -3.7688\n",
      "Epoch 1682/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5391 - val_loss: -2.8046\n",
      "Epoch 1683/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5467 - val_loss: -3.7088\n",
      "Epoch 1684/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5569 - val_loss: -3.2646\n",
      "Epoch 1685/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4969 - val_loss: -3.2612\n",
      "Epoch 1686/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0990 - val_loss: -4.5105\n",
      "Epoch 1687/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4895 - val_loss: -3.4732\n",
      "Epoch 1688/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5339 - val_loss: -3.7280\n",
      "Epoch 1689/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.5019 - val_loss: -2.9059\n",
      "Epoch 1690/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5371 - val_loss: -3.2605\n",
      "Epoch 1691/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4958 - val_loss: -4.1231\n",
      "Epoch 1692/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5790 - val_loss: -4.7206\n",
      "Epoch 1693/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5539 - val_loss: -3.7924\n",
      "Epoch 1694/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5372 - val_loss: 0.7368\n",
      "Epoch 1695/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5489 - val_loss: -3.1865\n",
      "Epoch 1696/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4558 - val_loss: -4.4773\n",
      "Epoch 1697/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5684 - val_loss: -4.3036\n",
      "Epoch 1698/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5197 - val_loss: -3.7698\n",
      "Epoch 1699/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5668 - val_loss: -4.5601\n",
      "Epoch 1700/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5406 - val_loss: -4.4533\n",
      "Epoch 1701/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5032 - val_loss: -4.6260\n",
      "Epoch 1702/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5358 - val_loss: -4.1938\n",
      "Epoch 1703/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5549 - val_loss: -4.3555\n",
      "Epoch 1704/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4344 - val_loss: -1.8986\n",
      "Epoch 1705/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4153 - val_loss: -4.0932\n",
      "Epoch 1706/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5884 - val_loss: -3.9753\n",
      "Epoch 1707/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5537 - val_loss: -4.1363\n",
      "Epoch 1708/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5871 - val_loss: -3.3398\n",
      "Epoch 1709/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4718 - val_loss: -4.2707\n",
      "Epoch 1710/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5678 - val_loss: -4.5953\n",
      "Epoch 1711/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2414 - val_loss: -3.3569\n",
      "Epoch 1712/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5362 - val_loss: -3.8241\n",
      "Epoch 1713/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4852 - val_loss: -3.1358\n",
      "Epoch 1714/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.6038 - val_loss: -3.8576\n",
      "Epoch 1715/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4278 - val_loss: -4.1622\n",
      "Epoch 1716/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5143 - val_loss: -4.5604\n",
      "Epoch 1717/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5782 - val_loss: -4.1093\n",
      "Epoch 1718/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4826 - val_loss: -4.3889\n",
      "Epoch 1719/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5926 - val_loss: -4.5235\n",
      "Epoch 1720/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2234 - val_loss: -4.2988\n",
      "Epoch 1721/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4233 - val_loss: -3.3610\n",
      "Epoch 1722/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4811 - val_loss: -3.8633\n",
      "Epoch 1723/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5176 - val_loss: -3.8754\n",
      "Epoch 1724/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5394 - val_loss: -4.4865\n",
      "Epoch 1725/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4182 - val_loss: -4.0067\n",
      "Epoch 1726/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5750 - val_loss: -3.5877\n",
      "Epoch 1727/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5501 - val_loss: -3.3162\n",
      "Epoch 1728/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5677 - val_loss: -4.0683\n",
      "Epoch 1729/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5544 - val_loss: -2.6622\n",
      "Epoch 1730/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5728 - val_loss: -2.6331\n",
      "Epoch 1731/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4663 - val_loss: -4.6542\n",
      "Epoch 1732/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6034 - val_loss: -3.4494\n",
      "Epoch 1733/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5324 - val_loss: -4.3885\n",
      "Epoch 1734/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5488 - val_loss: -2.5011\n",
      "Epoch 1735/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5229 - val_loss: -4.4860\n",
      "Epoch 1736/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5248 - val_loss: -4.0020\n",
      "Epoch 1737/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5523 - val_loss: -4.8316\n",
      "Epoch 1738/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5352 - val_loss: -3.5132\n",
      "Epoch 1739/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5895 - val_loss: -3.0947\n",
      "Epoch 1740/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3447 - val_loss: -4.2723\n",
      "Epoch 1741/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5325 - val_loss: -4.1343\n",
      "Epoch 1742/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5568 - val_loss: -3.8854\n",
      "Epoch 1743/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5981 - val_loss: -4.2246\n",
      "Epoch 1744/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4363 - val_loss: -4.4138\n",
      "Epoch 1745/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5873 - val_loss: -3.6235\n",
      "Epoch 1746/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6298 - val_loss: -2.7242\n",
      "Epoch 1747/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5466 - val_loss: -4.6285\n",
      "Epoch 1748/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5890 - val_loss: -1.4980\n",
      "Epoch 1749/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4458 - val_loss: -4.6090\n",
      "Epoch 1750/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.6212 - val_loss: -3.5385\n",
      "Epoch 1751/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5265 - val_loss: -4.1378\n",
      "Epoch 1752/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5324 - val_loss: -4.4225\n",
      "Epoch 1753/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.6046 - val_loss: -4.1962\n",
      "Epoch 1754/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6253 - val_loss: -3.6791\n",
      "Epoch 1755/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3519 - val_loss: -3.5420\n",
      "Epoch 1756/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5930 - val_loss: -4.2210\n",
      "Epoch 1757/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4603 - val_loss: -3.8702\n",
      "Epoch 1758/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6102 - val_loss: -3.4954\n",
      "Epoch 1759/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3745 - val_loss: -4.2197\n",
      "Epoch 1760/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6171 - val_loss: -4.0174\n",
      "Epoch 1761/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5735 - val_loss: -2.7196\n",
      "Epoch 1762/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5666 - val_loss: -2.9955\n",
      "Epoch 1763/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6084 - val_loss: -4.0191\n",
      "Epoch 1764/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5546 - val_loss: -4.2791\n",
      "Epoch 1765/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3030 - val_loss: -4.6053\n",
      "Epoch 1766/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6151 - val_loss: -4.0059\n",
      "Epoch 1767/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.6504 - val_loss: -4.0519\n",
      "Epoch 1768/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5996 - val_loss: -3.8369\n",
      "Epoch 1769/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.6107 - val_loss: -4.1255\n",
      "Epoch 1770/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5883 - val_loss: -4.7493\n",
      "Epoch 1771/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5894 - val_loss: -3.1929\n",
      "Epoch 1772/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6117 - val_loss: -4.2027\n",
      "Epoch 1773/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0198 - val_loss: -4.5314\n",
      "Epoch 1774/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3215 - val_loss: -4.7289\n",
      "Epoch 1775/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4858 - val_loss: -4.4408\n",
      "Epoch 1776/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1339 - val_loss: -4.2555\n",
      "Epoch 1777/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4575 - val_loss: -4.7112\n",
      "Epoch 1778/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5585 - val_loss: -4.1261\n",
      "Epoch 1779/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5621 - val_loss: -4.2777\n",
      "Epoch 1780/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5204 - val_loss: -3.8541\n",
      "Epoch 1781/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5935 - val_loss: -4.3986\n",
      "Epoch 1782/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6181 - val_loss: -4.5850\n",
      "Epoch 1783/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4660 - val_loss: -2.6954\n",
      "Epoch 1784/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5495 - val_loss: -4.0234\n",
      "Epoch 1785/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4802 - val_loss: -4.1998\n",
      "Epoch 1786/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5492 - val_loss: -2.7930\n",
      "Epoch 1787/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4172 - val_loss: -4.1835\n",
      "Epoch 1788/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4245 - val_loss: -4.7714\n",
      "Epoch 1789/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5342 - val_loss: -1.6461\n",
      "Epoch 1790/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5526 - val_loss: -4.0503\n",
      "Epoch 1791/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5891 - val_loss: -4.3118\n",
      "Epoch 1792/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5004 - val_loss: -3.5569\n",
      "Epoch 1793/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6044 - val_loss: -3.8068\n",
      "Epoch 1794/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5662 - val_loss: -4.5280\n",
      "Epoch 1795/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5346 - val_loss: -4.3378\n",
      "Epoch 1796/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5823 - val_loss: -4.5532\n",
      "Epoch 1797/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5808 - val_loss: -4.4231\n",
      "Epoch 1798/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5292 - val_loss: -2.2380\n",
      "Epoch 1799/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6382 - val_loss: -4.0751\n",
      "Epoch 1800/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5868 - val_loss: -4.0592\n",
      "Epoch 1801/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5956 - val_loss: -1.9846\n",
      "Epoch 1802/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5248 - val_loss: -3.0019\n",
      "Epoch 1803/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4343 - val_loss: -4.2438\n",
      "Epoch 1804/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5812 - val_loss: -4.3217\n",
      "Epoch 1805/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5553 - val_loss: -3.7248\n",
      "Epoch 1806/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5716 - val_loss: -3.9772\n",
      "Epoch 1807/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4032 - val_loss: -4.5817\n",
      "Epoch 1808/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6372 - val_loss: -3.7841\n",
      "Epoch 1809/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4905 - val_loss: -4.4239\n",
      "Epoch 1810/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4278 - val_loss: -4.8844\n",
      "Epoch 1811/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4301 - val_loss: -4.4710\n",
      "Epoch 1812/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4353 - val_loss: -4.1202\n",
      "Epoch 1813/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5887 - val_loss: -2.8324\n",
      "Epoch 1814/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9420 - val_loss: -4.5712\n",
      "Epoch 1815/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1004 - val_loss: -4.5888\n",
      "Epoch 1816/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4903 - val_loss: -3.7072\n",
      "Epoch 1817/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4586 - val_loss: -4.4892\n",
      "Epoch 1818/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5499 - val_loss: -3.7492\n",
      "Epoch 1819/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4427 - val_loss: -4.3181\n",
      "Epoch 1820/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5095 - val_loss: -4.0739\n",
      "Epoch 1821/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5614 - val_loss: -3.8405\n",
      "Epoch 1822/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5295 - val_loss: -2.6565\n",
      "Epoch 1823/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5639 - val_loss: -2.8545\n",
      "Epoch 1824/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5252 - val_loss: -4.8964\n",
      "Epoch 1825/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5887 - val_loss: -4.5992\n",
      "Epoch 1826/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.1694 - val_loss: -4.4036\n",
      "Epoch 1827/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5720 - val_loss: -4.5535\n",
      "Epoch 1828/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9896 - val_loss: -3.9462\n",
      "Epoch 1829/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5003 - val_loss: -3.8811\n",
      "Epoch 1830/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2177 - val_loss: -1.7788\n",
      "Epoch 1831/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4935 - val_loss: -4.6097\n",
      "Epoch 1832/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7607 - val_loss: -4.3521\n",
      "Epoch 1833/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7907 - val_loss: -4.1635\n",
      "Epoch 1834/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8617 - val_loss: -4.3587\n",
      "Epoch 1835/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6655 - val_loss: -4.0410\n",
      "Epoch 1836/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5618 - val_loss: -4.3496\n",
      "Epoch 1837/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8190 - val_loss: -2.9819\n",
      "Epoch 1838/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8620 - val_loss: -3.6131\n",
      "Epoch 1839/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7924 - val_loss: -3.9355\n",
      "Epoch 1840/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8545 - val_loss: -3.5359\n",
      "Epoch 1841/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9978 - val_loss: -2.3338\n",
      "Epoch 1842/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7472 - val_loss: -3.7343\n",
      "Epoch 1843/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9784 - val_loss: -3.7255\n",
      "Epoch 1844/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7377 - val_loss: -3.6203\n",
      "Epoch 1845/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8553 - val_loss: -4.1472\n",
      "Epoch 1846/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -4.5259 - val_loss: -2.8098\n",
      "Epoch 1847/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.6111 - val_loss: -4.4128\n",
      "Epoch 1848/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.9990 - val_loss: -4.3913\n",
      "Epoch 1849/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0405 - val_loss: -4.1175\n",
      "Epoch 1850/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2535 - val_loss: -4.4455\n",
      "Epoch 1851/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3647 - val_loss: -4.4350\n",
      "Epoch 1852/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3283 - val_loss: -3.9499\n",
      "Epoch 1853/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4659 - val_loss: -2.9545\n",
      "Epoch 1854/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4871 - val_loss: -4.0030\n",
      "Epoch 1855/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2528 - val_loss: -3.9202\n",
      "Epoch 1856/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4914 - val_loss: -3.7551\n",
      "Epoch 1857/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3050 - val_loss: -3.8268\n",
      "Epoch 1858/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3931 - val_loss: -3.2082\n",
      "Epoch 1859/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2820 - val_loss: -4.2396\n",
      "Epoch 1860/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3903 - val_loss: 0.0426\n",
      "Epoch 1861/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5211 - val_loss: -3.2280\n",
      "Epoch 1862/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4776 - val_loss: -4.1509\n",
      "Epoch 1863/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4476 - val_loss: -2.4146\n",
      "Epoch 1864/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4206 - val_loss: -4.0738\n",
      "Epoch 1865/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5144 - val_loss: -3.3306\n",
      "Epoch 1866/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5530 - val_loss: -4.2439\n",
      "Epoch 1867/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5058 - val_loss: -3.6485\n",
      "Epoch 1868/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6396 - val_loss: -2.6175\n",
      "Epoch 1869/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6632 - val_loss: -4.6007\n",
      "Epoch 1870/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2492 - val_loss: -4.0273\n",
      "Epoch 1871/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5219 - val_loss: 0.5734\n",
      "Epoch 1872/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4668 - val_loss: -3.4177\n",
      "Epoch 1873/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5236 - val_loss: -2.9125\n",
      "Epoch 1874/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6882 - val_loss: -2.8707\n",
      "Epoch 1875/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5961 - val_loss: -4.0377\n",
      "Epoch 1876/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6095 - val_loss: -3.0756\n",
      "Epoch 1877/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7061 - val_loss: -3.8233\n",
      "Epoch 1878/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4885 - val_loss: -4.6397\n",
      "Epoch 1879/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0044 - val_loss: -4.4752\n",
      "Epoch 1880/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6971 - val_loss: -2.5454\n",
      "Epoch 1881/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9257 - val_loss: -3.1496\n",
      "Epoch 1882/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0194 - val_loss: -4.2438\n",
      "Epoch 1883/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0257 - val_loss: -3.1290\n",
      "Epoch 1884/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8625 - val_loss: -4.6316\n",
      "Epoch 1885/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7102 - val_loss: -4.4643\n",
      "Epoch 1886/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6624 - val_loss: -3.7808\n",
      "Epoch 1887/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7949 - val_loss: -3.0826\n",
      "Epoch 1888/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7995 - val_loss: -4.0122\n",
      "Epoch 1889/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8922 - val_loss: -4.7173\n",
      "Epoch 1890/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8813 - val_loss: -3.4002\n",
      "Epoch 1891/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8733 - val_loss: -3.1401\n",
      "Epoch 1892/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7789 - val_loss: -3.9617\n",
      "Epoch 1893/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9739 - val_loss: -2.7308\n",
      "Epoch 1894/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9322 - val_loss: -4.7199\n",
      "Epoch 1895/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9472 - val_loss: -3.9069\n",
      "Epoch 1896/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9325 - val_loss: -3.9248\n",
      "Epoch 1897/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9501 - val_loss: -3.0045\n",
      "Epoch 1898/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5485 - val_loss: -0.5786\n",
      "Epoch 1899/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8800 - val_loss: -1.2700\n",
      "Epoch 1900/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6649 - val_loss: -4.8504\n",
      "Epoch 1901/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0522 - val_loss: -3.3554\n",
      "Epoch 1902/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9023 - val_loss: -3.8926\n",
      "Epoch 1903/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1061 - val_loss: -3.6747\n",
      "Epoch 1904/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2713 - val_loss: -3.9645\n",
      "Epoch 1905/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3353 - val_loss: -4.0877\n",
      "Epoch 1906/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3212 - val_loss: -4.2591\n",
      "Epoch 1907/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4074 - val_loss: -3.7325\n",
      "Epoch 1908/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3497 - val_loss: -4.2138\n",
      "Epoch 1909/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4528 - val_loss: -4.1682\n",
      "Epoch 1910/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4951 - val_loss: -4.0544\n",
      "Epoch 1911/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4016 - val_loss: -4.3799\n",
      "Epoch 1912/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3470 - val_loss: -4.7863\n",
      "Epoch 1913/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6021 - val_loss: 1.7702\n",
      "Epoch 1914/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1714 - val_loss: -4.7293\n",
      "Epoch 1915/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8113 - val_loss: -4.5667\n",
      "Epoch 1916/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0212 - val_loss: -2.4652\n",
      "Epoch 1917/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0228 - val_loss: -3.8461\n",
      "Epoch 1918/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1012 - val_loss: -4.5815\n",
      "Epoch 1919/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1696 - val_loss: -3.8149\n",
      "Epoch 1920/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1449 - val_loss: -4.8264\n",
      "Epoch 1921/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2327 - val_loss: -4.7426\n",
      "Epoch 1922/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1247 - val_loss: -4.6216\n",
      "Epoch 1923/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2490 - val_loss: -3.5685\n",
      "Epoch 1924/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1935 - val_loss: -4.7117\n",
      "Epoch 1925/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2742 - val_loss: -3.5967\n",
      "Epoch 1926/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2436 - val_loss: -4.1097\n",
      "Epoch 1927/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2034 - val_loss: -4.7056\n",
      "Epoch 1928/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3012 - val_loss: -1.6778\n",
      "Epoch 1929/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2527 - val_loss: -3.7517\n",
      "Epoch 1930/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2995 - val_loss: -3.5332\n",
      "Epoch 1931/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3297 - val_loss: -4.2680\n",
      "Epoch 1932/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0585 - val_loss: -4.8155\n",
      "Epoch 1933/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2349 - val_loss: -4.2891\n",
      "Epoch 1934/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3473 - val_loss: -3.7986\n",
      "Epoch 1935/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1522 - val_loss: -4.1637\n",
      "Epoch 1936/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2875 - val_loss: -4.6562\n",
      "Epoch 1937/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2373 - val_loss: -3.4221\n",
      "Epoch 1938/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1912 - val_loss: -4.1032\n",
      "Epoch 1939/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4005 - val_loss: -4.0230\n",
      "Epoch 1940/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1855 - val_loss: -4.7160\n",
      "Epoch 1941/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2869 - val_loss: -4.2922\n",
      "Epoch 1942/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3902 - val_loss: -4.1829\n",
      "Epoch 1943/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3223 - val_loss: -4.4840\n",
      "Epoch 1944/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4066 - val_loss: -3.8580\n",
      "Epoch 1945/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1103 - val_loss: -3.9226\n",
      "Epoch 1946/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0305 - val_loss: -4.7179\n",
      "Epoch 1947/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3257 - val_loss: -1.7841\n",
      "Epoch 1948/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1842 - val_loss: -5.0205\n",
      "Epoch 1949/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3972 - val_loss: -4.0741\n",
      "Epoch 1950/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2828 - val_loss: -4.8351\n",
      "Epoch 1951/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3114 - val_loss: -4.0003\n",
      "Epoch 1952/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4024 - val_loss: 2.4327\n",
      "Epoch 1953/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0131 - val_loss: -4.5679\n",
      "Epoch 1954/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3226 - val_loss: -3.8171\n",
      "Epoch 1955/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1651 - val_loss: -4.3294\n",
      "Epoch 1956/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1362 - val_loss: -3.5710\n",
      "Epoch 1957/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2743 - val_loss: -4.4754\n",
      "Epoch 1958/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2074 - val_loss: -4.3578\n",
      "Epoch 1959/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.2445 - val_loss: -2.0954\n",
      "Epoch 1960/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -3.4085 - val_loss: -3.2762\n",
      "Epoch 1961/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9932 - val_loss: -3.5313\n",
      "Epoch 1962/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0663 - val_loss: -3.9005\n",
      "Epoch 1963/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2735 - val_loss: -3.6709\n",
      "Epoch 1964/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3801 - val_loss: -3.1614\n",
      "Epoch 1965/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4359 - val_loss: -4.0746\n",
      "Epoch 1966/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4478 - val_loss: -4.1710\n",
      "Epoch 1967/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4932 - val_loss: -4.1786\n",
      "Epoch 1968/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5339 - val_loss: -2.9496\n",
      "Epoch 1969/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.9232 - val_loss: -3.4340\n",
      "Epoch 1970/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0115 - val_loss: -3.3526\n",
      "Epoch 1971/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1475 - val_loss: -3.3402\n",
      "Epoch 1972/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.3339 - val_loss: -3.8295\n",
      "Epoch 1973/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0906 - val_loss: -3.4260\n",
      "Epoch 1974/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1952 - val_loss: -3.5296\n",
      "Epoch 1975/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1779 - val_loss: -3.8954\n",
      "Epoch 1976/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2976 - val_loss: -3.9872\n",
      "Epoch 1977/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2650 - val_loss: -3.5271\n",
      "Epoch 1978/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2999 - val_loss: -4.0421\n",
      "Epoch 1979/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4381 - val_loss: -3.7144\n",
      "Epoch 1980/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3326 - val_loss: -4.0899\n",
      "Epoch 1981/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3330 - val_loss: -3.4946\n",
      "Epoch 1982/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2501 - val_loss: -3.1666\n",
      "Epoch 1983/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1291 - val_loss: -3.6375\n",
      "Epoch 1984/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2074 - val_loss: -3.9390\n",
      "Epoch 1985/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3106 - val_loss: -3.9435\n",
      "Epoch 1986/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2748 - val_loss: -3.7021\n",
      "Epoch 1987/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.1852 - val_loss: -3.6628\n",
      "Epoch 1988/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3837 - val_loss: -3.9903\n",
      "Epoch 1989/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3865 - val_loss: -3.4102\n",
      "Epoch 1990/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4265 - val_loss: -3.7843\n",
      "Epoch 1991/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3268 - val_loss: -3.7586\n",
      "Epoch 1992/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3376 - val_loss: -3.8456\n",
      "Epoch 1993/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4858 - val_loss: -3.3709\n",
      "Epoch 1994/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3040 - val_loss: -3.9700\n",
      "Epoch 1995/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4531 - val_loss: -4.0953\n",
      "Epoch 1996/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.4827 - val_loss: -4.0341\n",
      "Epoch 1997/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2210 - val_loss: -3.7595\n",
      "Epoch 1998/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -3.9922 - val_loss: -3.5144\n",
      "Epoch 1999/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8884 - val_loss: -3.4173\n",
      "Epoch 2000/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3641 - val_loss: -3.8524\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 7s 44ms/step - loss: -4.3648\n",
      "WARNING:tensorflow:Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 8s 45ms/step - loss: -5.2940\n",
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.1760"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 33s 253ms/step - loss: -1.1760 - val_loss: -1.4493\n",
      "Epoch 2/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.3452"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -2.3452 - val_loss: -2.5134\n",
      "Epoch 3/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.8971 - val_loss: -2.4517\n",
      "Epoch 4/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.2487"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -3.2487 - val_loss: -2.6784\n",
      "Epoch 5/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.3911"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -3.3911 - val_loss: -3.1196\n",
      "Epoch 6/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.5254 - val_loss: -2.6435\n",
      "Epoch 7/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.7283"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 246ms/step - loss: -3.7283 - val_loss: -3.3642\n",
      "Epoch 8/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.7653 - val_loss: -2.9599\n",
      "Epoch 9/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9456 - val_loss: -0.8609\n",
      "Epoch 10/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.8231 - val_loss: -2.7171\n",
      "Epoch 11/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.9527"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 244ms/step - loss: -3.9527 - val_loss: -3.4270\n",
      "Epoch 12/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.0354"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -4.0354 - val_loss: -3.5602\n",
      "Epoch 13/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.0705"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -4.0705 - val_loss: -3.6529\n",
      "Epoch 14/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0495 - val_loss: -3.3501\n",
      "Epoch 15/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1442 - val_loss: -3.5224\n",
      "Epoch 16/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9697 - val_loss: -2.4527\n",
      "Epoch 17/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.0402 - val_loss: -3.4716\n",
      "Epoch 18/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1754 - val_loss: -3.6376\n",
      "Epoch 19/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1802 - val_loss: -2.9643\n",
      "Epoch 20/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1172 - val_loss: -3.6497\n",
      "Epoch 21/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1761 - val_loss: -3.3783\n",
      "Epoch 22/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1070 - val_loss: -3.3292\n",
      "Epoch 23/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.2873"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 245ms/step - loss: -4.2873 - val_loss: -3.7198\n",
      "Epoch 24/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.3374"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 246ms/step - loss: -4.3374 - val_loss: -3.8524\n",
      "Epoch 25/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3178 - val_loss: -3.7751\n",
      "Epoch 26/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3058 - val_loss: -3.6333\n",
      "Epoch 27/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4053 - val_loss: 0.5157\n",
      "Epoch 28/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3654 - val_loss: -3.4219\n",
      "Epoch 29/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1093 - val_loss: -3.8495\n",
      "Epoch 30/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.2529 - val_loss: -3.1619\n",
      "Epoch 31/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9631 - val_loss: -3.5526\n",
      "Epoch 32/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3876 - val_loss: -3.0349\n",
      "Epoch 33/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4603 - val_loss: -3.8459\n",
      "Epoch 34/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4268"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -4.4268 - val_loss: -3.8532\n",
      "Epoch 35/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5058"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -4.5058 - val_loss: -3.9462\n",
      "Epoch 36/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.2899"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 246ms/step - loss: -4.2899 - val_loss: -3.9627\n",
      "Epoch 37/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4622"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -4.4622 - val_loss: -4.0039\n",
      "Epoch 38/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4645 - val_loss: -3.9391\n",
      "Epoch 39/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5602 - val_loss: -3.9912\n",
      "Epoch 40/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5632"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -4.5632 - val_loss: -4.0584\n",
      "Epoch 41/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4341"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -4.4341 - val_loss: -4.0608\n",
      "Epoch 42/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5423 - val_loss: -3.2604\n",
      "Epoch 43/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5211 - val_loss: -3.1861\n",
      "Epoch 44/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5429 - val_loss: -3.8583\n",
      "Epoch 45/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5027 - val_loss: -4.0242\n",
      "Epoch 46/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5354 - val_loss: -2.6609\n",
      "Epoch 47/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6028 - val_loss: -3.7864\n",
      "Epoch 48/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5617 - val_loss: -4.0233\n",
      "Epoch 49/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.6106 - val_loss: -3.6796\n",
      "Epoch 50/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1679 - val_loss: -3.8089\n",
      "Epoch 51/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5131 - val_loss: -1.4782\n",
      "Epoch 52/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4851 - val_loss: -3.9663\n",
      "Epoch 53/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5261 - val_loss: -3.7526\n",
      "Epoch 54/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5007"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -4.5007 - val_loss: -4.1895\n",
      "Epoch 55/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5441 - val_loss: -3.9730\n",
      "Epoch 56/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5633 - val_loss: -3.9100\n",
      "Epoch 57/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6257 - val_loss: -3.6066\n",
      "Epoch 58/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5471 - val_loss: -3.9725\n",
      "Epoch 59/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6248 - val_loss: -4.0746\n",
      "Epoch 60/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -4.6244 - val_loss: -4.2246\n",
      "Epoch 61/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6009 - val_loss: -4.0016\n",
      "Epoch 62/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6871 - val_loss: -4.2214\n",
      "Epoch 63/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5230 - val_loss: -3.4843\n",
      "Epoch 64/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5874 - val_loss: -4.0734\n",
      "Epoch 65/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6114 - val_loss: -3.9000\n",
      "Epoch 66/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6775 - val_loss: -4.1352\n",
      "Epoch 67/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6679 - val_loss: -3.9250\n",
      "Epoch 68/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6536 - val_loss: -3.9387\n",
      "Epoch 69/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7230 - val_loss: -4.0462\n",
      "Epoch 70/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7354 - val_loss: -2.5229\n",
      "Epoch 71/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3349 - val_loss: -4.0015\n",
      "Epoch 72/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7125 - val_loss: -3.9789\n",
      "Epoch 73/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6900 - val_loss: -4.1255\n",
      "Epoch 74/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7056"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -4.7056 - val_loss: -4.2344\n",
      "Epoch 75/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6322 - val_loss: -3.9855\n",
      "Epoch 76/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6382 - val_loss: -3.8848\n",
      "Epoch 77/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7431 - val_loss: -3.9592\n",
      "Epoch 78/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7891 - val_loss: -3.6722\n",
      "Epoch 79/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.6573 - val_loss: -3.6066\n",
      "Epoch 80/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.0680 - val_loss: -2.2156\n",
      "Epoch 81/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.4432 - val_loss: -3.5251\n",
      "Epoch 82/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1428 - val_loss: -3.5094\n",
      "Epoch 83/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4138 - val_loss: -3.8007\n",
      "Epoch 84/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5089 - val_loss: -2.8772\n",
      "Epoch 85/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5839 - val_loss: -4.0781\n",
      "Epoch 86/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5271 - val_loss: -3.6575\n",
      "Epoch 87/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6672 - val_loss: -2.8003\n",
      "Epoch 88/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.5689 - val_loss: -3.8001\n",
      "Epoch 89/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7104 - val_loss: -3.2339\n",
      "Epoch 90/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6553 - val_loss: -4.1483\n",
      "Epoch 91/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7428 - val_loss: -3.7344\n",
      "Epoch 92/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6563 - val_loss: -4.0817\n",
      "Epoch 93/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6266 - val_loss: -2.4494\n",
      "Epoch 94/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7160 - val_loss: -3.9388\n",
      "Epoch 95/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6410 - val_loss: -4.1157\n",
      "Epoch 96/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7267 - val_loss: -4.2241\n",
      "Epoch 97/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5717 - val_loss: -3.4659\n",
      "Epoch 98/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6995 - val_loss: -3.9127\n",
      "Epoch 99/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7247 - val_loss: -3.7982\n",
      "Epoch 100/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7695 - val_loss: -3.8587\n",
      "Epoch 101/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7591 - val_loss: -3.4624\n",
      "Epoch 102/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7994 - val_loss: -3.9920\n",
      "Epoch 103/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5563 - val_loss: -3.1855\n",
      "Epoch 104/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7151 - val_loss: -4.0507\n",
      "Epoch 105/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6616 - val_loss: -4.0478\n",
      "Epoch 106/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8004 - val_loss: -3.0761\n",
      "Epoch 107/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -4.8244 - val_loss: -4.2693\n",
      "Epoch 108/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7851 - val_loss: -3.5769\n",
      "Epoch 109/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8110 - val_loss: -2.9102\n",
      "Epoch 110/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7793 - val_loss: -3.4433\n",
      "Epoch 111/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7036 - val_loss: -3.7677\n",
      "Epoch 112/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8114 - val_loss: -3.8050\n",
      "Epoch 113/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8458 - val_loss: -3.6671\n",
      "Epoch 114/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8469 - val_loss: -2.4538\n",
      "Epoch 115/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7534 - val_loss: -4.0002\n",
      "Epoch 116/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8123 - val_loss: -4.1266\n",
      "Epoch 117/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0949 - val_loss: -2.8485\n",
      "Epoch 118/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -2.8491 - val_loss: -3.6083\n",
      "Epoch 119/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.4944 - val_loss: -3.8882\n",
      "Epoch 120/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.8395 - val_loss: -3.6735\n",
      "Epoch 121/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2461 - val_loss: -3.9487\n",
      "Epoch 122/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4601 - val_loss: -3.2970\n",
      "Epoch 123/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4418 - val_loss: -3.2065\n",
      "Epoch 124/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5812 - val_loss: -3.6071\n",
      "Epoch 125/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5267 - val_loss: -4.1772\n",
      "Epoch 126/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6622"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -4.6622 - val_loss: -4.3353\n",
      "Epoch 127/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6155 - val_loss: -2.8469\n",
      "Epoch 128/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6646 - val_loss: -4.0107\n",
      "Epoch 129/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6355 - val_loss: -4.0717\n",
      "Epoch 130/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7265 - val_loss: -3.9498\n",
      "Epoch 131/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6622 - val_loss: -4.1596\n",
      "Epoch 132/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6756 - val_loss: -3.7763\n",
      "Epoch 133/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7421 - val_loss: -4.0546\n",
      "Epoch 134/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3949 - val_loss: -4.0648\n",
      "Epoch 135/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6090 - val_loss: -3.1789\n",
      "Epoch 136/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6903 - val_loss: -3.8975\n",
      "Epoch 137/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6169 - val_loss: -4.1367\n",
      "Epoch 138/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5393 - val_loss: -4.1008\n",
      "Epoch 139/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7678 - val_loss: -4.1059\n",
      "Epoch 140/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8198 - val_loss: -4.0188\n",
      "Epoch 141/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6717 - val_loss: -4.1546\n",
      "Epoch 142/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7955 - val_loss: -4.0165\n",
      "Epoch 143/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7317 - val_loss: -4.2115\n",
      "Epoch 144/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6424 - val_loss: -4.0216\n",
      "Epoch 145/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3551 - val_loss: -3.9578\n",
      "Epoch 146/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6875 - val_loss: -4.1152\n",
      "Epoch 147/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8201 - val_loss: 0.2419\n",
      "Epoch 148/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7060 - val_loss: -3.4970\n",
      "Epoch 149/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7865 - val_loss: -4.0210\n",
      "Epoch 150/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7217 - val_loss: -3.7475\n",
      "Epoch 151/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8405 - val_loss: -4.0537\n",
      "Epoch 152/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7696 - val_loss: -3.1866\n",
      "Epoch 153/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8023 - val_loss: -3.5833\n",
      "Epoch 154/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8692 - val_loss: -4.1920\n",
      "Epoch 155/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6790 - val_loss: -4.0450\n",
      "Epoch 156/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8223 - val_loss: -3.0602\n",
      "Epoch 157/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7718 - val_loss: -3.8986\n",
      "Epoch 158/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8675 - val_loss: -4.2892\n",
      "Epoch 159/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8286 - val_loss: -3.8881\n",
      "Epoch 160/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8403 - val_loss: 6.0850\n",
      "Epoch 161/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7091 - val_loss: -3.8339\n",
      "Epoch 162/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8683 - val_loss: -4.1690\n",
      "Epoch 163/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8340 - val_loss: -4.1083\n",
      "Epoch 164/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8346"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -4.8346 - val_loss: -4.3367\n",
      "Epoch 165/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3597 - val_loss: -3.8253\n",
      "Epoch 166/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3239 - val_loss: -4.0423\n",
      "Epoch 167/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6634 - val_loss: -4.0387\n",
      "Epoch 168/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7708 - val_loss: -4.0765\n",
      "Epoch 169/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7517 - val_loss: -4.1968\n",
      "Epoch 170/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8585 - val_loss: -3.9745\n",
      "Epoch 171/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7396 - val_loss: -3.4913\n",
      "Epoch 172/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7652 - val_loss: -3.0886\n",
      "Epoch 173/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8314 - val_loss: -4.2210\n",
      "Epoch 174/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6424 - val_loss: -4.0745\n",
      "Epoch 175/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9468 - val_loss: -1.5969\n",
      "Epoch 176/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6575 - val_loss: -3.9938\n",
      "Epoch 177/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8202 - val_loss: -2.8167\n",
      "Epoch 178/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8553 - val_loss: -3.9757\n",
      "Epoch 179/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7997 - val_loss: -4.1495\n",
      "Epoch 180/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9124 - val_loss: -4.1854\n",
      "Epoch 181/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8830 - val_loss: -4.1746\n",
      "Epoch 182/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9370 - val_loss: -3.9203\n",
      "Epoch 183/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8631 - val_loss: -3.4554\n",
      "Epoch 184/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9507 - val_loss: -4.2306\n",
      "Epoch 185/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9451 - val_loss: -3.7655\n",
      "Epoch 186/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9032"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -4.9032 - val_loss: -4.3489\n",
      "Epoch 187/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.0955 - val_loss: -3.2954\n",
      "Epoch 188/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.7026 - val_loss: -3.5319\n",
      "Epoch 189/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.0407 - val_loss: -4.2523\n",
      "Epoch 190/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5307 - val_loss: -4.2756\n",
      "Epoch 191/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8013 - val_loss: -2.8399\n",
      "Epoch 192/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3549 - val_loss: -4.0811\n",
      "Epoch 193/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -2.9149 - val_loss: -3.4559\n",
      "Epoch 194/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.4551 - val_loss: -3.6207\n",
      "Epoch 195/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.6119 - val_loss: -3.5855\n",
      "Epoch 196/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.8480 - val_loss: -3.8512\n",
      "Epoch 197/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1503 - val_loss: -3.9893\n",
      "Epoch 198/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2244 - val_loss: -3.3631\n",
      "Epoch 199/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1057 - val_loss: -3.2644\n",
      "Epoch 200/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1569 - val_loss: -3.6722\n",
      "Epoch 201/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0919 - val_loss: -4.0628\n",
      "Epoch 202/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5624 - val_loss: -4.0453\n",
      "Epoch 203/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5136 - val_loss: -3.8949\n",
      "Epoch 204/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2371 - val_loss: -0.9801\n",
      "Epoch 205/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6394 - val_loss: -3.9365\n",
      "Epoch 206/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6116 - val_loss: -3.6760\n",
      "Epoch 207/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5472 - val_loss: -4.0862\n",
      "Epoch 208/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7007 - val_loss: -4.0440\n",
      "Epoch 209/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7481 - val_loss: -3.6928\n",
      "Epoch 210/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7846 - val_loss: -4.1344\n",
      "Epoch 211/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7987 - val_loss: -3.5763\n",
      "Epoch 212/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7489 - val_loss: -4.1049\n",
      "Epoch 213/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8456 - val_loss: -3.8760\n",
      "Epoch 214/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8363 - val_loss: -4.2408\n",
      "Epoch 215/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8698 - val_loss: -3.9256\n",
      "Epoch 216/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6315 - val_loss: -3.7637\n",
      "Epoch 217/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8721 - val_loss: -3.7526\n",
      "Epoch 218/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8720 - val_loss: -4.2697\n",
      "Epoch 219/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9147 - val_loss: -2.7725\n",
      "Epoch 220/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8312 - val_loss: -3.7919\n",
      "Epoch 221/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8200 - val_loss: -3.7990\n",
      "Epoch 222/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9096 - val_loss: -4.1147\n",
      "Epoch 223/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6090 - val_loss: -3.6144\n",
      "Epoch 224/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -2.9782 - val_loss: -4.1954\n",
      "Epoch 225/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.7878 - val_loss: -4.0840\n",
      "Epoch 226/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9286 - val_loss: -4.3080\n",
      "Epoch 227/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0406 - val_loss: -3.6218\n",
      "Epoch 228/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0917 - val_loss: -4.1988\n",
      "Epoch 229/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -3.3361 - val_loss: -3.9577\n",
      "Epoch 230/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -3.6641 - val_loss: -3.1622\n",
      "Epoch 231/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9168 - val_loss: -4.3055\n",
      "Epoch 232/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0143 - val_loss: -3.8010\n",
      "Epoch 233/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.7562 - val_loss: -3.0727\n",
      "Epoch 234/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.4928 - val_loss: -1.9131\n",
      "Epoch 235/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.6193 - val_loss: -3.3916\n",
      "Epoch 236/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.6912 - val_loss: -2.1403\n",
      "Epoch 237/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.5469 - val_loss: -3.3614\n",
      "Epoch 238/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7612 - val_loss: -2.6323\n",
      "Epoch 239/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.7965 - val_loss: -3.5820\n",
      "Epoch 240/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9475 - val_loss: -3.4532\n",
      "Epoch 241/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9184 - val_loss: -3.6409\n",
      "Epoch 242/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.8614 - val_loss: -3.0464\n",
      "Epoch 243/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9516 - val_loss: -3.5719\n",
      "Epoch 244/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9699 - val_loss: -3.6943\n",
      "Epoch 245/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.1384 - val_loss: -3.7692\n",
      "Epoch 246/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3133 - val_loss: -4.0219\n",
      "Epoch 247/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3067 - val_loss: -2.9524\n",
      "Epoch 248/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3357 - val_loss: -3.9140\n",
      "Epoch 249/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4591 - val_loss: -4.1776\n",
      "Epoch 250/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4606 - val_loss: -3.4776\n",
      "Epoch 251/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4633 - val_loss: -3.0792\n",
      "Epoch 252/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5295 - val_loss: -3.9715\n",
      "Epoch 253/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2836 - val_loss: -3.3980\n",
      "Epoch 254/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5427 - val_loss: -1.4874\n",
      "Epoch 255/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4537 - val_loss: -4.1048\n",
      "Epoch 256/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5157 - val_loss: -4.0133\n",
      "Epoch 257/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7373 - val_loss: -2.3566\n",
      "Epoch 258/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6844 - val_loss: -4.2154\n",
      "Epoch 259/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8050 - val_loss: -3.2469\n",
      "Epoch 260/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8088 - val_loss: -4.2797\n",
      "Epoch 261/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8245 - val_loss: -3.4524\n",
      "Epoch 262/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9720 - val_loss: -4.2808\n",
      "Epoch 263/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9470 - val_loss: -4.2219\n",
      "Epoch 264/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8509 - val_loss: -3.7964\n",
      "Epoch 265/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7091 - val_loss: -3.7301\n",
      "Epoch 266/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8957 - val_loss: -3.3919\n",
      "Epoch 267/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9827 - val_loss: -4.1731\n",
      "Epoch 268/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9334 - val_loss: -4.0956\n",
      "Epoch 269/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9551 - val_loss: -3.8749\n",
      "Epoch 270/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9212 - val_loss: -3.5694\n",
      "Epoch 271/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9810 - val_loss: -3.5673\n",
      "Epoch 272/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9776"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -4.9776 - val_loss: -4.4028\n",
      "Epoch 273/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9222 - val_loss: -4.1955\n",
      "Epoch 274/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0380 - val_loss: -4.3161\n",
      "Epoch 275/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9743 - val_loss: -4.2289\n",
      "Epoch 276/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8584 - val_loss: -3.7152\n",
      "Epoch 277/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0000 - val_loss: -4.1195\n",
      "Epoch 278/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9474 - val_loss: -4.1463\n",
      "Epoch 279/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0728"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -5.0728 - val_loss: -4.5064\n",
      "Epoch 280/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9310 - val_loss: -4.0154\n",
      "Epoch 281/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0130 - val_loss: -3.0139\n",
      "Epoch 282/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8523 - val_loss: -4.3726\n",
      "Epoch 283/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6584 - val_loss: -3.7968\n",
      "Epoch 284/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8480 - val_loss: -3.4043\n",
      "Epoch 285/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9714 - val_loss: -2.6978\n",
      "Epoch 286/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9932 - val_loss: -4.1434\n",
      "Epoch 287/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8681 - val_loss: -3.6182\n",
      "Epoch 288/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7340 - val_loss: -3.9664\n",
      "Epoch 289/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9917 - val_loss: -4.2080\n",
      "Epoch 290/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0625 - val_loss: -3.8125\n",
      "Epoch 291/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0014 - val_loss: -3.9419\n",
      "Epoch 292/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0362 - val_loss: -4.0586\n",
      "Epoch 293/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0222"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 246ms/step - loss: -5.0222 - val_loss: -4.5469\n",
      "Epoch 294/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0265 - val_loss: -4.2437\n",
      "Epoch 295/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9694 - val_loss: -4.2119\n",
      "Epoch 296/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0681 - val_loss: -3.8292\n",
      "Epoch 297/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9960 - val_loss: -4.1392\n",
      "Epoch 298/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4509 - val_loss: -3.9594\n",
      "Epoch 299/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0026 - val_loss: -4.1310\n",
      "Epoch 300/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9921 - val_loss: -4.2059\n",
      "Epoch 301/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9844 - val_loss: -3.8340\n",
      "Epoch 302/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0494 - val_loss: -3.6048\n",
      "Epoch 303/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9169 - val_loss: -3.8648\n",
      "Epoch 304/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0836 - val_loss: -4.0531\n",
      "Epoch 305/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0504 - val_loss: -4.3074\n",
      "Epoch 306/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9497 - val_loss: -4.1873\n",
      "Epoch 307/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0511 - val_loss: -3.3404\n",
      "Epoch 308/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0111 - val_loss: -4.4142\n",
      "Epoch 309/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0803 - val_loss: -4.2919\n",
      "Epoch 310/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1022 - val_loss: -3.8276\n",
      "Epoch 311/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3824 - val_loss: -4.2943\n",
      "Epoch 312/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9957 - val_loss: -3.8398\n",
      "Epoch 313/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9117 - val_loss: -4.1790\n",
      "Epoch 314/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9941 - val_loss: -4.1225\n",
      "Epoch 315/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1192 - val_loss: -4.0197\n",
      "Epoch 316/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8680 - val_loss: -3.7808\n",
      "Epoch 317/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1486 - val_loss: -3.5422\n",
      "Epoch 318/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0844 - val_loss: -3.9961\n",
      "Epoch 319/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1272 - val_loss: -4.0597\n",
      "Epoch 320/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9729 - val_loss: -4.5198\n",
      "Epoch 321/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1290 - val_loss: -4.1721\n",
      "Epoch 322/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9455 - val_loss: -4.3817\n",
      "Epoch 323/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0622 - val_loss: -4.3974\n",
      "Epoch 324/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1120 - val_loss: -2.3148\n",
      "Epoch 325/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0534 - val_loss: -3.5312\n",
      "Epoch 326/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0598 - val_loss: -4.1292\n",
      "Epoch 327/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.8456 - val_loss: -4.1900\n",
      "Epoch 328/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0332 - val_loss: -4.1945\n",
      "Epoch 329/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3932 - val_loss: -4.3393\n",
      "Epoch 330/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7001 - val_loss: -4.2800\n",
      "Epoch 331/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7339 - val_loss: -3.4847\n",
      "Epoch 332/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4626 - val_loss: -4.3977\n",
      "Epoch 333/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9183 - val_loss: -3.8418\n",
      "Epoch 334/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9068 - val_loss: -4.1105\n",
      "Epoch 335/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9409 - val_loss: -3.9916\n",
      "Epoch 336/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8086 - val_loss: -3.8406\n",
      "Epoch 337/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2731 - val_loss: -3.3381\n",
      "Epoch 338/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4062 - val_loss: -4.3760\n",
      "Epoch 339/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7745 - val_loss: -4.1228\n",
      "Epoch 340/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8965 - val_loss: -3.3271\n",
      "Epoch 341/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9019 - val_loss: 0.3223\n",
      "Epoch 342/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0288 - val_loss: -2.0950\n",
      "Epoch 343/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8584 - val_loss: -4.1498\n",
      "Epoch 344/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9512 - val_loss: -4.2491\n",
      "Epoch 345/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0312 - val_loss: -3.9577\n",
      "Epoch 346/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0114 - val_loss: -2.5403\n",
      "Epoch 347/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.4756 - val_loss: -3.8320\n",
      "Epoch 348/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4803 - val_loss: -3.5427\n",
      "Epoch 349/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6066 - val_loss: -2.3289\n",
      "Epoch 350/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6396 - val_loss: -3.3769\n",
      "Epoch 351/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8026 - val_loss: -3.7441\n",
      "Epoch 352/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.6318 - val_loss: -3.6125\n",
      "Epoch 353/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8197 - val_loss: -4.1340\n",
      "Epoch 354/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8503 - val_loss: -4.1525\n",
      "Epoch 355/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9222 - val_loss: -3.7894\n",
      "Epoch 356/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9705 - val_loss: -4.1659\n",
      "Epoch 357/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9867 - val_loss: -4.3288\n",
      "Epoch 358/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9029 - val_loss: -3.8038\n",
      "Epoch 359/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8996 - val_loss: -4.3156\n",
      "Epoch 360/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9299 - val_loss: -3.8804\n",
      "Epoch 361/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0015 - val_loss: -4.2222\n",
      "Epoch 362/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0182 - val_loss: -3.1459\n",
      "Epoch 363/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0149 - val_loss: -4.4554\n",
      "Epoch 364/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8302 - val_loss: -3.3988\n",
      "Epoch 365/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9703 - val_loss: -4.3087\n",
      "Epoch 366/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0603 - val_loss: -3.9190\n",
      "Epoch 367/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0508 - val_loss: -3.5182\n",
      "Epoch 368/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -2.9165 - val_loss: -3.5226\n",
      "Epoch 369/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0833 - val_loss: -3.5810\n",
      "Epoch 370/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3923 - val_loss: -3.9246\n",
      "Epoch 371/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3397 - val_loss: -3.6497\n",
      "Epoch 372/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4774 - val_loss: -3.9670\n",
      "Epoch 373/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5672 - val_loss: -4.0954\n",
      "Epoch 374/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.6639 - val_loss: -2.1602\n",
      "Epoch 375/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6767 - val_loss: -4.0916\n",
      "Epoch 376/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7202 - val_loss: -3.0617\n",
      "Epoch 377/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6225 - val_loss: -4.0869\n",
      "Epoch 378/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6991 - val_loss: -4.1530\n",
      "Epoch 379/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7714 - val_loss: -2.9475\n",
      "Epoch 380/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7425 - val_loss: -3.1460\n",
      "Epoch 381/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7194 - val_loss: -2.3244\n",
      "Epoch 382/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7274 - val_loss: -4.2136\n",
      "Epoch 383/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7800 - val_loss: -3.5580\n",
      "Epoch 384/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7594 - val_loss: -3.8376\n",
      "Epoch 385/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7997 - val_loss: -4.0512\n",
      "Epoch 386/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4869 - val_loss: -3.7584\n",
      "Epoch 387/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4080 - val_loss: -2.8203\n",
      "Epoch 388/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5599 - val_loss: -3.9376\n",
      "Epoch 389/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7553 - val_loss: -2.9365\n",
      "Epoch 390/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5197 - val_loss: -4.1780\n",
      "Epoch 391/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7240 - val_loss: -2.7274\n",
      "Epoch 392/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8071 - val_loss: -3.8897\n",
      "Epoch 393/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7588 - val_loss: -4.1473\n",
      "Epoch 394/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8198 - val_loss: -3.5129\n",
      "Epoch 395/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9806 - val_loss: -3.5114\n",
      "Epoch 396/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9294 - val_loss: -3.2083\n",
      "Epoch 397/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2401 - val_loss: -3.6502\n",
      "Epoch 398/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5295 - val_loss: -3.6149\n",
      "Epoch 399/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4847 - val_loss: -4.0343\n",
      "Epoch 400/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7261 - val_loss: -4.3575\n",
      "Epoch 401/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7391 - val_loss: -3.9901\n",
      "Epoch 402/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8268 - val_loss: -4.1286\n",
      "Epoch 403/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2868 - val_loss: -3.9287\n",
      "Epoch 404/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7167 - val_loss: -4.0427\n",
      "Epoch 405/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7255 - val_loss: -4.0148\n",
      "Epoch 406/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7568 - val_loss: -3.3568\n",
      "Epoch 407/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5353 - val_loss: -3.7551\n",
      "Epoch 408/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7845 - val_loss: -3.9430\n",
      "Epoch 409/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6055 - val_loss: -4.1831\n",
      "Epoch 410/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8845 - val_loss: -3.1719\n",
      "Epoch 411/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8361 - val_loss: -3.7432\n",
      "Epoch 412/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9309 - val_loss: -3.7894\n",
      "Epoch 413/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9168 - val_loss: -3.2894\n",
      "Epoch 414/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6300 - val_loss: -1.6262\n",
      "Epoch 415/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.8859 - val_loss: -4.0413\n",
      "Epoch 416/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9195 - val_loss: -4.2599\n",
      "Epoch 417/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8003 - val_loss: -3.9528\n",
      "Epoch 418/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9466 - val_loss: -4.2475\n",
      "Epoch 419/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9335 - val_loss: -3.9950\n",
      "Epoch 420/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9476 - val_loss: -4.1662\n",
      "Epoch 421/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0237 - val_loss: -3.8689\n",
      "Epoch 422/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9341 - val_loss: -4.2667\n",
      "Epoch 423/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9455 - val_loss: -3.8432\n",
      "Epoch 424/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9617 - val_loss: -4.2771\n",
      "Epoch 425/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9812 - val_loss: -4.0601\n",
      "Epoch 426/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0095 - val_loss: -2.5811\n",
      "Epoch 427/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9540 - val_loss: -3.4100\n",
      "Epoch 428/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8000 - val_loss: -3.6922\n",
      "Epoch 429/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0429 - val_loss: -4.2479\n",
      "Epoch 430/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8475 - val_loss: -4.2478\n",
      "Epoch 431/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0157 - val_loss: -4.3278\n",
      "Epoch 432/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9890 - val_loss: -4.0645\n",
      "Epoch 433/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0021 - val_loss: -3.9311\n",
      "Epoch 434/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9754 - val_loss: -3.9850\n",
      "Epoch 435/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0988 - val_loss: -4.3067\n",
      "Epoch 436/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9885 - val_loss: -4.2798\n",
      "Epoch 437/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1146 - val_loss: -4.1912\n",
      "Epoch 438/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.4910 - val_loss: -4.0918\n",
      "Epoch 439/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4300 - val_loss: -4.1591\n",
      "Epoch 440/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6300 - val_loss: -3.7776\n",
      "Epoch 441/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7565 - val_loss: 1.3306\n",
      "Epoch 442/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7443 - val_loss: -2.4003\n",
      "Epoch 443/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7680 - val_loss: -1.9837\n",
      "Epoch 444/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8115 - val_loss: -3.9797\n",
      "Epoch 445/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9401 - val_loss: -3.5054\n",
      "Epoch 446/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8925 - val_loss: -4.3517\n",
      "Epoch 447/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.7230 - val_loss: -3.4504\n",
      "Epoch 448/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1259 - val_loss: -3.8238\n",
      "Epoch 449/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2961 - val_loss: -3.8419\n",
      "Epoch 450/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4535 - val_loss: -4.1509\n",
      "Epoch 451/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4556 - val_loss: -4.3884\n",
      "Epoch 452/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4428 - val_loss: -4.2107\n",
      "Epoch 453/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6345 - val_loss: -4.4064\n",
      "Epoch 454/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6580 - val_loss: -4.3773\n",
      "Epoch 455/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6969 - val_loss: -4.3354\n",
      "Epoch 456/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7108 - val_loss: -3.0363\n",
      "Epoch 457/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7768 - val_loss: -4.2984\n",
      "Epoch 458/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8458 - val_loss: -4.4064\n",
      "Epoch 459/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7299 - val_loss: -3.6138\n",
      "Epoch 460/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7953 - val_loss: -3.9430\n",
      "Epoch 461/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8141 - val_loss: -3.8728\n",
      "Epoch 462/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9019 - val_loss: -3.8949\n",
      "Epoch 463/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6818 - val_loss: -4.4730\n",
      "Epoch 464/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8674 - val_loss: -4.3602\n",
      "Epoch 465/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9535 - val_loss: -4.3418\n",
      "Epoch 466/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9767 - val_loss: -4.4428\n",
      "Epoch 467/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9989 - val_loss: -4.2804\n",
      "Epoch 468/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0209 - val_loss: -3.9058\n",
      "Epoch 469/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9482 - val_loss: -4.3361\n",
      "Epoch 470/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9832 - val_loss: -3.9856\n",
      "Epoch 471/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9952 - val_loss: -4.3769\n",
      "Epoch 472/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9761 - val_loss: -4.4478\n",
      "Epoch 473/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9666 - val_loss: -3.7196\n",
      "Epoch 474/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5860 - val_loss: -4.0215\n",
      "Epoch 475/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0046 - val_loss: -1.1734\n",
      "Epoch 476/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0518 - val_loss: -4.0523\n",
      "Epoch 477/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9385 - val_loss: -4.2823\n",
      "Epoch 478/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8509 - val_loss: -3.9812\n",
      "Epoch 479/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0936 - val_loss: -4.5122\n",
      "Epoch 480/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6802 - val_loss: -4.3976\n",
      "Epoch 481/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9806 - val_loss: -4.4087\n",
      "Epoch 482/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9993 - val_loss: -4.2709\n",
      "Epoch 483/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0143 - val_loss: -4.2267\n",
      "Epoch 484/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1014 - val_loss: -3.5191\n",
      "Epoch 485/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1114 - val_loss: -4.3391\n",
      "Epoch 486/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0290 - val_loss: -4.5221\n",
      "Epoch 487/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3631 - val_loss: -4.1782\n",
      "Epoch 488/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8886 - val_loss: -3.5263\n",
      "Epoch 489/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0022 - val_loss: -4.4047\n",
      "Epoch 490/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0607 - val_loss: -3.9401\n",
      "Epoch 491/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9971 - val_loss: -4.4357\n",
      "Epoch 492/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9678 - val_loss: -4.1766\n",
      "Epoch 493/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0871 - val_loss: -4.4912\n",
      "Epoch 494/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0620 - val_loss: -4.3158\n",
      "Epoch 495/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1284 - val_loss: -4.3952\n",
      "Epoch 496/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0824 - val_loss: -2.4596\n",
      "Epoch 497/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9706 - val_loss: -4.1855\n",
      "Epoch 498/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0543 - val_loss: -4.4001\n",
      "Epoch 499/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1425 - val_loss: -4.4110\n",
      "Epoch 500/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1629 - val_loss: -4.4041\n",
      "Epoch 501/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0869 - val_loss: -4.3883\n",
      "Epoch 502/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2132 - val_loss: -4.3428\n",
      "Epoch 503/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1526 - val_loss: -3.0229\n",
      "Epoch 504/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7692 - val_loss: -3.7004\n",
      "Epoch 505/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8646 - val_loss: -3.7591\n",
      "Epoch 506/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9370 - val_loss: -4.0538\n",
      "Epoch 507/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0680 - val_loss: -4.0727\n",
      "Epoch 508/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0424 - val_loss: -4.4375\n",
      "Epoch 509/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0424 - val_loss: -3.2574\n",
      "Epoch 510/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9960 - val_loss: -4.1337\n",
      "Epoch 511/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1455 - val_loss: -3.7899\n",
      "Epoch 512/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0174 - val_loss: -4.1951\n",
      "Epoch 513/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9466 - val_loss: -3.3433\n",
      "Epoch 514/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1379 - val_loss: -3.9895\n",
      "Epoch 515/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0500 - val_loss: -4.3946\n",
      "Epoch 516/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0967 - val_loss: -4.3122\n",
      "Epoch 517/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0985 - val_loss: -3.9120\n",
      "Epoch 518/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2240 - val_loss: -4.1798\n",
      "Epoch 519/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0475 - val_loss: -3.5352\n",
      "Epoch 520/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1731 - val_loss: -4.4749\n",
      "Epoch 521/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1134 - val_loss: -4.3047\n",
      "Epoch 522/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.1324 - val_loss: -4.3441\n",
      "Epoch 523/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1329 - val_loss: -4.1199\n",
      "Epoch 524/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0668 - val_loss: -4.1071\n",
      "Epoch 525/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1758 - val_loss: -4.1354\n",
      "Epoch 526/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1695 - val_loss: -4.3387\n",
      "Epoch 527/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1812 - val_loss: -3.8260\n",
      "Epoch 528/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9815 - val_loss: -3.7650\n",
      "Epoch 529/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2823 - val_loss: -3.9678\n",
      "Epoch 530/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2080 - val_loss: -4.0924\n",
      "Epoch 531/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1178 - val_loss: -4.0906\n",
      "Epoch 532/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1986 - val_loss: -4.1876\n",
      "Epoch 533/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1922 - val_loss: -4.1481\n",
      "Epoch 534/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1715 - val_loss: -2.7356\n",
      "Epoch 535/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1717 - val_loss: -4.3379\n",
      "Epoch 536/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2152"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -5.2152 - val_loss: -4.5948\n",
      "Epoch 537/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8917 - val_loss: -3.8780\n",
      "Epoch 538/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2231 - val_loss: -4.3409\n",
      "Epoch 539/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0559 - val_loss: -2.9360\n",
      "Epoch 540/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2145 - val_loss: -4.0824\n",
      "Epoch 541/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1725 - val_loss: -4.4347\n",
      "Epoch 542/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2591 - val_loss: -4.1151\n",
      "Epoch 543/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0238 - val_loss: -4.1788\n",
      "Epoch 544/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2730 - val_loss: -3.1878\n",
      "Epoch 545/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2239 - val_loss: -4.4309\n",
      "Epoch 546/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2555 - val_loss: -2.9893\n",
      "Epoch 547/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1433 - val_loss: -4.2860\n",
      "Epoch 548/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0112 - val_loss: -3.1699\n",
      "Epoch 549/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1343 - val_loss: -3.5145\n",
      "Epoch 550/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2552 - val_loss: -4.5168\n",
      "Epoch 551/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0697 - val_loss: -4.2883\n",
      "Epoch 552/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2437 - val_loss: -4.2568\n",
      "Epoch 553/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1099 - val_loss: -3.0631\n",
      "Epoch 554/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2204 - val_loss: -4.4895\n",
      "Epoch 555/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2558 - val_loss: -4.2749\n",
      "Epoch 556/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0582 - val_loss: -4.4303\n",
      "Epoch 557/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1817 - val_loss: -4.4966\n",
      "Epoch 558/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2678 - val_loss: -4.5000\n",
      "Epoch 559/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1863 - val_loss: -3.6864\n",
      "Epoch 560/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2282 - val_loss: -3.5338\n",
      "Epoch 561/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2316 - val_loss: -4.4767\n",
      "Epoch 562/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2667 - val_loss: -3.9431\n",
      "Epoch 563/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2823 - val_loss: -4.1709\n",
      "Epoch 564/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2617 - val_loss: -3.8111\n",
      "Epoch 565/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2595 - val_loss: -4.4279\n",
      "Epoch 566/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2296 - val_loss: -4.1333\n",
      "Epoch 567/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2498 - val_loss: -4.3359\n",
      "Epoch 568/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2235 - val_loss: -3.9578\n",
      "Epoch 569/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2953 - val_loss: -4.3181\n",
      "Epoch 570/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1995 - val_loss: -4.3645\n",
      "Epoch 571/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2335 - val_loss: -3.7903\n",
      "Epoch 572/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2983 - val_loss: -4.0702\n",
      "Epoch 573/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3290 - val_loss: -3.6541\n",
      "Epoch 574/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1977 - val_loss: -3.8170\n",
      "Epoch 575/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2850 - val_loss: -2.5927\n",
      "Epoch 576/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2102 - val_loss: -4.4866\n",
      "Epoch 577/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3552 - val_loss: -3.7653\n",
      "Epoch 578/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1363 - val_loss: -4.1367\n",
      "Epoch 579/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2769 - val_loss: -4.4242\n",
      "Epoch 580/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0311 - val_loss: -3.6250\n",
      "Epoch 581/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1453 - val_loss: -4.3531\n",
      "Epoch 582/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2095 - val_loss: -2.0451\n",
      "Epoch 583/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1474 - val_loss: -4.3157\n",
      "Epoch 584/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2807 - val_loss: -2.6760\n",
      "Epoch 585/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2872 - val_loss: -3.8192\n",
      "Epoch 586/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2700 - val_loss: -4.3877\n",
      "Epoch 587/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1390 - val_loss: -3.3345\n",
      "Epoch 588/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2295"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -5.2295 - val_loss: -4.6309\n",
      "Epoch 589/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3535 - val_loss: -4.3251\n",
      "Epoch 590/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2156 - val_loss: -4.2961\n",
      "Epoch 591/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2037"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -5.2037 - val_loss: -4.7188\n",
      "Epoch 592/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3157 - val_loss: -3.7337\n",
      "Epoch 593/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2050 - val_loss: -4.3070\n",
      "Epoch 594/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3304 - val_loss: -4.3429\n",
      "Epoch 595/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2883 - val_loss: -4.1144\n",
      "Epoch 596/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2355 - val_loss: -2.1035\n",
      "Epoch 597/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2979 - val_loss: -4.1099\n",
      "Epoch 598/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2694 - val_loss: -4.3776\n",
      "Epoch 599/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2802 - val_loss: -3.0173\n",
      "Epoch 600/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3110 - val_loss: -4.6010\n",
      "Epoch 601/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2451 - val_loss: -4.4603\n",
      "Epoch 602/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2871"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 242ms/step - loss: -5.2871 - val_loss: -4.7491\n",
      "Epoch 603/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2925 - val_loss: -2.7526\n",
      "Epoch 604/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3261 - val_loss: -3.5101\n",
      "Epoch 605/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3285 - val_loss: -4.2736\n",
      "Epoch 606/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3205 - val_loss: -4.3729\n",
      "Epoch 607/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2331 - val_loss: -2.9451\n",
      "Epoch 608/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3378 - val_loss: -2.5259\n",
      "Epoch 609/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0829 - val_loss: -4.2302\n",
      "Epoch 610/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9106 - val_loss: -4.0499\n",
      "Epoch 611/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2119 - val_loss: -3.5676\n",
      "Epoch 612/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0692 - val_loss: -4.1121\n",
      "Epoch 613/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2882 - val_loss: -4.2144\n",
      "Epoch 614/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0384 - val_loss: -4.1862\n",
      "Epoch 615/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3299 - val_loss: -3.4257\n",
      "Epoch 616/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9770 - val_loss: -4.5896\n",
      "Epoch 617/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2093 - val_loss: -4.4693\n",
      "Epoch 618/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2665 - val_loss: -4.5297\n",
      "Epoch 619/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2461 - val_loss: 2.7242\n",
      "Epoch 620/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1589 - val_loss: -4.5724\n",
      "Epoch 621/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2645 - val_loss: -4.3448\n",
      "Epoch 622/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2954 - val_loss: -4.1474\n",
      "Epoch 623/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3257 - val_loss: -4.3112\n",
      "Epoch 624/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1405 - val_loss: -4.3503\n",
      "Epoch 625/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3340 - val_loss: -4.4991\n",
      "Epoch 626/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1917 - val_loss: -3.9659\n",
      "Epoch 627/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2961 - val_loss: -4.2855\n",
      "Epoch 628/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3158 - val_loss: -3.9612\n",
      "Epoch 629/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2317 - val_loss: -4.3289\n",
      "Epoch 630/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3460 - val_loss: -4.3158\n",
      "Epoch 631/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3139 - val_loss: -3.9781\n",
      "Epoch 632/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2618 - val_loss: -4.3984\n",
      "Epoch 633/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3394 - val_loss: -2.8663\n",
      "Epoch 634/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2327 - val_loss: -4.3434\n",
      "Epoch 635/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2680 - val_loss: -2.7353\n",
      "Epoch 636/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3765 - val_loss: -3.7927\n",
      "Epoch 637/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1530 - val_loss: -3.8014\n",
      "Epoch 638/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3733 - val_loss: -1.3950\n",
      "Epoch 639/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3204 - val_loss: -3.6591\n",
      "Epoch 640/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2749 - val_loss: -3.1191\n",
      "Epoch 641/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3457 - val_loss: -1.8684\n",
      "Epoch 642/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2720 - val_loss: -4.5700\n",
      "Epoch 643/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3067 - val_loss: -3.8531\n",
      "Epoch 644/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2915 - val_loss: -4.3842\n",
      "Epoch 645/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2768 - val_loss: -1.8563\n",
      "Epoch 646/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2875 - val_loss: -4.3891\n",
      "Epoch 647/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2756 - val_loss: -3.8061\n",
      "Epoch 648/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2499 - val_loss: -4.2764\n",
      "Epoch 649/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3760 - val_loss: -3.8595\n",
      "Epoch 650/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3054 - val_loss: -4.5341\n",
      "Epoch 651/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3640 - val_loss: -4.4993\n",
      "Epoch 652/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6773 - val_loss: -3.2295\n",
      "Epoch 653/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2175 - val_loss: -4.3056\n",
      "Epoch 654/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6983 - val_loss: -4.3955\n",
      "Epoch 655/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9651 - val_loss: -3.9496\n",
      "Epoch 656/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9471 - val_loss: -4.3055\n",
      "Epoch 657/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1154 - val_loss: -4.6601\n",
      "Epoch 658/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2034 - val_loss: -4.0943\n",
      "Epoch 659/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1211 - val_loss: -2.1316\n",
      "Epoch 660/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2112 - val_loss: -4.5054\n",
      "Epoch 661/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0798 - val_loss: -4.4657\n",
      "Epoch 662/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2763 - val_loss: -3.7730\n",
      "Epoch 663/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2402 - val_loss: -3.7678\n",
      "Epoch 664/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0492 - val_loss: -4.4045\n",
      "Epoch 665/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.7685 - val_loss: -3.8496\n",
      "Epoch 666/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.4442 - val_loss: -3.6286\n",
      "Epoch 667/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9160 - val_loss: -4.0889\n",
      "Epoch 668/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9983 - val_loss: -4.4397\n",
      "Epoch 669/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8002 - val_loss: -3.5867\n",
      "Epoch 670/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1027 - val_loss: -4.6142\n",
      "Epoch 671/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1775 - val_loss: -4.4190\n",
      "Epoch 672/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1218 - val_loss: -4.3926\n",
      "Epoch 673/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9262 - val_loss: -3.7825\n",
      "Epoch 674/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1481 - val_loss: -4.1962\n",
      "Epoch 675/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2078 - val_loss: -3.3692\n",
      "Epoch 676/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1955 - val_loss: -1.0945\n",
      "Epoch 677/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0999 - val_loss: -4.2683\n",
      "Epoch 678/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3129 - val_loss: -3.1485\n",
      "Epoch 679/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2546 - val_loss: -4.5960\n",
      "Epoch 680/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2435 - val_loss: -4.0324\n",
      "Epoch 681/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3422 - val_loss: -4.3758\n",
      "Epoch 682/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4082 - val_loss: -4.3372\n",
      "Epoch 683/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0485 - val_loss: -4.5830\n",
      "Epoch 684/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2222 - val_loss: -4.4342\n",
      "Epoch 685/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1361 - val_loss: -3.9794\n",
      "Epoch 686/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2015 - val_loss: -3.5048\n",
      "Epoch 687/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1637 - val_loss: -4.1032\n",
      "Epoch 688/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2286 - val_loss: -2.6232\n",
      "Epoch 689/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2545 - val_loss: -2.0931\n",
      "Epoch 690/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3494 - val_loss: -4.1513\n",
      "Epoch 691/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2356 - val_loss: -3.1544\n",
      "Epoch 692/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2619 - val_loss: -3.1246\n",
      "Epoch 693/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0320 - val_loss: -4.4069\n",
      "Epoch 694/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3234 - val_loss: -4.2408\n",
      "Epoch 695/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2509 - val_loss: -4.3251\n",
      "Epoch 696/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3098 - val_loss: -4.5590\n",
      "Epoch 697/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0187 - val_loss: -3.7129\n",
      "Epoch 698/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3094 - val_loss: -3.8108\n",
      "Epoch 699/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2351 - val_loss: -4.1795\n",
      "Epoch 700/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3186 - val_loss: -4.5245\n",
      "Epoch 701/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3192 - val_loss: -4.1088\n",
      "Epoch 702/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1498 - val_loss: -4.3163\n",
      "Epoch 703/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2114 - val_loss: -4.4412\n",
      "Epoch 704/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3011 - val_loss: -4.2373\n",
      "Epoch 705/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3020 - val_loss: -3.0412\n",
      "Epoch 706/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3354 - val_loss: -3.6737\n",
      "Epoch 707/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3040 - val_loss: -3.5913\n",
      "Epoch 708/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3348 - val_loss: -4.0921\n",
      "Epoch 709/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3526 - val_loss: -4.0713\n",
      "Epoch 710/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3008 - val_loss: -4.0181\n",
      "Epoch 711/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2655 - val_loss: -4.4086\n",
      "Epoch 712/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1938 - val_loss: -4.4890\n",
      "Epoch 713/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4078 - val_loss: -4.3967\n",
      "Epoch 714/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3919 - val_loss: -4.2097\n",
      "Epoch 715/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0414 - val_loss: -4.5587\n",
      "Epoch 716/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2151 - val_loss: -4.4905\n",
      "Epoch 717/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3472 - val_loss: -2.8606\n",
      "Epoch 718/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2675 - val_loss: -1.7046\n",
      "Epoch 719/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1868 - val_loss: -4.3572\n",
      "Epoch 720/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2872 - val_loss: -4.4320\n",
      "Epoch 721/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2596 - val_loss: -4.7133\n",
      "Epoch 722/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2677 - val_loss: -4.5082\n",
      "Epoch 723/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.4237 - val_loss: -3.7259\n",
      "Epoch 724/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4024 - val_loss: -4.5306\n",
      "Epoch 725/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8004 - val_loss: -3.5975\n",
      "Epoch 726/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0543 - val_loss: -4.4519\n",
      "Epoch 727/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0734 - val_loss: -3.6958\n",
      "Epoch 728/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0701 - val_loss: -4.5249\n",
      "Epoch 729/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0915 - val_loss: -4.3551\n",
      "Epoch 730/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1313 - val_loss: -4.4686\n",
      "Epoch 731/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2193 - val_loss: -4.2361\n",
      "Epoch 732/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1102 - val_loss: -4.1712\n",
      "Epoch 733/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3141 - val_loss: -4.1879\n",
      "Epoch 734/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1245 - val_loss: -3.2787\n",
      "Epoch 735/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2593 - val_loss: -3.7735\n",
      "Epoch 736/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1993 - val_loss: -3.9372\n",
      "Epoch 737/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3203 - val_loss: -3.6954\n",
      "Epoch 738/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2792 - val_loss: -1.9280\n",
      "Epoch 739/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2498 - val_loss: -4.2477\n",
      "Epoch 740/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3566 - val_loss: 1.6467\n",
      "Epoch 741/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9331 - val_loss: -2.4000\n",
      "Epoch 742/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9918 - val_loss: -3.7872\n",
      "Epoch 743/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2173 - val_loss: -4.3172\n",
      "Epoch 744/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1862 - val_loss: -4.4473\n",
      "Epoch 745/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2488 - val_loss: -4.5279\n",
      "Epoch 746/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2847 - val_loss: -4.1736\n",
      "Epoch 747/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8147 - val_loss: -3.6167\n",
      "Epoch 748/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9776 - val_loss: -4.1331\n",
      "Epoch 749/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2412 - val_loss: -1.7933\n",
      "Epoch 750/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1684 - val_loss: -4.4582\n",
      "Epoch 751/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0565 - val_loss: -4.3290\n",
      "Epoch 752/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3067 - val_loss: -4.1371\n",
      "Epoch 753/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2974 - val_loss: -3.4063\n",
      "Epoch 754/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1971 - val_loss: -4.3562\n",
      "Epoch 755/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2584 - val_loss: -4.4964\n",
      "Epoch 756/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2658 - val_loss: -3.7258\n",
      "Epoch 757/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3805 - val_loss: -3.4907\n",
      "Epoch 758/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9904 - val_loss: -3.3619\n",
      "Epoch 759/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2414 - val_loss: -4.3355\n",
      "Epoch 760/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2077 - val_loss: -4.4456\n",
      "Epoch 761/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2971 - val_loss: 1.5727\n",
      "Epoch 762/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3021 - val_loss: -2.9780\n",
      "Epoch 763/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3200 - val_loss: -4.0450\n",
      "Epoch 764/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2882 - val_loss: -4.6108\n",
      "Epoch 765/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3453 - val_loss: -1.3146\n",
      "Epoch 766/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2491 - val_loss: -4.6829\n",
      "Epoch 767/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3311 - val_loss: -3.8299\n",
      "Epoch 768/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2413 - val_loss: -4.0853\n",
      "Epoch 769/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3860 - val_loss: -4.2230\n",
      "Epoch 770/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3336 - val_loss: -4.4858\n",
      "Epoch 771/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3225 - val_loss: -4.3001\n",
      "Epoch 772/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3821 - val_loss: -4.3249\n",
      "Epoch 773/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4100 - val_loss: -4.6153\n",
      "Epoch 774/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2382 - val_loss: -4.5104\n",
      "Epoch 775/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3137 - val_loss: -4.0765\n",
      "Epoch 776/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2163 - val_loss: -4.3449\n",
      "Epoch 777/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4052 - val_loss: -4.4045\n",
      "Epoch 778/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2312 - val_loss: -4.5562\n",
      "Epoch 779/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.7565 - val_loss: -4.1274\n",
      "Epoch 780/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9165 - val_loss: -4.2993\n",
      "Epoch 781/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0755 - val_loss: -4.5857\n",
      "Epoch 782/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1436 - val_loss: -4.1742\n",
      "Epoch 783/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2246 - val_loss: -4.4649\n",
      "Epoch 784/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1652 - val_loss: -4.4371\n",
      "Epoch 785/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2916 - val_loss: -4.4297\n",
      "Epoch 786/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1922 - val_loss: -4.3028\n",
      "Epoch 787/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2986 - val_loss: -2.7041\n",
      "Epoch 788/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2406 - val_loss: -2.6964\n",
      "Epoch 789/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2669 - val_loss: -4.1587\n",
      "Epoch 790/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3168 - val_loss: -2.7218\n",
      "Epoch 791/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2154 - val_loss: -3.1180\n",
      "Epoch 792/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3925 - val_loss: -3.7668\n",
      "Epoch 793/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7353 - val_loss: -3.5825\n",
      "Epoch 794/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8627 - val_loss: -4.3893\n",
      "Epoch 795/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2351 - val_loss: 0.7084\n",
      "Epoch 796/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2425 - val_loss: -3.9661\n",
      "Epoch 797/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2473 - val_loss: -4.7045\n",
      "Epoch 798/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1726 - val_loss: -4.1977\n",
      "Epoch 799/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2254 - val_loss: -4.2987\n",
      "Epoch 800/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2910 - val_loss: -4.2845\n",
      "Epoch 801/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2508 - val_loss: -4.4934\n",
      "Epoch 802/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4051 - val_loss: -3.6644\n",
      "Epoch 803/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2903 - val_loss: -3.8460\n",
      "Epoch 804/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2163 - val_loss: -3.2801\n",
      "Epoch 805/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3524 - val_loss: -3.9800\n",
      "Epoch 806/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3723"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -5.3723 - val_loss: -4.7996\n",
      "Epoch 807/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3809 - val_loss: -4.1764\n",
      "Epoch 808/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3615 - val_loss: -3.1314\n",
      "Epoch 809/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5091 - val_loss: -3.2575\n",
      "Epoch 810/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9342 - val_loss: -4.1170\n",
      "Epoch 811/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9775 - val_loss: -4.2225\n",
      "Epoch 812/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9165 - val_loss: -2.8938\n",
      "Epoch 813/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1310 - val_loss: -3.8169\n",
      "Epoch 814/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1400 - val_loss: -3.9075\n",
      "Epoch 815/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1448 - val_loss: -4.6363\n",
      "Epoch 816/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1271 - val_loss: -3.6758\n",
      "Epoch 817/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2019 - val_loss: -4.3225\n",
      "Epoch 818/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2369 - val_loss: -4.5472\n",
      "Epoch 819/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2440 - val_loss: -4.7515\n",
      "Epoch 820/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1504 - val_loss: -1.6227\n",
      "Epoch 821/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2904 - val_loss: -4.1537\n",
      "Epoch 822/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1846 - val_loss: -4.5267\n",
      "Epoch 823/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3373 - val_loss: -3.5542\n",
      "Epoch 824/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2333 - val_loss: -4.4816\n",
      "Epoch 825/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1419 - val_loss: -4.3651\n",
      "Epoch 826/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3230 - val_loss: -4.4033\n",
      "Epoch 827/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2817 - val_loss: -4.3378\n",
      "Epoch 828/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3456 - val_loss: -3.3437\n",
      "Epoch 829/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7542 - val_loss: -4.3771\n",
      "Epoch 830/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2315 - val_loss: -4.4996\n",
      "Epoch 831/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2291 - val_loss: -3.8751\n",
      "Epoch 832/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3032 - val_loss: -4.1136\n",
      "Epoch 833/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3716 - val_loss: -4.4373\n",
      "Epoch 834/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2622 - val_loss: -4.0916\n",
      "Epoch 835/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0980 - val_loss: -4.1362\n",
      "Epoch 836/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3231"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 245ms/step - loss: -5.3231 - val_loss: -4.8024\n",
      "Epoch 837/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3789 - val_loss: -3.6531\n",
      "Epoch 838/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4450 - val_loss: -4.2474\n",
      "Epoch 839/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6322 - val_loss: -3.2802\n",
      "Epoch 840/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9671 - val_loss: -3.9754\n",
      "Epoch 841/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1971 - val_loss: -1.0531\n",
      "Epoch 842/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2066 - val_loss: -3.8356\n",
      "Epoch 843/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1999 - val_loss: -2.5946\n",
      "Epoch 844/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1804 - val_loss: -4.2657\n",
      "Epoch 845/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2736 - val_loss: -3.9204\n",
      "Epoch 846/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2731 - val_loss: -4.6876\n",
      "Epoch 847/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2389 - val_loss: -4.2474\n",
      "Epoch 848/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2273 - val_loss: -0.0605\n",
      "Epoch 849/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3614 - val_loss: -4.6831\n",
      "Epoch 850/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2463 - val_loss: -4.3955\n",
      "Epoch 851/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3746 - val_loss: -3.7858\n",
      "Epoch 852/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5023 - val_loss: -3.5457\n",
      "Epoch 853/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6101 - val_loss: -0.3341\n",
      "Epoch 854/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9714 - val_loss: -4.7103\n",
      "Epoch 855/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1699 - val_loss: -3.0758\n",
      "Epoch 856/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2838 - val_loss: -4.3542\n",
      "Epoch 857/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1734 - val_loss: -4.5696\n",
      "Epoch 858/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2577 - val_loss: -4.0966\n",
      "Epoch 859/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3396 - val_loss: -4.2234\n",
      "Epoch 860/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2540 - val_loss: -4.5331\n",
      "Epoch 861/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2787 - val_loss: -3.8339\n",
      "Epoch 862/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3196 - val_loss: -3.8273\n",
      "Epoch 863/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3932 - val_loss: -3.7843\n",
      "Epoch 864/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3688 - val_loss: -4.1557\n",
      "Epoch 865/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1174 - val_loss: -4.3498\n",
      "Epoch 866/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2697 - val_loss: -3.4531\n",
      "Epoch 867/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3807 - val_loss: -1.1541\n",
      "Epoch 868/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3614 - val_loss: -3.9838\n",
      "Epoch 869/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3483"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 245ms/step - loss: -5.3483 - val_loss: -4.8832\n",
      "Epoch 870/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3188 - val_loss: -4.3208\n",
      "Epoch 871/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3885 - val_loss: -4.5989\n",
      "Epoch 872/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2797 - val_loss: -3.7401\n",
      "Epoch 873/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4002 - val_loss: -3.4567\n",
      "Epoch 874/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3191 - val_loss: -4.4905\n",
      "Epoch 875/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2491 - val_loss: -4.5048\n",
      "Epoch 876/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3715 - val_loss: -0.2713\n",
      "Epoch 877/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3011 - val_loss: -4.2916\n",
      "Epoch 878/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3351 - val_loss: -4.8613\n",
      "Epoch 879/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3618 - val_loss: -1.0648\n",
      "Epoch 880/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3021 - val_loss: -4.4967\n",
      "Epoch 881/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4455 - val_loss: -1.8022\n",
      "Epoch 882/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2694 - val_loss: -4.4668\n",
      "Epoch 883/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4111 - val_loss: -4.1412\n",
      "Epoch 884/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3171 - val_loss: -3.6074\n",
      "Epoch 885/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3814 - val_loss: -4.3859\n",
      "Epoch 886/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2976 - val_loss: -4.5064\n",
      "Epoch 887/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3735 - val_loss: -3.8990\n",
      "Epoch 888/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2813 - val_loss: -4.5487\n",
      "Epoch 889/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4180 - val_loss: -4.4262\n",
      "Epoch 890/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4082 - val_loss: -4.5668\n",
      "Epoch 891/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4506 - val_loss: -3.6193\n",
      "Epoch 892/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3221 - val_loss: -4.3575\n",
      "Epoch 893/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2179 - val_loss: -3.0856\n",
      "Epoch 894/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3634 - val_loss: -3.8972\n",
      "Epoch 895/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8371 - val_loss: -3.3374\n",
      "Epoch 896/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8427 - val_loss: -4.4358\n",
      "Epoch 897/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2206 - val_loss: -4.3332\n",
      "Epoch 898/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2164 - val_loss: -4.7553\n",
      "Epoch 899/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2584 - val_loss: -4.2285\n",
      "Epoch 900/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2943 - val_loss: -4.7762\n",
      "Epoch 901/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3607 - val_loss: -4.3959\n",
      "Epoch 902/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3317 - val_loss: -4.7311\n",
      "Epoch 903/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3280 - val_loss: -3.8297\n",
      "Epoch 904/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3119 - val_loss: -4.6820\n",
      "Epoch 905/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4412 - val_loss: -4.1264\n",
      "Epoch 906/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2369 - val_loss: -3.7440\n",
      "Epoch 907/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3799 - val_loss: -4.2064\n",
      "Epoch 908/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3952 - val_loss: -4.6903\n",
      "Epoch 909/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4002 - val_loss: -4.2815\n",
      "Epoch 910/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3606 - val_loss: -4.3741\n",
      "Epoch 911/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3739 - val_loss: -4.2508\n",
      "Epoch 912/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3873 - val_loss: -4.4506\n",
      "Epoch 913/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2776 - val_loss: -4.3599\n",
      "Epoch 914/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4272 - val_loss: -4.0822\n",
      "Epoch 915/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4247 - val_loss: -3.4921\n",
      "Epoch 916/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3819 - val_loss: -3.6154\n",
      "Epoch 917/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4557 - val_loss: -4.5239\n",
      "Epoch 918/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3562 - val_loss: -3.8899\n",
      "Epoch 919/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3697 - val_loss: -2.5500\n",
      "Epoch 920/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4015 - val_loss: -4.6172\n",
      "Epoch 921/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3685 - val_loss: -3.7735\n",
      "Epoch 922/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3932 - val_loss: -3.2916\n",
      "Epoch 923/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2671 - val_loss: -3.5691\n",
      "Epoch 924/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3834 - val_loss: -4.4424\n",
      "Epoch 925/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2273 - val_loss: -4.4973\n",
      "Epoch 926/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3581 - val_loss: -3.7612\n",
      "Epoch 927/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3391 - val_loss: -4.8054\n",
      "Epoch 928/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4242 - val_loss: -1.5504\n",
      "Epoch 929/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2873 - val_loss: -3.4570\n",
      "Epoch 930/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4672 - val_loss: -3.4924\n",
      "Epoch 931/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3185 - val_loss: -4.6764\n",
      "Epoch 932/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4568 - val_loss: -4.4104\n",
      "Epoch 933/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4829 - val_loss: -4.6230\n",
      "Epoch 934/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3580 - val_loss: -4.0005\n",
      "Epoch 935/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4027 - val_loss: -4.2958\n",
      "Epoch 936/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4564 - val_loss: -4.4943\n",
      "Epoch 937/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3908 - val_loss: -4.6854\n",
      "Epoch 938/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2962 - val_loss: -4.3634\n",
      "Epoch 939/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4364 - val_loss: -4.8344\n",
      "Epoch 940/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4978 - val_loss: -3.8467\n",
      "Epoch 941/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0780 - val_loss: -4.2038\n",
      "Epoch 942/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3162 - val_loss: -4.6228\n",
      "Epoch 943/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2300 - val_loss: 0.0456\n",
      "Epoch 944/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3533 - val_loss: -3.6585\n",
      "Epoch 945/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3152 - val_loss: -3.9532\n",
      "Epoch 946/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3652 - val_loss: -4.4122\n",
      "Epoch 947/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4184 - val_loss: -3.0551\n",
      "Epoch 948/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4279 - val_loss: -2.2796\n",
      "Epoch 949/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3868 - val_loss: -4.6472\n",
      "Epoch 950/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4146 - val_loss: -3.2623\n",
      "Epoch 951/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0833 - val_loss: -4.2335\n",
      "Epoch 952/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3919 - val_loss: -4.5656\n",
      "Epoch 953/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.3665 - val_loss: -4.4626\n",
      "Epoch 954/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1956 - val_loss: -3.9618\n",
      "Epoch 955/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2829 - val_loss: -4.6440\n",
      "Epoch 956/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3663 - val_loss: -2.3901\n",
      "Epoch 957/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3896 - val_loss: -4.6330\n",
      "Epoch 958/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4404 - val_loss: -4.7374\n",
      "Epoch 959/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3599 - val_loss: -3.9314\n",
      "Epoch 960/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3565 - val_loss: -4.8318\n",
      "Epoch 961/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3237 - val_loss: -4.6010\n",
      "Epoch 962/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4099 - val_loss: -4.5633\n",
      "Epoch 963/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4106 - val_loss: -3.5619\n",
      "Epoch 964/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2979 - val_loss: -3.7493\n",
      "Epoch 965/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3002 - val_loss: -4.4093\n",
      "Epoch 966/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3059 - val_loss: -4.1816\n",
      "Epoch 967/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4518 - val_loss: -4.1028\n",
      "Epoch 968/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3830 - val_loss: -4.5043\n",
      "Epoch 969/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4546 - val_loss: -4.7419\n",
      "Epoch 970/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3691 - val_loss: -4.1123\n",
      "Epoch 971/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1881 - val_loss: -4.6660\n",
      "Epoch 972/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3362 - val_loss: -3.6983\n",
      "Epoch 973/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3517 - val_loss: -4.6940\n",
      "Epoch 974/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3478 - val_loss: -4.5321\n",
      "Epoch 975/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3726 - val_loss: -4.1718\n",
      "Epoch 976/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3794 - val_loss: -4.0943\n",
      "Epoch 977/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3707 - val_loss: -2.7993\n",
      "Epoch 978/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9755 - val_loss: -4.1443\n",
      "Epoch 979/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3329 - val_loss: -4.2158\n",
      "Epoch 980/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3197 - val_loss: -4.0231\n",
      "Epoch 981/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3091 - val_loss: -3.9269\n",
      "Epoch 982/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4061 - val_loss: -4.2738\n",
      "Epoch 983/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4649 - val_loss: -2.7208\n",
      "Epoch 984/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3642 - val_loss: -4.8015\n",
      "Epoch 985/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3559 - val_loss: -4.4268\n",
      "Epoch 986/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3905 - val_loss: -4.4569\n",
      "Epoch 987/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3844 - val_loss: -4.3152\n",
      "Epoch 988/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2718 - val_loss: -3.7758\n",
      "Epoch 989/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4467 - val_loss: -4.1235\n",
      "Epoch 990/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4776 - val_loss: -4.3404\n",
      "Epoch 991/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3960 - val_loss: -3.9022\n",
      "Epoch 992/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4252 - val_loss: -3.7210\n",
      "Epoch 993/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3540 - val_loss: -4.7171\n",
      "Epoch 994/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4520 - val_loss: -4.0815\n",
      "Epoch 995/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2494 - val_loss: -4.1800\n",
      "Epoch 996/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4953 - val_loss: -4.4822\n",
      "Epoch 997/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5065 - val_loss: -2.6598\n",
      "Epoch 998/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4276 - val_loss: -1.1494\n",
      "Epoch 999/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3037 - val_loss: -4.6847\n",
      "Epoch 1000/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4886 - val_loss: -3.6371\n",
      "Epoch 1001/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3728 - val_loss: -2.5510\n",
      "Epoch 1002/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3739 - val_loss: -4.0880\n",
      "Epoch 1003/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4477 - val_loss: -4.1440\n",
      "Epoch 1004/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4456 - val_loss: -4.0939\n",
      "Epoch 1005/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3383 - val_loss: -3.0599\n",
      "Epoch 1006/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3412 - val_loss: -4.2985\n",
      "Epoch 1007/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2625 - val_loss: -4.5946\n",
      "Epoch 1008/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4225 - val_loss: -3.4315\n",
      "Epoch 1009/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3432 - val_loss: -4.5778\n",
      "Epoch 1010/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4527 - val_loss: -4.5811\n",
      "Epoch 1011/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3882 - val_loss: -2.5370\n",
      "Epoch 1012/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1869 - val_loss: -3.6476\n",
      "Epoch 1013/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4712 - val_loss: -4.6383\n",
      "Epoch 1014/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3960 - val_loss: -4.4034\n",
      "Epoch 1015/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3358 - val_loss: -4.5648\n",
      "Epoch 1016/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4187 - val_loss: -3.2319\n",
      "Epoch 1017/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4903 - val_loss: -2.4055\n",
      "Epoch 1018/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4449 - val_loss: -4.5041\n",
      "Epoch 1019/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4218 - val_loss: -3.7402\n",
      "Epoch 1020/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5102 - val_loss: -3.6935\n",
      "Epoch 1021/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4393 - val_loss: -3.1177\n",
      "Epoch 1022/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1774 - val_loss: -3.7089\n",
      "Epoch 1023/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3292 - val_loss: -4.0922\n",
      "Epoch 1024/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3769 - val_loss: -4.4980\n",
      "Epoch 1025/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4722 - val_loss: -2.4436\n",
      "Epoch 1026/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2133 - val_loss: -4.6637\n",
      "Epoch 1027/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3569 - val_loss: -4.1865\n",
      "Epoch 1028/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3783 - val_loss: -2.7441\n",
      "Epoch 1029/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3809 - val_loss: -4.5869\n",
      "Epoch 1030/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4190 - val_loss: -1.2193\n",
      "Epoch 1031/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4507 - val_loss: -4.5592\n",
      "Epoch 1032/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5105 - val_loss: -0.0544\n",
      "Epoch 1033/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4354 - val_loss: -4.8633\n",
      "Epoch 1034/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2300 - val_loss: -3.9555\n",
      "Epoch 1035/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5773 - val_loss: -3.9495\n",
      "Epoch 1036/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2412 - val_loss: -1.5013\n",
      "Epoch 1037/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3746 - val_loss: -4.2879\n",
      "Epoch 1038/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2856 - val_loss: -4.4849\n",
      "Epoch 1039/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4469 - val_loss: -4.3429\n",
      "Epoch 1040/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3415 - val_loss: -4.4072\n",
      "Epoch 1041/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4549 - val_loss: -4.5468\n",
      "Epoch 1042/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3706 - val_loss: -4.4364\n",
      "Epoch 1043/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4213 - val_loss: -3.6175\n",
      "Epoch 1044/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4208 - val_loss: -3.7473\n",
      "Epoch 1045/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4714 - val_loss: -3.9495\n",
      "Epoch 1046/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4372 - val_loss: -4.4831\n",
      "Epoch 1047/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3411 - val_loss: -4.2425\n",
      "Epoch 1048/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3865 - val_loss: -4.5322\n",
      "Epoch 1049/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4680 - val_loss: -3.9328\n",
      "Epoch 1050/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4606 - val_loss: -2.6611\n",
      "Epoch 1051/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3869 - val_loss: -4.4725\n",
      "Epoch 1052/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4157 - val_loss: -4.4891\n",
      "Epoch 1053/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4707 - val_loss: -4.3596\n",
      "Epoch 1054/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4620 - val_loss: -3.3385\n",
      "Epoch 1055/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2731 - val_loss: -3.9396\n",
      "Epoch 1056/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3101 - val_loss: -2.5207\n",
      "Epoch 1057/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1874 - val_loss: -3.7518\n",
      "Epoch 1058/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4057 - val_loss: -4.4243\n",
      "Epoch 1059/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3896 - val_loss: -4.1864\n",
      "Epoch 1060/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2770 - val_loss: -4.3544\n",
      "Epoch 1061/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2047 - val_loss: -4.6060\n",
      "Epoch 1062/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3935 - val_loss: -4.1803\n",
      "Epoch 1063/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4016 - val_loss: -1.6557\n",
      "Epoch 1064/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3158 - val_loss: -4.3100\n",
      "Epoch 1065/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3752 - val_loss: -4.6157\n",
      "Epoch 1066/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5090 - val_loss: -4.4921\n",
      "Epoch 1067/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2851 - val_loss: -4.6856\n",
      "Epoch 1068/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4220 - val_loss: -4.6685\n",
      "Epoch 1069/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4397 - val_loss: -3.1718\n",
      "Epoch 1070/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4053 - val_loss: -4.6832\n",
      "Epoch 1071/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4249 - val_loss: -3.5520\n",
      "Epoch 1072/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4313 - val_loss: -4.2333\n",
      "Epoch 1073/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4650 - val_loss: -4.7055\n",
      "Epoch 1074/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4821 - val_loss: -4.1488\n",
      "Epoch 1075/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4599 - val_loss: -3.0250\n",
      "Epoch 1076/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4894 - val_loss: -4.7979\n",
      "Epoch 1077/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4195 - val_loss: -4.7136\n",
      "Epoch 1078/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4016 - val_loss: -3.5620\n",
      "Epoch 1079/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3892 - val_loss: -4.6884\n",
      "Epoch 1080/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4467 - val_loss: -4.6591\n",
      "Epoch 1081/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5411 - val_loss: -2.6769\n",
      "Epoch 1082/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2874 - val_loss: -1.7718\n",
      "Epoch 1083/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4379 - val_loss: -3.7272\n",
      "Epoch 1084/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4602 - val_loss: -2.0165\n",
      "Epoch 1085/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4905 - val_loss: -3.9173\n",
      "Epoch 1086/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2522 - val_loss: -3.7796\n",
      "Epoch 1087/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4528 - val_loss: -3.4683\n",
      "Epoch 1088/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3621 - val_loss: -4.8524\n",
      "Epoch 1089/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3876 - val_loss: -4.5371\n",
      "Epoch 1090/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1740 - val_loss: -4.2461\n",
      "Epoch 1091/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4270 - val_loss: -2.8041\n",
      "Epoch 1092/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4386 - val_loss: -4.1729\n",
      "Epoch 1093/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2905 - val_loss: -4.4159\n",
      "Epoch 1094/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4339 - val_loss: -3.0463\n",
      "Epoch 1095/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4000 - val_loss: -4.6953\n",
      "Epoch 1096/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4541 - val_loss: -4.1460\n",
      "Epoch 1097/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4240 - val_loss: -4.6664\n",
      "Epoch 1098/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4794 - val_loss: -4.3038\n",
      "Epoch 1099/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2632 - val_loss: -4.5441\n",
      "Epoch 1100/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4262 - val_loss: -4.6649\n",
      "Epoch 1101/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4580 - val_loss: -3.5349\n",
      "Epoch 1102/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4103 - val_loss: -4.3710\n",
      "Epoch 1103/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4764 - val_loss: -4.2885\n",
      "Epoch 1104/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4127 - val_loss: -4.6134\n",
      "Epoch 1105/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4587 - val_loss: -2.7359\n",
      "Epoch 1106/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4508 - val_loss: -4.1943\n",
      "Epoch 1107/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4970 - val_loss: -4.2850\n",
      "Epoch 1108/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4662 - val_loss: -3.7474\n",
      "Epoch 1109/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4632 - val_loss: -4.2640\n",
      "Epoch 1110/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6517 - val_loss: 10.5987\n",
      "Epoch 1111/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.4978 - val_loss: -4.0163\n",
      "Epoch 1112/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6089 - val_loss: -3.3391\n",
      "Epoch 1113/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8724 - val_loss: -3.4238\n",
      "Epoch 1114/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9585 - val_loss: -4.2657\n",
      "Epoch 1115/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0227 - val_loss: -3.1067\n",
      "Epoch 1116/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0179 - val_loss: -4.1771\n",
      "Epoch 1117/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9198 - val_loss: -4.1110\n",
      "Epoch 1118/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1330 - val_loss: -4.2847\n",
      "Epoch 1119/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1725 - val_loss: -4.1638\n",
      "Epoch 1120/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2077 - val_loss: -3.8208\n",
      "Epoch 1121/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1039 - val_loss: -3.4761\n",
      "Epoch 1122/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1754 - val_loss: -4.5232\n",
      "Epoch 1123/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2434 - val_loss: -2.5201\n",
      "Epoch 1124/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1569 - val_loss: -3.3561\n",
      "Epoch 1125/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1426 - val_loss: -3.9033\n",
      "Epoch 1126/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2687 - val_loss: -3.8382\n",
      "Epoch 1127/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2520 - val_loss: -2.4619\n",
      "Epoch 1128/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2088 - val_loss: -3.9604\n",
      "Epoch 1129/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2994 - val_loss: -3.6940\n",
      "Epoch 1130/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2390 - val_loss: -3.2656\n",
      "Epoch 1131/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2429 - val_loss: -4.0057\n",
      "Epoch 1132/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1815 - val_loss: -3.0372\n",
      "Epoch 1133/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2920 - val_loss: -3.9470\n",
      "Epoch 1134/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2290 - val_loss: -4.4583\n",
      "Epoch 1135/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3379 - val_loss: -3.3580\n",
      "Epoch 1136/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3489 - val_loss: -3.4194\n",
      "Epoch 1137/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2942 - val_loss: -4.2732\n",
      "Epoch 1138/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3212 - val_loss: -4.4140\n",
      "Epoch 1139/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3601 - val_loss: -2.5698\n",
      "Epoch 1140/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3170 - val_loss: -3.3097\n",
      "Epoch 1141/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2466 - val_loss: -4.2766\n",
      "Epoch 1142/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3372 - val_loss: -3.8090\n",
      "Epoch 1143/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3040 - val_loss: -3.4349\n",
      "Epoch 1144/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3183 - val_loss: -4.2456\n",
      "Epoch 1145/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4079 - val_loss: -4.2706\n",
      "Epoch 1146/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2901 - val_loss: -4.1841\n",
      "Epoch 1147/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3942 - val_loss: -4.4559\n",
      "Epoch 1148/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3828 - val_loss: -2.7853\n",
      "Epoch 1149/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3051 - val_loss: -4.1638\n",
      "Epoch 1150/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2636 - val_loss: -4.3587\n",
      "Epoch 1151/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3892 - val_loss: -4.6683\n",
      "Epoch 1152/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1852 - val_loss: -4.0410\n",
      "Epoch 1153/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4021 - val_loss: -3.3558\n",
      "Epoch 1154/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3295 - val_loss: -4.2982\n",
      "Epoch 1155/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3528 - val_loss: -3.2099\n",
      "Epoch 1156/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2564 - val_loss: -3.8892\n",
      "Epoch 1157/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3882 - val_loss: -1.7345\n",
      "Epoch 1158/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3290 - val_loss: -2.0739\n",
      "Epoch 1159/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2970 - val_loss: -4.6608\n",
      "Epoch 1160/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4405 - val_loss: -4.0064\n",
      "Epoch 1161/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3264 - val_loss: -4.4218\n",
      "Epoch 1162/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2687 - val_loss: -4.4273\n",
      "Epoch 1163/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3999 - val_loss: -4.0625\n",
      "Epoch 1164/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4069 - val_loss: -4.2792\n",
      "Epoch 1165/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4302 - val_loss: -3.1884\n",
      "Epoch 1166/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3145 - val_loss: -4.2081\n",
      "Epoch 1167/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3409 - val_loss: -4.2113\n",
      "Epoch 1168/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2127 - val_loss: -3.7976\n",
      "Epoch 1169/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3480 - val_loss: -4.3706\n",
      "Epoch 1170/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4412 - val_loss: -4.4915\n",
      "Epoch 1171/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2838 - val_loss: -4.3780\n",
      "Epoch 1172/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4307 - val_loss: -4.4622\n",
      "Epoch 1173/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4123 - val_loss: -0.6066\n",
      "Epoch 1174/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3602 - val_loss: -2.8797\n",
      "Epoch 1175/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8327 - val_loss: -4.3056\n",
      "Epoch 1176/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7440 - val_loss: -3.6630\n",
      "Epoch 1177/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9659 - val_loss: -3.6365\n",
      "Epoch 1178/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0948 - val_loss: -4.1114\n",
      "Epoch 1179/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1309 - val_loss: -4.4329\n",
      "Epoch 1180/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2063 - val_loss: -4.4345\n",
      "Epoch 1181/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0313 - val_loss: -4.4009\n",
      "Epoch 1182/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2126 - val_loss: -4.2150\n",
      "Epoch 1183/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2905 - val_loss: -4.0912\n",
      "Epoch 1184/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2170 - val_loss: -4.4031\n",
      "Epoch 1185/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2288 - val_loss: -4.1982\n",
      "Epoch 1186/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3141 - val_loss: -4.3441\n",
      "Epoch 1187/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2623 - val_loss: -4.4979\n",
      "Epoch 1188/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2694 - val_loss: -4.6387\n",
      "Epoch 1189/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3483 - val_loss: -0.5148\n",
      "Epoch 1190/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1215 - val_loss: -4.1569\n",
      "Epoch 1191/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3695 - val_loss: -4.5490\n",
      "Epoch 1192/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2815"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -5.2815 - val_loss: -4.9704\n",
      "Epoch 1193/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2670 - val_loss: -4.8111\n",
      "Epoch 1194/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3744 - val_loss: -4.7683\n",
      "Epoch 1195/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3483 - val_loss: -4.5626\n",
      "Epoch 1196/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3602 - val_loss: -3.9573\n",
      "Epoch 1197/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2904 - val_loss: -3.8474\n",
      "Epoch 1198/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3849 - val_loss: -4.6438\n",
      "Epoch 1199/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3873 - val_loss: -4.7046\n",
      "Epoch 1200/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2906 - val_loss: -1.5273\n",
      "Epoch 1201/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3396 - val_loss: -4.6897\n",
      "Epoch 1202/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3662 - val_loss: -4.0318\n",
      "Epoch 1203/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3832 - val_loss: -3.5224\n",
      "Epoch 1204/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3100 - val_loss: -0.7022\n",
      "Epoch 1205/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3567 - val_loss: -4.6045\n",
      "Epoch 1206/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3171 - val_loss: -4.2088\n",
      "Epoch 1207/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2759 - val_loss: -4.2127\n",
      "Epoch 1208/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3857 - val_loss: -3.8634\n",
      "Epoch 1209/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3041 - val_loss: -4.1958\n",
      "Epoch 1210/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4576 - val_loss: -3.4406\n",
      "Epoch 1211/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2678 - val_loss: -3.6397\n",
      "Epoch 1212/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3247 - val_loss: -2.9221\n",
      "Epoch 1213/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3495 - val_loss: -4.8530\n",
      "Epoch 1214/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2042 - val_loss: -3.9112\n",
      "Epoch 1215/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3276 - val_loss: -4.3769\n",
      "Epoch 1216/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3972 - val_loss: -3.5501\n",
      "Epoch 1217/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3455 - val_loss: -3.8831\n",
      "Epoch 1218/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4841 - val_loss: -4.6389\n",
      "Epoch 1219/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3581 - val_loss: -3.1692\n",
      "Epoch 1220/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3824 - val_loss: -4.0089\n",
      "Epoch 1221/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4047 - val_loss: -4.8161\n",
      "Epoch 1222/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4151 - val_loss: -4.6241\n",
      "Epoch 1223/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4602 - val_loss: -4.7095\n",
      "Epoch 1224/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4020 - val_loss: -2.5618\n",
      "Epoch 1225/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2531 - val_loss: -4.4412\n",
      "Epoch 1226/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4418 - val_loss: -3.6930\n",
      "Epoch 1227/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3906 - val_loss: -4.2108\n",
      "Epoch 1228/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4100 - val_loss: -4.7356\n",
      "Epoch 1229/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3958 - val_loss: -1.8877\n",
      "Epoch 1230/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3844 - val_loss: -4.6191\n",
      "Epoch 1231/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3350 - val_loss: -0.1451\n",
      "Epoch 1232/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3637 - val_loss: -3.4447\n",
      "Epoch 1233/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4832 - val_loss: -3.6964\n",
      "Epoch 1234/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4286 - val_loss: -2.7348\n",
      "Epoch 1235/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4583 - val_loss: 0.3411\n",
      "Epoch 1236/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2797 - val_loss: -4.7590\n",
      "Epoch 1237/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3852 - val_loss: -4.7553\n",
      "Epoch 1238/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3918 - val_loss: -4.4715\n",
      "Epoch 1239/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4793 - val_loss: -4.5325\n",
      "Epoch 1240/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3956 - val_loss: -4.4485\n",
      "Epoch 1241/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4118 - val_loss: -4.9016\n",
      "Epoch 1242/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2929 - val_loss: 1.6878\n",
      "Epoch 1243/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2799 - val_loss: -3.8022\n",
      "Epoch 1244/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4171 - val_loss: -4.2251\n",
      "Epoch 1245/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4633 - val_loss: -4.3235\n",
      "Epoch 1246/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4259 - val_loss: -4.0720\n",
      "Epoch 1247/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3981 - val_loss: -4.5155\n",
      "Epoch 1248/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4251 - val_loss: -4.2125\n",
      "Epoch 1249/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4768 - val_loss: -4.3966\n",
      "Epoch 1250/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7253 - val_loss: -4.1583\n",
      "Epoch 1251/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0945 - val_loss: -4.3085\n",
      "Epoch 1252/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2771 - val_loss: -3.7461\n",
      "Epoch 1253/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2881 - val_loss: -4.6366\n",
      "Epoch 1254/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3898 - val_loss: -3.4728\n",
      "Epoch 1255/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4266 - val_loss: -3.4571\n",
      "Epoch 1256/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1760 - val_loss: -4.4973\n",
      "Epoch 1257/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3036 - val_loss: -4.5926\n",
      "Epoch 1258/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3775 - val_loss: -4.3679\n",
      "Epoch 1259/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4391 - val_loss: -3.4534\n",
      "Epoch 1260/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0965 - val_loss: -3.8946\n",
      "Epoch 1261/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3930 - val_loss: -3.1016\n",
      "Epoch 1262/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4476 - val_loss: -3.2310\n",
      "Epoch 1263/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4560 - val_loss: -4.2742\n",
      "Epoch 1264/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4556 - val_loss: -4.3288\n",
      "Epoch 1265/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3978 - val_loss: -4.4348\n",
      "Epoch 1266/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3003 - val_loss: -3.0924\n",
      "Epoch 1267/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3738 - val_loss: -4.3171\n",
      "Epoch 1268/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3920 - val_loss: -3.7838\n",
      "Epoch 1269/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3772 - val_loss: -4.4538\n",
      "Epoch 1270/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4908 - val_loss: -3.9953\n",
      "Epoch 1271/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4657 - val_loss: -3.7588\n",
      "Epoch 1272/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4522 - val_loss: -4.3458\n",
      "Epoch 1273/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3938 - val_loss: -4.4997\n",
      "Epoch 1274/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4381 - val_loss: -4.1179\n",
      "Epoch 1275/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4922 - val_loss: -3.9705\n",
      "Epoch 1276/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4307 - val_loss: -2.3966\n",
      "Epoch 1277/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3908 - val_loss: -4.2436\n",
      "Epoch 1278/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5058 - val_loss: -3.9644\n",
      "Epoch 1279/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4811 - val_loss: -4.6164\n",
      "Epoch 1280/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4574 - val_loss: -4.0342\n",
      "Epoch 1281/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0665 - val_loss: -3.7119\n",
      "Epoch 1282/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0905 - val_loss: -3.4816\n",
      "Epoch 1283/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2963 - val_loss: -3.2884\n",
      "Epoch 1284/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2841 - val_loss: -4.4128\n",
      "Epoch 1285/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3259 - val_loss: -2.2013\n",
      "Epoch 1286/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3577 - val_loss: -4.1920\n",
      "Epoch 1287/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3902 - val_loss: -4.4259\n",
      "Epoch 1288/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4203 - val_loss: -4.2973\n",
      "Epoch 1289/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3670 - val_loss: -2.9507\n",
      "Epoch 1290/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3412 - val_loss: -3.1421\n",
      "Epoch 1291/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4318 - val_loss: -3.1659\n",
      "Epoch 1292/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4587 - val_loss: -4.3719\n",
      "Epoch 1293/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4527 - val_loss: -4.4623\n",
      "Epoch 1294/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4679 - val_loss: -3.8217\n",
      "Epoch 1295/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3251 - val_loss: -4.0509\n",
      "Epoch 1296/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4305 - val_loss: -1.1978\n",
      "Epoch 1297/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3393 - val_loss: -4.7583\n",
      "Epoch 1298/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4725 - val_loss: -3.6901\n",
      "Epoch 1299/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3730 - val_loss: -4.4615\n",
      "Epoch 1300/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1403 - val_loss: -3.8601\n",
      "Epoch 1301/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3937 - val_loss: -4.3042\n",
      "Epoch 1302/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3770 - val_loss: -4.3470\n",
      "Epoch 1303/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3154 - val_loss: -1.9028\n",
      "Epoch 1304/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4036 - val_loss: -4.3989\n",
      "Epoch 1305/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3560 - val_loss: -4.5745\n",
      "Epoch 1306/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4755 - val_loss: -4.1929\n",
      "Epoch 1307/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4610 - val_loss: -2.1376\n",
      "Epoch 1308/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3596 - val_loss: -4.6655\n",
      "Epoch 1309/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4451 - val_loss: -2.1573\n",
      "Epoch 1310/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4841 - val_loss: -3.9522\n",
      "Epoch 1311/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4577 - val_loss: -4.1222\n",
      "Epoch 1312/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4728 - val_loss: -3.5278\n",
      "Epoch 1313/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4813 - val_loss: -3.4857\n",
      "Epoch 1314/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2959 - val_loss: -4.6946\n",
      "Epoch 1315/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3705 - val_loss: -4.1909\n",
      "Epoch 1316/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4485 - val_loss: -3.8962\n",
      "Epoch 1317/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4431 - val_loss: -4.4185\n",
      "Epoch 1318/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5038 - val_loss: -4.4354\n",
      "Epoch 1319/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4965 - val_loss: -4.3939\n",
      "Epoch 1320/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4409 - val_loss: -3.5847\n",
      "Epoch 1321/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4154 - val_loss: -4.0222\n",
      "Epoch 1322/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1648 - val_loss: -4.5751\n",
      "Epoch 1323/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2317 - val_loss: -4.7111\n",
      "Epoch 1324/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4234 - val_loss: -4.1751\n",
      "Epoch 1325/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3375 - val_loss: -4.6073\n",
      "Epoch 1326/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2373 - val_loss: -3.4744\n",
      "Epoch 1327/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4655 - val_loss: -4.5711\n",
      "Epoch 1328/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4091 - val_loss: -3.6950\n",
      "Epoch 1329/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4480 - val_loss: -3.8350\n",
      "Epoch 1330/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1143 - val_loss: -3.4098\n",
      "Epoch 1331/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4069 - val_loss: -4.5341\n",
      "Epoch 1332/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1954 - val_loss: -4.6959\n",
      "Epoch 1333/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4175 - val_loss: -3.4199\n",
      "Epoch 1334/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4129 - val_loss: -1.9974\n",
      "Epoch 1335/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3461 - val_loss: -3.9923\n",
      "Epoch 1336/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3263 - val_loss: -4.0061\n",
      "Epoch 1337/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4606 - val_loss: -3.7683\n",
      "Epoch 1338/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4875 - val_loss: -3.9199\n",
      "Epoch 1339/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3846 - val_loss: -3.7732\n",
      "Epoch 1340/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4377 - val_loss: -3.4821\n",
      "Epoch 1341/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2574 - val_loss: -2.8629\n",
      "Epoch 1342/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4432 - val_loss: -4.2947\n",
      "Epoch 1343/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4264 - val_loss: -4.5891\n",
      "Epoch 1344/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5055 - val_loss: -4.3280\n",
      "Epoch 1345/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4651 - val_loss: -4.6257\n",
      "Epoch 1346/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5043 - val_loss: -4.0929\n",
      "Epoch 1347/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3906 - val_loss: -4.1873\n",
      "Epoch 1348/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3583 - val_loss: -4.2000\n",
      "Epoch 1349/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5297 - val_loss: -3.9020\n",
      "Epoch 1350/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4346 - val_loss: -4.5819\n",
      "Epoch 1351/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4997 - val_loss: -4.8837\n",
      "Epoch 1352/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2720 - val_loss: -1.4322\n",
      "Epoch 1353/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4889 - val_loss: -4.4201\n",
      "Epoch 1354/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5249 - val_loss: -4.2285\n",
      "Epoch 1355/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4397 - val_loss: -4.0345\n",
      "Epoch 1356/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5246 - val_loss: -4.0411\n",
      "Epoch 1357/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3562 - val_loss: -2.6920\n",
      "Epoch 1358/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0974 - val_loss: -4.6719\n",
      "Epoch 1359/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3665 - val_loss: -4.1438\n",
      "Epoch 1360/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3896 - val_loss: -3.5315\n",
      "Epoch 1361/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4240 - val_loss: -3.3244\n",
      "Epoch 1362/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4441 - val_loss: -3.4661\n",
      "Epoch 1363/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4627 - val_loss: -3.4069\n",
      "Epoch 1364/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4449 - val_loss: -2.9460\n",
      "Epoch 1365/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4600 - val_loss: -1.7796\n",
      "Epoch 1366/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4324 - val_loss: -2.0152\n",
      "Epoch 1367/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4357 - val_loss: -3.7218\n",
      "Epoch 1368/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4274 - val_loss: -3.7102\n",
      "Epoch 1369/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.5150 - val_loss: -0.4636\n",
      "Epoch 1370/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5294 - val_loss: -4.3092\n",
      "Epoch 1371/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4136 - val_loss: -4.8598\n",
      "Epoch 1372/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4833 - val_loss: -4.5667\n",
      "Epoch 1373/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4315 - val_loss: -4.5613\n",
      "Epoch 1374/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.5807 - val_loss: -4.4860\n",
      "Epoch 1375/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -2.2420 - val_loss: -3.6522\n",
      "Epoch 1376/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0813 - val_loss: -3.9841\n",
      "Epoch 1377/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8129 - val_loss: -4.0674\n",
      "Epoch 1378/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0091 - val_loss: -2.8729\n",
      "Epoch 1379/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0450 - val_loss: -4.0233\n",
      "Epoch 1380/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1626 - val_loss: -3.4013\n",
      "Epoch 1381/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1738 - val_loss: -4.1011\n",
      "Epoch 1382/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2309 - val_loss: -4.5456\n",
      "Epoch 1383/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7638 - val_loss: -4.5123\n",
      "Epoch 1384/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2560 - val_loss: -3.7783\n",
      "Epoch 1385/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2836 - val_loss: -3.9104\n",
      "Epoch 1386/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3248 - val_loss: -4.1410\n",
      "Epoch 1387/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3313 - val_loss: -2.7784\n",
      "Epoch 1388/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1598 - val_loss: -4.4997\n",
      "Epoch 1389/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2492 - val_loss: -3.3588\n",
      "Epoch 1390/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2289 - val_loss: -4.0131\n",
      "Epoch 1391/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3563 - val_loss: -3.1790\n",
      "Epoch 1392/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3309 - val_loss: -2.6921\n",
      "Epoch 1393/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3705 - val_loss: -3.2640\n",
      "Epoch 1394/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4125 - val_loss: -2.1724\n",
      "Epoch 1395/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3888 - val_loss: -3.4021\n",
      "Epoch 1396/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3483 - val_loss: -3.1990\n",
      "Epoch 1397/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4143 - val_loss: -2.5596\n",
      "Epoch 1398/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2961 - val_loss: -3.8231\n",
      "Epoch 1399/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4489 - val_loss: -3.8386\n",
      "Epoch 1400/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4055 - val_loss: -3.9375\n",
      "Epoch 1401/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3096 - val_loss: -3.5542\n",
      "Epoch 1402/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4614 - val_loss: -3.2284\n",
      "Epoch 1403/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4102 - val_loss: -3.9383\n",
      "Epoch 1404/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3223 - val_loss: -4.1081\n",
      "Epoch 1405/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3732 - val_loss: -3.1997\n",
      "Epoch 1406/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4277 - val_loss: -2.2344\n",
      "Epoch 1407/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4923 - val_loss: -4.3984\n",
      "Epoch 1408/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4229 - val_loss: -1.4659\n",
      "Epoch 1409/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4123 - val_loss: -4.4340\n",
      "Epoch 1410/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3645 - val_loss: -4.4811\n",
      "Epoch 1411/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4035 - val_loss: -4.0194\n",
      "Epoch 1412/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4050 - val_loss: -4.5497\n",
      "Epoch 1413/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3734 - val_loss: -3.1985\n",
      "Epoch 1414/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3992 - val_loss: -3.1078\n",
      "Epoch 1415/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3769 - val_loss: -3.9343\n",
      "Epoch 1416/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4498 - val_loss: -4.3485\n",
      "Epoch 1417/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3968 - val_loss: -4.3673\n",
      "Epoch 1418/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4526 - val_loss: -4.6009\n",
      "Epoch 1419/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5015 - val_loss: -3.6060\n",
      "Epoch 1420/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4249 - val_loss: -3.0533\n",
      "Epoch 1421/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4356 - val_loss: -4.6384\n",
      "Epoch 1422/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4843 - val_loss: -1.8427\n",
      "Epoch 1423/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4572 - val_loss: -3.4318\n",
      "Epoch 1424/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3891 - val_loss: -4.6543\n",
      "Epoch 1425/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3662 - val_loss: -4.1731\n",
      "Epoch 1426/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3566 - val_loss: -4.3927\n",
      "Epoch 1427/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4855 - val_loss: -3.6286\n",
      "Epoch 1428/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5062 - val_loss: -2.8757\n",
      "Epoch 1429/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4712 - val_loss: -4.0789\n",
      "Epoch 1430/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3721 - val_loss: -3.3647\n",
      "Epoch 1431/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4647 - val_loss: -3.7392\n",
      "Epoch 1432/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2920 - val_loss: -4.2771\n",
      "Epoch 1433/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5014 - val_loss: -4.1990\n",
      "Epoch 1434/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2352 - val_loss: -4.0872\n",
      "Epoch 1435/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5333 - val_loss: -3.6426\n",
      "Epoch 1436/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3694 - val_loss: -4.4242\n",
      "Epoch 1437/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4755 - val_loss: -3.9053\n",
      "Epoch 1438/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1439 - val_loss: -3.9674\n",
      "Epoch 1439/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3234 - val_loss: -4.1250\n",
      "Epoch 1440/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4378 - val_loss: -3.9720\n",
      "Epoch 1441/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3596 - val_loss: -4.6030\n",
      "Epoch 1442/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5041 - val_loss: -4.7052\n",
      "Epoch 1443/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4975 - val_loss: -4.8236\n",
      "Epoch 1444/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4660 - val_loss: -2.0175\n",
      "Epoch 1445/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4915 - val_loss: -4.2067\n",
      "Epoch 1446/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.5079 - val_loss: -4.3541\n",
      "Epoch 1447/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4517 - val_loss: -2.7254\n",
      "Epoch 1448/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1245 - val_loss: -3.8816\n",
      "Epoch 1449/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3849 - val_loss: -4.7192\n",
      "Epoch 1450/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4265 - val_loss: -3.4597\n",
      "Epoch 1451/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4858 - val_loss: -3.0358\n",
      "Epoch 1452/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4618 - val_loss: -4.4640\n",
      "Epoch 1453/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5130 - val_loss: -1.5187\n",
      "Epoch 1454/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4145 - val_loss: -3.4367\n",
      "Epoch 1455/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4021 - val_loss: -4.4269\n",
      "Epoch 1456/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4358 - val_loss: -4.0898\n",
      "Epoch 1457/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5149 - val_loss: -4.3180\n",
      "Epoch 1458/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4786 - val_loss: -4.4237\n",
      "Epoch 1459/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4582 - val_loss: -3.8121\n",
      "Epoch 1460/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.5097 - val_loss: -4.1328\n",
      "Epoch 1461/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3866 - val_loss: -3.9445\n",
      "Epoch 1462/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5210 - val_loss: -3.7769\n",
      "Epoch 1463/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3776 - val_loss: -4.3343\n",
      "Epoch 1464/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4488 - val_loss: -3.6086\n",
      "Epoch 1465/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4243 - val_loss: -4.5825\n",
      "Epoch 1466/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5027 - val_loss: -2.5860\n",
      "Epoch 1467/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3059 - val_loss: -3.8620\n",
      "Epoch 1468/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4798 - val_loss: -2.5070\n",
      "Epoch 1469/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4705 - val_loss: -2.5805\n",
      "Epoch 1470/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2877 - val_loss: -2.8897\n",
      "Epoch 1471/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4748 - val_loss: -3.4890\n",
      "Epoch 1472/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4894 - val_loss: -4.4123\n",
      "Epoch 1473/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3171 - val_loss: -4.7262\n",
      "Epoch 1474/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5382 - val_loss: -3.1316\n",
      "Epoch 1475/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5441 - val_loss: -4.2602\n",
      "Epoch 1476/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4963 - val_loss: -1.5472\n",
      "Epoch 1477/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2420 - val_loss: -3.2468\n",
      "Epoch 1478/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6565 - val_loss: -4.2016\n",
      "Epoch 1479/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0166 - val_loss: -4.5758\n",
      "Epoch 1480/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9836 - val_loss: -3.3594\n",
      "Epoch 1481/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1646 - val_loss: -4.4733\n",
      "Epoch 1482/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1695 - val_loss: -1.1872\n",
      "Epoch 1483/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2419 - val_loss: -4.4236\n",
      "Epoch 1484/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2792 - val_loss: -4.6989\n",
      "Epoch 1485/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2065 - val_loss: -4.7211\n",
      "Epoch 1486/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2532 - val_loss: -4.5586\n",
      "Epoch 1487/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3744 - val_loss: -3.9056\n",
      "Epoch 1488/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4341 - val_loss: -4.8102\n",
      "Epoch 1489/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3495 - val_loss: -4.7642\n",
      "Epoch 1490/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8397 - val_loss: -4.4521\n",
      "Epoch 1491/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1851 - val_loss: -3.6135\n",
      "Epoch 1492/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3375 - val_loss: -4.6366\n",
      "Epoch 1493/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3190 - val_loss: -4.5916\n",
      "Epoch 1494/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3057 - val_loss: -4.1779\n",
      "Epoch 1495/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4180 - val_loss: -4.5297\n",
      "Epoch 1496/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2156 - val_loss: -3.9418\n",
      "Epoch 1497/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3622 - val_loss: -3.1362\n",
      "Epoch 1498/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3868 - val_loss: -4.5402\n",
      "Epoch 1499/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4719 - val_loss: -4.3177\n",
      "Epoch 1500/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3562 - val_loss: -3.5339\n",
      "Epoch 1501/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5000 - val_loss: -4.4071\n",
      "Epoch 1502/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3040 - val_loss: -4.3586\n",
      "Epoch 1503/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3956 - val_loss: -4.0071\n",
      "Epoch 1504/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4564 - val_loss: -2.7470\n",
      "Epoch 1505/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2431 - val_loss: -4.1553\n",
      "Epoch 1506/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4531 - val_loss: -3.7091\n",
      "Epoch 1507/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4496 - val_loss: -4.4611\n",
      "Epoch 1508/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5040 - val_loss: -4.0485\n",
      "Epoch 1509/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4838 - val_loss: -4.3001\n",
      "Epoch 1510/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4367 - val_loss: -2.6278\n",
      "Epoch 1511/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4869 - val_loss: -4.0547\n",
      "Epoch 1512/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3091 - val_loss: -4.6191\n",
      "Epoch 1513/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4961 - val_loss: -3.2865\n",
      "Epoch 1514/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4737 - val_loss: -4.3417\n",
      "Epoch 1515/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4657 - val_loss: -3.8288\n",
      "Epoch 1516/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4952 - val_loss: -3.8298\n",
      "Epoch 1517/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3698 - val_loss: -4.1195\n",
      "Epoch 1518/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4847 - val_loss: -4.5416\n",
      "Epoch 1519/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4957 - val_loss: -3.8602\n",
      "Epoch 1520/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4716 - val_loss: -3.5252\n",
      "Epoch 1521/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4844 - val_loss: -3.2197\n",
      "Epoch 1522/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3900 - val_loss: -4.3723\n",
      "Epoch 1523/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4188 - val_loss: -4.0824\n",
      "Epoch 1524/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1547 - val_loss: -4.1253\n",
      "Epoch 1525/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4485 - val_loss: -2.9846\n",
      "Epoch 1526/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5002 - val_loss: -4.3237\n",
      "Epoch 1527/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4008 - val_loss: -4.3091\n",
      "Epoch 1528/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4362 - val_loss: -3.8254\n",
      "Epoch 1529/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4109 - val_loss: -2.4479\n",
      "Epoch 1530/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3963 - val_loss: -3.7271\n",
      "Epoch 1531/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.5074 - val_loss: -3.6645\n",
      "Epoch 1532/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4586 - val_loss: -2.9332\n",
      "Epoch 1533/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5455 - val_loss: -0.7865\n",
      "Epoch 1534/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5004 - val_loss: -3.9061\n",
      "Epoch 1535/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4775 - val_loss: -3.4411\n",
      "Epoch 1536/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3722 - val_loss: -3.7810\n",
      "Epoch 1537/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4913 - val_loss: -3.1089\n",
      "Epoch 1538/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4803 - val_loss: -4.4712\n",
      "Epoch 1539/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4739 - val_loss: -3.8283\n",
      "Epoch 1540/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4681 - val_loss: -3.9059\n",
      "Epoch 1541/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4381 - val_loss: -4.5258\n",
      "Epoch 1542/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5235 - val_loss: -3.5998\n",
      "Epoch 1543/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4524 - val_loss: -3.9114\n",
      "Epoch 1544/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4102 - val_loss: -4.3695\n",
      "Epoch 1545/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4056 - val_loss: -1.6604\n",
      "Epoch 1546/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4146 - val_loss: -4.1624\n",
      "Epoch 1547/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4623 - val_loss: -3.0070\n",
      "Epoch 1548/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5270 - val_loss: -4.5860\n",
      "Epoch 1549/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5426 - val_loss: -3.7813\n",
      "Epoch 1550/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0546 - val_loss: -4.3519\n",
      "Epoch 1551/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2485 - val_loss: -2.0103\n",
      "Epoch 1552/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3236 - val_loss: -4.4358\n",
      "Epoch 1553/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4235 - val_loss: -3.3574\n",
      "Epoch 1554/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4085 - val_loss: -4.6395\n",
      "Epoch 1555/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4607 - val_loss: -2.8520\n",
      "Epoch 1556/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3813 - val_loss: -3.0599\n",
      "Epoch 1557/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4186 - val_loss: -3.7621\n",
      "Epoch 1558/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4525 - val_loss: -1.9103\n",
      "Epoch 1559/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4886 - val_loss: -4.2516\n",
      "Epoch 1560/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4361 - val_loss: -4.2823\n",
      "Epoch 1561/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4131 - val_loss: -3.9468\n",
      "Epoch 1562/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4936 - val_loss: -4.3753\n",
      "Epoch 1563/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4078 - val_loss: -3.8446\n",
      "Epoch 1564/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4466 - val_loss: -4.2442\n",
      "Epoch 1565/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4949 - val_loss: -3.7342\n",
      "Epoch 1566/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5099 - val_loss: -4.2773\n",
      "Epoch 1567/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4818 - val_loss: -3.4937\n",
      "Epoch 1568/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5122 - val_loss: -2.4455\n",
      "Epoch 1569/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3377 - val_loss: -4.8099\n",
      "Epoch 1570/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3522 - val_loss: -4.4663\n",
      "Epoch 1571/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5144 - val_loss: -3.2308\n",
      "Epoch 1572/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4936 - val_loss: -4.4112\n",
      "Epoch 1573/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4907 - val_loss: -4.1463\n",
      "Epoch 1574/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4429 - val_loss: -2.8525\n",
      "Epoch 1575/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5174 - val_loss: -3.1315\n",
      "Epoch 1576/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4297 - val_loss: -4.5268\n",
      "Epoch 1577/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5539 - val_loss: -4.2917\n",
      "Epoch 1578/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5070 - val_loss: -4.1034\n",
      "Epoch 1579/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4295 - val_loss: -4.1545\n",
      "Epoch 1580/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4707 - val_loss: -0.9803\n",
      "Epoch 1581/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4577 - val_loss: -3.9419\n",
      "Epoch 1582/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4453 - val_loss: -3.7061\n",
      "Epoch 1583/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5265 - val_loss: -4.0947\n",
      "Epoch 1584/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5342 - val_loss: -4.0651\n",
      "Epoch 1585/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4157 - val_loss: -3.8895\n",
      "Epoch 1586/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4076 - val_loss: -1.2406\n",
      "Epoch 1587/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4586 - val_loss: -3.4163\n",
      "Epoch 1588/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4703 - val_loss: -4.1740\n",
      "Epoch 1589/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5191 - val_loss: -3.6551\n",
      "Epoch 1590/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5100 - val_loss: -3.8052\n",
      "Epoch 1591/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5170 - val_loss: -3.0778\n",
      "Epoch 1592/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5189 - val_loss: -3.5588\n",
      "Epoch 1593/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5707 - val_loss: -4.2685\n",
      "Epoch 1594/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4392 - val_loss: -2.2704\n",
      "Epoch 1595/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4546 - val_loss: -2.2117\n",
      "Epoch 1596/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5098 - val_loss: -4.1233\n",
      "Epoch 1597/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4833 - val_loss: -4.4213\n",
      "Epoch 1598/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5108 - val_loss: -4.6000\n",
      "Epoch 1599/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3984 - val_loss: -4.0185\n",
      "Epoch 1600/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3701 - val_loss: -4.2295\n",
      "Epoch 1601/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4782 - val_loss: -4.6635\n",
      "Epoch 1602/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5423 - val_loss: -4.2093\n",
      "Epoch 1603/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5434 - val_loss: -2.7094\n",
      "Epoch 1604/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5484 - val_loss: -3.9489\n",
      "Epoch 1605/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3965 - val_loss: -4.1214\n",
      "Epoch 1606/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5315 - val_loss: -3.4710\n",
      "Epoch 1607/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5223 - val_loss: -3.2008\n",
      "Epoch 1608/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5577 - val_loss: -1.2475\n",
      "Epoch 1609/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4220 - val_loss: -3.2160\n",
      "Epoch 1610/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5041 - val_loss: -4.1102\n",
      "Epoch 1611/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5215 - val_loss: -3.5146\n",
      "Epoch 1612/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3755 - val_loss: -4.3840\n",
      "Epoch 1613/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2981 - val_loss: -4.5069\n",
      "Epoch 1614/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4163 - val_loss: -4.4042\n",
      "Epoch 1615/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4577 - val_loss: -3.4241\n",
      "Epoch 1616/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.5179 - val_loss: -4.0404\n",
      "Epoch 1617/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3845 - val_loss: -4.7188\n",
      "Epoch 1618/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5319 - val_loss: -2.0045\n",
      "Epoch 1619/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5518 - val_loss: -3.3880\n",
      "Epoch 1620/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5170 - val_loss: -3.5921\n",
      "Epoch 1621/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5470 - val_loss: -3.3028\n",
      "Epoch 1622/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4750 - val_loss: -4.5972\n",
      "Epoch 1623/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2904 - val_loss: -4.4529\n",
      "Epoch 1624/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2645 - val_loss: -4.7044\n",
      "Epoch 1625/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3809 - val_loss: -3.6534\n",
      "Epoch 1626/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4631 - val_loss: 1.8972\n",
      "Epoch 1627/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5027 - val_loss: -3.8081\n",
      "Epoch 1628/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1166 - val_loss: -4.0015\n",
      "Epoch 1629/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3441 - val_loss: -4.2293\n",
      "Epoch 1630/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3229 - val_loss: -4.4335\n",
      "Epoch 1631/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4593 - val_loss: -4.5587\n",
      "Epoch 1632/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4305 - val_loss: -4.6423\n",
      "Epoch 1633/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3229 - val_loss: -3.6101\n",
      "Epoch 1634/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2949 - val_loss: -3.3039\n",
      "Epoch 1635/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4567 - val_loss: -4.6635\n",
      "Epoch 1636/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4123 - val_loss: -3.6851\n",
      "Epoch 1637/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4337 - val_loss: -4.4546\n",
      "Epoch 1638/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5013 - val_loss: -3.4447\n",
      "Epoch 1639/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4786 - val_loss: -3.4209\n",
      "Epoch 1640/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4357 - val_loss: -4.6572\n",
      "Epoch 1641/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4652 - val_loss: -2.7049\n",
      "Epoch 1642/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4876 - val_loss: -3.3010\n",
      "Epoch 1643/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5228 - val_loss: -4.2685\n",
      "Epoch 1644/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4732 - val_loss: -3.8339\n",
      "Epoch 1645/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4587 - val_loss: -4.6183\n",
      "Epoch 1646/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4785 - val_loss: -3.8551\n",
      "Epoch 1647/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4446 - val_loss: -3.4447\n",
      "Epoch 1648/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5280 - val_loss: -4.1565\n",
      "Epoch 1649/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4673 - val_loss: -4.3194\n",
      "Epoch 1650/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3370 - val_loss: -4.6342\n",
      "Epoch 1651/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4461 - val_loss: -3.3781\n",
      "Epoch 1652/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5463 - val_loss: -3.4977\n",
      "Epoch 1653/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4670 - val_loss: -3.7451\n",
      "Epoch 1654/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4987 - val_loss: -4.4284\n",
      "Epoch 1655/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4939 - val_loss: -3.5562\n",
      "Epoch 1656/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5258 - val_loss: -2.5051\n",
      "Epoch 1657/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4428 - val_loss: -0.6216\n",
      "Epoch 1658/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4930 - val_loss: -3.7710\n",
      "Epoch 1659/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5161 - val_loss: -3.8091\n",
      "Epoch 1660/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5376 - val_loss: -3.7796\n",
      "Epoch 1661/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4619 - val_loss: -4.6797\n",
      "Epoch 1662/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4646 - val_loss: -4.7245\n",
      "Epoch 1663/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5007 - val_loss: -3.4469\n",
      "Epoch 1664/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5288 - val_loss: -3.5872\n",
      "Epoch 1665/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5245 - val_loss: -4.0692\n",
      "Epoch 1666/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4893 - val_loss: -2.4537\n",
      "Epoch 1667/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1007 - val_loss: -4.3419\n",
      "Epoch 1668/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0909 - val_loss: -3.8507\n",
      "Epoch 1669/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2776 - val_loss: -4.1358\n",
      "Epoch 1670/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2573 - val_loss: -4.8824\n",
      "Epoch 1671/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3078 - val_loss: -4.2749\n",
      "Epoch 1672/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4663 - val_loss: -2.6229\n",
      "Epoch 1673/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4990 - val_loss: -3.6197\n",
      "Epoch 1674/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4458 - val_loss: -3.5570\n",
      "Epoch 1675/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4648 - val_loss: -3.7987\n",
      "Epoch 1676/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3828 - val_loss: -3.4247\n",
      "Epoch 1677/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4707 - val_loss: -4.0941\n",
      "Epoch 1678/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5318 - val_loss: -4.5131\n",
      "Epoch 1679/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4776 - val_loss: -2.5853\n",
      "Epoch 1680/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5453 - val_loss: -3.4705\n",
      "Epoch 1681/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4412 - val_loss: -4.2295\n",
      "Epoch 1682/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5393 - val_loss: -2.6454\n",
      "Epoch 1683/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4874 - val_loss: -1.7084\n",
      "Epoch 1684/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5498 - val_loss: -3.8466\n",
      "Epoch 1685/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5258 - val_loss: -3.8438\n",
      "Epoch 1686/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4312 - val_loss: -3.9825\n",
      "Epoch 1687/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5385 - val_loss: -4.3843\n",
      "Epoch 1688/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5476 - val_loss: -2.4996\n",
      "Epoch 1689/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4528 - val_loss: -4.3442\n",
      "Epoch 1690/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4439 - val_loss: -4.0672\n",
      "Epoch 1691/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5460 - val_loss: -3.5524\n",
      "Epoch 1692/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5369 - val_loss: -4.0534\n",
      "Epoch 1693/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5703 - val_loss: -3.5961\n",
      "Epoch 1694/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4550 - val_loss: -4.1333\n",
      "Epoch 1695/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5063 - val_loss: -3.6806\n",
      "Epoch 1696/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4581 - val_loss: -3.8974\n",
      "Epoch 1697/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5598 - val_loss: -3.4460\n",
      "Epoch 1698/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5497 - val_loss: -4.2252\n",
      "Epoch 1699/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4428 - val_loss: -4.4165\n",
      "Epoch 1700/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5268 - val_loss: -4.5918\n",
      "Epoch 1701/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5626 - val_loss: -2.6203\n",
      "Epoch 1702/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2838 - val_loss: -3.6879\n",
      "Epoch 1703/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4990 - val_loss: -3.6595\n",
      "Epoch 1704/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3832 - val_loss: -4.1062\n",
      "Epoch 1705/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5383 - val_loss: -2.2445\n",
      "Epoch 1706/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5323 - val_loss: -3.6708\n",
      "Epoch 1707/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3940 - val_loss: -2.2157\n",
      "Epoch 1708/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5098 - val_loss: -4.0797\n",
      "Epoch 1709/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4995 - val_loss: -3.6976\n",
      "Epoch 1710/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.5087"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -5.5087 - val_loss: -4.9727\n",
      "Epoch 1711/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5472 - val_loss: -3.3536\n",
      "Epoch 1712/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4689 - val_loss: -3.9336\n",
      "Epoch 1713/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5579 - val_loss: -3.5814\n",
      "Epoch 1714/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5098 - val_loss: -4.3091\n",
      "Epoch 1715/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4592 - val_loss: 1.9200\n",
      "Epoch 1716/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4297 - val_loss: -3.9267\n",
      "Epoch 1717/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5351 - val_loss: -2.6443\n",
      "Epoch 1718/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4175 - val_loss: -3.7181\n",
      "Epoch 1719/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5263 - val_loss: -4.3530\n",
      "Epoch 1720/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5571 - val_loss: -3.9338\n",
      "Epoch 1721/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5353 - val_loss: -3.3415\n",
      "Epoch 1722/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5026 - val_loss: -2.3919\n",
      "Epoch 1723/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5347 - val_loss: -4.0442\n",
      "Epoch 1724/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5612 - val_loss: -3.7296\n",
      "Epoch 1725/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5674 - val_loss: -3.8512\n",
      "Epoch 1726/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4964 - val_loss: -3.1547\n",
      "Epoch 1727/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5836 - val_loss: -4.1143\n",
      "Epoch 1728/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5440 - val_loss: -3.7571\n",
      "Epoch 1729/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.5796 - val_loss: -2.9866\n",
      "Epoch 1730/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5367 - val_loss: -3.7413\n",
      "Epoch 1731/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5072 - val_loss: -4.2539\n",
      "Epoch 1732/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4139 - val_loss: -4.4434\n",
      "Epoch 1733/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8985 - val_loss: -4.3532\n",
      "Epoch 1734/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1170 - val_loss: -4.1875\n",
      "Epoch 1735/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2458 - val_loss: -3.8303\n",
      "Epoch 1736/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3804 - val_loss: -3.9136\n",
      "Epoch 1737/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3771 - val_loss: -4.1135\n",
      "Epoch 1738/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4211 - val_loss: -2.7476\n",
      "Epoch 1739/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4418 - val_loss: -4.0648\n",
      "Epoch 1740/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5040 - val_loss: -2.4651\n",
      "Epoch 1741/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2678 - val_loss: -4.5902\n",
      "Epoch 1742/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5021 - val_loss: -4.3303\n",
      "Epoch 1743/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.9545 - val_loss: -4.2110\n",
      "Epoch 1744/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6288 - val_loss: -4.6032\n",
      "Epoch 1745/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9090 - val_loss: -4.7973\n",
      "Epoch 1746/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0665 - val_loss: -4.1043\n",
      "Epoch 1747/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1471 - val_loss: -4.3446\n",
      "Epoch 1748/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1682 - val_loss: -4.5572\n",
      "Epoch 1749/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2625 - val_loss: -4.0734\n",
      "Epoch 1750/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2641 - val_loss: -4.5080\n",
      "Epoch 1751/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3390 - val_loss: -4.6680\n",
      "Epoch 1752/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2977 - val_loss: -4.2016\n",
      "Epoch 1753/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3579 - val_loss: -4.7550\n",
      "Epoch 1754/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3966 - val_loss: -4.6292\n",
      "Epoch 1755/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3725 - val_loss: -4.4604\n",
      "Epoch 1756/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4201 - val_loss: -3.5291\n",
      "Epoch 1757/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3006 - val_loss: -3.7865\n",
      "Epoch 1758/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3607 - val_loss: -4.4287\n",
      "Epoch 1759/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4069 - val_loss: -4.4252\n",
      "Epoch 1760/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4076 - val_loss: -4.4760\n",
      "Epoch 1761/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4105 - val_loss: -0.9418\n",
      "Epoch 1762/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3510 - val_loss: -3.5982\n",
      "Epoch 1763/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4551 - val_loss: -4.4866\n",
      "Epoch 1764/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4606 - val_loss: -4.4774\n",
      "Epoch 1765/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3514 - val_loss: -3.6533\n",
      "Epoch 1766/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4251 - val_loss: -3.5644\n",
      "Epoch 1767/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5013 - val_loss: -4.3710\n",
      "Epoch 1768/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4106 - val_loss: -3.8988\n",
      "Epoch 1769/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5016 - val_loss: -4.2964\n",
      "Epoch 1770/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4395 - val_loss: -4.3509\n",
      "Epoch 1771/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4979 - val_loss: -4.0748\n",
      "Epoch 1772/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4236 - val_loss: -3.4847\n",
      "Epoch 1773/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3784 - val_loss: -4.1460\n",
      "Epoch 1774/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4378 - val_loss: -3.7336\n",
      "Epoch 1775/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4660 - val_loss: -4.6767\n",
      "Epoch 1776/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4791 - val_loss: -4.7482\n",
      "Epoch 1777/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3835"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_130_layer_call_fn, lstm_cell_130_layer_call_and_return_conditional_losses, lstm_cell_131_layer_call_fn, lstm_cell_131_layer_call_and_return_conditional_losses, lstm_cell_132_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -5.3835 - val_loss: -4.9874\n",
      "Epoch 1778/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4959 - val_loss: -4.2505\n",
      "Epoch 1779/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3979 - val_loss: -4.4899\n",
      "Epoch 1780/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4780 - val_loss: -4.1634\n",
      "Epoch 1781/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4872 - val_loss: -4.5227\n",
      "Epoch 1782/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4356 - val_loss: -3.4058\n",
      "Epoch 1783/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5132 - val_loss: -3.4254\n",
      "Epoch 1784/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5246 - val_loss: -3.6523\n",
      "Epoch 1785/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4067 - val_loss: -4.7615\n",
      "Epoch 1786/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3643 - val_loss: -4.3655\n",
      "Epoch 1787/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4896 - val_loss: -4.3861\n",
      "Epoch 1788/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5040 - val_loss: -3.0671\n",
      "Epoch 1789/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5109 - val_loss: -4.5767\n",
      "Epoch 1790/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5117 - val_loss: -4.3080\n",
      "Epoch 1791/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4087 - val_loss: -4.5662\n",
      "Epoch 1792/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4810 - val_loss: -4.1074\n",
      "Epoch 1793/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5213 - val_loss: -3.1924\n",
      "Epoch 1794/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4348 - val_loss: -4.3969\n",
      "Epoch 1795/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4947 - val_loss: -3.1916\n",
      "Epoch 1796/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4769 - val_loss: -4.3595\n",
      "Epoch 1797/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5592 - val_loss: -3.9655\n",
      "Epoch 1798/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4772 - val_loss: -3.1959\n",
      "Epoch 1799/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5449 - val_loss: -3.6660\n",
      "Epoch 1800/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4279 - val_loss: -2.8202\n",
      "Epoch 1801/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5454 - val_loss: -3.3739\n",
      "Epoch 1802/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3910 - val_loss: -2.5891\n",
      "Epoch 1803/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3189 - val_loss: -4.6812\n",
      "Epoch 1804/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5641 - val_loss: -4.4858\n",
      "Epoch 1805/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5592 - val_loss: -2.2629\n",
      "Epoch 1806/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4505 - val_loss: -4.3846\n",
      "Epoch 1807/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4013 - val_loss: -4.5017\n",
      "Epoch 1808/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5331 - val_loss: -1.8089\n",
      "Epoch 1809/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4121 - val_loss: -4.1716\n",
      "Epoch 1810/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5533 - val_loss: -4.3797\n",
      "Epoch 1811/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5344 - val_loss: -4.2828\n",
      "Epoch 1812/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4470 - val_loss: -4.2131\n",
      "Epoch 1813/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5839 - val_loss: -2.7763\n",
      "Epoch 1814/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4597 - val_loss: -3.2327\n",
      "Epoch 1815/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5001 - val_loss: -3.9449\n",
      "Epoch 1816/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4743 - val_loss: -2.4483\n",
      "Epoch 1817/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4185 - val_loss: -4.1946\n",
      "Epoch 1818/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4898 - val_loss: -3.5797\n",
      "Epoch 1819/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5112 - val_loss: -3.8498\n",
      "Epoch 1820/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5478 - val_loss: -3.9106\n",
      "Epoch 1821/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5224 - val_loss: -4.1266\n",
      "Epoch 1822/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4033 - val_loss: -3.7831\n",
      "Epoch 1823/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5512 - val_loss: -3.5744\n",
      "Epoch 1824/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5486 - val_loss: -4.4000\n",
      "Epoch 1825/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5606 - val_loss: -3.6438\n",
      "Epoch 1826/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4886 - val_loss: -4.0238\n",
      "Epoch 1827/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5530 - val_loss: 0.6550\n",
      "Epoch 1828/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5114 - val_loss: -2.6832\n",
      "Epoch 1829/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4581 - val_loss: -4.4804\n",
      "Epoch 1830/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4433 - val_loss: -0.8223\n",
      "Epoch 1831/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0650 - val_loss: -4.4871\n",
      "Epoch 1832/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4551 - val_loss: -4.0926\n",
      "Epoch 1833/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4402 - val_loss: -2.8168\n",
      "Epoch 1834/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4090 - val_loss: -3.7760\n",
      "Epoch 1835/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5242 - val_loss: -4.1648\n",
      "Epoch 1836/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5614 - val_loss: -4.0245\n",
      "Epoch 1837/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3978 - val_loss: -3.3764\n",
      "Epoch 1838/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3928 - val_loss: -3.2290\n",
      "Epoch 1839/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4945 - val_loss: -4.1589\n",
      "Epoch 1840/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4400 - val_loss: -3.8523\n",
      "Epoch 1841/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5216 - val_loss: -3.8506\n",
      "Epoch 1842/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5167 - val_loss: -4.1179\n",
      "Epoch 1843/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2931 - val_loss: -4.7204\n",
      "Epoch 1844/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5650 - val_loss: -3.8022\n",
      "Epoch 1845/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5874 - val_loss: -3.9184\n",
      "Epoch 1846/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5041 - val_loss: -4.1011\n",
      "Epoch 1847/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5086 - val_loss: -2.6716\n",
      "Epoch 1848/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4259 - val_loss: -3.9740\n",
      "Epoch 1849/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5915 - val_loss: -1.0127\n",
      "Epoch 1850/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5636 - val_loss: -2.5164\n",
      "Epoch 1851/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5724 - val_loss: -3.6836\n",
      "Epoch 1852/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5619 - val_loss: -3.5197\n",
      "Epoch 1853/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5707 - val_loss: -4.3276\n",
      "Epoch 1854/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5291 - val_loss: -4.6460\n",
      "Epoch 1855/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5964 - val_loss: -2.6059\n",
      "Epoch 1856/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4470 - val_loss: -4.5334\n",
      "Epoch 1857/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5529 - val_loss: -4.0009\n",
      "Epoch 1858/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5792 - val_loss: -3.8050\n",
      "Epoch 1859/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3700 - val_loss: -4.6358\n",
      "Epoch 1860/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5908 - val_loss: -3.6030\n",
      "Epoch 1861/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5581 - val_loss: -4.1672\n",
      "Epoch 1862/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5472 - val_loss: -3.4693\n",
      "Epoch 1863/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4728 - val_loss: -3.0476\n",
      "Epoch 1864/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5275 - val_loss: -4.4191\n",
      "Epoch 1865/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5183 - val_loss: -4.4735\n",
      "Epoch 1866/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4233 - val_loss: -3.4476\n",
      "Epoch 1867/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5420 - val_loss: -3.8402\n",
      "Epoch 1868/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5382 - val_loss: -3.2101\n",
      "Epoch 1869/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5650 - val_loss: -4.5250\n",
      "Epoch 1870/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5953 - val_loss: -2.3783\n",
      "Epoch 1871/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5674 - val_loss: -2.1461\n",
      "Epoch 1872/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5184 - val_loss: -4.6057\n",
      "Epoch 1873/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5657 - val_loss: -3.0806\n",
      "Epoch 1874/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5655 - val_loss: -4.3976\n",
      "Epoch 1875/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5115 - val_loss: -2.7219\n",
      "Epoch 1876/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4194 - val_loss: -3.6588\n",
      "Epoch 1877/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6160 - val_loss: -2.4688\n",
      "Epoch 1878/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5225 - val_loss: -4.0744\n",
      "Epoch 1879/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5293 - val_loss: -4.3142\n",
      "Epoch 1880/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5121 - val_loss: -3.3838\n",
      "Epoch 1881/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4560 - val_loss: -4.5588\n",
      "Epoch 1882/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5242 - val_loss: -3.9401\n",
      "Epoch 1883/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5287 - val_loss: -2.5574\n",
      "Epoch 1884/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4125 - val_loss: -3.3809\n",
      "Epoch 1885/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3832 - val_loss: -4.6887\n",
      "Epoch 1886/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4646 - val_loss: -4.3358\n",
      "Epoch 1887/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6029 - val_loss: -4.1201\n",
      "Epoch 1888/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6117 - val_loss: -3.9042\n",
      "Epoch 1889/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5813 - val_loss: -4.1171\n",
      "Epoch 1890/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5167 - val_loss: -4.6704\n",
      "Epoch 1891/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5806 - val_loss: -3.0983\n",
      "Epoch 1892/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4885 - val_loss: -2.9434\n",
      "Epoch 1893/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5646 - val_loss: -3.8188\n",
      "Epoch 1894/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5149 - val_loss: -4.0717\n",
      "Epoch 1895/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4134 - val_loss: -4.8549\n",
      "Epoch 1896/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5583 - val_loss: -3.5656\n",
      "Epoch 1897/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4891 - val_loss: -4.1344\n",
      "Epoch 1898/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5537 - val_loss: -3.9961\n",
      "Epoch 1899/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6144 - val_loss: -2.3871\n",
      "Epoch 1900/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5247 - val_loss: -4.3049\n",
      "Epoch 1901/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5832 - val_loss: -4.2250\n",
      "Epoch 1902/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5179 - val_loss: -3.9922\n",
      "Epoch 1903/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5717 - val_loss: -4.1806\n",
      "Epoch 1904/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5819 - val_loss: -4.5186\n",
      "Epoch 1905/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4936 - val_loss: -4.0066\n",
      "Epoch 1906/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5850 - val_loss: -4.6752\n",
      "Epoch 1907/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4879 - val_loss: -2.1491\n",
      "Epoch 1908/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5332 - val_loss: -4.3717\n",
      "Epoch 1909/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5657 - val_loss: -4.2781\n",
      "Epoch 1910/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3565 - val_loss: -3.8116\n",
      "Epoch 1911/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.6134 - val_loss: -3.1688\n",
      "Epoch 1912/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5852 - val_loss: -3.7195\n",
      "Epoch 1913/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5979 - val_loss: -4.0809\n",
      "Epoch 1914/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5536 - val_loss: -3.0862\n",
      "Epoch 1915/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5233 - val_loss: -4.2810\n",
      "Epoch 1916/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5908 - val_loss: -4.3090\n",
      "Epoch 1917/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4895 - val_loss: -4.6580\n",
      "Epoch 1918/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5892 - val_loss: -3.8811\n",
      "Epoch 1919/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5115 - val_loss: -3.9074\n",
      "Epoch 1920/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5997 - val_loss: -3.9358\n",
      "Epoch 1921/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5625 - val_loss: -4.1979\n",
      "Epoch 1922/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5722 - val_loss: -3.8229\n",
      "Epoch 1923/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5968 - val_loss: -2.4880\n",
      "Epoch 1924/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5099 - val_loss: -4.1145\n",
      "Epoch 1925/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5390 - val_loss: -1.1749\n",
      "Epoch 1926/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4723 - val_loss: -3.8602\n",
      "Epoch 1927/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6326 - val_loss: -3.9797\n",
      "Epoch 1928/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1883 - val_loss: -4.4628\n",
      "Epoch 1929/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3921 - val_loss: -4.4581\n",
      "Epoch 1930/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4955 - val_loss: -1.2889\n",
      "Epoch 1931/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4398 - val_loss: -2.0832\n",
      "Epoch 1932/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3631 - val_loss: -3.9242\n",
      "Epoch 1933/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4016 - val_loss: -4.0696\n",
      "Epoch 1934/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5336 - val_loss: -3.7300\n",
      "Epoch 1935/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4520 - val_loss: -3.6670\n",
      "Epoch 1936/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5150 - val_loss: -3.7860\n",
      "Epoch 1937/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5466 - val_loss: -2.4450\n",
      "Epoch 1938/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5618 - val_loss: 3.7625\n",
      "Epoch 1939/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5056 - val_loss: -3.4874\n",
      "Epoch 1940/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5305 - val_loss: -3.2148\n",
      "Epoch 1941/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5344 - val_loss: -3.8841\n",
      "Epoch 1942/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5308 - val_loss: -3.3367\n",
      "Epoch 1943/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5870 - val_loss: -3.6036\n",
      "Epoch 1944/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5796 - val_loss: -3.3865\n",
      "Epoch 1945/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5059 - val_loss: -4.2255\n",
      "Epoch 1946/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5945 - val_loss: -4.2184\n",
      "Epoch 1947/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5349 - val_loss: -4.3603\n",
      "Epoch 1948/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5335 - val_loss: -4.5623\n",
      "Epoch 1949/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6017 - val_loss: -4.2909\n",
      "Epoch 1950/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4496 - val_loss: -3.5094\n",
      "Epoch 1951/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5883 - val_loss: -4.2149\n",
      "Epoch 1952/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4902 - val_loss: -4.2219\n",
      "Epoch 1953/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5841 - val_loss: -4.3856\n",
      "Epoch 1954/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4154 - val_loss: -3.4672\n",
      "Epoch 1955/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5731 - val_loss: -4.3031\n",
      "Epoch 1956/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5334 - val_loss: -2.9007\n",
      "Epoch 1957/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4810 - val_loss: -3.4313\n",
      "Epoch 1958/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6114 - val_loss: -3.9392\n",
      "Epoch 1959/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6134 - val_loss: -4.0302\n",
      "Epoch 1960/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4367 - val_loss: -4.3417\n",
      "Epoch 1961/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.6079 - val_loss: -4.0303\n",
      "Epoch 1962/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5900 - val_loss: -4.1396\n",
      "Epoch 1963/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6078 - val_loss: -3.3020\n",
      "Epoch 1964/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5789 - val_loss: -3.6701\n",
      "Epoch 1965/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6111 - val_loss: -4.2892\n",
      "Epoch 1966/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5174 - val_loss: -3.7285\n",
      "Epoch 1967/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5772 - val_loss: -4.4103\n",
      "Epoch 1968/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5048 - val_loss: -3.9542\n",
      "Epoch 1969/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4990 - val_loss: -3.8431\n",
      "Epoch 1970/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6116 - val_loss: -3.6110\n",
      "Epoch 1971/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6122 - val_loss: 3.8079\n",
      "Epoch 1972/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2398 - val_loss: -3.7683\n",
      "Epoch 1973/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5417 - val_loss: -3.1557\n",
      "Epoch 1974/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.5882 - val_loss: -3.8615\n",
      "Epoch 1975/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5979 - val_loss: -3.3837\n",
      "Epoch 1976/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4835 - val_loss: -3.7323\n",
      "Epoch 1977/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5255 - val_loss: -3.1704\n",
      "Epoch 1978/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5233 - val_loss: -3.8884\n",
      "Epoch 1979/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5062 - val_loss: -3.7849\n",
      "Epoch 1980/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.6198 - val_loss: -3.2603\n",
      "Epoch 1981/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.6304 - val_loss: -4.2913\n",
      "Epoch 1982/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5119 - val_loss: -4.2098\n",
      "Epoch 1983/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.6142 - val_loss: -3.9777\n",
      "Epoch 1984/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.5770 - val_loss: -4.1346\n",
      "Epoch 1985/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.5959 - val_loss: -4.0871\n",
      "Epoch 1986/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5707 - val_loss: -4.0415\n",
      "Epoch 1987/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5375 - val_loss: -3.5123\n",
      "Epoch 1988/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6298 - val_loss: -3.9176\n",
      "Epoch 1989/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2017 - val_loss: -4.0898\n",
      "Epoch 1990/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5107 - val_loss: -3.8836\n",
      "Epoch 1991/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5276 - val_loss: -4.3242\n",
      "Epoch 1992/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5167 - val_loss: -4.3075\n",
      "Epoch 1993/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5843 - val_loss: -3.9962\n",
      "Epoch 1994/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5835 - val_loss: -4.5354\n",
      "Epoch 1995/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.6021 - val_loss: -3.0814\n",
      "Epoch 1996/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.6061 - val_loss: -3.8313\n",
      "Epoch 1997/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5536 - val_loss: -1.4465\n",
      "Epoch 1998/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5822 - val_loss: -3.5790\n",
      "Epoch 1999/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5672 - val_loss: -4.0843\n",
      "Epoch 2000/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5588 - val_loss: -4.2082\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 7s 45ms/step - loss: -5.2029\n",
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 8s 45ms/step - loss: -5.1614\n",
      "WARNING:tensorflow:Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.1066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 34s 252ms/step - loss: -1.1066 - val_loss: 0.4978\n",
      "Epoch 2/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.2553"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 244ms/step - loss: -1.2553 - val_loss: 0.4771\n",
      "Epoch 3/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -1.2568 - val_loss: 0.6930\n",
      "Epoch 4/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -1.3046 - val_loss: 1.1819\n",
      "Epoch 5/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.3522"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -1.3522 - val_loss: -0.9625\n",
      "Epoch 6/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.0476"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -2.0476 - val_loss: -1.1638\n",
      "Epoch 7/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.8280"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -2.8280 - val_loss: -2.6601\n",
      "Epoch 8/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.2244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -3.2244 - val_loss: -3.0778\n",
      "Epoch 9/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.4992"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -3.4992 - val_loss: -3.2612\n",
      "Epoch 10/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.6624"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -3.6624 - val_loss: -3.3263\n",
      "Epoch 11/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.7728 - val_loss: -1.6886\n",
      "Epoch 12/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.8091"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -3.8091 - val_loss: -3.3743\n",
      "Epoch 13/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.9448"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -3.9448 - val_loss: -3.6514\n",
      "Epoch 14/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8297 - val_loss: -2.4662\n",
      "Epoch 15/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.9719 - val_loss: -3.5119\n",
      "Epoch 16/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.9656 - val_loss: -3.3641\n",
      "Epoch 17/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0872 - val_loss: -3.1184\n",
      "Epoch 18/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.0873 - val_loss: -3.6084\n",
      "Epoch 19/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1033 - val_loss: -3.5042\n",
      "Epoch 20/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -3.6641 - val_loss: -2.4481\n",
      "Epoch 21/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.1067 - val_loss: -0.4868\n",
      "Epoch 22/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.1211"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -4.1211 - val_loss: -3.7097\n",
      "Epoch 23/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.2071 - val_loss: -3.1342\n",
      "Epoch 24/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1308 - val_loss: -3.5356\n",
      "Epoch 25/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2127 - val_loss: -2.0733\n",
      "Epoch 26/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.2131"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -4.2131 - val_loss: -3.8512\n",
      "Epoch 27/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2452 - val_loss: -3.8321\n",
      "Epoch 28/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.2172 - val_loss: -3.7597\n",
      "Epoch 29/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1798 - val_loss: -3.4624\n",
      "Epoch 30/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2516 - val_loss: -3.0274\n",
      "Epoch 31/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.3408"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -4.3408 - val_loss: -3.9411\n",
      "Epoch 32/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2132 - val_loss: -2.2119\n",
      "Epoch 33/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3555 - val_loss: -3.8132\n",
      "Epoch 34/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1523 - val_loss: -3.0592\n",
      "Epoch 35/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0282 - val_loss: -3.8863\n",
      "Epoch 36/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2910 - val_loss: -2.6679\n",
      "Epoch 37/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2484 - val_loss: -3.6917\n",
      "Epoch 38/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2743 - val_loss: -3.6690\n",
      "Epoch 39/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3872 - val_loss: -3.1510\n",
      "Epoch 40/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3701 - val_loss: -3.8231\n",
      "Epoch 41/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.3742 - val_loss: -3.8597\n",
      "Epoch 42/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4342"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -4.4342 - val_loss: -4.0591\n",
      "Epoch 43/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4103 - val_loss: -2.9814\n",
      "Epoch 44/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2034 - val_loss: -3.4549\n",
      "Epoch 45/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3623 - val_loss: -4.0257\n",
      "Epoch 46/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4157 - val_loss: -4.0031\n",
      "Epoch 47/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4400 - val_loss: -3.9776\n",
      "Epoch 48/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3789 - val_loss: -3.7617\n",
      "Epoch 49/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4834 - val_loss: -4.0342\n",
      "Epoch 50/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4181 - val_loss: -4.0198\n",
      "Epoch 51/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.3992"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -4.3992 - val_loss: -4.1468\n",
      "Epoch 52/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4516 - val_loss: -4.0851\n",
      "Epoch 53/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3449 - val_loss: -3.4938\n",
      "Epoch 54/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.3951 - val_loss: -3.8855\n",
      "Epoch 55/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.3913 - val_loss: -3.7601\n",
      "Epoch 56/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4852 - val_loss: -3.9867\n",
      "Epoch 57/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4985 - val_loss: -4.0663\n",
      "Epoch 58/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4761 - val_loss: -3.8366\n",
      "Epoch 59/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3531 - val_loss: -3.7764\n",
      "Epoch 60/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4784 - val_loss: -3.9820\n",
      "Epoch 61/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5005 - val_loss: -4.0267\n",
      "Epoch 62/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5629 - val_loss: -3.9855\n",
      "Epoch 63/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5739 - val_loss: -3.2149\n",
      "Epoch 64/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5256 - val_loss: -2.8753\n",
      "Epoch 65/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4266 - val_loss: -3.5146\n",
      "Epoch 66/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5169 - val_loss: -4.0170\n",
      "Epoch 67/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4764 - val_loss: -3.7267\n",
      "Epoch 68/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5276 - val_loss: -4.1463\n",
      "Epoch 69/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5880 - val_loss: -2.9651\n",
      "Epoch 70/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5322 - val_loss: -3.6094\n",
      "Epoch 71/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5808 - val_loss: -3.9030\n",
      "Epoch 72/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5743 - val_loss: -3.5577\n",
      "Epoch 73/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6221 - val_loss: -4.0740\n",
      "Epoch 74/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.4273 - val_loss: -3.5797\n",
      "Epoch 75/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1359 - val_loss: -3.8994\n",
      "Epoch 76/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4256 - val_loss: -4.0422\n",
      "Epoch 77/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4181 - val_loss: -4.0152\n",
      "Epoch 78/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5774 - val_loss: -3.2754\n",
      "Epoch 79/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5582 - val_loss: -3.1180\n",
      "Epoch 80/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4950 - val_loss: -3.3598\n",
      "Epoch 81/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6353 - val_loss: -3.9386\n",
      "Epoch 82/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5323 - val_loss: -3.5154\n",
      "Epoch 83/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0255 - val_loss: -4.0106\n",
      "Epoch 84/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4821 - val_loss: -3.2778\n",
      "Epoch 85/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4762 - val_loss: -3.9817\n",
      "Epoch 86/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5573 - val_loss: -3.3856\n",
      "Epoch 87/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6168 - val_loss: -3.0876\n",
      "Epoch 88/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5237 - val_loss: -3.5697\n",
      "Epoch 89/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6423"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 242ms/step - loss: -4.6423 - val_loss: -4.1501\n",
      "Epoch 90/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6132 - val_loss: -3.7522\n",
      "Epoch 91/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6113 - val_loss: -4.1382\n",
      "Epoch 92/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5989 - val_loss: -3.8019\n",
      "Epoch 93/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4936 - val_loss: -2.3215\n",
      "Epoch 94/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5889 - val_loss: -4.1396\n",
      "Epoch 95/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6355 - val_loss: -3.9548\n",
      "Epoch 96/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6465 - val_loss: -3.9870\n",
      "Epoch 97/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7203 - val_loss: -4.0375\n",
      "Epoch 98/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.2111 - val_loss: -4.0942\n",
      "Epoch 99/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6637 - val_loss: -4.0241\n",
      "Epoch 100/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -4.7066 - val_loss: -4.2227\n",
      "Epoch 101/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5495 - val_loss: -4.2071\n",
      "Epoch 102/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7094 - val_loss: -3.4710\n",
      "Epoch 103/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6394 - val_loss: -4.0579\n",
      "Epoch 104/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.2934"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -4.2934 - val_loss: -4.2757\n",
      "Epoch 105/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4269 - val_loss: -3.9740\n",
      "Epoch 106/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7436 - val_loss: -4.1924\n",
      "Epoch 107/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6177 - val_loss: -4.1942\n",
      "Epoch 108/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6888 - val_loss: -4.1732\n",
      "Epoch 109/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1622 - val_loss: -1.1604\n",
      "Epoch 110/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.6111 - val_loss: -3.4599\n",
      "Epoch 111/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.1565 - val_loss: -2.2793\n",
      "Epoch 112/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2933 - val_loss: -3.9210\n",
      "Epoch 113/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3435 - val_loss: -3.9695\n",
      "Epoch 114/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4721 - val_loss: -3.9315\n",
      "Epoch 115/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5370 - val_loss: -4.0855\n",
      "Epoch 116/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4675 - val_loss: -3.8094\n",
      "Epoch 117/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5501 - val_loss: -3.5799\n",
      "Epoch 118/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5820 - val_loss: -4.1185\n",
      "Epoch 119/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6068 - val_loss: -3.4267\n",
      "Epoch 120/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5704 - val_loss: -3.6478\n",
      "Epoch 121/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5439 - val_loss: -4.1018\n",
      "Epoch 122/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6482 - val_loss: -4.1301\n",
      "Epoch 123/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6163 - val_loss: -4.2127\n",
      "Epoch 124/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6418 - val_loss: -3.9443\n",
      "Epoch 125/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6668 - val_loss: -3.8925\n",
      "Epoch 126/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6256"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.6256 - val_loss: -4.3029\n",
      "Epoch 127/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5927 - val_loss: -4.2343\n",
      "Epoch 128/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6034 - val_loss: -4.0712\n",
      "Epoch 129/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5639 - val_loss: -3.4575\n",
      "Epoch 130/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7635 - val_loss: -3.7590\n",
      "Epoch 131/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5172 - val_loss: -4.2116\n",
      "Epoch 132/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7103 - val_loss: -3.4238\n",
      "Epoch 133/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6185 - val_loss: -4.2135\n",
      "Epoch 134/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6988 - val_loss: -4.1128\n",
      "Epoch 135/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6392 - val_loss: 2.3175\n",
      "Epoch 136/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4430 - val_loss: -3.8631\n",
      "Epoch 137/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6194 - val_loss: -4.2090\n",
      "Epoch 138/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7211 - val_loss: -4.2907\n",
      "Epoch 139/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6804 - val_loss: -4.0182\n",
      "Epoch 140/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -4.7130 - val_loss: -3.5485\n",
      "Epoch 141/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6587 - val_loss: -4.0322\n",
      "Epoch 142/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7532 - val_loss: -3.1405\n",
      "Epoch 143/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7107 - val_loss: -3.8603\n",
      "Epoch 144/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6510 - val_loss: -4.1165\n",
      "Epoch 145/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7456 - val_loss: -3.9908\n",
      "Epoch 146/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7373 - val_loss: -3.7815\n",
      "Epoch 147/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7131"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -4.7131 - val_loss: -4.3443\n",
      "Epoch 148/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6508 - val_loss: -4.3076\n",
      "Epoch 149/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7511 - val_loss: -3.1680\n",
      "Epoch 150/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6528 - val_loss: -2.8543\n",
      "Epoch 151/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7489 - val_loss: -4.0272\n",
      "Epoch 152/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4761 - val_loss: -1.1143\n",
      "Epoch 153/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6866 - val_loss: -2.7937\n",
      "Epoch 154/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7470 - val_loss: -4.0306\n",
      "Epoch 155/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7487 - val_loss: -4.1972\n",
      "Epoch 156/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6428 - val_loss: -4.2166\n",
      "Epoch 157/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7655 - val_loss: -4.0211\n",
      "Epoch 158/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7841 - val_loss: -4.1743\n",
      "Epoch 159/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8163 - val_loss: -2.7359\n",
      "Epoch 160/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8162 - val_loss: -2.0234\n",
      "Epoch 161/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7747 - val_loss: -4.1895\n",
      "Epoch 162/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5429 - val_loss: -4.2981\n",
      "Epoch 163/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7801 - val_loss: -3.9712\n",
      "Epoch 164/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.0621 - val_loss: -3.3940\n",
      "Epoch 165/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.8647 - val_loss: -2.5042\n",
      "Epoch 166/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3498 - val_loss: -4.0513\n",
      "Epoch 167/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4354 - val_loss: -4.0238\n",
      "Epoch 168/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4026 - val_loss: -3.9156\n",
      "Epoch 169/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6053 - val_loss: -4.1505\n",
      "Epoch 170/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6651 - val_loss: -2.9639\n",
      "Epoch 171/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6497 - val_loss: -4.1435\n",
      "Epoch 172/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6858 - val_loss: -1.1854\n",
      "Epoch 173/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6093 - val_loss: -3.6091\n",
      "Epoch 174/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4919 - val_loss: -4.1239\n",
      "Epoch 175/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6804 - val_loss: -3.7578\n",
      "Epoch 176/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6770 - val_loss: -4.0906\n",
      "Epoch 177/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6740 - val_loss: -4.1037\n",
      "Epoch 178/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7275 - val_loss: -3.7154\n",
      "Epoch 179/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7908 - val_loss: -4.1734\n",
      "Epoch 180/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5819 - val_loss: -4.0074\n",
      "Epoch 181/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7824 - val_loss: -4.0816\n",
      "Epoch 182/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8286 - val_loss: -3.5727\n",
      "Epoch 183/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7150 - val_loss: -3.3095\n",
      "Epoch 184/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7367 - val_loss: -4.2567\n",
      "Epoch 185/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8095 - val_loss: -3.4193\n",
      "Epoch 186/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7206"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -4.7206 - val_loss: -4.3475\n",
      "Epoch 187/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8111 - val_loss: -2.9807\n",
      "Epoch 188/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5829 - val_loss: -4.2174\n",
      "Epoch 189/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7446 - val_loss: -4.0126\n",
      "Epoch 190/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8263 - val_loss: -4.2089\n",
      "Epoch 191/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8604 - val_loss: -3.5871\n",
      "Epoch 192/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8224 - val_loss: -4.0319\n",
      "Epoch 193/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.1399 - val_loss: -4.1977\n",
      "Epoch 194/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7814 - val_loss: -3.4778\n",
      "Epoch 195/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8336 - val_loss: -3.7546\n",
      "Epoch 196/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6210 - val_loss: -4.1817\n",
      "Epoch 197/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8043 - val_loss: -3.7048\n",
      "Epoch 198/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7673 - val_loss: -4.2261\n",
      "Epoch 199/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7518 - val_loss: -4.2191\n",
      "Epoch 200/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7759 - val_loss: -4.2344\n",
      "Epoch 201/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8204 - val_loss: -3.5167\n",
      "Epoch 202/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8397 - val_loss: -4.2838\n",
      "Epoch 203/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1575 - val_loss: -3.9470\n",
      "Epoch 204/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.6168 - val_loss: -3.0677\n",
      "Epoch 205/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6648 - val_loss: -4.1778\n",
      "Epoch 206/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7052 - val_loss: -3.9305\n",
      "Epoch 207/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7649 - val_loss: -4.1954\n",
      "Epoch 208/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7160 - val_loss: -4.2040\n",
      "Epoch 209/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5690 - val_loss: -3.7460\n",
      "Epoch 210/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7549 - val_loss: -3.9058\n",
      "Epoch 211/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7640 - val_loss: -4.3426\n",
      "Epoch 212/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8692 - val_loss: -3.5880\n",
      "Epoch 213/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7677 - val_loss: -4.0576\n",
      "Epoch 214/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8574"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -4.8574 - val_loss: -4.3479\n",
      "Epoch 215/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7069 - val_loss: -4.2283\n",
      "Epoch 216/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9078 - val_loss: -3.7842\n",
      "Epoch 217/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7965 - val_loss: -3.7731\n",
      "Epoch 218/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7698 - val_loss: -3.9125\n",
      "Epoch 219/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9189 - val_loss: -4.1971\n",
      "Epoch 220/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7730 - val_loss: -3.6650\n",
      "Epoch 221/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8016 - val_loss: -3.3255\n",
      "Epoch 222/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8853 - val_loss: -4.1775\n",
      "Epoch 223/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7967 - val_loss: -3.0235\n",
      "Epoch 224/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8274 - val_loss: -4.3436\n",
      "Epoch 225/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8370 - val_loss: -3.7178\n",
      "Epoch 226/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4105 - val_loss: -3.8681\n",
      "Epoch 227/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8253 - val_loss: -4.0747\n",
      "Epoch 228/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8373 - val_loss: -4.1394\n",
      "Epoch 229/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8841 - val_loss: -4.3379\n",
      "Epoch 230/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7513 - val_loss: -3.8901\n",
      "Epoch 231/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8617 - val_loss: -4.2697\n",
      "Epoch 232/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8343 - val_loss: -4.2837\n",
      "Epoch 233/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8152 - val_loss: -3.9023\n",
      "Epoch 234/2000\n",
      "118/118 [==============================] - 23s 192ms/step - loss: -4.8872 - val_loss: -2.7174\n",
      "Epoch 235/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2304 - val_loss: -4.2768\n",
      "Epoch 236/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7970 - val_loss: -3.7396\n",
      "Epoch 237/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8773"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 250ms/step - loss: -4.8773 - val_loss: -4.3831\n",
      "Epoch 238/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7575 - val_loss: -3.9590\n",
      "Epoch 239/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7679 - val_loss: -4.1275\n",
      "Epoch 240/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5689 - val_loss: -3.8991\n",
      "Epoch 241/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8762 - val_loss: -4.3739\n",
      "Epoch 242/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9218"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -4.9218 - val_loss: -4.4319\n",
      "Epoch 243/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9243"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -4.9243 - val_loss: -4.4674\n",
      "Epoch 244/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7812 - val_loss: -4.2288\n",
      "Epoch 245/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9154 - val_loss: -4.0689\n",
      "Epoch 246/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9834 - val_loss: -4.4309\n",
      "Epoch 247/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8731 - val_loss: -3.7536\n",
      "Epoch 248/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -2.5716 - val_loss: -2.6200\n",
      "Epoch 249/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -2.8711 - val_loss: -3.4793\n",
      "Epoch 250/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -3.6006 - val_loss: -3.1689\n",
      "Epoch 251/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0517 - val_loss: -3.8160\n",
      "Epoch 252/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.8936 - val_loss: -3.8172\n",
      "Epoch 253/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2712 - val_loss: -3.9358\n",
      "Epoch 254/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3044 - val_loss: -3.9202\n",
      "Epoch 255/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.3218 - val_loss: -3.8702\n",
      "Epoch 256/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3110 - val_loss: -4.0434\n",
      "Epoch 257/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3798 - val_loss: -3.4681\n",
      "Epoch 258/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4487 - val_loss: -4.1520\n",
      "Epoch 259/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5130 - val_loss: -3.7207\n",
      "Epoch 260/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5044 - val_loss: -4.1344\n",
      "Epoch 261/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5223 - val_loss: -4.0639\n",
      "Epoch 262/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4551 - val_loss: -3.4913\n",
      "Epoch 263/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5422 - val_loss: -3.8671\n",
      "Epoch 264/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5125 - val_loss: -3.6161\n",
      "Epoch 265/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4898 - val_loss: -3.4335\n",
      "Epoch 266/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5729 - val_loss: -3.9846\n",
      "Epoch 267/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5713 - val_loss: -4.1371\n",
      "Epoch 268/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4272 - val_loss: -4.0441\n",
      "Epoch 269/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6200 - val_loss: -3.2340\n",
      "Epoch 270/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5572 - val_loss: -4.0496\n",
      "Epoch 271/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6210 - val_loss: -4.0888\n",
      "Epoch 272/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6724 - val_loss: -4.2198\n",
      "Epoch 273/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5912 - val_loss: -4.0747\n",
      "Epoch 274/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6224 - val_loss: -4.0103\n",
      "Epoch 275/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6659 - val_loss: -4.2567\n",
      "Epoch 276/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6103 - val_loss: -4.2549\n",
      "Epoch 277/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5753 - val_loss: -3.7861\n",
      "Epoch 278/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6628 - val_loss: -3.5545\n",
      "Epoch 279/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7209 - val_loss: -4.3220\n",
      "Epoch 280/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6261 - val_loss: -4.0371\n",
      "Epoch 281/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7243 - val_loss: -3.0708\n",
      "Epoch 282/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6725 - val_loss: -4.1333\n",
      "Epoch 283/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7204 - val_loss: -3.0273\n",
      "Epoch 284/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7371 - val_loss: -2.3339\n",
      "Epoch 285/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6248 - val_loss: -4.2986\n",
      "Epoch 286/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.5581 - val_loss: -3.4548\n",
      "Epoch 287/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4979 - val_loss: -3.8709\n",
      "Epoch 288/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6505 - val_loss: -4.2355\n",
      "Epoch 289/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5472 - val_loss: -3.6965\n",
      "Epoch 290/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7577 - val_loss: -4.1671\n",
      "Epoch 291/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6632 - val_loss: -4.2304\n",
      "Epoch 292/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5989 - val_loss: -3.9583\n",
      "Epoch 293/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5147 - val_loss: -3.9535\n",
      "Epoch 294/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7073 - val_loss: -3.5688\n",
      "Epoch 295/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7435 - val_loss: -4.2061\n",
      "Epoch 296/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7186 - val_loss: -2.6362\n",
      "Epoch 297/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7308 - val_loss: -4.0899\n",
      "Epoch 298/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -3.9107 - val_loss: -3.4034\n",
      "Epoch 299/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0929 - val_loss: -3.7866\n",
      "Epoch 300/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5229 - val_loss: -2.9024\n",
      "Epoch 301/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5913 - val_loss: -4.0872\n",
      "Epoch 302/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5546 - val_loss: -4.2274\n",
      "Epoch 303/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6258 - val_loss: -3.8395\n",
      "Epoch 304/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5617 - val_loss: -4.1491\n",
      "Epoch 305/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0760 - val_loss: -3.8571\n",
      "Epoch 306/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5230 - val_loss: -4.1161\n",
      "Epoch 307/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6369 - val_loss: -4.2576\n",
      "Epoch 308/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6887 - val_loss: -4.1399\n",
      "Epoch 309/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7251 - val_loss: -3.6917\n",
      "Epoch 310/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7518 - val_loss: -3.6917\n",
      "Epoch 311/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7073 - val_loss: -3.7511\n",
      "Epoch 312/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7764 - val_loss: -4.1553\n",
      "Epoch 313/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7605 - val_loss: -4.0326\n",
      "Epoch 314/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7930 - val_loss: -3.9681\n",
      "Epoch 315/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8182 - val_loss: -4.1936\n",
      "Epoch 316/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8433 - val_loss: -4.0944\n",
      "Epoch 317/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6002 - val_loss: -3.9211\n",
      "Epoch 318/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7755 - val_loss: -4.0293\n",
      "Epoch 319/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8563 - val_loss: -3.8483\n",
      "Epoch 320/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7101 - val_loss: -4.2092\n",
      "Epoch 321/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0975 - val_loss: -3.7660\n",
      "Epoch 322/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5020 - val_loss: -4.2326\n",
      "Epoch 323/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7696 - val_loss: -3.6401\n",
      "Epoch 324/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7002 - val_loss: -3.0622\n",
      "Epoch 325/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.6870 - val_loss: -4.1146\n",
      "Epoch 326/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8040 - val_loss: -4.1142\n",
      "Epoch 327/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7717 - val_loss: -3.8487\n",
      "Epoch 328/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7315 - val_loss: -4.1531\n",
      "Epoch 329/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8310 - val_loss: -4.0406\n",
      "Epoch 330/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8381 - val_loss: -4.4304\n",
      "Epoch 331/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8576 - val_loss: -4.1112\n",
      "Epoch 332/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8871 - val_loss: -4.2243\n",
      "Epoch 333/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7725 - val_loss: -3.6323\n",
      "Epoch 334/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8098 - val_loss: -4.2664\n",
      "Epoch 335/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9346 - val_loss: -4.0392\n",
      "Epoch 336/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7199 - val_loss: -4.1828\n",
      "Epoch 337/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8085 - val_loss: -4.1265\n",
      "Epoch 338/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8844 - val_loss: -3.7473\n",
      "Epoch 339/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8392 - val_loss: -3.9125\n",
      "Epoch 340/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.8275 - val_loss: -3.6958\n",
      "Epoch 341/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6723 - val_loss: -4.0643\n",
      "Epoch 342/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7705 - val_loss: -4.2438\n",
      "Epoch 343/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8082 - val_loss: -3.4423\n",
      "Epoch 344/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8303 - val_loss: -4.1803\n",
      "Epoch 345/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8539 - val_loss: -3.5625\n",
      "Epoch 346/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8705 - val_loss: -3.7686\n",
      "Epoch 347/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8585 - val_loss: -4.0826\n",
      "Epoch 348/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8572 - val_loss: -4.4620\n",
      "Epoch 349/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7976 - val_loss: -4.2210\n",
      "Epoch 350/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.1789 - val_loss: -3.7708\n",
      "Epoch 351/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -4.4436 - val_loss: -3.7322\n",
      "Epoch 352/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7417 - val_loss: -3.9765\n",
      "Epoch 353/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8301 - val_loss: -2.8167\n",
      "Epoch 354/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7845 - val_loss: -3.9586\n",
      "Epoch 355/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5862 - val_loss: 68.3013\n",
      "Epoch 356/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -2.7259 - val_loss: -2.7368\n",
      "Epoch 357/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.5642 - val_loss: -2.9667\n",
      "Epoch 358/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.2057 - val_loss: -3.5648\n",
      "Epoch 359/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4860 - val_loss: -3.6729\n",
      "Epoch 360/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5590 - val_loss: -3.8973\n",
      "Epoch 361/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.6586 - val_loss: -3.9865\n",
      "Epoch 362/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7202 - val_loss: -4.1722\n",
      "Epoch 363/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6810 - val_loss: -4.0033\n",
      "Epoch 364/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6851 - val_loss: -3.8174\n",
      "Epoch 365/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9102 - val_loss: -2.7313\n",
      "Epoch 366/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -3.7176 - val_loss: -3.7803\n",
      "Epoch 367/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3707 - val_loss: -3.8045\n",
      "Epoch 368/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5275 - val_loss: -4.0476\n",
      "Epoch 369/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3775 - val_loss: -3.6905\n",
      "Epoch 370/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5598 - val_loss: -4.0889\n",
      "Epoch 371/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6681 - val_loss: -2.4943\n",
      "Epoch 372/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5559 - val_loss: -2.9379\n",
      "Epoch 373/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6762 - val_loss: -4.0379\n",
      "Epoch 374/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6638 - val_loss: -3.0570\n",
      "Epoch 375/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7104 - val_loss: -3.6533\n",
      "Epoch 376/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7339 - val_loss: -4.2124\n",
      "Epoch 377/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7358 - val_loss: -4.0766\n",
      "Epoch 378/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8262 - val_loss: -3.7082\n",
      "Epoch 379/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7580 - val_loss: -3.9460\n",
      "Epoch 380/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7681 - val_loss: -3.6245\n",
      "Epoch 381/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8410 - val_loss: -4.1266\n",
      "Epoch 382/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7591 - val_loss: -3.9585\n",
      "Epoch 383/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8722 - val_loss: -3.4714\n",
      "Epoch 384/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6720 - val_loss: -4.2477\n",
      "Epoch 385/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7981 - val_loss: -4.0771\n",
      "Epoch 386/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8633 - val_loss: -1.8721\n",
      "Epoch 387/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5664 - val_loss: -4.1027\n",
      "Epoch 388/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8138 - val_loss: -4.2232\n",
      "Epoch 389/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8803 - val_loss: -4.1858\n",
      "Epoch 390/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7461 - val_loss: -3.8741\n",
      "Epoch 391/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8568 - val_loss: -4.2018\n",
      "Epoch 392/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8203 - val_loss: -4.0331\n",
      "Epoch 393/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8078 - val_loss: -3.7047\n",
      "Epoch 394/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8938 - val_loss: -4.1428\n",
      "Epoch 395/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8805 - val_loss: -3.5279\n",
      "Epoch 396/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8497 - val_loss: -4.3774\n",
      "Epoch 397/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9630 - val_loss: -4.1018\n",
      "Epoch 398/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.3207 - val_loss: -3.4812\n",
      "Epoch 399/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.5380 - val_loss: -3.7407\n",
      "Epoch 400/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7963 - val_loss: -4.0097\n",
      "Epoch 401/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8034 - val_loss: -3.8483\n",
      "Epoch 402/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8749 - val_loss: -4.0675\n",
      "Epoch 403/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5738 - val_loss: 6.4220\n",
      "Epoch 404/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.6254 - val_loss: -3.7262\n",
      "Epoch 405/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5099 - val_loss: -3.0189\n",
      "Epoch 406/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7094 - val_loss: -4.0646\n",
      "Epoch 407/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7296 - val_loss: -3.9575\n",
      "Epoch 408/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8373 - val_loss: -4.3546\n",
      "Epoch 409/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7643 - val_loss: -3.3270\n",
      "Epoch 410/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7989 - val_loss: -3.8823\n",
      "Epoch 411/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9133 - val_loss: -0.5977\n",
      "Epoch 412/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8084 - val_loss: -4.0029\n",
      "Epoch 413/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9021 - val_loss: -3.9288\n",
      "Epoch 414/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8787 - val_loss: -3.0313\n",
      "Epoch 415/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8789 - val_loss: -4.3457\n",
      "Epoch 416/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8483 - val_loss: -4.3359\n",
      "Epoch 417/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9006 - val_loss: -4.0687\n",
      "Epoch 418/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9723 - val_loss: -4.3606\n",
      "Epoch 419/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8919 - val_loss: -4.2130\n",
      "Epoch 420/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9598 - val_loss: -4.4367\n",
      "Epoch 421/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8879 - val_loss: -3.1463\n",
      "Epoch 422/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9359 - val_loss: -4.2602\n",
      "Epoch 423/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9460 - val_loss: -3.7903\n",
      "Epoch 424/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.3348 - val_loss: -3.6026\n",
      "Epoch 425/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3452 - val_loss: -3.9185\n",
      "Epoch 426/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7154 - val_loss: -3.9027\n",
      "Epoch 427/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8234 - val_loss: -3.0443\n",
      "Epoch 428/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8096 - val_loss: -4.0748\n",
      "Epoch 429/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8257 - val_loss: -4.0278\n",
      "Epoch 430/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8733 - val_loss: -4.0913\n",
      "Epoch 431/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8911 - val_loss: -4.1560\n",
      "Epoch 432/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8445 - val_loss: -4.1910\n",
      "Epoch 433/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9146 - val_loss: -3.9449\n",
      "Epoch 434/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8674 - val_loss: -3.9569\n",
      "Epoch 435/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9656 - val_loss: -3.9995\n",
      "Epoch 436/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8978 - val_loss: -4.1969\n",
      "Epoch 437/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9890 - val_loss: -3.9362\n",
      "Epoch 438/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5390 - val_loss: -4.0746\n",
      "Epoch 439/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9552 - val_loss: -3.4873\n",
      "Epoch 440/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8572 - val_loss: -4.2709\n",
      "Epoch 441/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8801 - val_loss: -3.7806\n",
      "Epoch 442/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9570 - val_loss: -4.4216\n",
      "Epoch 443/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9111 - val_loss: -3.9212\n",
      "Epoch 444/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9958 - val_loss: -1.8922\n",
      "Epoch 445/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8477 - val_loss: -4.1011\n",
      "Epoch 446/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9531 - val_loss: -4.3018\n",
      "Epoch 447/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9999 - val_loss: -2.3808\n",
      "Epoch 448/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7257 - val_loss: -4.0336\n",
      "Epoch 449/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9215 - val_loss: -3.8738\n",
      "Epoch 450/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0335 - val_loss: -4.0884\n",
      "Epoch 451/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9445 - val_loss: -4.1724\n",
      "Epoch 452/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0612 - val_loss: -4.4236\n",
      "Epoch 453/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9315 - val_loss: -4.0358\n",
      "Epoch 454/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9262 - val_loss: -3.2003\n",
      "Epoch 455/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9604 - val_loss: -4.1678\n",
      "Epoch 456/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0503 - val_loss: -4.2045\n",
      "Epoch 457/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9659 - val_loss: -3.9997\n",
      "Epoch 458/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0705 - val_loss: -2.7641\n",
      "Epoch 459/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0006 - val_loss: -4.0193\n",
      "Epoch 460/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0341 - val_loss: -3.7628\n",
      "Epoch 461/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9135 - val_loss: -4.1497\n",
      "Epoch 462/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.9657 - val_loss: -4.5090\n",
      "Epoch 463/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0555 - val_loss: -3.3120\n",
      "Epoch 464/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8765 - val_loss: -4.3343\n",
      "Epoch 465/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9053 - val_loss: -4.1563\n",
      "Epoch 466/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9130 - val_loss: -4.3197\n",
      "Epoch 467/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0454 - val_loss: -4.2669\n",
      "Epoch 468/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7943 - val_loss: -4.0588\n",
      "Epoch 469/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9613 - val_loss: -4.3638\n",
      "Epoch 470/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0159 - val_loss: -3.6956\n",
      "Epoch 471/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0782 - val_loss: -4.3571\n",
      "Epoch 472/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9065 - val_loss: -4.2030\n",
      "Epoch 473/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0528 - val_loss: -2.5971\n",
      "Epoch 474/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1036 - val_loss: -2.1378\n",
      "Epoch 475/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9766 - val_loss: -2.6881\n",
      "Epoch 476/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5278 - val_loss: -4.0198\n",
      "Epoch 477/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7917 - val_loss: -4.1349\n",
      "Epoch 478/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8045 - val_loss: -3.7735\n",
      "Epoch 479/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0766 - val_loss: -3.9700\n",
      "Epoch 480/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9060 - val_loss: -4.1696\n",
      "Epoch 481/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9828 - val_loss: -4.1651\n",
      "Epoch 482/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8299 - val_loss: -4.0121\n",
      "Epoch 483/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0115 - val_loss: -3.1248\n",
      "Epoch 484/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0186 - val_loss: -3.5943\n",
      "Epoch 485/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0021 - val_loss: -1.6850\n",
      "Epoch 486/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0582 - val_loss: -3.7118\n",
      "Epoch 487/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -2.2902 - val_loss: -2.5269\n",
      "Epoch 488/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.0112 - val_loss: -3.1097\n",
      "Epoch 489/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -3.3284 - val_loss: -2.8787\n",
      "Epoch 490/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.2037 - val_loss: -3.4913\n",
      "Epoch 491/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.6459 - val_loss: -3.8022\n",
      "Epoch 492/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.8476 - val_loss: -3.0784\n",
      "Epoch 493/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.9407 - val_loss: -3.9333\n",
      "Epoch 494/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3348 - val_loss: -4.1227\n",
      "Epoch 495/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.3669 - val_loss: -4.0110\n",
      "Epoch 496/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5124 - val_loss: -2.9002\n",
      "Epoch 497/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5167 - val_loss: -3.4895\n",
      "Epoch 498/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6654 - val_loss: -4.1036\n",
      "Epoch 499/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6423 - val_loss: -4.1293\n",
      "Epoch 500/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6592 - val_loss: -4.1931\n",
      "Epoch 501/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7353 - val_loss: -4.0781\n",
      "Epoch 502/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7263 - val_loss: -3.2413\n",
      "Epoch 503/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7478 - val_loss: -4.1937\n",
      "Epoch 504/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7767 - val_loss: -3.9473\n",
      "Epoch 505/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4685 - val_loss: -3.8239\n",
      "Epoch 506/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6318 - val_loss: -3.0171\n",
      "Epoch 507/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7347 - val_loss: -4.2621\n",
      "Epoch 508/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7368 - val_loss: -3.9968\n",
      "Epoch 509/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7676 - val_loss: -3.9282\n",
      "Epoch 510/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1443 - val_loss: -4.0240\n",
      "Epoch 511/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7335 - val_loss: -4.1246\n",
      "Epoch 512/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7937 - val_loss: -2.5120\n",
      "Epoch 513/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7008 - val_loss: -3.8931\n",
      "Epoch 514/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5986 - val_loss: -3.6290\n",
      "Epoch 515/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5645 - val_loss: -4.1664\n",
      "Epoch 516/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7516 - val_loss: -4.0301\n",
      "Epoch 517/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8410 - val_loss: -3.4630\n",
      "Epoch 518/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6734 - val_loss: -3.9163\n",
      "Epoch 519/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2042 - val_loss: -2.1277\n",
      "Epoch 520/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -3.6828 - val_loss: -3.5947\n",
      "Epoch 521/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2474 - val_loss: -3.9882\n",
      "Epoch 522/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4462 - val_loss: -3.9374\n",
      "Epoch 523/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5060 - val_loss: -4.1423\n",
      "Epoch 524/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2530 - val_loss: -3.6171\n",
      "Epoch 525/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3362 - val_loss: -4.1366\n",
      "Epoch 526/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5287 - val_loss: -2.7298\n",
      "Epoch 527/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3643 - val_loss: -4.0692\n",
      "Epoch 528/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6881 - val_loss: -3.6220\n",
      "Epoch 529/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8049 - val_loss: -4.0277\n",
      "Epoch 530/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7642 - val_loss: -3.3842\n",
      "Epoch 531/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7927 - val_loss: -4.2016\n",
      "Epoch 532/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8131 - val_loss: -3.8419\n",
      "Epoch 533/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8477 - val_loss: -3.9353\n",
      "Epoch 534/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7080 - val_loss: -4.2438\n",
      "Epoch 535/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8026 - val_loss: -3.8181\n",
      "Epoch 536/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6839 - val_loss: -4.0487\n",
      "Epoch 537/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7618 - val_loss: -3.7758\n",
      "Epoch 538/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8572 - val_loss: -4.0692\n",
      "Epoch 539/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8157 - val_loss: -4.2833\n",
      "Epoch 540/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8889 - val_loss: -4.3363\n",
      "Epoch 541/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8252 - val_loss: -4.3375\n",
      "Epoch 542/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9281 - val_loss: -4.4175\n",
      "Epoch 543/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7716 - val_loss: -4.3981\n",
      "Epoch 544/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8946 - val_loss: -4.3174\n",
      "Epoch 545/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8874 - val_loss: -4.2735\n",
      "Epoch 546/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9234 - val_loss: -2.7663\n",
      "Epoch 547/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9110 - val_loss: -3.3831\n",
      "Epoch 548/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8623 - val_loss: -4.2778\n",
      "Epoch 549/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9131 - val_loss: -4.3068\n",
      "Epoch 550/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9898 - val_loss: -4.5055\n",
      "Epoch 551/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8683 - val_loss: -2.6325\n",
      "Epoch 552/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9539 - val_loss: -4.4635\n",
      "Epoch 553/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6911 - val_loss: -3.9432\n",
      "Epoch 554/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8814 - val_loss: -4.3330\n",
      "Epoch 555/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9776 - val_loss: -2.9812\n",
      "Epoch 556/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -2.4411 - val_loss: -2.4468\n",
      "Epoch 557/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.5493 - val_loss: -3.3129\n",
      "Epoch 558/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.8504 - val_loss: -3.5637\n",
      "Epoch 559/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.9942 - val_loss: -3.7038\n",
      "Epoch 560/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0752 - val_loss: -3.7463\n",
      "Epoch 561/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.2549 - val_loss: -3.1514\n",
      "Epoch 562/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2070 - val_loss: -3.7801\n",
      "Epoch 563/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3437 - val_loss: -3.7795\n",
      "Epoch 564/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.2433 - val_loss: -3.3219\n",
      "Epoch 565/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4282 - val_loss: -4.1009\n",
      "Epoch 566/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3069 - val_loss: -3.7981\n",
      "Epoch 567/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4571 - val_loss: -4.1590\n",
      "Epoch 568/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5305 - val_loss: -4.2073\n",
      "Epoch 569/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4108 - val_loss: -4.0743\n",
      "Epoch 570/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2594 - val_loss: -4.1428\n",
      "Epoch 571/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5423 - val_loss: -4.1970\n",
      "Epoch 572/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5847 - val_loss: -3.7244\n",
      "Epoch 573/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5780 - val_loss: -4.1148\n",
      "Epoch 574/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6254 - val_loss: -4.2365\n",
      "Epoch 575/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6694 - val_loss: -4.0034\n",
      "Epoch 576/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6782 - val_loss: -4.0468\n",
      "Epoch 577/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7356 - val_loss: -3.2977\n",
      "Epoch 578/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7356 - val_loss: -4.1178\n",
      "Epoch 579/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7908 - val_loss: -4.0646\n",
      "Epoch 580/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6846 - val_loss: -4.0639\n",
      "Epoch 581/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8375 - val_loss: -4.3557\n",
      "Epoch 582/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7397 - val_loss: -3.7650\n",
      "Epoch 583/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.6947 - val_loss: -4.2011\n",
      "Epoch 584/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8658 - val_loss: -3.9242\n",
      "Epoch 585/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7650 - val_loss: -3.8106\n",
      "Epoch 586/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8618 - val_loss: -2.1633\n",
      "Epoch 587/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5249 - val_loss: -3.3031\n",
      "Epoch 588/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8502 - val_loss: -4.2002\n",
      "Epoch 589/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7820 - val_loss: -3.9744\n",
      "Epoch 590/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8057 - val_loss: -4.0797\n",
      "Epoch 591/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8635 - val_loss: -3.9983\n",
      "Epoch 592/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8709 - val_loss: -1.4652\n",
      "Epoch 593/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4499 - val_loss: -4.1392\n",
      "Epoch 594/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7923 - val_loss: -1.8278\n",
      "Epoch 595/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8530 - val_loss: -4.0251\n",
      "Epoch 596/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7453 - val_loss: -4.2796\n",
      "Epoch 597/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8103 - val_loss: -3.6081\n",
      "Epoch 598/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7538 - val_loss: -3.8843\n",
      "Epoch 599/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7152 - val_loss: -3.3485\n",
      "Epoch 600/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8840 - val_loss: -4.2918\n",
      "Epoch 601/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8431 - val_loss: -3.2921\n",
      "Epoch 602/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9162 - val_loss: -4.0387\n",
      "Epoch 603/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8741 - val_loss: -3.7924\n",
      "Epoch 604/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7342 - val_loss: -4.4297\n",
      "Epoch 605/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9047 - val_loss: -4.4515\n",
      "Epoch 606/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6881 - val_loss: -4.0425\n",
      "Epoch 607/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8383 - val_loss: -4.2902\n",
      "Epoch 608/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9167 - val_loss: -4.3425\n",
      "Epoch 609/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9065 - val_loss: -2.8312\n",
      "Epoch 610/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6633 - val_loss: -3.7886\n",
      "Epoch 611/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8881 - val_loss: -1.8474\n",
      "Epoch 612/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9409 - val_loss: -4.3063\n",
      "Epoch 613/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8943 - val_loss: -4.1665\n",
      "Epoch 614/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9377 - val_loss: -4.2605\n",
      "Epoch 615/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9193 - val_loss: -3.6730\n",
      "Epoch 616/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8684 - val_loss: -4.2861\n",
      "Epoch 617/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9887 - val_loss: -4.1977\n",
      "Epoch 618/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9426 - val_loss: -4.3989\n",
      "Epoch 619/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9350 - val_loss: -4.3607\n",
      "Epoch 620/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9569 - val_loss: -4.3985\n",
      "Epoch 621/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9523 - val_loss: -1.0417\n",
      "Epoch 622/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9314 - val_loss: -4.2943\n",
      "Epoch 623/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9012 - val_loss: -4.3417\n",
      "Epoch 624/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7620 - val_loss: -4.0477\n",
      "Epoch 625/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9283 - val_loss: -4.2313\n",
      "Epoch 626/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9031 - val_loss: -4.1376\n",
      "Epoch 627/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9248 - val_loss: -4.1089\n",
      "Epoch 628/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9723 - val_loss: -1.2548\n",
      "Epoch 629/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8010 - val_loss: -4.1834\n",
      "Epoch 630/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9550 - val_loss: -3.6019\n",
      "Epoch 631/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8947 - val_loss: -4.1033\n",
      "Epoch 632/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8908 - val_loss: -3.8148\n",
      "Epoch 633/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9095 - val_loss: -3.5990\n",
      "Epoch 634/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.3498 - val_loss: -3.9507\n",
      "Epoch 635/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6060 - val_loss: -3.1781\n",
      "Epoch 636/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7688 - val_loss: -4.3139\n",
      "Epoch 637/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8986 - val_loss: -3.3050\n",
      "Epoch 638/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9223 - val_loss: -2.5118\n",
      "Epoch 639/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8896 - val_loss: -4.0857\n",
      "Epoch 640/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9608 - val_loss: -3.7424\n",
      "Epoch 641/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9033 - val_loss: -4.2040\n",
      "Epoch 642/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9608 - val_loss: -4.2846\n",
      "Epoch 643/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8554 - val_loss: -4.0574\n",
      "Epoch 644/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9533"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -4.9533 - val_loss: -4.5249\n",
      "Epoch 645/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9852 - val_loss: -3.6881\n",
      "Epoch 646/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9587 - val_loss: -3.3897\n",
      "Epoch 647/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9087 - val_loss: -3.9986\n",
      "Epoch 648/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9892 - val_loss: -3.3602\n",
      "Epoch 649/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0028 - val_loss: -2.7998\n",
      "Epoch 650/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8504 - val_loss: -4.2548\n",
      "Epoch 651/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9307 - val_loss: -3.6455\n",
      "Epoch 652/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9919 - val_loss: -4.2028\n",
      "Epoch 653/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9534 - val_loss: -4.3509\n",
      "Epoch 654/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8998 - val_loss: -3.6716\n",
      "Epoch 655/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7562 - val_loss: -4.2557\n",
      "Epoch 656/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8866 - val_loss: -4.2077\n",
      "Epoch 657/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0559 - val_loss: -4.3200\n",
      "Epoch 658/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9061 - val_loss: -4.3881\n",
      "Epoch 659/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9820 - val_loss: -3.7434\n",
      "Epoch 660/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0232 - val_loss: -3.6327\n",
      "Epoch 661/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8349 - val_loss: -3.9783\n",
      "Epoch 662/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9057 - val_loss: -3.6159\n",
      "Epoch 663/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0120 - val_loss: -4.3206\n",
      "Epoch 664/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9799 - val_loss: -4.3255\n",
      "Epoch 665/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9351 - val_loss: -4.1439\n",
      "Epoch 666/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8523 - val_loss: -4.3931\n",
      "Epoch 667/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0308 - val_loss: -3.7356\n",
      "Epoch 668/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0523 - val_loss: -3.9896\n",
      "Epoch 669/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0125 - val_loss: -4.2405\n",
      "Epoch 670/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8698 - val_loss: -4.1665\n",
      "Epoch 671/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9929 - val_loss: -4.1586\n",
      "Epoch 672/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0833 - val_loss: -4.1515\n",
      "Epoch 673/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9926 - val_loss: -4.3800\n",
      "Epoch 674/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0057 - val_loss: -3.9101\n",
      "Epoch 675/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0161 - val_loss: -2.5124\n",
      "Epoch 676/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0413 - val_loss: -1.3156\n",
      "Epoch 677/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7591 - val_loss: -3.5825\n",
      "Epoch 678/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9989 - val_loss: -3.8942\n",
      "Epoch 679/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8072 - val_loss: -4.1972\n",
      "Epoch 680/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9735 - val_loss: -3.8191\n",
      "Epoch 681/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0281 - val_loss: -3.9747\n",
      "Epoch 682/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9344 - val_loss: -4.2548\n",
      "Epoch 683/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9864 - val_loss: -3.7508\n",
      "Epoch 684/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0395 - val_loss: -3.8355\n",
      "Epoch 685/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0421 - val_loss: 1.0027\n",
      "Epoch 686/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9812 - val_loss: -3.7501\n",
      "Epoch 687/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0047 - val_loss: -3.4140\n",
      "Epoch 688/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0730 - val_loss: -3.5647\n",
      "Epoch 689/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0256 - val_loss: -4.2154\n",
      "Epoch 690/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0076 - val_loss: -4.2068\n",
      "Epoch 691/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0955 - val_loss: -4.3460\n",
      "Epoch 692/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0407 - val_loss: -4.3501\n",
      "Epoch 693/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0442"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -5.0442 - val_loss: -4.5266\n",
      "Epoch 694/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9953 - val_loss: -3.4842\n",
      "Epoch 695/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0728 - val_loss: -1.6794\n",
      "Epoch 696/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -3.4510 - val_loss: -3.9338\n",
      "Epoch 697/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3698 - val_loss: -2.6558\n",
      "Epoch 698/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4444 - val_loss: -4.0912\n",
      "Epoch 699/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.0126 - val_loss: -4.4706\n",
      "Epoch 700/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3774 - val_loss: -4.2709\n",
      "Epoch 701/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5732 - val_loss: -1.4581\n",
      "Epoch 702/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4015"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 242ms/step - loss: -4.4015 - val_loss: -4.5714\n",
      "Epoch 703/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6818 - val_loss: -4.2239\n",
      "Epoch 704/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5435 - val_loss: -3.8287\n",
      "Epoch 705/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7465 - val_loss: -3.7302\n",
      "Epoch 706/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6965 - val_loss: -4.2658\n",
      "Epoch 707/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7452 - val_loss: -4.3630\n",
      "Epoch 708/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8104"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -4.8104 - val_loss: -4.6367\n",
      "Epoch 709/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6870 - val_loss: -3.4061\n",
      "Epoch 710/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7221 - val_loss: -3.6287\n",
      "Epoch 711/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8106 - val_loss: -4.2808\n",
      "Epoch 712/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7931 - val_loss: -2.6278\n",
      "Epoch 713/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7463 - val_loss: -3.9367\n",
      "Epoch 714/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7648 - val_loss: -4.3487\n",
      "Epoch 715/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8674 - val_loss: -3.9008\n",
      "Epoch 716/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7086 - val_loss: -4.4341\n",
      "Epoch 717/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8621 - val_loss: -4.2707\n",
      "Epoch 718/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8596 - val_loss: -1.5171\n",
      "Epoch 719/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6880 - val_loss: -3.6535\n",
      "Epoch 720/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8535 - val_loss: -4.0567\n",
      "Epoch 721/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8975 - val_loss: -4.0113\n",
      "Epoch 722/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8338 - val_loss: -3.5974\n",
      "Epoch 723/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8773 - val_loss: 0.3508\n",
      "Epoch 724/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8202 - val_loss: -4.3066\n",
      "Epoch 725/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8986 - val_loss: -4.2502\n",
      "Epoch 726/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9153 - val_loss: -4.1549\n",
      "Epoch 727/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6026 - val_loss: 8.9597\n",
      "Epoch 728/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -3.7888 - val_loss: -4.0591\n",
      "Epoch 729/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6083 - val_loss: -3.0405\n",
      "Epoch 730/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6959 - val_loss: -3.7067\n",
      "Epoch 731/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7491 - val_loss: -4.0691\n",
      "Epoch 732/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7613 - val_loss: -4.1384\n",
      "Epoch 733/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6940 - val_loss: -4.2852\n",
      "Epoch 734/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8381 - val_loss: -3.4352\n",
      "Epoch 735/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8685 - val_loss: -3.6371\n",
      "Epoch 736/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8032 - val_loss: -4.0086\n",
      "Epoch 737/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5826 - val_loss: -4.5063\n",
      "Epoch 738/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8633 - val_loss: -4.1005\n",
      "Epoch 739/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8363 - val_loss: -4.2185\n",
      "Epoch 740/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8853 - val_loss: -4.1410\n",
      "Epoch 741/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8532 - val_loss: -4.2585\n",
      "Epoch 742/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -3.7095 - val_loss: -1.9722\n",
      "Epoch 743/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.3628 - val_loss: -3.3295\n",
      "Epoch 744/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2088 - val_loss: -3.7066\n",
      "Epoch 745/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4347 - val_loss: -3.8821\n",
      "Epoch 746/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5177 - val_loss: -3.9519\n",
      "Epoch 747/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5641 - val_loss: -3.9691\n",
      "Epoch 748/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6033 - val_loss: -4.0221\n",
      "Epoch 749/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6741 - val_loss: -3.6456\n",
      "Epoch 750/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6661 - val_loss: -4.2367\n",
      "Epoch 751/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6126 - val_loss: -4.1021\n",
      "Epoch 752/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6938 - val_loss: -3.8344\n",
      "Epoch 753/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7201 - val_loss: -4.0228\n",
      "Epoch 754/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7123 - val_loss: -4.1824\n",
      "Epoch 755/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7691 - val_loss: -3.2406\n",
      "Epoch 756/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8189 - val_loss: -3.8709\n",
      "Epoch 757/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4418 - val_loss: -4.3209\n",
      "Epoch 758/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7722 - val_loss: -3.8384\n",
      "Epoch 759/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7790 - val_loss: -4.1837\n",
      "Epoch 760/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7648 - val_loss: -3.6765\n",
      "Epoch 761/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8398 - val_loss: -3.8233\n",
      "Epoch 762/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7572 - val_loss: -4.0677\n",
      "Epoch 763/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8238 - val_loss: -3.5322\n",
      "Epoch 764/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8162 - val_loss: -4.1161\n",
      "Epoch 765/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8167 - val_loss: -4.1873\n",
      "Epoch 766/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9313 - val_loss: -3.8958\n",
      "Epoch 767/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7080 - val_loss: -4.2855\n",
      "Epoch 768/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9382 - val_loss: -3.4168\n",
      "Epoch 769/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8767 - val_loss: -4.4208\n",
      "Epoch 770/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8666 - val_loss: -4.1143\n",
      "Epoch 771/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8410 - val_loss: -4.1944\n",
      "Epoch 772/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8805 - val_loss: -4.2863\n",
      "Epoch 773/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9490 - val_loss: -4.3122\n",
      "Epoch 774/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5751 - val_loss: -2.7481\n",
      "Epoch 775/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4430 - val_loss: -3.9123\n",
      "Epoch 776/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7224 - val_loss: -4.0367\n",
      "Epoch 777/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7974 - val_loss: -4.1141\n",
      "Epoch 778/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8955 - val_loss: -4.3934\n",
      "Epoch 779/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7973 - val_loss: -4.4747\n",
      "Epoch 780/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9266 - val_loss: -4.1511\n",
      "Epoch 781/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8862 - val_loss: -3.7545\n",
      "Epoch 782/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8765 - val_loss: -3.8532\n",
      "Epoch 783/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9510 - val_loss: -3.8612\n",
      "Epoch 784/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8710 - val_loss: -4.1740\n",
      "Epoch 785/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9218 - val_loss: -4.4088\n",
      "Epoch 786/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9468 - val_loss: -3.5784\n",
      "Epoch 787/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8186 - val_loss: -2.7821\n",
      "Epoch 788/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8932 - val_loss: -4.1142\n",
      "Epoch 789/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8011 - val_loss: -3.3285\n",
      "Epoch 790/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9254 - val_loss: -1.9853\n",
      "Epoch 791/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9525 - val_loss: -3.9901\n",
      "Epoch 792/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9473 - val_loss: -4.1544\n",
      "Epoch 793/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9827 - val_loss: -3.8260\n",
      "Epoch 794/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0146 - val_loss: -4.1631\n",
      "Epoch 795/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8964 - val_loss: -2.0457\n",
      "Epoch 796/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9782 - val_loss: -3.9659\n",
      "Epoch 797/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9547 - val_loss: -3.0743\n",
      "Epoch 798/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0027 - val_loss: -3.0841\n",
      "Epoch 799/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9603 - val_loss: -4.0683\n",
      "Epoch 800/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9737 - val_loss: -3.9898\n",
      "Epoch 801/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5354 - val_loss: -3.2319\n",
      "Epoch 802/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2784 - val_loss: -3.8455\n",
      "Epoch 803/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6486 - val_loss: -4.0921\n",
      "Epoch 804/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7829 - val_loss: -4.1900\n",
      "Epoch 805/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8207 - val_loss: -4.1163\n",
      "Epoch 806/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9445 - val_loss: -4.3127\n",
      "Epoch 807/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8780 - val_loss: -3.7768\n",
      "Epoch 808/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9683 - val_loss: -4.3812\n",
      "Epoch 809/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8295 - val_loss: -4.0460\n",
      "Epoch 810/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9797 - val_loss: -2.1285\n",
      "Epoch 811/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9595 - val_loss: -2.4297\n",
      "Epoch 812/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8672 - val_loss: -3.5729\n",
      "Epoch 813/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8833 - val_loss: -4.4216\n",
      "Epoch 814/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9230 - val_loss: -3.9996\n",
      "Epoch 815/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8994 - val_loss: -2.2329\n",
      "Epoch 816/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8915 - val_loss: -3.8260\n",
      "Epoch 817/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9978 - val_loss: -4.3607\n",
      "Epoch 818/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9918 - val_loss: -4.3657\n",
      "Epoch 819/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9545 - val_loss: -2.6964\n",
      "Epoch 820/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9894 - val_loss: -4.1607\n",
      "Epoch 821/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9104 - val_loss: -4.0784\n",
      "Epoch 822/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0149 - val_loss: -4.3727\n",
      "Epoch 823/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0510 - val_loss: -3.7700\n",
      "Epoch 824/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9868 - val_loss: -3.7462\n",
      "Epoch 825/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7730 - val_loss: -4.2404\n",
      "Epoch 826/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0424 - val_loss: -4.0839\n",
      "Epoch 827/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9912 - val_loss: -4.4408\n",
      "Epoch 828/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9716 - val_loss: -3.8371\n",
      "Epoch 829/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0274 - val_loss: -4.0784\n",
      "Epoch 830/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9950 - val_loss: -4.2217\n",
      "Epoch 831/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0674 - val_loss: -3.8391\n",
      "Epoch 832/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0078 - val_loss: -4.2187\n",
      "Epoch 833/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1245 - val_loss: -2.6725\n",
      "Epoch 834/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8655 - val_loss: -4.5496\n",
      "Epoch 835/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0086 - val_loss: -4.3395\n",
      "Epoch 836/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0693 - val_loss: -3.0374\n",
      "Epoch 837/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0680 - val_loss: -2.7228\n",
      "Epoch 838/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8533 - val_loss: -3.7675\n",
      "Epoch 839/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0609 - val_loss: -4.3695\n",
      "Epoch 840/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -4.9618 - val_loss: -4.1948\n",
      "Epoch 841/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1677 - val_loss: -4.3404\n",
      "Epoch 842/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0501 - val_loss: -4.1648\n",
      "Epoch 843/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0612 - val_loss: -3.5567\n",
      "Epoch 844/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9902 - val_loss: -3.2175\n",
      "Epoch 845/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9866 - val_loss: -4.0487\n",
      "Epoch 846/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1003 - val_loss: -4.1708\n",
      "Epoch 847/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0689 - val_loss: -4.3234\n",
      "Epoch 848/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1616 - val_loss: -3.8986\n",
      "Epoch 849/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0956 - val_loss: -3.9946\n",
      "Epoch 850/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9675 - val_loss: -3.9188\n",
      "Epoch 851/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0577 - val_loss: -4.1293\n",
      "Epoch 852/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1413 - val_loss: -3.7142\n",
      "Epoch 853/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -3.7541 - val_loss: -3.4058\n",
      "Epoch 854/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3479 - val_loss: -4.0426\n",
      "Epoch 855/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6758 - val_loss: -3.7759\n",
      "Epoch 856/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8456 - val_loss: -4.3403\n",
      "Epoch 857/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7433 - val_loss: -3.7084\n",
      "Epoch 858/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9701 - val_loss: -4.3135\n",
      "Epoch 859/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9612 - val_loss: -3.6038\n",
      "Epoch 860/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9385 - val_loss: -4.4212\n",
      "Epoch 861/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9363 - val_loss: -3.9664\n",
      "Epoch 862/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9773 - val_loss: -3.5212\n",
      "Epoch 863/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0347 - val_loss: -2.8025\n",
      "Epoch 864/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0369 - val_loss: -2.6803\n",
      "Epoch 865/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9893 - val_loss: -3.6738\n",
      "Epoch 866/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9705 - val_loss: -2.8301\n",
      "Epoch 867/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0068 - val_loss: -3.8413\n",
      "Epoch 868/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0677 - val_loss: -4.0431\n",
      "Epoch 869/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0288 - val_loss: -4.5731\n",
      "Epoch 870/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9775 - val_loss: -4.3528\n",
      "Epoch 871/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0317 - val_loss: -3.9252\n",
      "Epoch 872/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0064 - val_loss: -2.7744\n",
      "Epoch 873/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9996 - val_loss: -4.5432\n",
      "Epoch 874/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9376 - val_loss: -3.8140\n",
      "Epoch 875/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -4.9766 - val_loss: -4.6856\n",
      "Epoch 876/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0599 - val_loss: -4.1982\n",
      "Epoch 877/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0161 - val_loss: -4.2478\n",
      "Epoch 878/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1145 - val_loss: -4.4339\n",
      "Epoch 879/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8911 - val_loss: -4.0713\n",
      "Epoch 880/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0753 - val_loss: -3.4427\n",
      "Epoch 881/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0664 - val_loss: -4.0333\n",
      "Epoch 882/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0990 - val_loss: -4.5588\n",
      "Epoch 883/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1391 - val_loss: -4.4297\n",
      "Epoch 884/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0489 - val_loss: -3.3675\n",
      "Epoch 885/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1068 - val_loss: -3.7086\n",
      "Epoch 886/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9992 - val_loss: -4.0342\n",
      "Epoch 887/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1502 - val_loss: -4.1104\n",
      "Epoch 888/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1063 - val_loss: -2.2378\n",
      "Epoch 889/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1314 - val_loss: -3.5482\n",
      "Epoch 890/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1173 - val_loss: -4.2478\n",
      "Epoch 891/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5149 - val_loss: -3.4727\n",
      "Epoch 892/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6733 - val_loss: -4.1788\n",
      "Epoch 893/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7271 - val_loss: -2.5551\n",
      "Epoch 894/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7676 - val_loss: -4.5867\n",
      "Epoch 895/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5589 - val_loss: -3.9894\n",
      "Epoch 896/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.2029 - val_loss: -4.4620\n",
      "Epoch 897/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0665 - val_loss: -4.4296\n",
      "Epoch 898/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.3999 - val_loss: -4.4847\n",
      "Epoch 899/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5155 - val_loss: -4.1658\n",
      "Epoch 900/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5992 - val_loss: -3.0498\n",
      "Epoch 901/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5829 - val_loss: -4.1329\n",
      "Epoch 902/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6938 - val_loss: -3.1791\n",
      "Epoch 903/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6767 - val_loss: -4.3509\n",
      "Epoch 904/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7676 - val_loss: -3.8167\n",
      "Epoch 905/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4226 - val_loss: -3.9063\n",
      "Epoch 906/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7320 - val_loss: -4.2689\n",
      "Epoch 907/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8580 - val_loss: -3.1804\n",
      "Epoch 908/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7983 - val_loss: -4.2832\n",
      "Epoch 909/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7061 - val_loss: -2.4475\n",
      "Epoch 910/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7837 - val_loss: -3.8909\n",
      "Epoch 911/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6850 - val_loss: -3.7607\n",
      "Epoch 912/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.0442 - val_loss: -3.9255\n",
      "Epoch 913/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6567 - val_loss: -4.0050\n",
      "Epoch 914/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7079 - val_loss: -4.1155\n",
      "Epoch 915/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9013 - val_loss: -4.1220\n",
      "Epoch 916/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7735 - val_loss: -4.2525\n",
      "Epoch 917/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9523 - val_loss: -4.0215\n",
      "Epoch 918/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8422 - val_loss: -3.1847\n",
      "Epoch 919/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7019 - val_loss: -1.9693\n",
      "Epoch 920/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8608 - val_loss: -3.8224\n",
      "Epoch 921/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8649 - val_loss: -4.2370\n",
      "Epoch 922/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8589 - val_loss: -3.6678\n",
      "Epoch 923/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9093 - val_loss: -3.7839\n",
      "Epoch 924/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9476 - val_loss: -4.5446\n",
      "Epoch 925/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9221 - val_loss: -4.3703\n",
      "Epoch 926/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8283 - val_loss: -3.7310\n",
      "Epoch 927/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7924 - val_loss: -3.8617\n",
      "Epoch 928/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9713 - val_loss: -3.7652\n",
      "Epoch 929/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9675 - val_loss: -4.1012\n",
      "Epoch 930/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0507 - val_loss: -3.7460\n",
      "Epoch 931/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0333 - val_loss: -4.6347\n",
      "Epoch 932/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0254 - val_loss: -4.6770\n",
      "Epoch 933/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8457 - val_loss: -3.6070\n",
      "Epoch 934/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0641 - val_loss: -4.3016\n",
      "Epoch 935/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0760 - val_loss: -4.4845\n",
      "Epoch 936/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0867 - val_loss: -3.6475\n",
      "Epoch 937/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9371 - val_loss: -4.4214\n",
      "Epoch 938/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1073 - val_loss: -4.0728\n",
      "Epoch 939/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0781 - val_loss: -4.1391\n",
      "Epoch 940/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0196 - val_loss: -4.4406\n",
      "Epoch 941/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1487 - val_loss: -4.0330\n",
      "Epoch 942/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0546 - val_loss: -4.4229\n",
      "Epoch 943/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9923 - val_loss: -4.4880\n",
      "Epoch 944/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0534 - val_loss: -3.2914\n",
      "Epoch 945/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0386"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -5.0386 - val_loss: -4.7222\n",
      "Epoch 946/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1383 - val_loss: -2.0466\n",
      "Epoch 947/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0983 - val_loss: -3.3796\n",
      "Epoch 948/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0797 - val_loss: -4.3012\n",
      "Epoch 949/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9590 - val_loss: -4.4508\n",
      "Epoch 950/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0712 - val_loss: -4.2461\n",
      "Epoch 951/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9329 - val_loss: -4.4908\n",
      "Epoch 952/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1301 - val_loss: -1.3124\n",
      "Epoch 953/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0113 - val_loss: -2.0650\n",
      "Epoch 954/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0237 - val_loss: -4.0170\n",
      "Epoch 955/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1312 - val_loss: -3.5153\n",
      "Epoch 956/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0467 - val_loss: -3.7119\n",
      "Epoch 957/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0195 - val_loss: -3.8205\n",
      "Epoch 958/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1136 - val_loss: -4.0007\n",
      "Epoch 959/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6632 - val_loss: -3.8521\n",
      "Epoch 960/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2789 - val_loss: -4.4113\n",
      "Epoch 961/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8206 - val_loss: -2.6329\n",
      "Epoch 962/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8984 - val_loss: -4.3413\n",
      "Epoch 963/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0234 - val_loss: -4.1509\n",
      "Epoch 964/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0336 - val_loss: -3.2327\n",
      "Epoch 965/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0209 - val_loss: -4.3720\n",
      "Epoch 966/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3521 - val_loss: -4.1838\n",
      "Epoch 967/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7061 - val_loss: -3.6531\n",
      "Epoch 968/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9436 - val_loss: -3.9379\n",
      "Epoch 969/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0428 - val_loss: -4.4715\n",
      "Epoch 970/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9587 - val_loss: -3.9698\n",
      "Epoch 971/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0845 - val_loss: -4.2101\n",
      "Epoch 972/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1154 - val_loss: -3.0219\n",
      "Epoch 973/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1439 - val_loss: -4.3196\n",
      "Epoch 974/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1642 - val_loss: -3.2307\n",
      "Epoch 975/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0204 - val_loss: -4.1213\n",
      "Epoch 976/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8148 - val_loss: -4.0588\n",
      "Epoch 977/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0509 - val_loss: -2.9353\n",
      "Epoch 978/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1874 - val_loss: -3.6762\n",
      "Epoch 979/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0850 - val_loss: -3.9947\n",
      "Epoch 980/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1218 - val_loss: -3.4487\n",
      "Epoch 981/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0378 - val_loss: -4.3153\n",
      "Epoch 982/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1775 - val_loss: -3.7409\n",
      "Epoch 983/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1288 - val_loss: -4.2321\n",
      "Epoch 984/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1019 - val_loss: -4.5993\n",
      "Epoch 985/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0892 - val_loss: -4.4772\n",
      "Epoch 986/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1868 - val_loss: -4.0438\n",
      "Epoch 987/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1887 - val_loss: -4.6353\n",
      "Epoch 988/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1933 - val_loss: -3.4239\n",
      "Epoch 989/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1815 - val_loss: -3.6413\n",
      "Epoch 990/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5555 - val_loss: -3.8118\n",
      "Epoch 991/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9637 - val_loss: -4.1827\n",
      "Epoch 992/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9666 - val_loss: -3.1812\n",
      "Epoch 993/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9962 - val_loss: -4.2733\n",
      "Epoch 994/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1461 - val_loss: -4.2976\n",
      "Epoch 995/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0404 - val_loss: -3.9585\n",
      "Epoch 996/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1891 - val_loss: -3.9891\n",
      "Epoch 997/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1310 - val_loss: -4.1580\n",
      "Epoch 998/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2111 - val_loss: -4.2855\n",
      "Epoch 999/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0812 - val_loss: -1.8601\n",
      "Epoch 1000/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0380 - val_loss: -3.9005\n",
      "Epoch 1001/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2388 - val_loss: -4.7037\n",
      "Epoch 1002/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1935 - val_loss: -4.6925\n",
      "Epoch 1003/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0780 - val_loss: -2.4750\n",
      "Epoch 1004/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1313 - val_loss: -3.7302\n",
      "Epoch 1005/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1666 - val_loss: -4.2613\n",
      "Epoch 1006/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1838 - val_loss: -4.5905\n",
      "Epoch 1007/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0070 - val_loss: -4.1445\n",
      "Epoch 1008/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1949 - val_loss: -1.1799\n",
      "Epoch 1009/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1065 - val_loss: -4.2598\n",
      "Epoch 1010/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1891 - val_loss: -4.4346\n",
      "Epoch 1011/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1974 - val_loss: -3.0294\n",
      "Epoch 1012/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9886 - val_loss: -4.3468\n",
      "Epoch 1013/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1630 - val_loss: -4.6211\n",
      "Epoch 1014/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1202 - val_loss: -3.4369\n",
      "Epoch 1015/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1046 - val_loss: -3.8759\n",
      "Epoch 1016/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2395 - val_loss: -3.9415\n",
      "Epoch 1017/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0432 - val_loss: -4.0206\n",
      "Epoch 1018/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1623 - val_loss: -3.8677\n",
      "Epoch 1019/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1249 - val_loss: -4.4542\n",
      "Epoch 1020/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2429 - val_loss: -3.2252\n",
      "Epoch 1021/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1739 - val_loss: -4.5555\n",
      "Epoch 1022/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2446 - val_loss: -3.9232\n",
      "Epoch 1023/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0046 - val_loss: -4.5844\n",
      "Epoch 1024/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2634 - val_loss: -4.3084\n",
      "Epoch 1025/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3149 - val_loss: -3.8126\n",
      "Epoch 1026/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9232 - val_loss: -1.2709\n",
      "Epoch 1027/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1947 - val_loss: -4.5099\n",
      "Epoch 1028/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2444 - val_loss: -4.1911\n",
      "Epoch 1029/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2814 - val_loss: -3.7810\n",
      "Epoch 1030/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1578 - val_loss: -4.3879\n",
      "Epoch 1031/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2706 - val_loss: -4.4248\n",
      "Epoch 1032/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9690 - val_loss: -4.3356\n",
      "Epoch 1033/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2095 - val_loss: -4.5148\n",
      "Epoch 1034/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1772 - val_loss: -4.5104\n",
      "Epoch 1035/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2545 - val_loss: -3.6648\n",
      "Epoch 1036/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0950 - val_loss: -4.2144\n",
      "Epoch 1037/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2722 - val_loss: -4.5770\n",
      "Epoch 1038/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2401 - val_loss: -4.0900\n",
      "Epoch 1039/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2776 - val_loss: -4.4597\n",
      "Epoch 1040/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2052 - val_loss: -4.1808\n",
      "Epoch 1041/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0632 - val_loss: -3.9956\n",
      "Epoch 1042/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1866 - val_loss: -4.0702\n",
      "Epoch 1043/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2268 - val_loss: -4.0427\n",
      "Epoch 1044/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2229 - val_loss: -4.3264\n",
      "Epoch 1045/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8631 - val_loss: -4.2738\n",
      "Epoch 1046/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2503 - val_loss: -4.4348\n",
      "Epoch 1047/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2023 - val_loss: -4.2434\n",
      "Epoch 1048/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2230 - val_loss: -4.4633\n",
      "Epoch 1049/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2015 - val_loss: -3.8431\n",
      "Epoch 1050/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2669 - val_loss: -3.4182\n",
      "Epoch 1051/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1083 - val_loss: -4.1413\n",
      "Epoch 1052/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1963 - val_loss: -4.3254\n",
      "Epoch 1053/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3061 - val_loss: -3.8218\n",
      "Epoch 1054/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2781 - val_loss: -4.3218\n",
      "Epoch 1055/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2455 - val_loss: -2.4837\n",
      "Epoch 1056/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2792 - val_loss: -3.7510\n",
      "Epoch 1057/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1680 - val_loss: -3.6483\n",
      "Epoch 1058/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2482 - val_loss: -4.5319\n",
      "Epoch 1059/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1890 - val_loss: -4.2222\n",
      "Epoch 1060/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2628 - val_loss: -4.6394\n",
      "Epoch 1061/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2932 - val_loss: -4.5627\n",
      "Epoch 1062/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2723 - val_loss: -4.3891\n",
      "Epoch 1063/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3004 - val_loss: -2.4084\n",
      "Epoch 1064/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1367 - val_loss: -3.2152\n",
      "Epoch 1065/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5594 - val_loss: -3.5588\n",
      "Epoch 1066/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6056 - val_loss: -4.3838\n",
      "Epoch 1067/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0676 - val_loss: -3.9925\n",
      "Epoch 1068/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0463 - val_loss: -4.6989\n",
      "Epoch 1069/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0729 - val_loss: -1.1030\n",
      "Epoch 1070/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1290 - val_loss: -4.3076\n",
      "Epoch 1071/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1984 - val_loss: -4.1928\n",
      "Epoch 1072/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1422 - val_loss: -4.3248\n",
      "Epoch 1073/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1600 - val_loss: -2.9253\n",
      "Epoch 1074/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0605 - val_loss: -3.6767\n",
      "Epoch 1075/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9902 - val_loss: -4.2238\n",
      "Epoch 1076/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0581 - val_loss: -3.9986\n",
      "Epoch 1077/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2253 - val_loss: -4.6599\n",
      "Epoch 1078/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1255 - val_loss: -4.3950\n",
      "Epoch 1079/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1422 - val_loss: -3.8188\n",
      "Epoch 1080/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2601 - val_loss: -4.6748\n",
      "Epoch 1081/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1093 - val_loss: -4.1736\n",
      "Epoch 1082/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2172 - val_loss: -3.7967\n",
      "Epoch 1083/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2176 - val_loss: -4.6151\n",
      "Epoch 1084/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2169 - val_loss: -2.9919\n",
      "Epoch 1085/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2755 - val_loss: -3.8343\n",
      "Epoch 1086/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2696 - val_loss: -4.4586\n",
      "Epoch 1087/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1851 - val_loss: -4.1182\n",
      "Epoch 1088/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1887 - val_loss: -4.5542\n",
      "Epoch 1089/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2440 - val_loss: -4.6136\n",
      "Epoch 1090/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2747 - val_loss: -4.0568\n",
      "Epoch 1091/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2344 - val_loss: -4.1115\n",
      "Epoch 1092/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1712 - val_loss: -3.9329\n",
      "Epoch 1093/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1879 - val_loss: -4.0910\n",
      "Epoch 1094/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2721 - val_loss: -4.2533\n",
      "Epoch 1095/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2121 - val_loss: -4.4174\n",
      "Epoch 1096/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2065 - val_loss: -4.3750\n",
      "Epoch 1097/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2291 - val_loss: -4.3766\n",
      "Epoch 1098/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2219 - val_loss: -3.1010\n",
      "Epoch 1099/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2211 - val_loss: -4.1956\n",
      "Epoch 1100/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2312 - val_loss: -3.9521\n",
      "Epoch 1101/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2976 - val_loss: -3.9192\n",
      "Epoch 1102/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1589 - val_loss: -3.3538\n",
      "Epoch 1103/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2826"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -5.2826 - val_loss: -4.7390\n",
      "Epoch 1104/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2733 - val_loss: -4.6384\n",
      "Epoch 1105/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -5.2973 - val_loss: -4.2422\n",
      "Epoch 1106/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1760 - val_loss: -4.0213\n",
      "Epoch 1107/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2069 - val_loss: -4.4780\n",
      "Epoch 1108/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1800 - val_loss: -4.5315\n",
      "Epoch 1109/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2835 - val_loss: -4.3556\n",
      "Epoch 1110/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2788 - val_loss: -3.8457\n",
      "Epoch 1111/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1981 - val_loss: -4.2315\n",
      "Epoch 1112/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3689 - val_loss: -1.1824\n",
      "Epoch 1113/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2610 - val_loss: -3.3138\n",
      "Epoch 1114/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2003 - val_loss: -3.8827\n",
      "Epoch 1115/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2677 - val_loss: -3.7118\n",
      "Epoch 1116/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2414 - val_loss: -3.5882\n",
      "Epoch 1117/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2914 - val_loss: -4.0405\n",
      "Epoch 1118/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2712 - val_loss: -3.7041\n",
      "Epoch 1119/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2987 - val_loss: -4.2736\n",
      "Epoch 1120/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2117 - val_loss: -4.0969\n",
      "Epoch 1121/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3088 - val_loss: -4.1095\n",
      "Epoch 1122/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2890 - val_loss: -3.1555\n",
      "Epoch 1123/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2935 - val_loss: -4.3727\n",
      "Epoch 1124/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2486 - val_loss: -3.8865\n",
      "Epoch 1125/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3162 - val_loss: -4.3522\n",
      "Epoch 1126/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2622 - val_loss: -4.0135\n",
      "Epoch 1127/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2102 - val_loss: -4.5793\n",
      "Epoch 1128/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -5.3083 - val_loss: -4.7923\n",
      "Epoch 1129/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2605 - val_loss: -4.5811\n",
      "Epoch 1130/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0894 - val_loss: -4.6126\n",
      "Epoch 1131/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2291 - val_loss: -4.4881\n",
      "Epoch 1132/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2296 - val_loss: -4.2124\n",
      "Epoch 1133/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2582 - val_loss: -3.9878\n",
      "Epoch 1134/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3201 - val_loss: -4.6410\n",
      "Epoch 1135/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2753 - val_loss: -4.3719\n",
      "Epoch 1136/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3076 - val_loss: -3.3443\n",
      "Epoch 1137/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2017 - val_loss: -4.5689\n",
      "Epoch 1138/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3035 - val_loss: -3.9296\n",
      "Epoch 1139/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2884 - val_loss: -3.5146\n",
      "Epoch 1140/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2986 - val_loss: -4.4909\n",
      "Epoch 1141/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0430 - val_loss: -4.2045\n",
      "Epoch 1142/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0433 - val_loss: -4.0528\n",
      "Epoch 1143/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1668 - val_loss: -4.4053\n",
      "Epoch 1144/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9962 - val_loss: -3.4705\n",
      "Epoch 1145/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1300 - val_loss: -4.3017\n",
      "Epoch 1146/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2232 - val_loss: -3.2973\n",
      "Epoch 1147/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2177 - val_loss: -4.4656\n",
      "Epoch 1148/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1873 - val_loss: -3.7006\n",
      "Epoch 1149/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2594 - val_loss: -4.4911\n",
      "Epoch 1150/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2305 - val_loss: 0.0958\n",
      "Epoch 1151/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4951 - val_loss: -4.2514\n",
      "Epoch 1152/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0855 - val_loss: -4.3528\n",
      "Epoch 1153/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0156 - val_loss: -4.4377\n",
      "Epoch 1154/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1366 - val_loss: -4.2957\n",
      "Epoch 1155/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1997 - val_loss: -4.4955\n",
      "Epoch 1156/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2028 - val_loss: 0.4917\n",
      "Epoch 1157/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1792 - val_loss: -2.2771\n",
      "Epoch 1158/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1783 - val_loss: -2.4861\n",
      "Epoch 1159/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1805 - val_loss: -4.1839\n",
      "Epoch 1160/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2538 - val_loss: -4.4054\n",
      "Epoch 1161/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2365 - val_loss: -4.3885\n",
      "Epoch 1162/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2404 - val_loss: -4.4368\n",
      "Epoch 1163/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2658 - val_loss: -4.2461\n",
      "Epoch 1164/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3031 - val_loss: -4.6878\n",
      "Epoch 1165/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2870 - val_loss: -4.3225\n",
      "Epoch 1166/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2630 - val_loss: -4.7107\n",
      "Epoch 1167/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1830 - val_loss: -4.4462\n",
      "Epoch 1168/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3363 - val_loss: -3.9564\n",
      "Epoch 1169/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3260 - val_loss: -4.1100\n",
      "Epoch 1170/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3363 - val_loss: -3.8874\n",
      "Epoch 1171/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2051 - val_loss: -4.4667\n",
      "Epoch 1172/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3221 - val_loss: -4.2528\n",
      "Epoch 1173/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2958 - val_loss: -4.0394\n",
      "Epoch 1174/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2597 - val_loss: -4.0975\n",
      "Epoch 1175/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2998 - val_loss: -4.1997\n",
      "Epoch 1176/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3748 - val_loss: -0.6371\n",
      "Epoch 1177/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1544 - val_loss: -4.2971\n",
      "Epoch 1178/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2958 - val_loss: -4.1040\n",
      "Epoch 1179/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2495 - val_loss: -4.6506\n",
      "Epoch 1180/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2841 - val_loss: -2.4215\n",
      "Epoch 1181/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2291 - val_loss: -4.5519\n",
      "Epoch 1182/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2986 - val_loss: -4.6721\n",
      "Epoch 1183/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3749 - val_loss: -4.6336\n",
      "Epoch 1184/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3800 - val_loss: -3.7170\n",
      "Epoch 1185/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1511 - val_loss: -4.3797\n",
      "Epoch 1186/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3669 - val_loss: -4.4013\n",
      "Epoch 1187/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4305 - val_loss: -2.8859\n",
      "Epoch 1188/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7760 - val_loss: -4.3875\n",
      "Epoch 1189/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0420 - val_loss: -4.6452\n",
      "Epoch 1190/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0306 - val_loss: -3.3203\n",
      "Epoch 1191/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9536 - val_loss: -4.5775\n",
      "Epoch 1192/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1656 - val_loss: -3.7158\n",
      "Epoch 1193/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1559 - val_loss: -4.5016\n",
      "Epoch 1194/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1519 - val_loss: -4.2124\n",
      "Epoch 1195/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2092 - val_loss: -4.2111\n",
      "Epoch 1196/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2908 - val_loss: -4.1761\n",
      "Epoch 1197/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2766 - val_loss: -3.2129\n",
      "Epoch 1198/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2685 - val_loss: -3.5862\n",
      "Epoch 1199/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0845 - val_loss: -4.3065\n",
      "Epoch 1200/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3005 - val_loss: -2.5668\n",
      "Epoch 1201/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2685 - val_loss: -4.2136\n",
      "Epoch 1202/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2124 - val_loss: -3.9568\n",
      "Epoch 1203/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8753 - val_loss: -4.4810\n",
      "Epoch 1204/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2233 - val_loss: -1.7668\n",
      "Epoch 1205/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2028 - val_loss: -4.5267\n",
      "Epoch 1206/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2791 - val_loss: -4.5842\n",
      "Epoch 1207/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2680 - val_loss: -4.5103\n",
      "Epoch 1208/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2934 - val_loss: -4.5383\n",
      "Epoch 1209/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3047 - val_loss: -4.3317\n",
      "Epoch 1210/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2334 - val_loss: -3.8689\n",
      "Epoch 1211/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2758 - val_loss: -3.9336\n",
      "Epoch 1212/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2357 - val_loss: -3.2608\n",
      "Epoch 1213/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2915 - val_loss: -4.4351\n",
      "Epoch 1214/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2607 - val_loss: -4.4475\n",
      "Epoch 1215/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1728 - val_loss: -4.2793\n",
      "Epoch 1216/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3512 - val_loss: -2.2965\n",
      "Epoch 1217/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1767 - val_loss: -4.3591\n",
      "Epoch 1218/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3554 - val_loss: -4.5825\n",
      "Epoch 1219/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2997 - val_loss: -4.7015\n",
      "Epoch 1220/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3059 - val_loss: -3.4471\n",
      "Epoch 1221/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3635 - val_loss: -4.7312\n",
      "Epoch 1222/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3394 - val_loss: -4.2201\n",
      "Epoch 1223/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3710 - val_loss: -4.0099\n",
      "Epoch 1224/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2091 - val_loss: -4.4835\n",
      "Epoch 1225/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2128 - val_loss: -4.3309\n",
      "Epoch 1226/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3580 - val_loss: -4.1398\n",
      "Epoch 1227/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2473 - val_loss: -3.9736\n",
      "Epoch 1228/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3647 - val_loss: -4.2663\n",
      "Epoch 1229/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3540 - val_loss: -4.4580\n",
      "Epoch 1230/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2305 - val_loss: -3.9096\n",
      "Epoch 1231/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3628 - val_loss: -1.8600\n",
      "Epoch 1232/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2812 - val_loss: -4.4330\n",
      "Epoch 1233/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3028 - val_loss: -3.7328\n",
      "Epoch 1234/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.3495 - val_loss: -2.8613\n",
      "Epoch 1235/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3455 - val_loss: -3.5036\n",
      "Epoch 1236/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3842 - val_loss: -3.3526\n",
      "Epoch 1237/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1361 - val_loss: -4.6451\n",
      "Epoch 1238/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3319 - val_loss: -3.7354\n",
      "Epoch 1239/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2132 - val_loss: -4.1739\n",
      "Epoch 1240/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2537 - val_loss: -4.5925\n",
      "Epoch 1241/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2245 - val_loss: -4.3937\n",
      "Epoch 1242/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2543 - val_loss: -4.2921\n",
      "Epoch 1243/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4032 - val_loss: -3.6711\n",
      "Epoch 1244/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3747 - val_loss: -4.5551\n",
      "Epoch 1245/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2268 - val_loss: -2.2311\n",
      "Epoch 1246/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1701 - val_loss: -4.2989\n",
      "Epoch 1247/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3252 - val_loss: -3.4818\n",
      "Epoch 1248/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4100 - val_loss: -4.2605\n",
      "Epoch 1249/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3147 - val_loss: -4.6299\n",
      "Epoch 1250/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3557 - val_loss: -0.5537\n",
      "Epoch 1251/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3462 - val_loss: -4.4350\n",
      "Epoch 1252/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3528 - val_loss: -3.3068\n",
      "Epoch 1253/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2510 - val_loss: -4.1972\n",
      "Epoch 1254/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3807 - val_loss: -4.2212\n",
      "Epoch 1255/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3430 - val_loss: -4.0074\n",
      "Epoch 1256/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1980 - val_loss: -4.6479\n",
      "Epoch 1257/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3476 - val_loss: -4.0268\n",
      "Epoch 1258/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2259 - val_loss: -4.3852\n",
      "Epoch 1259/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3732 - val_loss: -4.0529\n",
      "Epoch 1260/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3540 - val_loss: -4.5666\n",
      "Epoch 1261/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3337 - val_loss: -4.0582\n",
      "Epoch 1262/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4137 - val_loss: -4.6609\n",
      "Epoch 1263/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4295 - val_loss: -3.6948\n",
      "Epoch 1264/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2786 - val_loss: -2.9765\n",
      "Epoch 1265/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2516 - val_loss: -2.6816\n",
      "Epoch 1266/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3770 - val_loss: -4.4207\n",
      "Epoch 1267/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2805 - val_loss: -3.8988\n",
      "Epoch 1268/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4236 - val_loss: -4.3562\n",
      "Epoch 1269/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3799 - val_loss: -4.4362\n",
      "Epoch 1270/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3815 - val_loss: -3.6627\n",
      "Epoch 1271/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2856 - val_loss: -3.8447\n",
      "Epoch 1272/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2838 - val_loss: -3.9249\n",
      "Epoch 1273/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3471 - val_loss: -4.3040\n",
      "Epoch 1274/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2455 - val_loss: -4.2052\n",
      "Epoch 1275/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3155 - val_loss: -4.4472\n",
      "Epoch 1276/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4055 - val_loss: -3.4107\n",
      "Epoch 1277/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3586 - val_loss: -4.6099\n",
      "Epoch 1278/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3090 - val_loss: -4.6146\n",
      "Epoch 1279/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3076 - val_loss: -3.6902\n",
      "Epoch 1280/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.3911 - val_loss: -4.0705\n",
      "Epoch 1281/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2623 - val_loss: -4.4243\n",
      "Epoch 1282/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3462 - val_loss: -4.3971\n",
      "Epoch 1283/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3350 - val_loss: -2.9504\n",
      "Epoch 1284/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2977 - val_loss: -4.5722\n",
      "Epoch 1285/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3122 - val_loss: -3.1045\n",
      "Epoch 1286/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3921 - val_loss: -4.5065\n",
      "Epoch 1287/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4128 - val_loss: -4.4522\n",
      "Epoch 1288/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2644 - val_loss: -4.2433\n",
      "Epoch 1289/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4287 - val_loss: -2.2414\n",
      "Epoch 1290/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2734 - val_loss: -4.6065\n",
      "Epoch 1291/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3793 - val_loss: -2.4480\n",
      "Epoch 1292/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4173 - val_loss: -4.7655\n",
      "Epoch 1293/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3581 - val_loss: -1.9458\n",
      "Epoch 1294/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3165 - val_loss: -4.0093\n",
      "Epoch 1295/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2031 - val_loss: -4.5769\n",
      "Epoch 1296/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0602"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -5.0602 - val_loss: -4.8077\n",
      "Epoch 1297/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2247 - val_loss: -4.6937\n",
      "Epoch 1298/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3756 - val_loss: -4.5587\n",
      "Epoch 1299/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3288 - val_loss: -4.4084\n",
      "Epoch 1300/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2672 - val_loss: -4.6896\n",
      "Epoch 1301/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3805 - val_loss: -4.5415\n",
      "Epoch 1302/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3210 - val_loss: -3.7868\n",
      "Epoch 1303/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3555 - val_loss: -4.4818\n",
      "Epoch 1304/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3276 - val_loss: -4.2718\n",
      "Epoch 1305/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3873 - val_loss: -2.9002\n",
      "Epoch 1306/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2757 - val_loss: -4.5708\n",
      "Epoch 1307/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3664 - val_loss: -4.3313\n",
      "Epoch 1308/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.3926 - val_loss: -3.7988\n",
      "Epoch 1309/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3728 - val_loss: -3.9928\n",
      "Epoch 1310/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4135 - val_loss: -3.9916\n",
      "Epoch 1311/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4392 - val_loss: -1.6894\n",
      "Epoch 1312/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2182 - val_loss: -4.2519\n",
      "Epoch 1313/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3997 - val_loss: -4.4381\n",
      "Epoch 1314/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9600 - val_loss: -4.4687\n",
      "Epoch 1315/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1998 - val_loss: -4.4439\n",
      "Epoch 1316/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -5.3462 - val_loss: -4.3782\n",
      "Epoch 1317/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2791 - val_loss: -4.5554\n",
      "Epoch 1318/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3646 - val_loss: -4.2123\n",
      "Epoch 1319/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3200 - val_loss: -4.4666\n",
      "Epoch 1320/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2539 - val_loss: -3.4895\n",
      "Epoch 1321/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3997 - val_loss: -4.1281\n",
      "Epoch 1322/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3642 - val_loss: -4.6278\n",
      "Epoch 1323/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3773 - val_loss: -4.5777\n",
      "Epoch 1324/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.4144"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -5.4144 - val_loss: -4.8388\n",
      "Epoch 1325/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2104 - val_loss: -4.2505\n",
      "Epoch 1326/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3574 - val_loss: -4.1356\n",
      "Epoch 1327/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4201 - val_loss: -4.8105\n",
      "Epoch 1328/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4127 - val_loss: -4.5789\n",
      "Epoch 1329/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8454 - val_loss: -3.5585\n",
      "Epoch 1330/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2794 - val_loss: -3.5888\n",
      "Epoch 1331/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3041 - val_loss: -4.4424\n",
      "Epoch 1332/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1702 - val_loss: -4.5307\n",
      "Epoch 1333/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3167 - val_loss: -3.3479\n",
      "Epoch 1334/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3844 - val_loss: -4.0524\n",
      "Epoch 1335/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2792 - val_loss: -4.0253\n",
      "Epoch 1336/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2983 - val_loss: -2.9194\n",
      "Epoch 1337/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3391 - val_loss: -4.7252\n",
      "Epoch 1338/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1565 - val_loss: -3.3539\n",
      "Epoch 1339/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2512 - val_loss: -4.3127\n",
      "Epoch 1340/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2976 - val_loss: 0.1830\n",
      "Epoch 1341/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2313 - val_loss: -3.8685\n",
      "Epoch 1342/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3687 - val_loss: -3.4405\n",
      "Epoch 1343/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0607 - val_loss: -4.7590\n",
      "Epoch 1344/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3957 - val_loss: -4.1600\n",
      "Epoch 1345/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3635 - val_loss: -4.7042\n",
      "Epoch 1346/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3698 - val_loss: -4.2541\n",
      "Epoch 1347/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4229 - val_loss: -4.5889\n",
      "Epoch 1348/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3203 - val_loss: -3.7926\n",
      "Epoch 1349/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4131 - val_loss: -3.2685\n",
      "Epoch 1350/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3092 - val_loss: -4.4813\n",
      "Epoch 1351/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3916 - val_loss: -4.7436\n",
      "Epoch 1352/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3930 - val_loss: -3.9667\n",
      "Epoch 1353/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4351 - val_loss: -4.0094\n",
      "Epoch 1354/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3715 - val_loss: -4.5872\n",
      "Epoch 1355/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3764 - val_loss: -4.4976\n",
      "Epoch 1356/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3590 - val_loss: -3.7684\n",
      "Epoch 1357/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3888 - val_loss: -4.4351\n",
      "Epoch 1358/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3711 - val_loss: -4.0678\n",
      "Epoch 1359/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2297 - val_loss: -4.5361\n",
      "Epoch 1360/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3457 - val_loss: -3.5821\n",
      "Epoch 1361/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4018 - val_loss: -4.1923\n",
      "Epoch 1362/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3098 - val_loss: -4.1586\n",
      "Epoch 1363/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4278 - val_loss: -3.2498\n",
      "Epoch 1364/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3369 - val_loss: -3.0226\n",
      "Epoch 1365/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4019 - val_loss: -3.2938\n",
      "Epoch 1366/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3383 - val_loss: -4.6221\n",
      "Epoch 1367/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4062 - val_loss: -4.2437\n",
      "Epoch 1368/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3695 - val_loss: -3.3537\n",
      "Epoch 1369/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3621 - val_loss: -4.2305\n",
      "Epoch 1370/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3693 - val_loss: -4.0066\n",
      "Epoch 1371/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4035 - val_loss: -4.7022\n",
      "Epoch 1372/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2038 - val_loss: -4.3690\n",
      "Epoch 1373/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2695 - val_loss: -3.8818\n",
      "Epoch 1374/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3916 - val_loss: 0.4997\n",
      "Epoch 1375/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2469 - val_loss: -4.2429\n",
      "Epoch 1376/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3925 - val_loss: -4.4494\n",
      "Epoch 1377/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3908 - val_loss: -3.4905\n",
      "Epoch 1378/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4191 - val_loss: -4.2844\n",
      "Epoch 1379/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3529 - val_loss: -4.1591\n",
      "Epoch 1380/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2734 - val_loss: -4.5665\n",
      "Epoch 1381/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3767 - val_loss: -4.3392\n",
      "Epoch 1382/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2752 - val_loss: -4.3218\n",
      "Epoch 1383/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2538 - val_loss: -4.2300\n",
      "Epoch 1384/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4831 - val_loss: -4.1660\n",
      "Epoch 1385/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3880 - val_loss: -4.0879\n",
      "Epoch 1386/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4295 - val_loss: -4.1377\n",
      "Epoch 1387/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4503 - val_loss: -2.4347\n",
      "Epoch 1388/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3108 - val_loss: -4.1863\n",
      "Epoch 1389/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4744 - val_loss: -3.9837\n",
      "Epoch 1390/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3951 - val_loss: -4.1971\n",
      "Epoch 1391/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8036 - val_loss: -3.9265\n",
      "Epoch 1392/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0906 - val_loss: -3.6742\n",
      "Epoch 1393/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2308 - val_loss: -3.6835\n",
      "Epoch 1394/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2932 - val_loss: -4.4954\n",
      "Epoch 1395/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3455 - val_loss: -3.2981\n",
      "Epoch 1396/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2808 - val_loss: -4.6007\n",
      "Epoch 1397/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2645 - val_loss: -4.7063\n",
      "Epoch 1398/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3580 - val_loss: -4.3570\n",
      "Epoch 1399/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3193 - val_loss: -2.7127\n",
      "Epoch 1400/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3396 - val_loss: -4.0002\n",
      "Epoch 1401/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3159 - val_loss: -2.4555\n",
      "Epoch 1402/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3742 - val_loss: -1.7488\n",
      "Epoch 1403/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2038 - val_loss: -3.9658\n",
      "Epoch 1404/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3861 - val_loss: -4.2723\n",
      "Epoch 1405/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3155 - val_loss: -3.3793\n",
      "Epoch 1406/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3403 - val_loss: -4.2520\n",
      "Epoch 1407/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3687 - val_loss: -3.5703\n",
      "Epoch 1408/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2706 - val_loss: -3.3684\n",
      "Epoch 1409/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4259 - val_loss: -3.4820\n",
      "Epoch 1410/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2656 - val_loss: -4.5587\n",
      "Epoch 1411/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3815 - val_loss: -1.6166\n",
      "Epoch 1412/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3435 - val_loss: -4.3352\n",
      "Epoch 1413/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4095 - val_loss: -3.0378\n",
      "Epoch 1414/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3752 - val_loss: -4.4026\n",
      "Epoch 1415/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3619 - val_loss: -2.3278\n",
      "Epoch 1416/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3862 - val_loss: -4.1727\n",
      "Epoch 1417/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3970 - val_loss: -4.7154\n",
      "Epoch 1418/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3515 - val_loss: -3.9222\n",
      "Epoch 1419/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3927 - val_loss: -4.3794\n",
      "Epoch 1420/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3351 - val_loss: -4.4908\n",
      "Epoch 1421/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3903 - val_loss: -4.5926\n",
      "Epoch 1422/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4559 - val_loss: -4.4623\n",
      "Epoch 1423/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3811 - val_loss: -4.1226\n",
      "Epoch 1424/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1425 - val_loss: -4.6721\n",
      "Epoch 1425/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3342 - val_loss: -3.9203\n",
      "Epoch 1426/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2641 - val_loss: -4.5381\n",
      "Epoch 1427/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2921 - val_loss: -4.4199\n",
      "Epoch 1428/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3644 - val_loss: -4.5971\n",
      "Epoch 1429/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3713 - val_loss: -4.5520\n",
      "Epoch 1430/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3856 - val_loss: -1.3525\n",
      "Epoch 1431/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3927 - val_loss: -2.9256\n",
      "Epoch 1432/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4414 - val_loss: -4.0772\n",
      "Epoch 1433/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3551 - val_loss: -3.2906\n",
      "Epoch 1434/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3856 - val_loss: -3.3014\n",
      "Epoch 1435/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4265 - val_loss: 1.4342\n",
      "Epoch 1436/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2640 - val_loss: -2.5563\n",
      "Epoch 1437/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4250 - val_loss: -4.2714\n",
      "Epoch 1438/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -5.3188 - val_loss: -3.3174\n",
      "Epoch 1439/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4405 - val_loss: -4.5963\n",
      "Epoch 1440/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4636 - val_loss: -4.3996\n",
      "Epoch 1441/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2382 - val_loss: -3.5704\n",
      "Epoch 1442/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4123 - val_loss: -4.7316\n",
      "Epoch 1443/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4289 - val_loss: -3.6197\n",
      "Epoch 1444/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4786 - val_loss: -4.5945\n",
      "Epoch 1445/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3396 - val_loss: -4.5738\n",
      "Epoch 1446/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3862 - val_loss: -4.6566\n",
      "Epoch 1447/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4147 - val_loss: -3.9584\n",
      "Epoch 1448/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4505 - val_loss: -4.3202\n",
      "Epoch 1449/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3646 - val_loss: -2.6430\n",
      "Epoch 1450/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3994 - val_loss: -2.8194\n",
      "Epoch 1451/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4389 - val_loss: -3.9985\n",
      "Epoch 1452/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3400 - val_loss: -4.1561\n",
      "Epoch 1453/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4774 - val_loss: 0.1724\n",
      "Epoch 1454/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4605 - val_loss: -3.2991\n",
      "Epoch 1455/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7118 - val_loss: -3.9217\n",
      "Epoch 1456/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4094 - val_loss: -3.9613\n",
      "Epoch 1457/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9750 - val_loss: -3.5195\n",
      "Epoch 1458/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1642 - val_loss: -4.3764\n",
      "Epoch 1459/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1496 - val_loss: -4.5914\n",
      "Epoch 1460/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2150 - val_loss: -4.2881\n",
      "Epoch 1461/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2394 - val_loss: -4.5157\n",
      "Epoch 1462/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3335 - val_loss: -3.5655\n",
      "Epoch 1463/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1721 - val_loss: -4.5268\n",
      "Epoch 1464/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3122 - val_loss: -2.8006\n",
      "Epoch 1465/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2071 - val_loss: -1.6750\n",
      "Epoch 1466/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3384 - val_loss: -4.0162\n",
      "Epoch 1467/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3413 - val_loss: -3.1436\n",
      "Epoch 1468/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3118 - val_loss: -1.8208\n",
      "Epoch 1469/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3695 - val_loss: -0.7513\n",
      "Epoch 1470/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2549 - val_loss: -1.3562\n",
      "Epoch 1471/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2151 - val_loss: -1.9540\n",
      "Epoch 1472/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4156 - val_loss: -4.3376\n",
      "Epoch 1473/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2935 - val_loss: -4.5807\n",
      "Epoch 1474/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4194 - val_loss: -4.3582\n",
      "Epoch 1475/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2848 - val_loss: -4.3442\n",
      "Epoch 1476/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4165 - val_loss: -2.4203\n",
      "Epoch 1477/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4065 - val_loss: -3.6431\n",
      "Epoch 1478/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4078 - val_loss: -4.7877\n",
      "Epoch 1479/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3919 - val_loss: -4.8217\n",
      "Epoch 1480/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4151 - val_loss: -3.8687\n",
      "Epoch 1481/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3918 - val_loss: -4.1847\n",
      "Epoch 1482/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4241 - val_loss: -4.3433\n",
      "Epoch 1483/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3643 - val_loss: -4.0123\n",
      "Epoch 1484/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4635 - val_loss: -4.4016\n",
      "Epoch 1485/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2739 - val_loss: -4.5709\n",
      "Epoch 1486/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4482 - val_loss: -3.6651\n",
      "Epoch 1487/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3258 - val_loss: -3.7846\n",
      "Epoch 1488/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4664 - val_loss: -4.6364\n",
      "Epoch 1489/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4261 - val_loss: -4.2884\n",
      "Epoch 1490/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4528 - val_loss: -4.4889\n",
      "Epoch 1491/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3980 - val_loss: -4.5071\n",
      "Epoch 1492/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4070 - val_loss: -3.7692\n",
      "Epoch 1493/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3589 - val_loss: -4.6121\n",
      "Epoch 1494/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4203 - val_loss: -4.1053\n",
      "Epoch 1495/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4680 - val_loss: -3.0940\n",
      "Epoch 1496/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3585 - val_loss: -2.1493\n",
      "Epoch 1497/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -4.9090 - val_loss: -3.8918\n",
      "Epoch 1498/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3504 - val_loss: -4.5995\n",
      "Epoch 1499/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3877 - val_loss: -4.2949\n",
      "Epoch 1500/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2078 - val_loss: -3.4701\n",
      "Epoch 1501/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3538 - val_loss: -4.7172\n",
      "Epoch 1502/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3404 - val_loss: -4.2576\n",
      "Epoch 1503/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4269 - val_loss: -4.3914\n",
      "Epoch 1504/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3371 - val_loss: -4.6178\n",
      "Epoch 1505/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4121 - val_loss: -4.6611\n",
      "Epoch 1506/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4365 - val_loss: -3.1147\n",
      "Epoch 1507/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2878 - val_loss: -4.1245\n",
      "Epoch 1508/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1770 - val_loss: -4.4913\n",
      "Epoch 1509/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3847 - val_loss: -4.2711\n",
      "Epoch 1510/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2913 - val_loss: -2.9598\n",
      "Epoch 1511/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3820 - val_loss: -3.7729\n",
      "Epoch 1512/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3423 - val_loss: -4.1430\n",
      "Epoch 1513/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4269 - val_loss: -3.2353\n",
      "Epoch 1514/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4636 - val_loss: -1.9342\n",
      "Epoch 1515/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3914 - val_loss: -3.2185\n",
      "Epoch 1516/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3748 - val_loss: -4.3403\n",
      "Epoch 1517/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1657 - val_loss: -4.5444\n",
      "Epoch 1518/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1482 - val_loss: -4.3423\n",
      "Epoch 1519/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2845 - val_loss: 0.1472\n",
      "Epoch 1520/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2344 - val_loss: -4.4557\n",
      "Epoch 1521/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2566 - val_loss: -4.0778\n",
      "Epoch 1522/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3465 - val_loss: -4.2893\n",
      "Epoch 1523/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3170 - val_loss: -3.6701\n",
      "Epoch 1524/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3839 - val_loss: -4.1254\n",
      "Epoch 1525/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4116 - val_loss: -3.1694\n",
      "Epoch 1526/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4639 - val_loss: -4.4434\n",
      "Epoch 1527/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.4548 - val_loss: -3.5707\n",
      "Epoch 1528/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4502 - val_loss: -3.5025\n",
      "Epoch 1529/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2316 - val_loss: -4.3942\n",
      "Epoch 1530/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4686 - val_loss: -4.3575\n",
      "Epoch 1531/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3133 - val_loss: -4.3487\n",
      "Epoch 1532/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4524 - val_loss: -3.2689\n",
      "Epoch 1533/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3932 - val_loss: -3.7491\n",
      "Epoch 1534/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4203 - val_loss: -4.3926\n",
      "Epoch 1535/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4434 - val_loss: -4.4406\n",
      "Epoch 1536/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4916 - val_loss: -3.4223\n",
      "Epoch 1537/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3822 - val_loss: -4.2504\n",
      "Epoch 1538/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4449 - val_loss: -4.6121\n",
      "Epoch 1539/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4360 - val_loss: -4.6734\n",
      "Epoch 1540/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3949 - val_loss: -3.2635\n",
      "Epoch 1541/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2676 - val_loss: -3.6401\n",
      "Epoch 1542/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4748 - val_loss: -3.2335\n",
      "Epoch 1543/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4281 - val_loss: -4.2670\n",
      "Epoch 1544/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.5563 - val_loss: -4.5225\n",
      "Epoch 1545/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2940 - val_loss: -4.6222\n",
      "Epoch 1546/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3712 - val_loss: -3.9864\n",
      "Epoch 1547/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2535 - val_loss: -3.0094\n",
      "Epoch 1548/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.4098"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_140_layer_call_fn, lstm_cell_140_layer_call_and_return_conditional_losses, lstm_cell_141_layer_call_fn, lstm_cell_141_layer_call_and_return_conditional_losses, lstm_cell_142_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 243ms/step - loss: -5.4098 - val_loss: -4.8688\n",
      "Epoch 1549/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3312 - val_loss: -4.3845\n",
      "Epoch 1550/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2531 - val_loss: -4.5627\n",
      "Epoch 1551/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4078 - val_loss: -3.1421\n",
      "Epoch 1552/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2973 - val_loss: -4.6836\n",
      "Epoch 1553/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4269 - val_loss: -3.3639\n",
      "Epoch 1554/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3825 - val_loss: -4.1962\n",
      "Epoch 1555/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3974 - val_loss: -4.1758\n",
      "Epoch 1556/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3123 - val_loss: -4.1590\n",
      "Epoch 1557/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4254 - val_loss: -3.7128\n",
      "Epoch 1558/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4127 - val_loss: -3.2425\n",
      "Epoch 1559/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3144 - val_loss: -4.5742\n",
      "Epoch 1560/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4448 - val_loss: -1.9763\n",
      "Epoch 1561/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3568 - val_loss: -3.8094\n",
      "Epoch 1562/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4931 - val_loss: -4.4902\n",
      "Epoch 1563/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3399 - val_loss: -4.2003\n",
      "Epoch 1564/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3251 - val_loss: -4.1467\n",
      "Epoch 1565/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3932 - val_loss: -3.2090\n",
      "Epoch 1566/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5055 - val_loss: -4.2590\n",
      "Epoch 1567/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2960 - val_loss: -4.5596\n",
      "Epoch 1568/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4528 - val_loss: -0.7648\n",
      "Epoch 1569/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3283 - val_loss: -1.0134\n",
      "Epoch 1570/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3204 - val_loss: -3.9052\n",
      "Epoch 1571/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4799 - val_loss: -3.6484\n",
      "Epoch 1572/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4269 - val_loss: -4.1959\n",
      "Epoch 1573/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3987 - val_loss: -1.8631\n",
      "Epoch 1574/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4086 - val_loss: -4.1728\n",
      "Epoch 1575/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4853 - val_loss: -3.9703\n",
      "Epoch 1576/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3364 - val_loss: -4.2470\n",
      "Epoch 1577/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3479 - val_loss: -3.9279\n",
      "Epoch 1578/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4564 - val_loss: -3.3337\n",
      "Epoch 1579/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3051 - val_loss: -3.9423\n",
      "Epoch 1580/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4634 - val_loss: -4.5505\n",
      "Epoch 1581/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3947 - val_loss: -4.3030\n",
      "Epoch 1582/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4169 - val_loss: -3.2725\n",
      "Epoch 1583/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4235 - val_loss: -4.3942\n",
      "Epoch 1584/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4891 - val_loss: -3.5202\n",
      "Epoch 1585/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3361 - val_loss: -4.1734\n",
      "Epoch 1586/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4796 - val_loss: 8.6920\n",
      "Epoch 1587/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2214 - val_loss: -3.7879\n",
      "Epoch 1588/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5116 - val_loss: -3.5819\n",
      "Epoch 1589/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4612 - val_loss: -3.5060\n",
      "Epoch 1590/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4349 - val_loss: -3.2297\n",
      "Epoch 1591/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0762 - val_loss: -4.2668\n",
      "Epoch 1592/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9931 - val_loss: -4.4972\n",
      "Epoch 1593/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3657 - val_loss: -3.3576\n",
      "Epoch 1594/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1579 - val_loss: -4.5977\n",
      "Epoch 1595/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3005 - val_loss: -4.0718\n",
      "Epoch 1596/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3304 - val_loss: -4.4938\n",
      "Epoch 1597/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3875 - val_loss: -2.2689\n",
      "Epoch 1598/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4329 - val_loss: -4.4064\n",
      "Epoch 1599/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3365 - val_loss: -2.3433\n",
      "Epoch 1600/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3536 - val_loss: -3.1721\n",
      "Epoch 1601/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3728 - val_loss: -4.7642\n",
      "Epoch 1602/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3147 - val_loss: -4.1755\n",
      "Epoch 1603/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4272 - val_loss: -1.4103\n",
      "Epoch 1604/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4282 - val_loss: -4.0382\n",
      "Epoch 1605/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4267 - val_loss: -3.5013\n",
      "Epoch 1606/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4658 - val_loss: -4.4990\n",
      "Epoch 1607/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4399 - val_loss: -4.6166\n",
      "Epoch 1608/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3957 - val_loss: -3.9618\n",
      "Epoch 1609/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4502 - val_loss: -4.7398\n",
      "Epoch 1610/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3933 - val_loss: -4.3314\n",
      "Epoch 1611/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4880 - val_loss: -3.7568\n",
      "Epoch 1612/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4711 - val_loss: -4.0886\n",
      "Epoch 1613/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4024 - val_loss: -4.4940\n",
      "Epoch 1614/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4860 - val_loss: -0.0686\n",
      "Epoch 1615/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2301 - val_loss: -3.4280\n",
      "Epoch 1616/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4383 - val_loss: -4.3090\n",
      "Epoch 1617/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3626 - val_loss: -4.3365\n",
      "Epoch 1618/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3607 - val_loss: -4.3629\n",
      "Epoch 1619/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4846 - val_loss: -4.4159\n",
      "Epoch 1620/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4161 - val_loss: -4.1887\n",
      "Epoch 1621/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5240 - val_loss: -4.1287\n",
      "Epoch 1622/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3580 - val_loss: -4.0462\n",
      "Epoch 1623/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3708 - val_loss: -4.3069\n",
      "Epoch 1624/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4249 - val_loss: -4.2393\n",
      "Epoch 1625/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5141 - val_loss: -3.2434\n",
      "Epoch 1626/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3909 - val_loss: -1.9068\n",
      "Epoch 1627/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3802 - val_loss: -2.7155\n",
      "Epoch 1628/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4546 - val_loss: -3.5757\n",
      "Epoch 1629/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4522 - val_loss: -4.8356\n",
      "Epoch 1630/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5323 - val_loss: -4.0724\n",
      "Epoch 1631/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4715 - val_loss: -4.3136\n",
      "Epoch 1632/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4501 - val_loss: -2.9976\n",
      "Epoch 1633/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3082 - val_loss: -4.0723\n",
      "Epoch 1634/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5171 - val_loss: -3.5913\n",
      "Epoch 1635/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4104 - val_loss: -3.9825\n",
      "Epoch 1636/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4470 - val_loss: -3.4947\n",
      "Epoch 1637/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4467 - val_loss: -4.4707\n",
      "Epoch 1638/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4409 - val_loss: -4.2138\n",
      "Epoch 1639/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5279 - val_loss: -0.7984\n",
      "Epoch 1640/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4612 - val_loss: -2.2631\n",
      "Epoch 1641/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4273 - val_loss: -3.5905\n",
      "Epoch 1642/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5248 - val_loss: -1.6982\n",
      "Epoch 1643/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4177 - val_loss: -4.3557\n",
      "Epoch 1644/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3978 - val_loss: -3.9893\n",
      "Epoch 1645/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5178 - val_loss: -4.6790\n",
      "Epoch 1646/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4776 - val_loss: -3.5721\n",
      "Epoch 1647/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4660 - val_loss: -4.3549\n",
      "Epoch 1648/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4258 - val_loss: -4.2253\n",
      "Epoch 1649/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5406 - val_loss: -4.1781\n",
      "Epoch 1650/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4143 - val_loss: -4.5640\n",
      "Epoch 1651/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4146 - val_loss: -4.5120\n",
      "Epoch 1652/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5505 - val_loss: -4.0538\n",
      "Epoch 1653/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4597 - val_loss: -4.3480\n",
      "Epoch 1654/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4638 - val_loss: -4.0593\n",
      "Epoch 1655/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3866 - val_loss: -3.8521\n",
      "Epoch 1656/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6735 - val_loss: -4.0884\n",
      "Epoch 1657/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1075 - val_loss: -4.2747\n",
      "Epoch 1658/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2110 - val_loss: -4.1482\n",
      "Epoch 1659/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3175 - val_loss: -3.7817\n",
      "Epoch 1660/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3361 - val_loss: -4.2110\n",
      "Epoch 1661/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3324 - val_loss: -3.6953\n",
      "Epoch 1662/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3465 - val_loss: -4.5537\n",
      "Epoch 1663/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3477 - val_loss: -3.3827\n",
      "Epoch 1664/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4173 - val_loss: -4.1598\n",
      "Epoch 1665/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3818 - val_loss: -4.0206\n",
      "Epoch 1666/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3133 - val_loss: -3.7966\n",
      "Epoch 1667/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4744 - val_loss: -3.6399\n",
      "Epoch 1668/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4484 - val_loss: -3.6486\n",
      "Epoch 1669/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3639 - val_loss: -3.8391\n",
      "Epoch 1670/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4826 - val_loss: -3.1236\n",
      "Epoch 1671/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0395 - val_loss: -3.6605\n",
      "Epoch 1672/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2355 - val_loss: -3.2966\n",
      "Epoch 1673/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3697 - val_loss: -4.2552\n",
      "Epoch 1674/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3836 - val_loss: -3.8750\n",
      "Epoch 1675/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4274 - val_loss: -3.0303\n",
      "Epoch 1676/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3542 - val_loss: -2.3250\n",
      "Epoch 1677/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4716 - val_loss: -4.0359\n",
      "Epoch 1678/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3827 - val_loss: -4.6176\n",
      "Epoch 1679/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4860 - val_loss: -1.6030\n",
      "Epoch 1680/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4442 - val_loss: -4.2920\n",
      "Epoch 1681/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3659 - val_loss: -4.4497\n",
      "Epoch 1682/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4411 - val_loss: -1.1960\n",
      "Epoch 1683/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3463 - val_loss: -3.9350\n",
      "Epoch 1684/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5232 - val_loss: -3.1716\n",
      "Epoch 1685/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5129 - val_loss: -3.1304\n",
      "Epoch 1686/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3499 - val_loss: -3.0219\n",
      "Epoch 1687/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4519 - val_loss: -4.2649\n",
      "Epoch 1688/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4692 - val_loss: -3.9518\n",
      "Epoch 1689/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4335 - val_loss: -4.0127\n",
      "Epoch 1690/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3115 - val_loss: -4.0782\n",
      "Epoch 1691/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4095 - val_loss: -3.1908\n",
      "Epoch 1692/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5059 - val_loss: -3.4899\n",
      "Epoch 1693/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4113 - val_loss: -4.0197\n",
      "Epoch 1694/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4579 - val_loss: -4.2961\n",
      "Epoch 1695/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4744 - val_loss: -2.4178\n",
      "Epoch 1696/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3922 - val_loss: -4.1698\n",
      "Epoch 1697/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4487 - val_loss: -4.0795\n",
      "Epoch 1698/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3919 - val_loss: -4.3491\n",
      "Epoch 1699/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4713 - val_loss: -4.4427\n",
      "Epoch 1700/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3898 - val_loss: -4.3519\n",
      "Epoch 1701/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4524 - val_loss: -4.1822\n",
      "Epoch 1702/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4714 - val_loss: -4.3196\n",
      "Epoch 1703/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4713 - val_loss: -2.9571\n",
      "Epoch 1704/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2899 - val_loss: -4.4424\n",
      "Epoch 1705/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5022 - val_loss: -2.5224\n",
      "Epoch 1706/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4341 - val_loss: -3.9004\n",
      "Epoch 1707/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -5.5419 - val_loss: -4.0414\n",
      "Epoch 1708/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5259 - val_loss: -4.0927\n",
      "Epoch 1709/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4755 - val_loss: -3.8333\n",
      "Epoch 1710/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4550 - val_loss: -4.1462\n",
      "Epoch 1711/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4141 - val_loss: -3.4745\n",
      "Epoch 1712/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5319 - val_loss: -3.9324\n",
      "Epoch 1713/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4313 - val_loss: -3.6831\n",
      "Epoch 1714/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5240 - val_loss: -0.6085\n",
      "Epoch 1715/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5146 - val_loss: -3.0568\n",
      "Epoch 1716/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1970 - val_loss: -4.2015\n",
      "Epoch 1717/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4480 - val_loss: -2.5582\n",
      "Epoch 1718/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3617 - val_loss: -4.3885\n",
      "Epoch 1719/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5568 - val_loss: -4.1417\n",
      "Epoch 1720/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5031 - val_loss: -4.2938\n",
      "Epoch 1721/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5321 - val_loss: -3.8205\n",
      "Epoch 1722/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4466 - val_loss: -4.2217\n",
      "Epoch 1723/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4790 - val_loss: -3.4694\n",
      "Epoch 1724/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5566 - val_loss: -3.6398\n",
      "Epoch 1725/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4489 - val_loss: -4.0028\n",
      "Epoch 1726/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4458 - val_loss: -1.7476\n",
      "Epoch 1727/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4442 - val_loss: -4.6216\n",
      "Epoch 1728/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4532 - val_loss: -3.4141\n",
      "Epoch 1729/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5371 - val_loss: -2.1652\n",
      "Epoch 1730/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5387 - val_loss: -3.8283\n",
      "Epoch 1731/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4234 - val_loss: -4.5711\n",
      "Epoch 1732/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4330 - val_loss: -2.5951\n",
      "Epoch 1733/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3054 - val_loss: -3.9388\n",
      "Epoch 1734/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5213 - val_loss: -4.3794\n",
      "Epoch 1735/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4967 - val_loss: -4.0749\n",
      "Epoch 1736/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4386 - val_loss: -3.9443\n",
      "Epoch 1737/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3909 - val_loss: 2.0972\n",
      "Epoch 1738/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8737 - val_loss: -4.0286\n",
      "Epoch 1739/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3912 - val_loss: -3.8478\n",
      "Epoch 1740/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4279 - val_loss: -3.5204\n",
      "Epoch 1741/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4684 - val_loss: -1.8472\n",
      "Epoch 1742/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4222 - val_loss: -4.0750\n",
      "Epoch 1743/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4194 - val_loss: -4.3910\n",
      "Epoch 1744/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4253 - val_loss: -3.9615\n",
      "Epoch 1745/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4847 - val_loss: -3.8408\n",
      "Epoch 1746/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4900 - val_loss: -1.7127\n",
      "Epoch 1747/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3932 - val_loss: -3.3189\n",
      "Epoch 1748/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4579 - val_loss: -4.1886\n",
      "Epoch 1749/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5097 - val_loss: -3.0447\n",
      "Epoch 1750/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4932 - val_loss: -3.4626\n",
      "Epoch 1751/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5142 - val_loss: -3.5826\n",
      "Epoch 1752/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4764 - val_loss: -3.8299\n",
      "Epoch 1753/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5215 - val_loss: -3.3936\n",
      "Epoch 1754/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5030 - val_loss: -1.2241\n",
      "Epoch 1755/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2650 - val_loss: -3.5081\n",
      "Epoch 1756/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4441 - val_loss: -3.8276\n",
      "Epoch 1757/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3975 - val_loss: -4.0598\n",
      "Epoch 1758/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3525 - val_loss: -4.5210\n",
      "Epoch 1759/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4761 - val_loss: -4.0645\n",
      "Epoch 1760/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4637 - val_loss: -4.2380\n",
      "Epoch 1761/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5053 - val_loss: -4.2483\n",
      "Epoch 1762/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5126 - val_loss: -3.5106\n",
      "Epoch 1763/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5210 - val_loss: -3.4290\n",
      "Epoch 1764/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4999 - val_loss: -3.2864\n",
      "Epoch 1765/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3507 - val_loss: -2.1255\n",
      "Epoch 1766/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4846 - val_loss: -2.8876\n",
      "Epoch 1767/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4962 - val_loss: -1.8518\n",
      "Epoch 1768/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5174 - val_loss: -3.5432\n",
      "Epoch 1769/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2733 - val_loss: -3.8669\n",
      "Epoch 1770/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5089 - val_loss: -3.9820\n",
      "Epoch 1771/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4947 - val_loss: -2.7792\n",
      "Epoch 1772/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5148 - val_loss: -0.7305\n",
      "Epoch 1773/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3778 - val_loss: -3.3707\n",
      "Epoch 1774/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4967 - val_loss: -3.9129\n",
      "Epoch 1775/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0193 - val_loss: -4.2132\n",
      "Epoch 1776/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4991 - val_loss: -3.5287\n",
      "Epoch 1777/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3518 - val_loss: -3.4199\n",
      "Epoch 1778/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5633 - val_loss: -4.0389\n",
      "Epoch 1779/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4700 - val_loss: -3.7279\n",
      "Epoch 1780/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5368 - val_loss: -3.6761\n",
      "Epoch 1781/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5472 - val_loss: -3.9008\n",
      "Epoch 1782/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4638 - val_loss: -4.4439\n",
      "Epoch 1783/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4829 - val_loss: -4.2009\n",
      "Epoch 1784/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4487 - val_loss: -3.8438\n",
      "Epoch 1785/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4043 - val_loss: -4.2598\n",
      "Epoch 1786/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4700 - val_loss: -2.8625\n",
      "Epoch 1787/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5323 - val_loss: -4.5228\n",
      "Epoch 1788/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5038 - val_loss: -2.2368\n",
      "Epoch 1789/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5095 - val_loss: -4.3235\n",
      "Epoch 1790/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5279 - val_loss: -3.9780\n",
      "Epoch 1791/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4711 - val_loss: -3.9145\n",
      "Epoch 1792/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5542 - val_loss: -3.9661\n",
      "Epoch 1793/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5343 - val_loss: -4.1579\n",
      "Epoch 1794/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2527 - val_loss: -0.5982\n",
      "Epoch 1795/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2476 - val_loss: -2.5657\n",
      "Epoch 1796/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.4090 - val_loss: -4.5594\n",
      "Epoch 1797/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4565 - val_loss: -4.2166\n",
      "Epoch 1798/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4529 - val_loss: -4.3762\n",
      "Epoch 1799/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3936 - val_loss: -4.0094\n",
      "Epoch 1800/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4604 - val_loss: -4.0355\n",
      "Epoch 1801/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5460 - val_loss: -2.8448\n",
      "Epoch 1802/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4067 - val_loss: -3.5208\n",
      "Epoch 1803/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5402 - val_loss: -0.7209\n",
      "Epoch 1804/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4320 - val_loss: -3.9557\n",
      "Epoch 1805/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5714 - val_loss: -3.3451\n",
      "Epoch 1806/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5160 - val_loss: -2.4592\n",
      "Epoch 1807/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.4559 - val_loss: -4.1757\n",
      "Epoch 1808/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8158 - val_loss: -2.8782\n",
      "Epoch 1809/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0464 - val_loss: -3.1436\n",
      "Epoch 1810/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0747 - val_loss: -3.5071\n",
      "Epoch 1811/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1721 - val_loss: -4.5251\n",
      "Epoch 1812/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2378 - val_loss: -3.6631\n",
      "Epoch 1813/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2040 - val_loss: -1.9673\n",
      "Epoch 1814/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2295 - val_loss: -3.8229\n",
      "Epoch 1815/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.3135 - val_loss: -4.3262\n",
      "Epoch 1816/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2097 - val_loss: -2.9320\n",
      "Epoch 1817/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3098 - val_loss: -3.9661\n",
      "Epoch 1818/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3478 - val_loss: -4.1928\n",
      "Epoch 1819/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3111 - val_loss: -4.0474\n",
      "Epoch 1820/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3615 - val_loss: -4.1629\n",
      "Epoch 1821/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3551 - val_loss: -4.1040\n",
      "Epoch 1822/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2754 - val_loss: -3.3040\n",
      "Epoch 1823/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3389 - val_loss: -3.9867\n",
      "Epoch 1824/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3500 - val_loss: -3.0394\n",
      "Epoch 1825/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4113 - val_loss: -4.3249\n",
      "Epoch 1826/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3928 - val_loss: -3.0734\n",
      "Epoch 1827/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4030 - val_loss: -3.9625\n",
      "Epoch 1828/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2815 - val_loss: -4.5168\n",
      "Epoch 1829/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1367 - val_loss: -4.5394\n",
      "Epoch 1830/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1456 - val_loss: -2.0821\n",
      "Epoch 1831/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3064 - val_loss: -3.8794\n",
      "Epoch 1832/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3007 - val_loss: -4.2563\n",
      "Epoch 1833/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2412 - val_loss: -4.4028\n",
      "Epoch 1834/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4005 - val_loss: -3.5832\n",
      "Epoch 1835/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2989 - val_loss: -4.4495\n",
      "Epoch 1836/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4910 - val_loss: -3.5907\n",
      "Epoch 1837/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4304 - val_loss: -1.1283\n",
      "Epoch 1838/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4132 - val_loss: -1.9596\n",
      "Epoch 1839/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4735 - val_loss: -2.7611\n",
      "Epoch 1840/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3309 - val_loss: -4.2095\n",
      "Epoch 1841/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4846 - val_loss: -4.3169\n",
      "Epoch 1842/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4006 - val_loss: -4.6356\n",
      "Epoch 1843/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4624 - val_loss: -0.8996\n",
      "Epoch 1844/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3830 - val_loss: -3.8926\n",
      "Epoch 1845/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0360 - val_loss: -3.9237\n",
      "Epoch 1846/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2575 - val_loss: -4.2180\n",
      "Epoch 1847/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4268 - val_loss: -3.7825\n",
      "Epoch 1848/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4517 - val_loss: -4.1733\n",
      "Epoch 1849/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4046 - val_loss: -2.5774\n",
      "Epoch 1850/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4478 - val_loss: -3.8952\n",
      "Epoch 1851/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3505 - val_loss: -3.7328\n",
      "Epoch 1852/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5355 - val_loss: -3.8441\n",
      "Epoch 1853/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4474 - val_loss: -4.0395\n",
      "Epoch 1854/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5167 - val_loss: -2.5279\n",
      "Epoch 1855/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4740 - val_loss: -3.6685\n",
      "Epoch 1856/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4827 - val_loss: -2.8241\n",
      "Epoch 1857/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3472 - val_loss: -3.8862\n",
      "Epoch 1858/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5268 - val_loss: -3.1457\n",
      "Epoch 1859/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5659 - val_loss: -3.5717\n",
      "Epoch 1860/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4793 - val_loss: -3.8842\n",
      "Epoch 1861/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2602 - val_loss: -4.5977\n",
      "Epoch 1862/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3059 - val_loss: -4.3874\n",
      "Epoch 1863/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4417 - val_loss: -3.5081\n",
      "Epoch 1864/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5063 - val_loss: -4.2249\n",
      "Epoch 1865/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3289 - val_loss: -3.0190\n",
      "Epoch 1866/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5324 - val_loss: -2.8108\n",
      "Epoch 1867/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4804 - val_loss: -3.9083\n",
      "Epoch 1868/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4292 - val_loss: -3.5485\n",
      "Epoch 1869/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4410 - val_loss: -4.4494\n",
      "Epoch 1870/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5118 - val_loss: -1.4342\n",
      "Epoch 1871/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5338 - val_loss: -2.4639\n",
      "Epoch 1872/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4472 - val_loss: -2.5101\n",
      "Epoch 1873/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4653 - val_loss: -3.2963\n",
      "Epoch 1874/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5260 - val_loss: -4.0151\n",
      "Epoch 1875/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4659 - val_loss: -4.4112\n",
      "Epoch 1876/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5482 - val_loss: -4.0337\n",
      "Epoch 1877/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3951 - val_loss: -3.5430\n",
      "Epoch 1878/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4832 - val_loss: -3.0720\n",
      "Epoch 1879/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4773 - val_loss: -4.4648\n",
      "Epoch 1880/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5297 - val_loss: -3.3431\n",
      "Epoch 1881/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4410 - val_loss: -3.2343\n",
      "Epoch 1882/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5134 - val_loss: -3.0216\n",
      "Epoch 1883/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5208 - val_loss: -1.0507\n",
      "Epoch 1884/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5186 - val_loss: -1.9623\n",
      "Epoch 1885/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4894 - val_loss: -3.9793\n",
      "Epoch 1886/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3913 - val_loss: -4.0318\n",
      "Epoch 1887/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0566 - val_loss: -4.3206\n",
      "Epoch 1888/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4039 - val_loss: -4.4472\n",
      "Epoch 1889/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3280 - val_loss: -4.4922\n",
      "Epoch 1890/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4617 - val_loss: -4.4018\n",
      "Epoch 1891/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5131 - val_loss: -3.4070\n",
      "Epoch 1892/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4652 - val_loss: -3.0908\n",
      "Epoch 1893/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4783 - val_loss: -4.4359\n",
      "Epoch 1894/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5354 - val_loss: -4.2584\n",
      "Epoch 1895/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8321 - val_loss: -3.9946\n",
      "Epoch 1896/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2655 - val_loss: -1.2553\n",
      "Epoch 1897/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3618 - val_loss: -2.9703\n",
      "Epoch 1898/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3646 - val_loss: 1.0543\n",
      "Epoch 1899/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4785 - val_loss: -3.9340\n",
      "Epoch 1900/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4455 - val_loss: -4.4471\n",
      "Epoch 1901/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4811 - val_loss: -2.4301\n",
      "Epoch 1902/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5338 - val_loss: -2.3091\n",
      "Epoch 1903/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3421 - val_loss: -2.9296\n",
      "Epoch 1904/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3749 - val_loss: -4.3788\n",
      "Epoch 1905/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4637 - val_loss: -3.4920\n",
      "Epoch 1906/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4214 - val_loss: -4.2469\n",
      "Epoch 1907/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4348 - val_loss: -3.2200\n",
      "Epoch 1908/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4910 - val_loss: -3.5875\n",
      "Epoch 1909/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4432 - val_loss: -2.2017\n",
      "Epoch 1910/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4533 - val_loss: -3.4948\n",
      "Epoch 1911/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5088 - val_loss: -2.3953\n",
      "Epoch 1912/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4626 - val_loss: -4.0639\n",
      "Epoch 1913/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5119 - val_loss: -4.5081\n",
      "Epoch 1914/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4957 - val_loss: -3.2729\n",
      "Epoch 1915/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4792 - val_loss: -4.0498\n",
      "Epoch 1916/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5717 - val_loss: -3.9235\n",
      "Epoch 1917/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4207 - val_loss: -3.7239\n",
      "Epoch 1918/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5836 - val_loss: -4.0505\n",
      "Epoch 1919/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4921 - val_loss: -3.0158\n",
      "Epoch 1920/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.5588 - val_loss: -4.0936\n",
      "Epoch 1921/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4381 - val_loss: -4.1211\n",
      "Epoch 1922/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5334 - val_loss: -3.7802\n",
      "Epoch 1923/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3857 - val_loss: -3.4378\n",
      "Epoch 1924/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5734 - val_loss: -3.2764\n",
      "Epoch 1925/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5028 - val_loss: -3.3082\n",
      "Epoch 1926/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5363 - val_loss: -3.2253\n",
      "Epoch 1927/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5172 - val_loss: -2.9735\n",
      "Epoch 1928/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5395 - val_loss: -0.2863\n",
      "Epoch 1929/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5206 - val_loss: -1.9091\n",
      "Epoch 1930/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4768 - val_loss: -2.4132\n",
      "Epoch 1931/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4381 - val_loss: -3.0158\n",
      "Epoch 1932/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4904 - val_loss: -3.3367\n",
      "Epoch 1933/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4701 - val_loss: -3.6784\n",
      "Epoch 1934/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5929 - val_loss: -2.6707\n",
      "Epoch 1935/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4165 - val_loss: -3.9020\n",
      "Epoch 1936/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5646 - val_loss: -4.0155\n",
      "Epoch 1937/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3350 - val_loss: -1.8330\n",
      "Epoch 1938/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5164 - val_loss: -2.9141\n",
      "Epoch 1939/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3675 - val_loss: -3.2654\n",
      "Epoch 1940/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5001 - val_loss: -4.1032\n",
      "Epoch 1941/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5208 - val_loss: -2.5614\n",
      "Epoch 1942/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4742 - val_loss: -3.7685\n",
      "Epoch 1943/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4688 - val_loss: -2.6819\n",
      "Epoch 1944/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3922 - val_loss: -2.4964\n",
      "Epoch 1945/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4899 - val_loss: -3.7334\n",
      "Epoch 1946/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5409 - val_loss: -3.7795\n",
      "Epoch 1947/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4457 - val_loss: -4.5459\n",
      "Epoch 1948/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5737 - val_loss: -3.8272\n",
      "Epoch 1949/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.5693 - val_loss: -3.6134\n",
      "Epoch 1950/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5385 - val_loss: -2.2140\n",
      "Epoch 1951/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3945 - val_loss: -3.2475\n",
      "Epoch 1952/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5866 - val_loss: -2.8637\n",
      "Epoch 1953/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5196 - val_loss: -4.4070\n",
      "Epoch 1954/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5698 - val_loss: -4.1494\n",
      "Epoch 1955/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5339 - val_loss: -3.8193\n",
      "Epoch 1956/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3132 - val_loss: -3.9297\n",
      "Epoch 1957/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5639 - val_loss: -4.1773\n",
      "Epoch 1958/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.6117 - val_loss: -3.7877\n",
      "Epoch 1959/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5494 - val_loss: -3.8697\n",
      "Epoch 1960/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4839 - val_loss: -4.4766\n",
      "Epoch 1961/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5710 - val_loss: -1.8184\n",
      "Epoch 1962/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5669 - val_loss: -3.7635\n",
      "Epoch 1963/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5831 - val_loss: -3.6075\n",
      "Epoch 1964/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5041 - val_loss: -2.5819\n",
      "Epoch 1965/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5606 - val_loss: -3.5232\n",
      "Epoch 1966/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5084 - val_loss: -4.0481\n",
      "Epoch 1967/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5720 - val_loss: -3.9970\n",
      "Epoch 1968/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5891 - val_loss: -3.7525\n",
      "Epoch 1969/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5806 - val_loss: -3.9611\n",
      "Epoch 1970/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9272 - val_loss: -3.7625\n",
      "Epoch 1971/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4552 - val_loss: -1.4386\n",
      "Epoch 1972/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4501 - val_loss: -2.6863\n",
      "Epoch 1973/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4738 - val_loss: -2.5159\n",
      "Epoch 1974/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5559 - val_loss: -3.7287\n",
      "Epoch 1975/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3844 - val_loss: -3.3145\n",
      "Epoch 1976/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5500 - val_loss: -2.8916\n",
      "Epoch 1977/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3831 - val_loss: -3.0560\n",
      "Epoch 1978/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5436 - val_loss: -3.5919\n",
      "Epoch 1979/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5468 - val_loss: 0.2353\n",
      "Epoch 1980/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4794 - val_loss: -3.9032\n",
      "Epoch 1981/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5427 - val_loss: -4.3892\n",
      "Epoch 1982/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5398 - val_loss: -1.2112\n",
      "Epoch 1983/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5694 - val_loss: -3.1810\n",
      "Epoch 1984/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5566 - val_loss: -3.2388\n",
      "Epoch 1985/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4277 - val_loss: -4.6820\n",
      "Epoch 1986/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5567 - val_loss: -3.6165\n",
      "Epoch 1987/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5463 - val_loss: -4.2742\n",
      "Epoch 1988/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.6152 - val_loss: -3.8468\n",
      "Epoch 1989/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5019 - val_loss: -4.1003\n",
      "Epoch 1990/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5348 - val_loss: -2.9987\n",
      "Epoch 1991/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5565 - val_loss: -3.3943\n",
      "Epoch 1992/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4748 - val_loss: -0.5236\n",
      "Epoch 1993/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5688 - val_loss: -4.0669\n",
      "Epoch 1994/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5656 - val_loss: -3.5999\n",
      "Epoch 1995/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5391 - val_loss: -1.3085\n",
      "Epoch 1996/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4056 - val_loss: -3.3012\n",
      "Epoch 1997/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4117 - val_loss: 4.0893\n",
      "Epoch 1998/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2193 - val_loss: -2.9401\n",
      "Epoch 1999/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4975 - val_loss: -3.5375\n",
      "Epoch 2000/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3389 - val_loss: -4.2529\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 7s 44ms/step - loss: -4.8215\n",
      "WARNING:tensorflow:Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 8s 44ms/step - loss: -5.2562\n",
      "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.0080"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 34s 251ms/step - loss: -1.0080 - val_loss: 0.3810\n",
      "Epoch 2/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -1.2541 - val_loss: 0.4393\n",
      "Epoch 3/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.2564 - val_loss: 0.5682\n",
      "Epoch 4/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -1.2579 - val_loss: 0.5759\n",
      "Epoch 5/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -1.2590 - val_loss: 0.4826\n",
      "Epoch 6/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -1.2604 - val_loss: 0.5303\n",
      "Epoch 7/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.2600"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -1.2600 - val_loss: 0.3286\n",
      "Epoch 8/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -1.2609 - val_loss: 0.5049\n",
      "Epoch 9/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.2607 - val_loss: 0.6761\n",
      "Epoch 10/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -1.2647 - val_loss: 0.4618\n",
      "Epoch 11/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.7370"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -1.7370 - val_loss: -1.4211\n",
      "Epoch 12/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.7591"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -2.7591 - val_loss: -1.6828\n",
      "Epoch 13/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.2283"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 243ms/step - loss: -3.2283 - val_loss: -3.1452\n",
      "Epoch 14/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.5669 - val_loss: -2.3180\n",
      "Epoch 15/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.6219 - val_loss: -2.8606\n",
      "Epoch 16/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.7450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -3.7450 - val_loss: -3.4692\n",
      "Epoch 17/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.8886 - val_loss: -3.0646\n",
      "Epoch 18/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.9564"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -3.9564 - val_loss: -3.5699\n",
      "Epoch 19/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.0231 - val_loss: -1.3203\n",
      "Epoch 20/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0048 - val_loss: -3.4410\n",
      "Epoch 21/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.1167"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 242ms/step - loss: -4.1167 - val_loss: -3.6078\n",
      "Epoch 22/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1121 - val_loss: -3.5515\n",
      "Epoch 23/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -3.9857 - val_loss: -2.0916\n",
      "Epoch 24/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0571 - val_loss: -3.1959\n",
      "Epoch 25/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1911 - val_loss: -2.5624\n",
      "Epoch 26/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.1782"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -4.1782 - val_loss: -3.7690\n",
      "Epoch 27/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2749 - val_loss: -3.5725\n",
      "Epoch 28/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.1751"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -4.1751 - val_loss: -3.7803\n",
      "Epoch 29/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.2870 - val_loss: -2.5497\n",
      "Epoch 30/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2567 - val_loss: -2.9088\n",
      "Epoch 31/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3346 - val_loss: -3.7465\n",
      "Epoch 32/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3850 - val_loss: -3.4501\n",
      "Epoch 33/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.3305"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -4.3305 - val_loss: -3.9183\n",
      "Epoch 34/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.1186 - val_loss: -3.0904\n",
      "Epoch 35/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.1547 - val_loss: -3.7006\n",
      "Epoch 36/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.3498"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -4.3498 - val_loss: -3.9803\n",
      "Epoch 37/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.3988"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -4.3988 - val_loss: -4.0127\n",
      "Epoch 38/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4099 - val_loss: -3.6294\n",
      "Epoch 39/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4002"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -4.4002 - val_loss: -4.1623\n",
      "Epoch 40/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4381 - val_loss: -4.0855\n",
      "Epoch 41/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4070 - val_loss: -3.7994\n",
      "Epoch 42/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3775 - val_loss: 5.2892\n",
      "Epoch 43/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.5431 - val_loss: -3.7204\n",
      "Epoch 44/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2775 - val_loss: -3.9143\n",
      "Epoch 45/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4164 - val_loss: -3.8996\n",
      "Epoch 46/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4134 - val_loss: -3.5658\n",
      "Epoch 47/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4631"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -4.4631 - val_loss: -4.1740\n",
      "Epoch 48/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4042 - val_loss: -3.9858\n",
      "Epoch 49/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4927 - val_loss: -4.0300\n",
      "Epoch 50/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2790 - val_loss: -3.8922\n",
      "Epoch 51/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5385 - val_loss: -4.1556\n",
      "Epoch 52/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4829 - val_loss: -3.9384\n",
      "Epoch 53/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4680 - val_loss: -3.4091\n",
      "Epoch 54/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1569 - val_loss: -3.6509\n",
      "Epoch 55/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.3357 - val_loss: -3.7268\n",
      "Epoch 56/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4943 - val_loss: -3.0235\n",
      "Epoch 57/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5120"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -4.5120 - val_loss: -4.1948\n",
      "Epoch 58/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5079 - val_loss: -3.7368\n",
      "Epoch 59/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5705 - val_loss: -3.8106\n",
      "Epoch 60/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5350 - val_loss: -3.9825\n",
      "Epoch 61/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5047 - val_loss: -3.8860\n",
      "Epoch 62/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5172 - val_loss: -3.6781\n",
      "Epoch 63/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5153 - val_loss: -3.9629\n",
      "Epoch 64/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5707 - val_loss: -4.1695\n",
      "Epoch 65/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5132 - val_loss: -3.8353\n",
      "Epoch 66/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5217 - val_loss: -4.0560\n",
      "Epoch 67/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5810 - val_loss: -3.7146\n",
      "Epoch 68/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6331 - val_loss: -4.1699\n",
      "Epoch 69/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5770 - val_loss: -3.5997\n",
      "Epoch 70/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5394 - val_loss: -3.1149\n",
      "Epoch 71/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5816 - val_loss: -3.7060\n",
      "Epoch 72/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3505 - val_loss: -3.9638\n",
      "Epoch 73/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6209 - val_loss: -3.8637\n",
      "Epoch 74/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5142 - val_loss: -4.0621\n",
      "Epoch 75/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5931 - val_loss: -4.1356\n",
      "Epoch 76/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6678"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -4.6678 - val_loss: -4.2501\n",
      "Epoch 77/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -4.5135 - val_loss: -3.9922\n",
      "Epoch 78/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6854 - val_loss: -4.1350\n",
      "Epoch 79/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6287 - val_loss: -4.0711\n",
      "Epoch 80/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5916 - val_loss: -2.0849\n",
      "Epoch 81/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6788 - val_loss: -3.6117\n",
      "Epoch 82/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5675 - val_loss: -3.6015\n",
      "Epoch 83/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5720 - val_loss: -4.1403\n",
      "Epoch 84/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6334 - val_loss: -4.2470\n",
      "Epoch 85/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5584 - val_loss: -4.1777\n",
      "Epoch 86/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7056 - val_loss: -2.8091\n",
      "Epoch 87/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.6486 - val_loss: -3.5987\n",
      "Epoch 88/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6372 - val_loss: -3.3524\n",
      "Epoch 89/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6680 - val_loss: -3.7174\n",
      "Epoch 90/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.9853 - val_loss: -3.0210\n",
      "Epoch 91/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.8235 - val_loss: -3.5841\n",
      "Epoch 92/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1300 - val_loss: -3.6096\n",
      "Epoch 93/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.3205 - val_loss: -3.2700\n",
      "Epoch 94/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3396 - val_loss: -3.7127\n",
      "Epoch 95/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4862 - val_loss: -3.7352\n",
      "Epoch 96/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0994 - val_loss: -3.3156\n",
      "Epoch 97/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3993 - val_loss: -3.6373\n",
      "Epoch 98/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5258 - val_loss: -3.6237\n",
      "Epoch 99/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5649 - val_loss: -4.0111\n",
      "Epoch 100/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5616 - val_loss: -4.1054\n",
      "Epoch 101/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4651 - val_loss: -3.1758\n",
      "Epoch 102/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5138 - val_loss: -4.1076\n",
      "Epoch 103/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.4894 - val_loss: -3.8727\n",
      "Epoch 104/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6264 - val_loss: -3.5981\n",
      "Epoch 105/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.7569 - val_loss: -3.9312\n",
      "Epoch 106/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2264 - val_loss: -4.2239\n",
      "Epoch 107/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.2951 - val_loss: -3.9519\n",
      "Epoch 108/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5611 - val_loss: -2.6877\n",
      "Epoch 109/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5300 - val_loss: -4.0429\n",
      "Epoch 110/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6206"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -4.6206 - val_loss: -4.3269\n",
      "Epoch 111/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5768 - val_loss: -3.5927\n",
      "Epoch 112/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5619 - val_loss: -4.1164\n",
      "Epoch 113/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5881 - val_loss: -3.7430\n",
      "Epoch 114/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7165 - val_loss: -4.0256\n",
      "Epoch 115/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6655 - val_loss: -3.0069\n",
      "Epoch 116/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6765 - val_loss: -4.1812\n",
      "Epoch 117/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6264 - val_loss: -3.9670\n",
      "Epoch 118/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6616 - val_loss: -4.1329\n",
      "Epoch 119/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7051 - val_loss: -3.6024\n",
      "Epoch 120/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4636 - val_loss: -4.1320\n",
      "Epoch 121/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4696 - val_loss: -3.8010\n",
      "Epoch 122/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6835 - val_loss: -3.7064\n",
      "Epoch 123/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -3.9042 - val_loss: -3.0486\n",
      "Epoch 124/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1922 - val_loss: -3.6452\n",
      "Epoch 125/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.2189 - val_loss: -2.8821\n",
      "Epoch 126/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.5934 - val_loss: -3.9391\n",
      "Epoch 127/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2236 - val_loss: -3.7482\n",
      "Epoch 128/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4797 - val_loss: -3.8735\n",
      "Epoch 129/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5598 - val_loss: -3.8199\n",
      "Epoch 130/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.6438 - val_loss: -3.8420\n",
      "Epoch 131/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4968 - val_loss: -3.8008\n",
      "Epoch 132/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6688 - val_loss: -4.0346\n",
      "Epoch 133/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.1467 - val_loss: -3.4043\n",
      "Epoch 134/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4681 - val_loss: -3.7197\n",
      "Epoch 135/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5920 - val_loss: -3.9382\n",
      "Epoch 136/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5757 - val_loss: -4.0266\n",
      "Epoch 137/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6772 - val_loss: -4.0702\n",
      "Epoch 138/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1287 - val_loss: -3.7392\n",
      "Epoch 139/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4318 - val_loss: -4.1391\n",
      "Epoch 140/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6325 - val_loss: -4.1079\n",
      "Epoch 141/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7088 - val_loss: -4.1378\n",
      "Epoch 142/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6249 - val_loss: -4.1620\n",
      "Epoch 143/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7523 - val_loss: -3.3728\n",
      "Epoch 144/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5875 - val_loss: -3.8885\n",
      "Epoch 145/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6502 - val_loss: -3.5830\n",
      "Epoch 146/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6661 - val_loss: -3.7973\n",
      "Epoch 147/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6684 - val_loss: -4.1688\n",
      "Epoch 148/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6612"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -4.6612 - val_loss: -4.3315\n",
      "Epoch 149/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7237 - val_loss: -3.9622\n",
      "Epoch 150/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7473 - val_loss: -4.1009\n",
      "Epoch 151/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7655 - val_loss: -2.6880\n",
      "Epoch 152/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7077 - val_loss: -3.8827\n",
      "Epoch 153/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7880 - val_loss: -3.7363\n",
      "Epoch 154/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7909 - val_loss: -4.2336\n",
      "Epoch 155/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7292 - val_loss: -3.9057\n",
      "Epoch 156/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.6679 - val_loss: -3.7799\n",
      "Epoch 157/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2591 - val_loss: -3.9028\n",
      "Epoch 158/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4338 - val_loss: -4.1006\n",
      "Epoch 159/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5129 - val_loss: -3.9554\n",
      "Epoch 160/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5357 - val_loss: 1.7326\n",
      "Epoch 161/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5812 - val_loss: -3.7454\n",
      "Epoch 162/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3484 - val_loss: -3.6303\n",
      "Epoch 163/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5921 - val_loss: -3.8703\n",
      "Epoch 164/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5899 - val_loss: -3.3825\n",
      "Epoch 165/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.6366 - val_loss: -2.3701\n",
      "Epoch 166/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6184 - val_loss: -4.0920\n",
      "Epoch 167/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5964 - val_loss: -3.7272\n",
      "Epoch 168/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7308 - val_loss: -4.3112\n",
      "Epoch 169/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4812 - val_loss: -4.0638\n",
      "Epoch 170/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7425 - val_loss: -3.0573\n",
      "Epoch 171/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6671 - val_loss: -4.2922\n",
      "Epoch 172/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.2615 - val_loss: -3.4028\n",
      "Epoch 173/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.0539 - val_loss: -3.8631\n",
      "Epoch 174/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.4760 - val_loss: -4.1004\n",
      "Epoch 175/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5254 - val_loss: -4.1138\n",
      "Epoch 176/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6119 - val_loss: -4.1340\n",
      "Epoch 177/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3300 - val_loss: -4.0188\n",
      "Epoch 178/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6381 - val_loss: -3.8149\n",
      "Epoch 179/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7143 - val_loss: -3.9958\n",
      "Epoch 180/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6511 - val_loss: -3.4954\n",
      "Epoch 181/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6925 - val_loss: -1.1056\n",
      "Epoch 182/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6432 - val_loss: -4.2662\n",
      "Epoch 183/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7638 - val_loss: -4.0978\n",
      "Epoch 184/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7316 - val_loss: -4.1665\n",
      "Epoch 185/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7484 - val_loss: -3.8757\n",
      "Epoch 186/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8018 - val_loss: -4.3045\n",
      "Epoch 187/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7360 - val_loss: -3.1053\n",
      "Epoch 188/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7428 - val_loss: -3.3374\n",
      "Epoch 189/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1020 - val_loss: -4.1363\n",
      "Epoch 190/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5728 - val_loss: -4.1994\n",
      "Epoch 191/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7074 - val_loss: -3.9763\n",
      "Epoch 192/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5808 - val_loss: -4.2306\n",
      "Epoch 193/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7338"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -4.7338 - val_loss: -4.3479\n",
      "Epoch 194/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7441 - val_loss: -4.1001\n",
      "Epoch 195/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7544 - val_loss: -4.0148\n",
      "Epoch 196/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6000 - val_loss: -4.0233\n",
      "Epoch 197/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7137 - val_loss: -4.1267\n",
      "Epoch 198/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7764 - val_loss: -2.3783\n",
      "Epoch 199/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7300"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 242ms/step - loss: -4.7300 - val_loss: -4.4445\n",
      "Epoch 200/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.4886 - val_loss: -3.7132\n",
      "Epoch 201/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8363 - val_loss: -4.2545\n",
      "Epoch 202/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7535 - val_loss: -3.7062\n",
      "Epoch 203/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8027 - val_loss: -4.0613\n",
      "Epoch 204/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8922 - val_loss: -3.6746\n",
      "Epoch 205/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.4566 - val_loss: -3.8397\n",
      "Epoch 206/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4427 - val_loss: -3.9120\n",
      "Epoch 207/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6187 - val_loss: -4.1395\n",
      "Epoch 208/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6010 - val_loss: -4.2759\n",
      "Epoch 209/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6928 - val_loss: -3.8373\n",
      "Epoch 210/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7497 - val_loss: -4.2343\n",
      "Epoch 211/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7235 - val_loss: -4.2560\n",
      "Epoch 212/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7297 - val_loss: -4.0023\n",
      "Epoch 213/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6908 - val_loss: -3.9354\n",
      "Epoch 214/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6870 - val_loss: -4.3457\n",
      "Epoch 215/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7992 - val_loss: -4.2122\n",
      "Epoch 216/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7455 - val_loss: -3.7142\n",
      "Epoch 217/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7318 - val_loss: -4.1736\n",
      "Epoch 218/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5762 - val_loss: -4.1981\n",
      "Epoch 219/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8479 - val_loss: -3.8792\n",
      "Epoch 220/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7665 - val_loss: -3.6912\n",
      "Epoch 221/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7265 - val_loss: -3.7396\n",
      "Epoch 222/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8118 - val_loss: -4.1958\n",
      "Epoch 223/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7024 - val_loss: -3.6247\n",
      "Epoch 224/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7766 - val_loss: -3.1975\n",
      "Epoch 225/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8040 - val_loss: -4.1852\n",
      "Epoch 226/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7963 - val_loss: -4.3937\n",
      "Epoch 227/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8053 - val_loss: -3.9937\n",
      "Epoch 228/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8599 - val_loss: -3.7092\n",
      "Epoch 229/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7775 - val_loss: -2.2702\n",
      "Epoch 230/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7869 - val_loss: -4.2833\n",
      "Epoch 231/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8833 - val_loss: -3.9680\n",
      "Epoch 232/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8743 - val_loss: -4.1334\n",
      "Epoch 233/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7655 - val_loss: -2.5027\n",
      "Epoch 234/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2359 - val_loss: -3.9185\n",
      "Epoch 235/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7528 - val_loss: -4.0863\n",
      "Epoch 236/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8979 - val_loss: -4.0956\n",
      "Epoch 237/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8374 - val_loss: -3.2407\n",
      "Epoch 238/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8803 - val_loss: -4.0610\n",
      "Epoch 239/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3806 - val_loss: -4.2367\n",
      "Epoch 240/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8015 - val_loss: -4.0736\n",
      "Epoch 241/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7804 - val_loss: -4.0114\n",
      "Epoch 242/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8748 - val_loss: -4.1033\n",
      "Epoch 243/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7953 - val_loss: -3.4947\n",
      "Epoch 244/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7218 - val_loss: -4.0494\n",
      "Epoch 245/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8282 - val_loss: -4.0283\n",
      "Epoch 246/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8926 - val_loss: -3.9704\n",
      "Epoch 247/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8703 - val_loss: -4.2010\n",
      "Epoch 248/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7912 - val_loss: -3.9595\n",
      "Epoch 249/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8454 - val_loss: -4.0646\n",
      "Epoch 250/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9121 - val_loss: -4.1421\n",
      "Epoch 251/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -4.8030 - val_loss: -4.5141\n",
      "Epoch 252/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7512 - val_loss: -2.8647\n",
      "Epoch 253/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9172 - val_loss: -4.4413\n",
      "Epoch 254/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6585 - val_loss: -3.7318\n",
      "Epoch 255/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9957 - val_loss: -4.0280\n",
      "Epoch 256/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9113 - val_loss: -4.5030\n",
      "Epoch 257/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8633 - val_loss: -0.5585\n",
      "Epoch 258/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8456 - val_loss: -4.3788\n",
      "Epoch 259/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9018 - val_loss: -3.5706\n",
      "Epoch 260/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8277 - val_loss: -4.1788\n",
      "Epoch 261/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9706 - val_loss: -3.8105\n",
      "Epoch 262/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8464 - val_loss: -4.3935\n",
      "Epoch 263/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8637 - val_loss: -3.1267\n",
      "Epoch 264/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3452 - val_loss: -4.0241\n",
      "Epoch 265/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7911 - val_loss: -3.9738\n",
      "Epoch 266/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8727 - val_loss: -4.1187\n",
      "Epoch 267/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9295 - val_loss: -3.9850\n",
      "Epoch 268/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8567 - val_loss: -3.9680\n",
      "Epoch 269/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8943 - val_loss: -3.8223\n",
      "Epoch 270/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8833 - val_loss: -4.0816\n",
      "Epoch 271/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8959 - val_loss: -3.5546\n",
      "Epoch 272/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8025 - val_loss: -4.1812\n",
      "Epoch 273/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9498 - val_loss: -4.0402\n",
      "Epoch 274/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8987 - val_loss: -4.3545\n",
      "Epoch 275/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9268 - val_loss: -4.1746\n",
      "Epoch 276/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9152 - val_loss: -2.6267\n",
      "Epoch 277/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8815 - val_loss: -4.1373\n",
      "Epoch 278/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9430 - val_loss: -3.8696\n",
      "Epoch 279/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8869 - val_loss: -4.2123\n",
      "Epoch 280/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9164 - val_loss: -3.8056\n",
      "Epoch 281/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9145 - val_loss: -3.9971\n",
      "Epoch 282/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6219 - val_loss: -1.3383\n",
      "Epoch 283/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4588 - val_loss: -1.8670\n",
      "Epoch 284/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8589 - val_loss: -3.7146\n",
      "Epoch 285/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8700 - val_loss: -3.8406\n",
      "Epoch 286/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9478 - val_loss: -2.8442\n",
      "Epoch 287/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9056 - val_loss: -3.3069\n",
      "Epoch 288/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7589 - val_loss: -4.3947\n",
      "Epoch 289/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9918 - val_loss: -3.8482\n",
      "Epoch 290/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8858 - val_loss: -4.1122\n",
      "Epoch 291/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9536 - val_loss: -4.1330\n",
      "Epoch 292/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9391 - val_loss: -4.0042\n",
      "Epoch 293/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9636 - val_loss: -4.0132\n",
      "Epoch 294/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8625 - val_loss: -3.9393\n",
      "Epoch 295/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8919 - val_loss: -4.2944\n",
      "Epoch 296/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9844 - val_loss: -4.1571\n",
      "Epoch 297/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8985 - val_loss: -4.1445\n",
      "Epoch 298/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8569 - val_loss: -3.8865\n",
      "Epoch 299/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8276 - val_loss: -1.8845\n",
      "Epoch 300/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.4847 - val_loss: -3.8628\n",
      "Epoch 301/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9115 - val_loss: -3.2108\n",
      "Epoch 302/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9997 - val_loss: -1.2403\n",
      "Epoch 303/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8605 - val_loss: -4.0555\n",
      "Epoch 304/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8560 - val_loss: -4.3293\n",
      "Epoch 305/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9436 - val_loss: -3.6051\n",
      "Epoch 306/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0096 - val_loss: -4.3675\n",
      "Epoch 307/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8633 - val_loss: -3.1438\n",
      "Epoch 308/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9999 - val_loss: -3.6596\n",
      "Epoch 309/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9840 - val_loss: -4.2326\n",
      "Epoch 310/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8264 - val_loss: -3.6370\n",
      "Epoch 311/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0146 - val_loss: -4.0850\n",
      "Epoch 312/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9332 - val_loss: -4.4414\n",
      "Epoch 313/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9301 - val_loss: -4.0159\n",
      "Epoch 314/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0164 - val_loss: -4.2460\n",
      "Epoch 315/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7494 - val_loss: -3.9106\n",
      "Epoch 316/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8464 - val_loss: -3.9749\n",
      "Epoch 317/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9435 - val_loss: -2.4085\n",
      "Epoch 318/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9970 - val_loss: -4.2663\n",
      "Epoch 319/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9487 - val_loss: -3.6043\n",
      "Epoch 320/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9159 - val_loss: -4.4332\n",
      "Epoch 321/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9633 - val_loss: -4.4354\n",
      "Epoch 322/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9168 - val_loss: -3.9596\n",
      "Epoch 323/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9001 - val_loss: -4.2996\n",
      "Epoch 324/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9717 - val_loss: -2.9152\n",
      "Epoch 325/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9117 - val_loss: -3.9956\n",
      "Epoch 326/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9989 - val_loss: -3.4110\n",
      "Epoch 327/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0080 - val_loss: -4.1934\n",
      "Epoch 328/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9599 - val_loss: -3.1814\n",
      "Epoch 329/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9693"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -4.9693 - val_loss: -4.5212\n",
      "Epoch 330/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0633 - val_loss: -4.3309\n",
      "Epoch 331/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8921 - val_loss: -3.8053\n",
      "Epoch 332/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0538 - val_loss: -4.2957\n",
      "Epoch 333/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4951 - val_loss: -4.0133\n",
      "Epoch 334/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9760 - val_loss: -4.1207\n",
      "Epoch 335/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9592 - val_loss: -4.0446\n",
      "Epoch 336/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9402 - val_loss: -3.6493\n",
      "Epoch 337/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9126 - val_loss: -3.0427\n",
      "Epoch 338/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0534 - val_loss: -3.5371\n",
      "Epoch 339/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9721 - val_loss: -4.0731\n",
      "Epoch 340/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4949 - val_loss: -3.7775\n",
      "Epoch 341/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0177 - val_loss: -4.0309\n",
      "Epoch 342/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9319 - val_loss: -4.3811\n",
      "Epoch 343/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8871 - val_loss: -4.2161\n",
      "Epoch 344/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0308 - val_loss: -4.3986\n",
      "Epoch 345/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9819 - val_loss: -4.2394\n",
      "Epoch 346/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9921 - val_loss: -3.9595\n",
      "Epoch 347/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0382 - val_loss: -4.0539\n",
      "Epoch 348/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0000 - val_loss: -4.4719\n",
      "Epoch 349/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7996 - val_loss: -3.2739\n",
      "Epoch 350/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9496 - val_loss: -3.4885\n",
      "Epoch 351/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0182 - val_loss: -4.4337\n",
      "Epoch 352/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9391 - val_loss: -3.1628\n",
      "Epoch 353/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0333 - val_loss: 3.6418\n",
      "Epoch 354/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9817 - val_loss: -4.2111\n",
      "Epoch 355/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9758 - val_loss: -3.3928\n",
      "Epoch 356/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0041 - val_loss: -3.9631\n",
      "Epoch 357/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0546 - val_loss: -4.1952\n",
      "Epoch 358/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9698 - val_loss: -4.1722\n",
      "Epoch 359/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0169 - val_loss: -4.1091\n",
      "Epoch 360/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9800 - val_loss: -3.8654\n",
      "Epoch 361/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0754 - val_loss: -4.1781\n",
      "Epoch 362/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0707 - val_loss: -3.3865\n",
      "Epoch 363/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7744 - val_loss: -4.1393\n",
      "Epoch 364/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7939 - val_loss: -4.0157\n",
      "Epoch 365/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0939 - val_loss: -3.0692\n",
      "Epoch 366/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0397 - val_loss: -3.9567\n",
      "Epoch 367/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0405 - val_loss: -4.0103\n",
      "Epoch 368/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7639"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.7639 - val_loss: -4.6011\n",
      "Epoch 369/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9981 - val_loss: -4.5303\n",
      "Epoch 370/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8692 - val_loss: -2.3600\n",
      "Epoch 371/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9511 - val_loss: -4.0632\n",
      "Epoch 372/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0578 - val_loss: -2.6447\n",
      "Epoch 373/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0041 - val_loss: -4.0686\n",
      "Epoch 374/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9947 - val_loss: -3.4033\n",
      "Epoch 375/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9944 - val_loss: -3.8479\n",
      "Epoch 376/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6581 - val_loss: -4.0886\n",
      "Epoch 377/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0211 - val_loss: -4.4153\n",
      "Epoch 378/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9560 - val_loss: -1.2012\n",
      "Epoch 379/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0237 - val_loss: -3.8173\n",
      "Epoch 380/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0694 - val_loss: -3.7682\n",
      "Epoch 381/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1006 - val_loss: -3.9313\n",
      "Epoch 382/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9118 - val_loss: -4.3458\n",
      "Epoch 383/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0378 - val_loss: -3.7588\n",
      "Epoch 384/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8371 - val_loss: -3.6373\n",
      "Epoch 385/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9294 - val_loss: -4.1117\n",
      "Epoch 386/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0769 - val_loss: -3.7830\n",
      "Epoch 387/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0526 - val_loss: -3.5145\n",
      "Epoch 388/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9379 - val_loss: -4.4404\n",
      "Epoch 389/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0884 - val_loss: -3.9661\n",
      "Epoch 390/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0495 - val_loss: -4.1400\n",
      "Epoch 391/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9565 - val_loss: -2.7437\n",
      "Epoch 392/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9692 - val_loss: -4.1852\n",
      "Epoch 393/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9991 - val_loss: -4.2533\n",
      "Epoch 394/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1416 - val_loss: -3.9741\n",
      "Epoch 395/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2350 - val_loss: -3.9604\n",
      "Epoch 396/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7924 - val_loss: -4.0391\n",
      "Epoch 397/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9332 - val_loss: -3.8695\n",
      "Epoch 398/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9375 - val_loss: -4.0347\n",
      "Epoch 399/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8839 - val_loss: -4.2919\n",
      "Epoch 400/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0131 - val_loss: -3.7963\n",
      "Epoch 401/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8862 - val_loss: -4.2890\n",
      "Epoch 402/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0163 - val_loss: -3.6333\n",
      "Epoch 403/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9596 - val_loss: -3.7436\n",
      "Epoch 404/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9977 - val_loss: -4.2372\n",
      "Epoch 405/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0948 - val_loss: -3.5759\n",
      "Epoch 406/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9688 - val_loss: -3.2898\n",
      "Epoch 407/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9453 - val_loss: -4.4796\n",
      "Epoch 408/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1420 - val_loss: -3.9923\n",
      "Epoch 409/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0317 - val_loss: -4.4229\n",
      "Epoch 410/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.7774 - val_loss: -2.1711\n",
      "Epoch 411/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.5217 - val_loss: -2.9066\n",
      "Epoch 412/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3617 - val_loss: -3.7398\n",
      "Epoch 413/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5304 - val_loss: -2.7336\n",
      "Epoch 414/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6486 - val_loss: -4.0988\n",
      "Epoch 415/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6964 - val_loss: -3.7234\n",
      "Epoch 416/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6351 - val_loss: -2.9125\n",
      "Epoch 417/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7050 - val_loss: -3.8700\n",
      "Epoch 418/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7757 - val_loss: -3.7630\n",
      "Epoch 419/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7645 - val_loss: -4.0334\n",
      "Epoch 420/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8602 - val_loss: -4.2312\n",
      "Epoch 421/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.0636 - val_loss: -3.7323\n",
      "Epoch 422/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6872 - val_loss: -4.1935\n",
      "Epoch 423/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7881 - val_loss: -4.2921\n",
      "Epoch 424/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4871 - val_loss: -3.3433\n",
      "Epoch 425/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3957 - val_loss: -4.2007\n",
      "Epoch 426/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7409 - val_loss: -4.1368\n",
      "Epoch 427/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8394 - val_loss: -3.9386\n",
      "Epoch 428/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8210 - val_loss: -4.0882\n",
      "Epoch 429/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9160 - val_loss: -4.2127\n",
      "Epoch 430/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8192 - val_loss: -4.3128\n",
      "Epoch 431/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9030 - val_loss: -4.1998\n",
      "Epoch 432/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8670 - val_loss: -4.3831\n",
      "Epoch 433/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8744 - val_loss: -4.0571\n",
      "Epoch 434/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9419 - val_loss: -4.1953\n",
      "Epoch 435/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8800 - val_loss: -3.2355\n",
      "Epoch 436/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9071 - val_loss: -4.1948\n",
      "Epoch 437/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9712 - val_loss: -4.3083\n",
      "Epoch 438/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9855 - val_loss: -4.3145\n",
      "Epoch 439/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9405 - val_loss: -3.7876\n",
      "Epoch 440/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7691 - val_loss: -3.9198\n",
      "Epoch 441/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9278 - val_loss: -3.8668\n",
      "Epoch 442/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9276 - val_loss: -4.2683\n",
      "Epoch 443/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9820 - val_loss: -3.1385\n",
      "Epoch 444/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9872 - val_loss: -3.1607\n",
      "Epoch 445/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9933 - val_loss: -4.4098\n",
      "Epoch 446/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9641 - val_loss: -2.5936\n",
      "Epoch 447/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9845 - val_loss: -0.0429\n",
      "Epoch 448/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8065 - val_loss: -4.1008\n",
      "Epoch 449/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9566 - val_loss: -4.1465\n",
      "Epoch 450/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8959 - val_loss: -4.0839\n",
      "Epoch 451/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0640 - val_loss: -4.4425\n",
      "Epoch 452/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9280 - val_loss: -4.2631\n",
      "Epoch 453/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9554 - val_loss: -2.4638\n",
      "Epoch 454/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9779 - val_loss: -4.1897\n",
      "Epoch 455/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9861 - val_loss: -4.2582\n",
      "Epoch 456/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0410 - val_loss: -4.4095\n",
      "Epoch 457/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9826 - val_loss: -4.2356\n",
      "Epoch 458/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0626 - val_loss: -3.1104\n",
      "Epoch 459/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9926 - val_loss: -3.9686\n",
      "Epoch 460/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8993 - val_loss: -4.1550\n",
      "Epoch 461/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9044 - val_loss: -4.0412\n",
      "Epoch 462/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0038 - val_loss: -4.3955\n",
      "Epoch 463/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0631 - val_loss: -3.2466\n",
      "Epoch 464/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9726 - val_loss: -4.1183\n",
      "Epoch 465/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0682 - val_loss: -3.3040\n",
      "Epoch 466/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4880 - val_loss: -4.1174\n",
      "Epoch 467/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0218 - val_loss: -4.2679\n",
      "Epoch 468/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9910 - val_loss: -4.3333\n",
      "Epoch 469/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0284 - val_loss: -3.9006\n",
      "Epoch 470/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0085 - val_loss: -4.1938\n",
      "Epoch 471/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0737 - val_loss: -4.2539\n",
      "Epoch 472/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0269 - val_loss: -4.1727\n",
      "Epoch 473/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0317 - val_loss: -3.7660\n",
      "Epoch 474/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0006 - val_loss: -4.2850\n",
      "Epoch 475/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0407 - val_loss: -4.1528\n",
      "Epoch 476/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0370 - val_loss: -4.3623\n",
      "Epoch 477/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0819 - val_loss: -3.4686\n",
      "Epoch 478/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8983 - val_loss: -4.4203\n",
      "Epoch 479/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1121 - val_loss: -4.2633\n",
      "Epoch 480/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9887 - val_loss: -4.5396\n",
      "Epoch 481/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0337 - val_loss: -4.0453\n",
      "Epoch 482/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0853 - val_loss: -3.6818\n",
      "Epoch 483/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9988 - val_loss: -3.7877\n",
      "Epoch 484/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9663 - val_loss: -4.5500\n",
      "Epoch 485/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0914 - val_loss: -4.2104\n",
      "Epoch 486/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0163 - val_loss: -3.5291\n",
      "Epoch 487/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9632 - val_loss: -2.4759\n",
      "Epoch 488/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0593 - val_loss: -1.8947\n",
      "Epoch 489/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0657 - val_loss: -3.8360\n",
      "Epoch 490/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0127 - val_loss: -4.4829\n",
      "Epoch 491/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0235 - val_loss: -4.3609\n",
      "Epoch 492/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1409 - val_loss: -3.8915\n",
      "Epoch 493/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0398 - val_loss: -4.3336\n",
      "Epoch 494/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0935 - val_loss: -4.3684\n",
      "Epoch 495/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0258 - val_loss: -4.1470\n",
      "Epoch 496/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0955 - val_loss: -3.1092\n",
      "Epoch 497/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0244 - val_loss: -3.4858\n",
      "Epoch 498/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0559 - val_loss: -4.0952\n",
      "Epoch 499/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0049 - val_loss: -4.4938\n",
      "Epoch 500/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0888 - val_loss: -4.4126\n",
      "Epoch 501/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0058 - val_loss: 2.6655\n",
      "Epoch 502/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5430 - val_loss: -4.2182\n",
      "Epoch 503/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9018 - val_loss: -4.3489\n",
      "Epoch 504/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0029 - val_loss: -4.3759\n",
      "Epoch 505/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1150 - val_loss: -1.7449\n",
      "Epoch 506/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0109 - val_loss: -2.4122\n",
      "Epoch 507/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0942 - val_loss: -3.9279\n",
      "Epoch 508/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0093 - val_loss: -4.1854\n",
      "Epoch 509/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0336 - val_loss: -2.9668\n",
      "Epoch 510/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9122 - val_loss: -4.1949\n",
      "Epoch 511/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8679 - val_loss: -3.2382\n",
      "Epoch 512/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1051 - val_loss: -3.6792\n",
      "Epoch 513/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0391 - val_loss: -3.7923\n",
      "Epoch 514/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1226 - val_loss: -4.3219\n",
      "Epoch 515/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0797 - val_loss: 6.9610\n",
      "Epoch 516/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3210 - val_loss: -3.8548\n",
      "Epoch 517/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9717 - val_loss: -4.2958\n",
      "Epoch 518/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1135 - val_loss: -4.2524\n",
      "Epoch 519/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0578 - val_loss: -4.2458\n",
      "Epoch 520/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9506 - val_loss: -4.4864\n",
      "Epoch 521/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1330 - val_loss: -4.3696\n",
      "Epoch 522/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9891 - val_loss: -4.3999\n",
      "Epoch 523/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1159 - val_loss: -4.3107\n",
      "Epoch 524/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1049 - val_loss: -3.1367\n",
      "Epoch 525/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1093 - val_loss: -3.6381\n",
      "Epoch 526/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0503 - val_loss: -3.8811\n",
      "Epoch 527/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0963 - val_loss: -4.5097\n",
      "Epoch 528/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0856 - val_loss: -4.4764\n",
      "Epoch 529/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0584 - val_loss: -4.2165\n",
      "Epoch 530/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1532 - val_loss: -1.3621\n",
      "Epoch 531/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5316 - val_loss: -4.1315\n",
      "Epoch 532/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1574"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -5.1574 - val_loss: -4.6324\n",
      "Epoch 533/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0702 - val_loss: -3.1746\n",
      "Epoch 534/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1197 - val_loss: -3.0972\n",
      "Epoch 535/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0725 - val_loss: -4.1799\n",
      "Epoch 536/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1018 - val_loss: -4.3329\n",
      "Epoch 537/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4897 - val_loss: -3.7151\n",
      "Epoch 538/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0192 - val_loss: -3.4885\n",
      "Epoch 539/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0576 - val_loss: -3.6774\n",
      "Epoch 540/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0943 - val_loss: -4.4554\n",
      "Epoch 541/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0054 - val_loss: -4.3155\n",
      "Epoch 542/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1632 - val_loss: -2.9191\n",
      "Epoch 543/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7523 - val_loss: -4.0571\n",
      "Epoch 544/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0875 - val_loss: -1.1255\n",
      "Epoch 545/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0908 - val_loss: -4.3694\n",
      "Epoch 546/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0897 - val_loss: -3.5019\n",
      "Epoch 547/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1416 - val_loss: -4.2221\n",
      "Epoch 548/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9756 - val_loss: -4.4177\n",
      "Epoch 549/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1484 - val_loss: -1.5523\n",
      "Epoch 550/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0901 - val_loss: -4.4023\n",
      "Epoch 551/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1261 - val_loss: -4.5720\n",
      "Epoch 552/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0275 - val_loss: -4.1763\n",
      "Epoch 553/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1476 - val_loss: -4.2750\n",
      "Epoch 554/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0787 - val_loss: -3.3300\n",
      "Epoch 555/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1668 - val_loss: -4.5937\n",
      "Epoch 556/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1681 - val_loss: -4.2925\n",
      "Epoch 557/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0445 - val_loss: -4.2730\n",
      "Epoch 558/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1327 - val_loss: -4.4856\n",
      "Epoch 559/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1860 - val_loss: -3.8748\n",
      "Epoch 560/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1897 - val_loss: -2.0926\n",
      "Epoch 561/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9816 - val_loss: -3.4573\n",
      "Epoch 562/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0654 - val_loss: -4.3200\n",
      "Epoch 563/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1135 - val_loss: -1.9460\n",
      "Epoch 564/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1711 - val_loss: -2.0832\n",
      "Epoch 565/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1390 - val_loss: -4.3388\n",
      "Epoch 566/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0953 - val_loss: -4.3240\n",
      "Epoch 567/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1212 - val_loss: -4.4267\n",
      "Epoch 568/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1901 - val_loss: -4.3545\n",
      "Epoch 569/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0456 - val_loss: -4.1436\n",
      "Epoch 570/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2210 - val_loss: -2.5968\n",
      "Epoch 571/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0999 - val_loss: -3.9346\n",
      "Epoch 572/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1338 - val_loss: 0.8625\n",
      "Epoch 573/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0508 - val_loss: -4.3589\n",
      "Epoch 574/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0156 - val_loss: -3.9889\n",
      "Epoch 575/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1223 - val_loss: -4.2162\n",
      "Epoch 576/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1572 - val_loss: -3.8097\n",
      "Epoch 577/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1600 - val_loss: -4.2282\n",
      "Epoch 578/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1739 - val_loss: -3.9501\n",
      "Epoch 579/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1025 - val_loss: -4.1690\n",
      "Epoch 580/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1842 - val_loss: -4.4842\n",
      "Epoch 581/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1943 - val_loss: -2.8502\n",
      "Epoch 582/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1122 - val_loss: 0.5800\n",
      "Epoch 583/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0443 - val_loss: -4.0646\n",
      "Epoch 584/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1915 - val_loss: -2.7617\n",
      "Epoch 585/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1116 - val_loss: -4.2474\n",
      "Epoch 586/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2339 - val_loss: -4.5799\n",
      "Epoch 587/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2020 - val_loss: -3.3188\n",
      "Epoch 588/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2118 - val_loss: -4.2802\n",
      "Epoch 589/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0582 - val_loss: -3.0892\n",
      "Epoch 590/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1576 - val_loss: -3.5512\n",
      "Epoch 591/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1509 - val_loss: -4.2478\n",
      "Epoch 592/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2144 - val_loss: -4.1040\n",
      "Epoch 593/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1535 - val_loss: -4.5324\n",
      "Epoch 594/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1811 - val_loss: -4.2145\n",
      "Epoch 595/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2067 - val_loss: -4.3292\n",
      "Epoch 596/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9469 - val_loss: -4.2708\n",
      "Epoch 597/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1775 - val_loss: -2.6880\n",
      "Epoch 598/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2437 - val_loss: -3.5779\n",
      "Epoch 599/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1834 - val_loss: -3.8022\n",
      "Epoch 600/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1844 - val_loss: -1.8160\n",
      "Epoch 601/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0404 - val_loss: -3.8352\n",
      "Epoch 602/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2149 - val_loss: -3.2171\n",
      "Epoch 603/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1817 - val_loss: -3.2668\n",
      "Epoch 604/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1958 - val_loss: -4.2210\n",
      "Epoch 605/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9661 - val_loss: -4.1096\n",
      "Epoch 606/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2152 - val_loss: -2.9476\n",
      "Epoch 607/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1193 - val_loss: -3.2830\n",
      "Epoch 608/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1095 - val_loss: -4.0888\n",
      "Epoch 609/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2593 - val_loss: -4.1124\n",
      "Epoch 610/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0256 - val_loss: -4.4106\n",
      "Epoch 611/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2658 - val_loss: -3.9112\n",
      "Epoch 612/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1814 - val_loss: -3.2481\n",
      "Epoch 613/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0191 - val_loss: -4.3492\n",
      "Epoch 614/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2456 - val_loss: -4.2581\n",
      "Epoch 615/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1446 - val_loss: -4.0537\n",
      "Epoch 616/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2498 - val_loss: -4.5142\n",
      "Epoch 617/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1183 - val_loss: -4.5142\n",
      "Epoch 618/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2149 - val_loss: -3.6225\n",
      "Epoch 619/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2438 - val_loss: 7.1111\n",
      "Epoch 620/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6194 - val_loss: -3.4717\n",
      "Epoch 621/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1840 - val_loss: -3.9479\n",
      "Epoch 622/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7659 - val_loss: -3.6524\n",
      "Epoch 623/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8919 - val_loss: -3.9523\n",
      "Epoch 624/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9231 - val_loss: -4.0123\n",
      "Epoch 625/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9707 - val_loss: -3.9931\n",
      "Epoch 626/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9661 - val_loss: -3.9480\n",
      "Epoch 627/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0825 - val_loss: -4.4353\n",
      "Epoch 628/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9876 - val_loss: -4.0348\n",
      "Epoch 629/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0572 - val_loss: -3.0977\n",
      "Epoch 630/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8990 - val_loss: -4.3485\n",
      "Epoch 631/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0189 - val_loss: -4.4148\n",
      "Epoch 632/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0634 - val_loss: -4.3542\n",
      "Epoch 633/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0685 - val_loss: -3.7614\n",
      "Epoch 634/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1478 - val_loss: -3.5385\n",
      "Epoch 635/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1010 - val_loss: -4.3642\n",
      "Epoch 636/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1598 - val_loss: -4.1239\n",
      "Epoch 637/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9988 - val_loss: -4.0409\n",
      "Epoch 638/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1438 - val_loss: -4.3078\n",
      "Epoch 639/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1734 - val_loss: -3.6164\n",
      "Epoch 640/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0932 - val_loss: -3.8105\n",
      "Epoch 641/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1833 - val_loss: -3.5420\n",
      "Epoch 642/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9382 - val_loss: -2.6612\n",
      "Epoch 643/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0062 - val_loss: -4.1049\n",
      "Epoch 644/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0435 - val_loss: -3.8017\n",
      "Epoch 645/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0852 - val_loss: -1.9618\n",
      "Epoch 646/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0453 - val_loss: -4.4098\n",
      "Epoch 647/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0670 - val_loss: -3.5351\n",
      "Epoch 648/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0809 - val_loss: -4.4997\n",
      "Epoch 649/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1957 - val_loss: -2.8570\n",
      "Epoch 650/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0936 - val_loss: -4.3360\n",
      "Epoch 651/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2075"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -5.2075 - val_loss: -4.6581\n",
      "Epoch 652/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1717 - val_loss: -4.0065\n",
      "Epoch 653/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0565 - val_loss: -3.8661\n",
      "Epoch 654/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0669 - val_loss: -3.8671\n",
      "Epoch 655/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1569 - val_loss: -4.1168\n",
      "Epoch 656/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0944 - val_loss: -3.8323\n",
      "Epoch 657/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1897 - val_loss: -4.4931\n",
      "Epoch 658/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1415 - val_loss: -3.7666\n",
      "Epoch 659/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1819 - val_loss: -4.2559\n",
      "Epoch 660/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2454 - val_loss: -0.8494\n",
      "Epoch 661/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1330 - val_loss: -4.0215\n",
      "Epoch 662/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1998 - val_loss: -3.6889\n",
      "Epoch 663/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0516 - val_loss: -4.2199\n",
      "Epoch 664/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2360 - val_loss: -3.5544\n",
      "Epoch 665/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2313 - val_loss: -4.0329\n",
      "Epoch 666/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1167 - val_loss: -4.4761\n",
      "Epoch 667/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0830 - val_loss: -4.0932\n",
      "Epoch 668/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2999 - val_loss: -4.0337\n",
      "Epoch 669/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1111 - val_loss: -3.8435\n",
      "Epoch 670/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1671 - val_loss: -4.3443\n",
      "Epoch 671/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1009 - val_loss: -4.2464\n",
      "Epoch 672/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1442 - val_loss: -4.0325\n",
      "Epoch 673/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1845 - val_loss: -4.2002\n",
      "Epoch 674/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2398 - val_loss: -3.9756\n",
      "Epoch 675/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1417 - val_loss: -4.3364\n",
      "Epoch 676/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2250 - val_loss: -4.1823\n",
      "Epoch 677/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2114 - val_loss: -3.7658\n",
      "Epoch 678/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5510 - val_loss: -3.8375\n",
      "Epoch 679/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9044 - val_loss: -4.3640\n",
      "Epoch 680/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9441 - val_loss: -3.4218\n",
      "Epoch 681/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0357 - val_loss: -4.0844\n",
      "Epoch 682/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0281 - val_loss: -4.3711\n",
      "Epoch 683/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0226 - val_loss: -4.0538\n",
      "Epoch 684/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9838 - val_loss: -4.4525\n",
      "Epoch 685/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0952 - val_loss: -4.1596\n",
      "Epoch 686/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1749 - val_loss: -3.3126\n",
      "Epoch 687/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1920 - val_loss: -3.4136\n",
      "Epoch 688/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0570 - val_loss: -2.4135\n",
      "Epoch 689/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1423 - val_loss: -0.9926\n",
      "Epoch 690/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1599 - val_loss: -4.4289\n",
      "Epoch 691/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0790 - val_loss: -3.8565\n",
      "Epoch 692/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1850 - val_loss: -4.3598\n",
      "Epoch 693/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8566 - val_loss: -4.3026\n",
      "Epoch 694/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1903 - val_loss: -3.9558\n",
      "Epoch 695/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1538 - val_loss: -4.1557\n",
      "Epoch 696/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2582 - val_loss: -4.3447\n",
      "Epoch 697/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0789 - val_loss: -1.7842\n",
      "Epoch 698/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1390 - val_loss: -3.9306\n",
      "Epoch 699/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2004 - val_loss: -3.8207\n",
      "Epoch 700/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1479 - val_loss: -4.3176\n",
      "Epoch 701/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2560 - val_loss: -2.9068\n",
      "Epoch 702/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1575 - val_loss: -4.1051\n",
      "Epoch 703/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2303 - val_loss: -4.0838\n",
      "Epoch 704/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2312 - val_loss: -4.3499\n",
      "Epoch 705/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2350 - val_loss: -3.8994\n",
      "Epoch 706/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1838 - val_loss: -3.8001\n",
      "Epoch 707/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1312 - val_loss: -4.3716\n",
      "Epoch 708/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2981 - val_loss: -4.5565\n",
      "Epoch 709/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2191 - val_loss: -2.1289\n",
      "Epoch 710/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2105 - val_loss: -4.3396\n",
      "Epoch 711/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1908 - val_loss: -3.0178\n",
      "Epoch 712/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1405 - val_loss: -4.1249\n",
      "Epoch 713/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2904"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -5.2904 - val_loss: -4.6666\n",
      "Epoch 714/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4574 - val_loss: -3.4403\n",
      "Epoch 715/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7532 - val_loss: -4.2035\n",
      "Epoch 716/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9412 - val_loss: -4.5183\n",
      "Epoch 717/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0602 - val_loss: -4.2464\n",
      "Epoch 718/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0221 - val_loss: -4.0197\n",
      "Epoch 719/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1688 - val_loss: -3.9742\n",
      "Epoch 720/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0379 - val_loss: -4.4575\n",
      "Epoch 721/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1720 - val_loss: -4.0911\n",
      "Epoch 722/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1154 - val_loss: -4.1018\n",
      "Epoch 723/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1506 - val_loss: 5.9808\n",
      "Epoch 724/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9407 - val_loss: -4.2978\n",
      "Epoch 725/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0842 - val_loss: -3.6731\n",
      "Epoch 726/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1633 - val_loss: -4.3380\n",
      "Epoch 727/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2362 - val_loss: -4.1483\n",
      "Epoch 728/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0417 - val_loss: -4.6512\n",
      "Epoch 729/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2003 - val_loss: -4.0102\n",
      "Epoch 730/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2318 - val_loss: -4.0513\n",
      "Epoch 731/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2089 - val_loss: -3.6397\n",
      "Epoch 732/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1821 - val_loss: -4.4508\n",
      "Epoch 733/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7854 - val_loss: -3.5888\n",
      "Epoch 734/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2514 - val_loss: -2.8098\n",
      "Epoch 735/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1700 - val_loss: -2.7440\n",
      "Epoch 736/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1397 - val_loss: -4.0709\n",
      "Epoch 737/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2099 - val_loss: -4.4418\n",
      "Epoch 738/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0041 - val_loss: -4.2301\n",
      "Epoch 739/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2508 - val_loss: -4.0712\n",
      "Epoch 740/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1127 - val_loss: -4.5684\n",
      "Epoch 741/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2201 - val_loss: -4.0962\n",
      "Epoch 742/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0985 - val_loss: -4.0106\n",
      "Epoch 743/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1735 - val_loss: -4.2458\n",
      "Epoch 744/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2665 - val_loss: -3.1578\n",
      "Epoch 745/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2380 - val_loss: -3.9923\n",
      "Epoch 746/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2679 - val_loss: -4.0546\n",
      "Epoch 747/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2342 - val_loss: -3.4174\n",
      "Epoch 748/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2080 - val_loss: -4.0109\n",
      "Epoch 749/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2238 - val_loss: -1.2691\n",
      "Epoch 750/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2972 - val_loss: -4.2516\n",
      "Epoch 751/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2781 - val_loss: -3.8560\n",
      "Epoch 752/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1969 - val_loss: -4.2488\n",
      "Epoch 753/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2717 - val_loss: -0.8023\n",
      "Epoch 754/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.0962 - val_loss: -3.0738\n",
      "Epoch 755/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9128 - val_loss: -4.1843\n",
      "Epoch 756/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0652 - val_loss: -3.3864\n",
      "Epoch 757/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0502 - val_loss: -2.7756\n",
      "Epoch 758/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6769 - val_loss: -3.5217\n",
      "Epoch 759/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1147 - val_loss: -2.7321\n",
      "Epoch 760/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1229 - val_loss: -4.1018\n",
      "Epoch 761/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1840 - val_loss: 1.9376\n",
      "Epoch 762/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0815 - val_loss: -4.1886\n",
      "Epoch 763/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2406 - val_loss: -4.0987\n",
      "Epoch 764/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1329 - val_loss: -3.5905\n",
      "Epoch 765/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1853 - val_loss: -3.8485\n",
      "Epoch 766/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2251 - val_loss: -4.5052\n",
      "Epoch 767/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0704 - val_loss: -4.1519\n",
      "Epoch 768/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2903 - val_loss: -3.5761\n",
      "Epoch 769/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3149 - val_loss: -4.6036\n",
      "Epoch 770/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1227 - val_loss: -3.4244\n",
      "Epoch 771/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2169 - val_loss: -4.4057\n",
      "Epoch 772/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2933 - val_loss: -4.6302\n",
      "Epoch 773/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2155 - val_loss: -4.3587\n",
      "Epoch 774/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9931 - val_loss: -4.5841\n",
      "Epoch 775/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0653 - val_loss: -3.3401\n",
      "Epoch 776/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1881 - val_loss: -3.5727\n",
      "Epoch 777/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2017 - val_loss: -4.4121\n",
      "Epoch 778/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2132 - val_loss: -4.2388\n",
      "Epoch 779/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2511 - val_loss: -3.6444\n",
      "Epoch 780/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2796 - val_loss: -3.8713\n",
      "Epoch 781/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2404 - val_loss: -3.9873\n",
      "Epoch 782/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3420 - val_loss: -3.0384\n",
      "Epoch 783/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1389 - val_loss: -3.4324\n",
      "Epoch 784/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3532 - val_loss: -4.3455\n",
      "Epoch 785/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3236 - val_loss: -4.2725\n",
      "Epoch 786/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1803 - val_loss: -3.9317\n",
      "Epoch 787/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2238 - val_loss: -4.2814\n",
      "Epoch 788/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2682 - val_loss: -2.7985\n",
      "Epoch 789/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2851 - val_loss: -4.5396\n",
      "Epoch 790/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2225 - val_loss: -4.0299\n",
      "Epoch 791/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2871 - val_loss: -3.4489\n",
      "Epoch 792/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3282 - val_loss: -4.5051\n",
      "Epoch 793/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2015 - val_loss: -3.5662\n",
      "Epoch 794/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2841 - val_loss: -2.7351\n",
      "Epoch 795/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1885 - val_loss: -3.6662\n",
      "Epoch 796/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2013 - val_loss: -4.1091\n",
      "Epoch 797/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2603 - val_loss: -3.7993\n",
      "Epoch 798/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2645 - val_loss: -4.0112\n",
      "Epoch 799/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2253 - val_loss: -4.0022\n",
      "Epoch 800/2000\n",
      "118/118 [==============================] - 21s 180ms/step - loss: -5.2862 - val_loss: -4.2474\n",
      "Epoch 801/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8470 - val_loss: -4.0954\n",
      "Epoch 802/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2856 - val_loss: -4.1087\n",
      "Epoch 803/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3035 - val_loss: -4.4398\n",
      "Epoch 804/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2623 - val_loss: -3.9684\n",
      "Epoch 805/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2674 - val_loss: -4.4294\n",
      "Epoch 806/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2776 - val_loss: -3.6701\n",
      "Epoch 807/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2867 - val_loss: -3.6688\n",
      "Epoch 808/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2487 - val_loss: -4.3318\n",
      "Epoch 809/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2277 - val_loss: -4.4428\n",
      "Epoch 810/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1704 - val_loss: -3.3055\n",
      "Epoch 811/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3914 - val_loss: -3.0787\n",
      "Epoch 812/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2185 - val_loss: -2.9770\n",
      "Epoch 813/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1487 - val_loss: -3.3882\n",
      "Epoch 814/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2671 - val_loss: -3.9604\n",
      "Epoch 815/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3401 - val_loss: -4.3873\n",
      "Epoch 816/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3713 - val_loss: -3.5862\n",
      "Epoch 817/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1744 - val_loss: -4.5526\n",
      "Epoch 818/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2180 - val_loss: -4.2069\n",
      "Epoch 819/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3515 - val_loss: -3.5136\n",
      "Epoch 820/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1890 - val_loss: -4.1215\n",
      "Epoch 821/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1939 - val_loss: -3.0256\n",
      "Epoch 822/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3236 - val_loss: -3.6113\n",
      "Epoch 823/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2257 - val_loss: -4.1496\n",
      "Epoch 824/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2881 - val_loss: -4.0265\n",
      "Epoch 825/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2573 - val_loss: -4.2062\n",
      "Epoch 826/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3679 - val_loss: -4.1544\n",
      "Epoch 827/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1850 - val_loss: -4.4916\n",
      "Epoch 828/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2968 - val_loss: -4.0628\n",
      "Epoch 829/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2622 - val_loss: -3.8357\n",
      "Epoch 830/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3107 - val_loss: -4.3092\n",
      "Epoch 831/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2413 - val_loss: -2.6832\n",
      "Epoch 832/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1281 - val_loss: -3.9175\n",
      "Epoch 833/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3068 - val_loss: -4.1766\n",
      "Epoch 834/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3158 - val_loss: -4.1547\n",
      "Epoch 835/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2850 - val_loss: -4.2631\n",
      "Epoch 836/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2226 - val_loss: -3.8895\n",
      "Epoch 837/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3537 - val_loss: -4.2484\n",
      "Epoch 838/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4006 - val_loss: -3.9501\n",
      "Epoch 839/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8740 - val_loss: -4.3410\n",
      "Epoch 840/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9998 - val_loss: -3.6036\n",
      "Epoch 841/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2138 - val_loss: -4.0446\n",
      "Epoch 842/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2732 - val_loss: -3.4766\n",
      "Epoch 843/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3173 - val_loss: -4.2602\n",
      "Epoch 844/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0134 - val_loss: -2.2700\n",
      "Epoch 845/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3480 - val_loss: -4.3602\n",
      "Epoch 846/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -5.2853 - val_loss: -2.6367\n",
      "Epoch 847/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1671"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -5.1671 - val_loss: -4.7043\n",
      "Epoch 848/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3047 - val_loss: -3.1446\n",
      "Epoch 849/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2428 - val_loss: -3.2602\n",
      "Epoch 850/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3192 - val_loss: -3.4392\n",
      "Epoch 851/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3468 - val_loss: -3.6668\n",
      "Epoch 852/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2467 - val_loss: -4.4010\n",
      "Epoch 853/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3946 - val_loss: -2.7247\n",
      "Epoch 854/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3512 - val_loss: -4.1847\n",
      "Epoch 855/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0957 - val_loss: -4.0619\n",
      "Epoch 856/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3303 - val_loss: -4.4519\n",
      "Epoch 857/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9257 - val_loss: -4.0457\n",
      "Epoch 858/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3779 - val_loss: -3.6087\n",
      "Epoch 859/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2370 - val_loss: -3.5927\n",
      "Epoch 860/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2551 - val_loss: -4.5838\n",
      "Epoch 861/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2835 - val_loss: -3.5374\n",
      "Epoch 862/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3178 - val_loss: -4.3859\n",
      "Epoch 863/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3098 - val_loss: -2.9033\n",
      "Epoch 864/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2821 - val_loss: -1.0721\n",
      "Epoch 865/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3145 - val_loss: -4.2636\n",
      "Epoch 866/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3490 - val_loss: 3.0458\n",
      "Epoch 867/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1918 - val_loss: -3.1665\n",
      "Epoch 868/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4022 - val_loss: -4.2459\n",
      "Epoch 869/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2351 - val_loss: -4.4940\n",
      "Epoch 870/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3364 - val_loss: -3.6502\n",
      "Epoch 871/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2616 - val_loss: -4.3403\n",
      "Epoch 872/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3125 - val_loss: -4.0417\n",
      "Epoch 873/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2848 - val_loss: -4.2053\n",
      "Epoch 874/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8887 - val_loss: -3.7238\n",
      "Epoch 875/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0775 - val_loss: -3.6972\n",
      "Epoch 876/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1754 - val_loss: -4.5658\n",
      "Epoch 877/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2888 - val_loss: -4.1398\n",
      "Epoch 878/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2712 - val_loss: -4.2243\n",
      "Epoch 879/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2507 - val_loss: -4.2402\n",
      "Epoch 880/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2052 - val_loss: -4.2310\n",
      "Epoch 881/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3171 - val_loss: -1.9853\n",
      "Epoch 882/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3276 - val_loss: -3.5135\n",
      "Epoch 883/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0013 - val_loss: -3.8094\n",
      "Epoch 884/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2069 - val_loss: -3.7612\n",
      "Epoch 885/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2617 - val_loss: -4.0916\n",
      "Epoch 886/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2547 - val_loss: -3.8662\n",
      "Epoch 887/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2787 - val_loss: -1.4866\n",
      "Epoch 888/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2354 - val_loss: -4.3998\n",
      "Epoch 889/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2404 - val_loss: -3.8878\n",
      "Epoch 890/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4082 - val_loss: -3.8195\n",
      "Epoch 891/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2307 - val_loss: -3.5448\n",
      "Epoch 892/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7629 - val_loss: -1.9209\n",
      "Epoch 893/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.7152 - val_loss: -3.3861\n",
      "Epoch 894/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3849 - val_loss: -3.6635\n",
      "Epoch 895/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6899 - val_loss: -3.9990\n",
      "Epoch 896/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8582 - val_loss: -3.9570\n",
      "Epoch 897/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9247 - val_loss: -3.9436\n",
      "Epoch 898/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8759 - val_loss: -4.1143\n",
      "Epoch 899/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9980 - val_loss: -3.9945\n",
      "Epoch 900/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0576 - val_loss: -3.7836\n",
      "Epoch 901/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7663 - val_loss: -3.3371\n",
      "Epoch 902/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9565 - val_loss: -4.2881\n",
      "Epoch 903/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0631 - val_loss: -2.6993\n",
      "Epoch 904/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0197 - val_loss: -4.0235\n",
      "Epoch 905/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1174 - val_loss: -4.2458\n",
      "Epoch 906/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0863 - val_loss: -3.0819\n",
      "Epoch 907/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1857 - val_loss: -3.1288\n",
      "Epoch 908/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0647 - val_loss: -3.7870\n",
      "Epoch 909/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1029 - val_loss: -4.0092\n",
      "Epoch 910/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4995 - val_loss: -3.1283\n",
      "Epoch 911/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4697 - val_loss: -4.0375\n",
      "Epoch 912/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0063 - val_loss: -4.0402\n",
      "Epoch 913/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1180 - val_loss: -4.4767\n",
      "Epoch 914/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1125 - val_loss: -4.3494\n",
      "Epoch 915/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1970 - val_loss: -4.1296\n",
      "Epoch 916/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9828 - val_loss: -3.4404\n",
      "Epoch 917/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1862 - val_loss: -3.9831\n",
      "Epoch 918/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1231 - val_loss: -4.3074\n",
      "Epoch 919/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1635 - val_loss: -4.1232\n",
      "Epoch 920/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1215 - val_loss: -4.1685\n",
      "Epoch 921/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6522 - val_loss: -3.7897\n",
      "Epoch 922/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8949 - val_loss: -4.0880\n",
      "Epoch 923/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1367 - val_loss: -2.4807\n",
      "Epoch 924/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8427 - val_loss: -4.1452\n",
      "Epoch 925/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1296 - val_loss: -4.2206\n",
      "Epoch 926/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1018 - val_loss: -3.4210\n",
      "Epoch 927/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1546 - val_loss: -3.8403\n",
      "Epoch 928/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1831 - val_loss: -3.9206\n",
      "Epoch 929/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0956 - val_loss: -4.0563\n",
      "Epoch 930/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2616 - val_loss: -4.1185\n",
      "Epoch 931/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1328 - val_loss: -4.4643\n",
      "Epoch 932/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1953 - val_loss: -3.5941\n",
      "Epoch 933/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1830 - val_loss: -4.2318\n",
      "Epoch 934/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2331 - val_loss: -4.2278\n",
      "Epoch 935/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2672 - val_loss: -3.8344\n",
      "Epoch 936/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2853 - val_loss: -2.7792\n",
      "Epoch 937/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0896 - val_loss: -4.4621\n",
      "Epoch 938/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2726 - val_loss: -3.4950\n",
      "Epoch 939/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2296 - val_loss: -3.1557\n",
      "Epoch 940/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2578 - val_loss: -4.2287\n",
      "Epoch 941/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7893 - val_loss: -2.7924\n",
      "Epoch 942/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4839 - val_loss: -4.4442\n",
      "Epoch 943/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0858 - val_loss: -3.8723\n",
      "Epoch 944/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1781 - val_loss: -3.8270\n",
      "Epoch 945/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0670 - val_loss: -4.0445\n",
      "Epoch 946/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2317 - val_loss: -3.4690\n",
      "Epoch 947/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2072 - val_loss: 0.1703\n",
      "Epoch 948/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0703 - val_loss: -3.7210\n",
      "Epoch 949/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2408 - val_loss: -2.4083\n",
      "Epoch 950/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2083 - val_loss: -3.7278\n",
      "Epoch 951/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2901 - val_loss: -3.9567\n",
      "Epoch 952/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2493 - val_loss: -4.2353\n",
      "Epoch 953/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2226 - val_loss: -3.6757\n",
      "Epoch 954/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1844 - val_loss: -3.6845\n",
      "Epoch 955/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2732 - val_loss: -3.5211\n",
      "Epoch 956/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2925 - val_loss: -4.0757\n",
      "Epoch 957/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2824 - val_loss: -4.2846\n",
      "Epoch 958/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0585 - val_loss: -4.2784\n",
      "Epoch 959/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2501 - val_loss: -3.8423\n",
      "Epoch 960/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3173 - val_loss: -4.0730\n",
      "Epoch 961/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2823 - val_loss: -4.3542\n",
      "Epoch 962/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3223 - val_loss: -4.0386\n",
      "Epoch 963/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1841 - val_loss: -3.9181\n",
      "Epoch 964/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2837 - val_loss: -3.1775\n",
      "Epoch 965/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2908 - val_loss: -4.2116\n",
      "Epoch 966/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2690 - val_loss: -3.1596\n",
      "Epoch 967/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3539 - val_loss: -2.1538\n",
      "Epoch 968/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2757 - val_loss: -2.1289\n",
      "Epoch 969/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2871 - val_loss: -4.3771\n",
      "Epoch 970/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3566 - val_loss: -4.4230\n",
      "Epoch 971/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3265 - val_loss: -4.1059\n",
      "Epoch 972/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0710 - val_loss: -4.1857\n",
      "Epoch 973/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9642 - val_loss: -4.0131\n",
      "Epoch 974/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1891 - val_loss: -4.0263\n",
      "Epoch 975/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2096 - val_loss: -3.1077\n",
      "Epoch 976/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2813 - val_loss: -3.5510\n",
      "Epoch 977/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3020 - val_loss: -3.5307\n",
      "Epoch 978/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3266 - val_loss: -3.5925\n",
      "Epoch 979/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2462 - val_loss: -4.0839\n",
      "Epoch 980/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2024 - val_loss: -3.2671\n",
      "Epoch 981/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2866 - val_loss: -4.2261\n",
      "Epoch 982/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3639 - val_loss: -3.1049\n",
      "Epoch 983/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2096 - val_loss: -3.0972\n",
      "Epoch 984/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3262 - val_loss: -4.6751\n",
      "Epoch 985/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2008 - val_loss: -3.9390\n",
      "Epoch 986/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3550 - val_loss: -3.5328\n",
      "Epoch 987/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2460 - val_loss: -2.7061\n",
      "Epoch 988/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.3477 - val_loss: -2.9905\n",
      "Epoch 989/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3657 - val_loss: -4.1588\n",
      "Epoch 990/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0525 - val_loss: -4.0978\n",
      "Epoch 991/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2747 - val_loss: -4.2184\n",
      "Epoch 992/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2821 - val_loss: -4.3155\n",
      "Epoch 993/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4785 - val_loss: -2.9200\n",
      "Epoch 994/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4852 - val_loss: -4.0294\n",
      "Epoch 995/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8712 - val_loss: -4.0548\n",
      "Epoch 996/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0233 - val_loss: -4.2097\n",
      "Epoch 997/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0606 - val_loss: -4.6768\n",
      "Epoch 998/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1366 - val_loss: -3.3879\n",
      "Epoch 999/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1056 - val_loss: -4.6426\n",
      "Epoch 1000/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1545 - val_loss: -3.0058\n",
      "Epoch 1001/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1825 - val_loss: -4.4350\n",
      "Epoch 1002/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2195 - val_loss: -4.5043\n",
      "Epoch 1003/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0716 - val_loss: -3.9587\n",
      "Epoch 1004/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1563 - val_loss: -0.1329\n",
      "Epoch 1005/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2185 - val_loss: -4.4102\n",
      "Epoch 1006/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2737 - val_loss: -3.7423\n",
      "Epoch 1007/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8797 - val_loss: -4.1172\n",
      "Epoch 1008/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2914 - val_loss: -2.4758\n",
      "Epoch 1009/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1992 - val_loss: -0.6268\n",
      "Epoch 1010/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1393 - val_loss: -4.4148\n",
      "Epoch 1011/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1969 - val_loss: -2.5356\n",
      "Epoch 1012/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2362 - val_loss: -3.8088\n",
      "Epoch 1013/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2963 - val_loss: -4.5011\n",
      "Epoch 1014/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1464 - val_loss: -4.1387\n",
      "Epoch 1015/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2787 - val_loss: -4.1876\n",
      "Epoch 1016/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1764 - val_loss: -2.5516\n",
      "Epoch 1017/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3094 - val_loss: -2.9844\n",
      "Epoch 1018/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2014 - val_loss: -3.5204\n",
      "Epoch 1019/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2745 - val_loss: -4.2533\n",
      "Epoch 1020/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3744 - val_loss: -2.7415\n",
      "Epoch 1021/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2686 - val_loss: -3.2845\n",
      "Epoch 1022/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2638 - val_loss: -2.6248\n",
      "Epoch 1023/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1103 - val_loss: -3.7347\n",
      "Epoch 1024/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2600 - val_loss: -4.1743\n",
      "Epoch 1025/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3500 - val_loss: -1.0881\n",
      "Epoch 1026/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2322 - val_loss: -4.3027\n",
      "Epoch 1027/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3813 - val_loss: -4.3892\n",
      "Epoch 1028/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2569 - val_loss: -4.6712\n",
      "Epoch 1029/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3426 - val_loss: -3.3470\n",
      "Epoch 1030/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2764 - val_loss: -1.1458\n",
      "Epoch 1031/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3317 - val_loss: -4.3211\n",
      "Epoch 1032/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2073 - val_loss: -4.0676\n",
      "Epoch 1033/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3947 - val_loss: -3.5452\n",
      "Epoch 1034/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2070 - val_loss: -3.7442\n",
      "Epoch 1035/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2853 - val_loss: -3.3945\n",
      "Epoch 1036/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2459 - val_loss: -4.2040\n",
      "Epoch 1037/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3795 - val_loss: -4.5596\n",
      "Epoch 1038/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2907 - val_loss: -4.2317\n",
      "Epoch 1039/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3387 - val_loss: -4.2446\n",
      "Epoch 1040/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2893 - val_loss: -3.3184\n",
      "Epoch 1041/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3336 - val_loss: -3.3452\n",
      "Epoch 1042/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2777 - val_loss: -3.6158\n",
      "Epoch 1043/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3298 - val_loss: -2.1762\n",
      "Epoch 1044/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3156 - val_loss: -4.4403\n",
      "Epoch 1045/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3059 - val_loss: -2.8310\n",
      "Epoch 1046/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3348 - val_loss: -3.6024\n",
      "Epoch 1047/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3043 - val_loss: -4.6432\n",
      "Epoch 1048/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3479 - val_loss: -3.9150\n",
      "Epoch 1049/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3540 - val_loss: -4.0723\n",
      "Epoch 1050/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0716 - val_loss: -4.4292\n",
      "Epoch 1051/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3521 - val_loss: -2.4378\n",
      "Epoch 1052/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2110 - val_loss: -4.5134\n",
      "Epoch 1053/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3849 - val_loss: -3.0576\n",
      "Epoch 1054/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2897 - val_loss: -3.4075\n",
      "Epoch 1055/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3269 - val_loss: -4.2928\n",
      "Epoch 1056/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3556 - val_loss: -3.9154\n",
      "Epoch 1057/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2629 - val_loss: -2.9123\n",
      "Epoch 1058/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3407 - val_loss: -4.6677\n",
      "Epoch 1059/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3561 - val_loss: -3.7160\n",
      "Epoch 1060/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3344 - val_loss: -4.5393\n",
      "Epoch 1061/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3804 - val_loss: -3.4721\n",
      "Epoch 1062/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3774 - val_loss: -3.2043\n",
      "Epoch 1063/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3170 - val_loss: -2.9998\n",
      "Epoch 1064/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0861 - val_loss: -2.5197\n",
      "Epoch 1065/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3289 - val_loss: -4.4122\n",
      "Epoch 1066/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2931 - val_loss: -2.6945\n",
      "Epoch 1067/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2959 - val_loss: -3.5778\n",
      "Epoch 1068/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3759 - val_loss: -4.3956\n",
      "Epoch 1069/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2225 - val_loss: -2.3338\n",
      "Epoch 1070/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3253 - val_loss: -4.1720\n",
      "Epoch 1071/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4227 - val_loss: -3.6818\n",
      "Epoch 1072/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3134 - val_loss: -4.2475\n",
      "Epoch 1073/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3798 - val_loss: -3.6303\n",
      "Epoch 1074/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3081 - val_loss: -4.2430\n",
      "Epoch 1075/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3725 - val_loss: -4.4077\n",
      "Epoch 1076/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2864 - val_loss: -4.2258\n",
      "Epoch 1077/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3767 - val_loss: -4.5603\n",
      "Epoch 1078/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3038 - val_loss: -2.8415\n",
      "Epoch 1079/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3260 - val_loss: -4.2985\n",
      "Epoch 1080/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0833 - val_loss: -4.1333\n",
      "Epoch 1081/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2741 - val_loss: -4.2904\n",
      "Epoch 1082/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3407 - val_loss: -4.2599\n",
      "Epoch 1083/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3226 - val_loss: -2.3984\n",
      "Epoch 1084/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4455 - val_loss: -3.3471\n",
      "Epoch 1085/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3551 - val_loss: -4.5927\n",
      "Epoch 1086/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3932 - val_loss: -3.9564\n",
      "Epoch 1087/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1850 - val_loss: -2.6168\n",
      "Epoch 1088/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3047 - val_loss: -4.2272\n",
      "Epoch 1089/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4117 - val_loss: -4.6248\n",
      "Epoch 1090/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3807 - val_loss: -4.3080\n",
      "Epoch 1091/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3330 - val_loss: -3.8713\n",
      "Epoch 1092/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3228 - val_loss: -3.9698\n",
      "Epoch 1093/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4839 - val_loss: -3.1161\n",
      "Epoch 1094/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5169 - val_loss: -4.1682\n",
      "Epoch 1095/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0429 - val_loss: -4.3166\n",
      "Epoch 1096/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1519 - val_loss: -4.3808\n",
      "Epoch 1097/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1006 - val_loss: -4.3481\n",
      "Epoch 1098/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1552 - val_loss: -3.8740\n",
      "Epoch 1099/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1713 - val_loss: -1.0702\n",
      "Epoch 1100/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0029 - val_loss: -3.7113\n",
      "Epoch 1101/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3047 - val_loss: -4.4565\n",
      "Epoch 1102/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1788 - val_loss: -4.4452\n",
      "Epoch 1103/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1825 - val_loss: -4.1032\n",
      "Epoch 1104/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2344 - val_loss: -3.8411\n",
      "Epoch 1105/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3222 - val_loss: -3.4413\n",
      "Epoch 1106/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2483 - val_loss: -4.4561\n",
      "Epoch 1107/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2706 - val_loss: -4.0495\n",
      "Epoch 1108/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3195 - val_loss: -4.3227\n",
      "Epoch 1109/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2647 - val_loss: 3.4466\n",
      "Epoch 1110/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2941 - val_loss: -3.0543\n",
      "Epoch 1111/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3431 - val_loss: -4.3107\n",
      "Epoch 1112/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2944 - val_loss: -4.3057\n",
      "Epoch 1113/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3680 - val_loss: -4.4655\n",
      "Epoch 1114/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3355 - val_loss: -3.6153\n",
      "Epoch 1115/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2241 - val_loss: -3.5077\n",
      "Epoch 1116/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3508 - val_loss: -3.6610\n",
      "Epoch 1117/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2928 - val_loss: -4.3124\n",
      "Epoch 1118/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2979 - val_loss: -2.9478\n",
      "Epoch 1119/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3434 - val_loss: -4.5090\n",
      "Epoch 1120/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3688 - val_loss: -3.0921\n",
      "Epoch 1121/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3541 - val_loss: -4.0714\n",
      "Epoch 1122/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -5.2868 - val_loss: -3.7408\n",
      "Epoch 1123/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3045 - val_loss: -3.9466\n",
      "Epoch 1124/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9510 - val_loss: -3.9554\n",
      "Epoch 1125/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2045 - val_loss: -4.0588\n",
      "Epoch 1126/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1904 - val_loss: -4.3833\n",
      "Epoch 1127/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2366 - val_loss: -3.7480\n",
      "Epoch 1128/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3191 - val_loss: -3.1489\n",
      "Epoch 1129/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2472 - val_loss: -4.1997\n",
      "Epoch 1130/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3097 - val_loss: -4.6615\n",
      "Epoch 1131/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2256 - val_loss: -4.4232\n",
      "Epoch 1132/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2845 - val_loss: -4.1606\n",
      "Epoch 1133/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3393 - val_loss: -3.8866\n",
      "Epoch 1134/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2925 - val_loss: -4.4022\n",
      "Epoch 1135/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3636 - val_loss: -4.1827\n",
      "Epoch 1136/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1638 - val_loss: -3.1470\n",
      "Epoch 1137/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2799 - val_loss: -4.1872\n",
      "Epoch 1138/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2830 - val_loss: -4.5643\n",
      "Epoch 1139/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3397 - val_loss: -3.0191\n",
      "Epoch 1140/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2968 - val_loss: -4.2210\n",
      "Epoch 1141/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2747 - val_loss: -3.0099\n",
      "Epoch 1142/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3888 - val_loss: -4.3474\n",
      "Epoch 1143/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3858 - val_loss: -4.1853\n",
      "Epoch 1144/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4030 - val_loss: -4.5392\n",
      "Epoch 1145/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3544 - val_loss: -3.8935\n",
      "Epoch 1146/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4205 - val_loss: -3.7707\n",
      "Epoch 1147/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3262 - val_loss: -4.2031\n",
      "Epoch 1148/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2632 - val_loss: -3.1647\n",
      "Epoch 1149/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3541 - val_loss: -4.3256\n",
      "Epoch 1150/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4510 - val_loss: -4.0153\n",
      "Epoch 1151/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3268 - val_loss: -4.1766\n",
      "Epoch 1152/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3365 - val_loss: -3.9809\n",
      "Epoch 1153/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2944 - val_loss: -4.4457\n",
      "Epoch 1154/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4359 - val_loss: -4.5216\n",
      "Epoch 1155/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3704 - val_loss: -2.1192\n",
      "Epoch 1156/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4061 - val_loss: -4.3437\n",
      "Epoch 1157/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4199 - val_loss: -2.5456\n",
      "Epoch 1158/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0872 - val_loss: -3.7628\n",
      "Epoch 1159/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2057 - val_loss: -1.2935\n",
      "Epoch 1160/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0657 - val_loss: -4.4069\n",
      "Epoch 1161/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3761 - val_loss: -4.5847\n",
      "Epoch 1162/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3360 - val_loss: -4.2586\n",
      "Epoch 1163/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4063 - val_loss: -4.4263\n",
      "Epoch 1164/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3713 - val_loss: -3.6024\n",
      "Epoch 1165/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2885 - val_loss: -3.2052\n",
      "Epoch 1166/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3878 - val_loss: -3.8629\n",
      "Epoch 1167/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3675 - val_loss: -3.9761\n",
      "Epoch 1168/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3489 - val_loss: -2.2948\n",
      "Epoch 1169/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4404 - val_loss: -4.3402\n",
      "Epoch 1170/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5481 - val_loss: -3.4730\n",
      "Epoch 1171/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0112 - val_loss: -3.6692\n",
      "Epoch 1172/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1973 - val_loss: -4.2673\n",
      "Epoch 1173/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1835 - val_loss: -3.2773\n",
      "Epoch 1174/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2416 - val_loss: -3.3775\n",
      "Epoch 1175/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2548 - val_loss: -4.3914\n",
      "Epoch 1176/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3688 - val_loss: -2.3916\n",
      "Epoch 1177/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0829 - val_loss: -3.6189\n",
      "Epoch 1178/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3229 - val_loss: -4.4897\n",
      "Epoch 1179/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3849 - val_loss: -4.1302\n",
      "Epoch 1180/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3423 - val_loss: -4.1763\n",
      "Epoch 1181/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2450 - val_loss: -4.4542\n",
      "Epoch 1182/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3025 - val_loss: -4.3311\n",
      "Epoch 1183/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4334 - val_loss: -3.9256\n",
      "Epoch 1184/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4411 - val_loss: -3.9861\n",
      "Epoch 1185/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1695 - val_loss: -4.3587\n",
      "Epoch 1186/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3879 - val_loss: -3.8249\n",
      "Epoch 1187/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2522 - val_loss: -4.2131\n",
      "Epoch 1188/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2869 - val_loss: -4.0254\n",
      "Epoch 1189/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4052 - val_loss: -4.1416\n",
      "Epoch 1190/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4143 - val_loss: -4.2538\n",
      "Epoch 1191/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3369 - val_loss: -4.0149\n",
      "Epoch 1192/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3663 - val_loss: -4.5881\n",
      "Epoch 1193/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4492 - val_loss: -4.6176\n",
      "Epoch 1194/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3135 - val_loss: -4.1873\n",
      "Epoch 1195/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4351 - val_loss: -4.3517\n",
      "Epoch 1196/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3783 - val_loss: -4.1549\n",
      "Epoch 1197/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3154 - val_loss: -3.1374\n",
      "Epoch 1198/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4415 - val_loss: -4.6811\n",
      "Epoch 1199/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3175 - val_loss: -4.5399\n",
      "Epoch 1200/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4416 - val_loss: -2.2849\n",
      "Epoch 1201/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2684 - val_loss: -3.7402\n",
      "Epoch 1202/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3927 - val_loss: -4.3465\n",
      "Epoch 1203/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3459 - val_loss: -4.2181\n",
      "Epoch 1204/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4351 - val_loss: -4.3040\n",
      "Epoch 1205/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4117 - val_loss: -4.5050\n",
      "Epoch 1206/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3484 - val_loss: -4.5442\n",
      "Epoch 1207/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0577 - val_loss: -4.5693\n",
      "Epoch 1208/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3794 - val_loss: -4.3601\n",
      "Epoch 1209/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -5.3644 - val_loss: -4.4655\n",
      "Epoch 1210/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3925 - val_loss: -3.2254\n",
      "Epoch 1211/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4030 - val_loss: -2.7756\n",
      "Epoch 1212/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3390 - val_loss: -4.2821\n",
      "Epoch 1213/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3899 - val_loss: -4.6673\n",
      "Epoch 1214/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3631 - val_loss: -4.3568\n",
      "Epoch 1215/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0972 - val_loss: -3.3767\n",
      "Epoch 1216/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3586 - val_loss: -3.8024\n",
      "Epoch 1217/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2282 - val_loss: -3.7406\n",
      "Epoch 1218/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4264 - val_loss: -3.9201\n",
      "Epoch 1219/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3381 - val_loss: -4.2485\n",
      "Epoch 1220/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4072 - val_loss: -3.9305\n",
      "Epoch 1221/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3820 - val_loss: -4.6693\n",
      "Epoch 1222/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1251 - val_loss: -4.6629\n",
      "Epoch 1223/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4137 - val_loss: -4.2351\n",
      "Epoch 1224/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4458 - val_loss: -4.1716\n",
      "Epoch 1225/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2601 - val_loss: -3.9041\n",
      "Epoch 1226/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3591 - val_loss: -3.2268\n",
      "Epoch 1227/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4712 - val_loss: -4.3296\n",
      "Epoch 1228/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1945 - val_loss: 0.0626\n",
      "Epoch 1229/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3443 - val_loss: -2.3791\n",
      "Epoch 1230/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2895 - val_loss: -4.6061\n",
      "Epoch 1231/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4290 - val_loss: -4.2921\n",
      "Epoch 1232/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3692 - val_loss: -3.8421\n",
      "Epoch 1233/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3257 - val_loss: -4.0277\n",
      "Epoch 1234/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3828 - val_loss: -4.2821\n",
      "Epoch 1235/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4770 - val_loss: -1.8528\n",
      "Epoch 1236/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2786 - val_loss: -4.0309\n",
      "Epoch 1237/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4213 - val_loss: -3.7205\n",
      "Epoch 1238/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3586 - val_loss: -4.4667\n",
      "Epoch 1239/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4599 - val_loss: -4.3264\n",
      "Epoch 1240/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3697 - val_loss: -4.5581\n",
      "Epoch 1241/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4362 - val_loss: -4.4076\n",
      "Epoch 1242/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3889 - val_loss: -4.5612\n",
      "Epoch 1243/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4107 - val_loss: -3.6634\n",
      "Epoch 1244/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2609 - val_loss: -4.4430\n",
      "Epoch 1245/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3930 - val_loss: -4.3963\n",
      "Epoch 1246/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3571 - val_loss: -4.4479\n",
      "Epoch 1247/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4834 - val_loss: -3.7616\n",
      "Epoch 1248/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4005 - val_loss: -2.4568\n",
      "Epoch 1249/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4016 - val_loss: -4.6361\n",
      "Epoch 1250/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3804 - val_loss: -3.7620\n",
      "Epoch 1251/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3027 - val_loss: -4.3991\n",
      "Epoch 1252/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4185 - val_loss: -4.6634\n",
      "Epoch 1253/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2854 - val_loss: -3.7196\n",
      "Epoch 1254/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4356 - val_loss: -3.1895\n",
      "Epoch 1255/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4560 - val_loss: -3.6194\n",
      "Epoch 1256/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2826 - val_loss: -4.3668\n",
      "Epoch 1257/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4398 - val_loss: -4.1321\n",
      "Epoch 1258/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4167 - val_loss: -4.4541\n",
      "Epoch 1259/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4132 - val_loss: -4.5600\n",
      "Epoch 1260/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.4198"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -5.4198 - val_loss: -4.8066\n",
      "Epoch 1261/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3281 - val_loss: -4.4294\n",
      "Epoch 1262/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4843 - val_loss: -3.9399\n",
      "Epoch 1263/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4542 - val_loss: -3.0220\n",
      "Epoch 1264/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4103 - val_loss: -1.9830\n",
      "Epoch 1265/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3735 - val_loss: -3.6919\n",
      "Epoch 1266/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4528 - val_loss: -4.3683\n",
      "Epoch 1267/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3758 - val_loss: -4.2650\n",
      "Epoch 1268/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4290 - val_loss: -4.1761\n",
      "Epoch 1269/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2684 - val_loss: -4.5460\n",
      "Epoch 1270/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3785 - val_loss: -3.8888\n",
      "Epoch 1271/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2884 - val_loss: -4.2819\n",
      "Epoch 1272/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4571 - val_loss: -3.7590\n",
      "Epoch 1273/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4844 - val_loss: -4.5661\n",
      "Epoch 1274/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4523 - val_loss: -4.1619\n",
      "Epoch 1275/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4489 - val_loss: -4.0696\n",
      "Epoch 1276/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4884 - val_loss: -4.5425\n",
      "Epoch 1277/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4343 - val_loss: -4.4262\n",
      "Epoch 1278/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3315 - val_loss: -4.5827\n",
      "Epoch 1279/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4468 - val_loss: -2.6840\n",
      "Epoch 1280/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4442 - val_loss: -4.5002\n",
      "Epoch 1281/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4330 - val_loss: -4.2844\n",
      "Epoch 1282/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2786 - val_loss: -3.6521\n",
      "Epoch 1283/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4629 - val_loss: -3.6916\n",
      "Epoch 1284/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3462 - val_loss: -3.7460\n",
      "Epoch 1285/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4943 - val_loss: -3.2545\n",
      "Epoch 1286/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4530 - val_loss: -3.2101\n",
      "Epoch 1287/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4241 - val_loss: -3.2355\n",
      "Epoch 1288/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4560 - val_loss: -3.2667\n",
      "Epoch 1289/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3447 - val_loss: -4.0265\n",
      "Epoch 1290/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4649 - val_loss: -3.6223\n",
      "Epoch 1291/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3269 - val_loss: -3.0875\n",
      "Epoch 1292/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9264 - val_loss: -4.5652\n",
      "Epoch 1293/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3485 - val_loss: -0.7276\n",
      "Epoch 1294/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4154 - val_loss: -3.4768\n",
      "Epoch 1295/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4167 - val_loss: -4.1863\n",
      "Epoch 1296/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3024 - val_loss: -4.2411\n",
      "Epoch 1297/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3703 - val_loss: -3.6966\n",
      "Epoch 1298/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4222 - val_loss: -3.6346\n",
      "Epoch 1299/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3996 - val_loss: -4.3995\n",
      "Epoch 1300/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4229 - val_loss: -2.4845\n",
      "Epoch 1301/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4094 - val_loss: -3.9498\n",
      "Epoch 1302/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2587 - val_loss: -3.7074\n",
      "Epoch 1303/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4865 - val_loss: -3.8944\n",
      "Epoch 1304/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4238 - val_loss: -3.4754\n",
      "Epoch 1305/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4573 - val_loss: -4.1214\n",
      "Epoch 1306/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2854 - val_loss: -4.0692\n",
      "Epoch 1307/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4283 - val_loss: -4.2310\n",
      "Epoch 1308/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4235 - val_loss: -3.9043\n",
      "Epoch 1309/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4823 - val_loss: -4.3805\n",
      "Epoch 1310/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.0221 - val_loss: -4.2774\n",
      "Epoch 1311/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8081 - val_loss: -4.4328\n",
      "Epoch 1312/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0383 - val_loss: -4.0697\n",
      "Epoch 1313/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1163 - val_loss: -4.6345\n",
      "Epoch 1314/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9917 - val_loss: -4.5888\n",
      "Epoch 1315/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1663 - val_loss: -4.4301\n",
      "Epoch 1316/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2652 - val_loss: -4.3322\n",
      "Epoch 1317/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2675 - val_loss: -4.5441\n",
      "Epoch 1318/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1554 - val_loss: -4.1611\n",
      "Epoch 1319/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2030 - val_loss: -4.0563\n",
      "Epoch 1320/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2923 - val_loss: -2.0658\n",
      "Epoch 1321/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2422 - val_loss: -3.7999\n",
      "Epoch 1322/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3297 - val_loss: -4.4848\n",
      "Epoch 1323/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3500 - val_loss: -4.1775\n",
      "Epoch 1324/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3435 - val_loss: -4.1989\n",
      "Epoch 1325/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9073 - val_loss: -3.6710\n",
      "Epoch 1326/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8676 - val_loss: -4.1458\n",
      "Epoch 1327/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2199 - val_loss: -4.3142\n",
      "Epoch 1328/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1879 - val_loss: -3.8119\n",
      "Epoch 1329/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2447 - val_loss: -3.6275\n",
      "Epoch 1330/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2606 - val_loss: -3.6088\n",
      "Epoch 1331/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2692 - val_loss: -3.8499\n",
      "Epoch 1332/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3132"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -5.3132 - val_loss: -4.8252\n",
      "Epoch 1333/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2256 - val_loss: -3.8243\n",
      "Epoch 1334/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3282 - val_loss: -4.2855\n",
      "Epoch 1335/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3492 - val_loss: -4.3295\n",
      "Epoch 1336/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0333 - val_loss: -4.5354\n",
      "Epoch 1337/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3498 - val_loss: -3.9565\n",
      "Epoch 1338/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3409 - val_loss: -4.5606\n",
      "Epoch 1339/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3097 - val_loss: -3.7865\n",
      "Epoch 1340/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3305 - val_loss: 1.6853\n",
      "Epoch 1341/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3240 - val_loss: -4.3352\n",
      "Epoch 1342/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3416 - val_loss: -4.2062\n",
      "Epoch 1343/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3658 - val_loss: -4.4086\n",
      "Epoch 1344/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4017 - val_loss: -3.2604\n",
      "Epoch 1345/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3496 - val_loss: -4.6298\n",
      "Epoch 1346/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2814 - val_loss: -4.5555\n",
      "Epoch 1347/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3841 - val_loss: -3.5537\n",
      "Epoch 1348/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2510 - val_loss: -4.0704\n",
      "Epoch 1349/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4422 - val_loss: -4.2110\n",
      "Epoch 1350/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2788 - val_loss: -4.1711\n",
      "Epoch 1351/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3894 - val_loss: -4.5154\n",
      "Epoch 1352/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2435 - val_loss: -4.5789\n",
      "Epoch 1353/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4428 - val_loss: -3.7722\n",
      "Epoch 1354/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3448 - val_loss: -3.6778\n",
      "Epoch 1355/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3899 - val_loss: -4.5915\n",
      "Epoch 1356/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2295 - val_loss: -3.7622\n",
      "Epoch 1357/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2781 - val_loss: -4.2697\n",
      "Epoch 1358/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3827 - val_loss: -4.3828\n",
      "Epoch 1359/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3769 - val_loss: -4.1711\n",
      "Epoch 1360/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1425 - val_loss: -4.5790\n",
      "Epoch 1361/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3815 - val_loss: -4.1093\n",
      "Epoch 1362/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3355 - val_loss: -4.0632\n",
      "Epoch 1363/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4366 - val_loss: -4.0141\n",
      "Epoch 1364/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3502 - val_loss: -2.9486\n",
      "Epoch 1365/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1010 - val_loss: -2.6852\n",
      "Epoch 1366/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3330 - val_loss: -4.5500\n",
      "Epoch 1367/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3939 - val_loss: -4.6527\n",
      "Epoch 1368/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2365 - val_loss: -4.3473\n",
      "Epoch 1369/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3565 - val_loss: -4.2743\n",
      "Epoch 1370/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4070 - val_loss: -4.6398\n",
      "Epoch 1371/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3733 - val_loss: -4.6396\n",
      "Epoch 1372/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4089 - val_loss: -4.0412\n",
      "Epoch 1373/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4626 - val_loss: -1.7856\n",
      "Epoch 1374/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3470 - val_loss: -3.5490\n",
      "Epoch 1375/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3834 - val_loss: -4.0770\n",
      "Epoch 1376/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4808 - val_loss: -4.1825\n",
      "Epoch 1377/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3980 - val_loss: -3.9791\n",
      "Epoch 1378/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3910 - val_loss: -2.9987\n",
      "Epoch 1379/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3597 - val_loss: -4.3230\n",
      "Epoch 1380/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3395 - val_loss: -3.8959\n",
      "Epoch 1381/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3387 - val_loss: -4.1459\n",
      "Epoch 1382/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3947 - val_loss: -4.4185\n",
      "Epoch 1383/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3777 - val_loss: -4.6467\n",
      "Epoch 1384/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4340 - val_loss: -3.9315\n",
      "Epoch 1385/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4347 - val_loss: -4.4172\n",
      "Epoch 1386/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3337 - val_loss: -3.8624\n",
      "Epoch 1387/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4089 - val_loss: -3.4582\n",
      "Epoch 1388/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3967 - val_loss: -4.0044\n",
      "Epoch 1389/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4418 - val_loss: -3.9782\n",
      "Epoch 1390/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4756 - val_loss: -4.1409\n",
      "Epoch 1391/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3568 - val_loss: -4.2999\n",
      "Epoch 1392/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3033 - val_loss: -3.9003\n",
      "Epoch 1393/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4610 - val_loss: -2.7780\n",
      "Epoch 1394/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3265 - val_loss: -4.7284\n",
      "Epoch 1395/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3479 - val_loss: -4.5356\n",
      "Epoch 1396/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4579 - val_loss: -3.7942\n",
      "Epoch 1397/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2203 - val_loss: -4.3043\n",
      "Epoch 1398/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3108 - val_loss: 2.4844\n",
      "Epoch 1399/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1436 - val_loss: -4.3211\n",
      "Epoch 1400/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3958 - val_loss: -4.0533\n",
      "Epoch 1401/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3479 - val_loss: -4.2245\n",
      "Epoch 1402/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3822 - val_loss: -4.0597\n",
      "Epoch 1403/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3511 - val_loss: -3.0782\n",
      "Epoch 1404/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4205 - val_loss: -4.0333\n",
      "Epoch 1405/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3462 - val_loss: -4.5455\n",
      "Epoch 1406/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4806 - val_loss: -4.1838\n",
      "Epoch 1407/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4608 - val_loss: -3.6155\n",
      "Epoch 1408/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3251 - val_loss: -3.1265\n",
      "Epoch 1409/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4796 - val_loss: -2.2666\n",
      "Epoch 1410/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3778 - val_loss: -4.3972\n",
      "Epoch 1411/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1970 - val_loss: -3.8954\n",
      "Epoch 1412/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2682 - val_loss: -4.4862\n",
      "Epoch 1413/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -5.3860 - val_loss: -3.3788\n",
      "Epoch 1414/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2041 - val_loss: -4.3263\n",
      "Epoch 1415/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4638 - val_loss: -4.0603\n",
      "Epoch 1416/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3575 - val_loss: -4.3062\n",
      "Epoch 1417/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4104 - val_loss: -4.5162\n",
      "Epoch 1418/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4659 - val_loss: -4.6509\n",
      "Epoch 1419/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3648 - val_loss: -4.2112\n",
      "Epoch 1420/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4410 - val_loss: -2.9423\n",
      "Epoch 1421/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4339 - val_loss: -2.9268\n",
      "Epoch 1422/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3824 - val_loss: -2.8191\n",
      "Epoch 1423/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3993 - val_loss: -4.0689\n",
      "Epoch 1424/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2205 - val_loss: -4.3228\n",
      "Epoch 1425/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4135 - val_loss: -3.8587\n",
      "Epoch 1426/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3885 - val_loss: -4.5175\n",
      "Epoch 1427/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4240 - val_loss: -4.4097\n",
      "Epoch 1428/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4855 - val_loss: -4.7010\n",
      "Epoch 1429/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1130 - val_loss: -3.7880\n",
      "Epoch 1430/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8729 - val_loss: -4.3113\n",
      "Epoch 1431/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2534 - val_loss: -4.1952\n",
      "Epoch 1432/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2032 - val_loss: -4.3764\n",
      "Epoch 1433/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3387 - val_loss: -4.5909\n",
      "Epoch 1434/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3886 - val_loss: -3.8871\n",
      "Epoch 1435/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3884 - val_loss: -4.4266\n",
      "Epoch 1436/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4429 - val_loss: -3.7062\n",
      "Epoch 1437/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3824 - val_loss: -3.9913\n",
      "Epoch 1438/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3254 - val_loss: -3.8828\n",
      "Epoch 1439/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3993 - val_loss: -2.8802\n",
      "Epoch 1440/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4384 - val_loss: -4.0915\n",
      "Epoch 1441/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3028 - val_loss: -3.7371\n",
      "Epoch 1442/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4352 - val_loss: -4.4694\n",
      "Epoch 1443/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4032 - val_loss: -4.6316\n",
      "Epoch 1444/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4503 - val_loss: -4.3343\n",
      "Epoch 1445/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4500 - val_loss: -4.5576\n",
      "Epoch 1446/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4299 - val_loss: -3.3399\n",
      "Epoch 1447/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4316 - val_loss: -3.8108\n",
      "Epoch 1448/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1483 - val_loss: -4.0664\n",
      "Epoch 1449/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0310 - val_loss: -4.2002\n",
      "Epoch 1450/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2283 - val_loss: -2.9373\n",
      "Epoch 1451/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2475 - val_loss: -3.6113\n",
      "Epoch 1452/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2067 - val_loss: -4.5965\n",
      "Epoch 1453/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3066 - val_loss: 0.4390\n",
      "Epoch 1454/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.3423 - val_loss: -4.1760\n",
      "Epoch 1455/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1495 - val_loss: -4.3095\n",
      "Epoch 1456/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2899 - val_loss: -4.1083\n",
      "Epoch 1457/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3728 - val_loss: -3.7896\n",
      "Epoch 1458/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3796 - val_loss: -4.1241\n",
      "Epoch 1459/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3158 - val_loss: -3.6854\n",
      "Epoch 1460/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3291 - val_loss: -4.3411\n",
      "Epoch 1461/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3621 - val_loss: -3.5607\n",
      "Epoch 1462/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3650 - val_loss: -3.6959\n",
      "Epoch 1463/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1088 - val_loss: -4.2502\n",
      "Epoch 1464/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2452 - val_loss: -4.5368\n",
      "Epoch 1465/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3728 - val_loss: -3.1286\n",
      "Epoch 1466/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4503 - val_loss: -4.4549\n",
      "Epoch 1467/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3684 - val_loss: -2.6738\n",
      "Epoch 1468/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1194 - val_loss: -3.3374\n",
      "Epoch 1469/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4206 - val_loss: -4.0639\n",
      "Epoch 1470/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3049 - val_loss: -2.0625\n",
      "Epoch 1471/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3433 - val_loss: -3.2378\n",
      "Epoch 1472/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4292 - val_loss: -4.5351\n",
      "Epoch 1473/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3661 - val_loss: -4.8003\n",
      "Epoch 1474/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3577 - val_loss: -4.3334\n",
      "Epoch 1475/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4692 - val_loss: -4.1318\n",
      "Epoch 1476/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4446 - val_loss: -0.5365\n",
      "Epoch 1477/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3791 - val_loss: -3.6610\n",
      "Epoch 1478/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2578 - val_loss: -4.6726\n",
      "Epoch 1479/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3773 - val_loss: -4.1503\n",
      "Epoch 1480/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3950 - val_loss: -4.4256\n",
      "Epoch 1481/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4132 - val_loss: -4.1342\n",
      "Epoch 1482/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4115 - val_loss: -4.0456\n",
      "Epoch 1483/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1967 - val_loss: -4.2679\n",
      "Epoch 1484/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4303 - val_loss: -4.2556\n",
      "Epoch 1485/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4597 - val_loss: -4.0103\n",
      "Epoch 1486/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3290 - val_loss: -4.2828\n",
      "Epoch 1487/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3823 - val_loss: -4.5690\n",
      "Epoch 1488/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2440 - val_loss: -4.2523\n",
      "Epoch 1489/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3126 - val_loss: -4.7000\n",
      "Epoch 1490/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1599 - val_loss: -4.2966\n",
      "Epoch 1491/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3497 - val_loss: -3.9146\n",
      "Epoch 1492/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4864 - val_loss: -4.6063\n",
      "Epoch 1493/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4126 - val_loss: -4.4289\n",
      "Epoch 1494/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3882 - val_loss: -2.0024\n",
      "Epoch 1495/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4769 - val_loss: -4.4560\n",
      "Epoch 1496/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4112 - val_loss: -2.6211\n",
      "Epoch 1497/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4274 - val_loss: -4.6803\n",
      "Epoch 1498/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3839 - val_loss: -4.5842\n",
      "Epoch 1499/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3259 - val_loss: -3.8828\n",
      "Epoch 1500/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3385 - val_loss: -2.9041\n",
      "Epoch 1501/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3995 - val_loss: -4.5320\n",
      "Epoch 1502/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3466 - val_loss: -4.5625\n",
      "Epoch 1503/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4391 - val_loss: -4.3949\n",
      "Epoch 1504/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3692 - val_loss: -3.6817\n",
      "Epoch 1505/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3949 - val_loss: -4.0469\n",
      "Epoch 1506/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4449 - val_loss: -3.9798\n",
      "Epoch 1507/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4692 - val_loss: -3.5289\n",
      "Epoch 1508/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3098 - val_loss: -4.4840\n",
      "Epoch 1509/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4945 - val_loss: -2.6423\n",
      "Epoch 1510/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4338 - val_loss: -3.2653\n",
      "Epoch 1511/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3986 - val_loss: -4.3021\n",
      "Epoch 1512/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.3509 - val_loss: -4.0250\n",
      "Epoch 1513/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4852 - val_loss: -3.1646\n",
      "Epoch 1514/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4336 - val_loss: -4.2112\n",
      "Epoch 1515/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4021 - val_loss: -3.7735\n",
      "Epoch 1516/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4556 - val_loss: -3.4249\n",
      "Epoch 1517/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3700 - val_loss: -3.3998\n",
      "Epoch 1518/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4199 - val_loss: -3.8398\n",
      "Epoch 1519/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4268 - val_loss: -4.3803\n",
      "Epoch 1520/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3998 - val_loss: -4.4014\n",
      "Epoch 1521/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5194 - val_loss: -4.1335\n",
      "Epoch 1522/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4260 - val_loss: -4.4961\n",
      "Epoch 1523/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2640 - val_loss: -4.1302\n",
      "Epoch 1524/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.0344 - val_loss: -3.7728\n",
      "Epoch 1525/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3493 - val_loss: -3.5244\n",
      "Epoch 1526/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4328 - val_loss: -4.1491\n",
      "Epoch 1527/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4549 - val_loss: -3.3834\n",
      "Epoch 1528/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4242 - val_loss: -3.4324\n",
      "Epoch 1529/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4474 - val_loss: -4.0827\n",
      "Epoch 1530/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4095 - val_loss: -4.6286\n",
      "Epoch 1531/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4211 - val_loss: -4.2350\n",
      "Epoch 1532/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5010 - val_loss: -4.3873\n",
      "Epoch 1533/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4881 - val_loss: -1.5611\n",
      "Epoch 1534/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3974 - val_loss: -4.1049\n",
      "Epoch 1535/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5048 - val_loss: -4.6210\n",
      "Epoch 1536/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3923 - val_loss: -4.1062\n",
      "Epoch 1537/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4306 - val_loss: -3.9606\n",
      "Epoch 1538/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4079 - val_loss: -4.8222\n",
      "Epoch 1539/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5309 - val_loss: -4.3278\n",
      "Epoch 1540/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3376 - val_loss: -3.3383\n",
      "Epoch 1541/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3410 - val_loss: -4.2977\n",
      "Epoch 1542/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.4348 - val_loss: -4.2657\n",
      "Epoch 1543/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4362 - val_loss: -4.4308\n",
      "Epoch 1544/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4903 - val_loss: -4.6190\n",
      "Epoch 1545/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4838 - val_loss: -3.2023\n",
      "Epoch 1546/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3306 - val_loss: -4.0848\n",
      "Epoch 1547/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3333 - val_loss: -2.6195\n",
      "Epoch 1548/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5181 - val_loss: -4.4454\n",
      "Epoch 1549/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5049 - val_loss: -4.0966\n",
      "Epoch 1550/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4920 - val_loss: -4.4443\n",
      "Epoch 1551/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3953 - val_loss: -4.5593\n",
      "Epoch 1552/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3752 - val_loss: -4.5025\n",
      "Epoch 1553/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4749 - val_loss: -2.8992\n",
      "Epoch 1554/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4924 - val_loss: -3.3374\n",
      "Epoch 1555/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2813 - val_loss: -4.4743\n",
      "Epoch 1556/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1714 - val_loss: -4.4998\n",
      "Epoch 1557/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4227 - val_loss: -3.0486\n",
      "Epoch 1558/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4150 - val_loss: -2.8334\n",
      "Epoch 1559/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3575 - val_loss: -4.4884\n",
      "Epoch 1560/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5106 - val_loss: -3.8502\n",
      "Epoch 1561/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4366 - val_loss: -4.5001\n",
      "Epoch 1562/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4500 - val_loss: -4.4375\n",
      "Epoch 1563/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2710 - val_loss: -3.1866\n",
      "Epoch 1564/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4401 - val_loss: -3.7796\n",
      "Epoch 1565/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4210 - val_loss: -4.3923\n",
      "Epoch 1566/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5290 - val_loss: -4.2373\n",
      "Epoch 1567/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4545 - val_loss: -4.1633\n",
      "Epoch 1568/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4081 - val_loss: -4.2882\n",
      "Epoch 1569/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3888 - val_loss: -3.9937\n",
      "Epoch 1570/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5192 - val_loss: -4.0268\n",
      "Epoch 1571/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5156 - val_loss: -4.0165\n",
      "Epoch 1572/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3441 - val_loss: -3.7356\n",
      "Epoch 1573/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4282 - val_loss: -2.9258\n",
      "Epoch 1574/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4376 - val_loss: -4.2212\n",
      "Epoch 1575/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4897 - val_loss: -3.9636\n",
      "Epoch 1576/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4879 - val_loss: -4.3964\n",
      "Epoch 1577/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4441 - val_loss: -4.3859\n",
      "Epoch 1578/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4596 - val_loss: -3.7113\n",
      "Epoch 1579/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3523 - val_loss: -4.2825\n",
      "Epoch 1580/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4749 - val_loss: -4.3288\n",
      "Epoch 1581/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4651 - val_loss: -4.5762\n",
      "Epoch 1582/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7592 - val_loss: -4.1308\n",
      "Epoch 1583/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1880 - val_loss: -4.4636\n",
      "Epoch 1584/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2897 - val_loss: -3.4068\n",
      "Epoch 1585/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3402 - val_loss: -4.4359\n",
      "Epoch 1586/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4640 - val_loss: -1.4412\n",
      "Epoch 1587/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3739 - val_loss: -3.7069\n",
      "Epoch 1588/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4762 - val_loss: -2.0431\n",
      "Epoch 1589/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4013 - val_loss: -3.3895\n",
      "Epoch 1590/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4291 - val_loss: -4.2448\n",
      "Epoch 1591/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.2886 - val_loss: -4.3739\n",
      "Epoch 1592/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1475 - val_loss: -4.3651\n",
      "Epoch 1593/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3385 - val_loss: -3.8219\n",
      "Epoch 1594/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3611 - val_loss: -3.8437\n",
      "Epoch 1595/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3434 - val_loss: -4.4631\n",
      "Epoch 1596/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3309 - val_loss: -4.5391\n",
      "Epoch 1597/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3865 - val_loss: -4.1510\n",
      "Epoch 1598/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4389 - val_loss: -4.3240\n",
      "Epoch 1599/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4818 - val_loss: -3.9558\n",
      "Epoch 1600/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3663 - val_loss: -2.8140\n",
      "Epoch 1601/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4154 - val_loss: -4.3890\n",
      "Epoch 1602/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4085 - val_loss: -4.3638\n",
      "Epoch 1603/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4213 - val_loss: -0.4808\n",
      "Epoch 1604/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4367 - val_loss: -4.3271\n",
      "Epoch 1605/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4637 - val_loss: -3.9284\n",
      "Epoch 1606/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5064 - val_loss: -3.2082\n",
      "Epoch 1607/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3440 - val_loss: -4.5528\n",
      "Epoch 1608/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3572 - val_loss: -4.3283\n",
      "Epoch 1609/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4676 - val_loss: -3.3106\n",
      "Epoch 1610/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4886 - val_loss: -3.9200\n",
      "Epoch 1611/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4914 - val_loss: -3.3053\n",
      "Epoch 1612/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5249 - val_loss: -4.3388\n",
      "Epoch 1613/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4857 - val_loss: -4.3626\n",
      "Epoch 1614/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0853 - val_loss: -4.1635\n",
      "Epoch 1615/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4270 - val_loss: -4.3680\n",
      "Epoch 1616/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4463 - val_loss: -4.6422\n",
      "Epoch 1617/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4312 - val_loss: -3.5763\n",
      "Epoch 1618/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3618 - val_loss: -4.4915\n",
      "Epoch 1619/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5076 - val_loss: -4.3413\n",
      "Epoch 1620/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4639 - val_loss: -3.7466\n",
      "Epoch 1621/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5448 - val_loss: -3.6871\n",
      "Epoch 1622/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3546 - val_loss: -3.0987\n",
      "Epoch 1623/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3944 - val_loss: -4.5541\n",
      "Epoch 1624/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4524 - val_loss: -4.4031\n",
      "Epoch 1625/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5181 - val_loss: -4.2519\n",
      "Epoch 1626/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4979 - val_loss: -3.4439\n",
      "Epoch 1627/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1336 - val_loss: -4.7435\n",
      "Epoch 1628/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5336 - val_loss: -4.1736\n",
      "Epoch 1629/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4567 - val_loss: -4.3983\n",
      "Epoch 1630/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4405 - val_loss: -4.2625\n",
      "Epoch 1631/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5022 - val_loss: -4.2223\n",
      "Epoch 1632/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3517 - val_loss: -4.1517\n",
      "Epoch 1633/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -5.5342 - val_loss: -4.4418\n",
      "Epoch 1634/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4113 - val_loss: -3.8513\n",
      "Epoch 1635/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4910 - val_loss: 3.0638\n",
      "Epoch 1636/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3299 - val_loss: -4.6843\n",
      "Epoch 1637/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5367 - val_loss: -3.7338\n",
      "Epoch 1638/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4039 - val_loss: -4.5179\n",
      "Epoch 1639/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5357 - val_loss: -3.9647\n",
      "Epoch 1640/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3431 - val_loss: -3.9951\n",
      "Epoch 1641/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4016 - val_loss: -3.1552\n",
      "Epoch 1642/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5435 - val_loss: -3.8996\n",
      "Epoch 1643/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5242 - val_loss: -4.5707\n",
      "Epoch 1644/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4533 - val_loss: -4.5523\n",
      "Epoch 1645/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4625 - val_loss: -3.9882\n",
      "Epoch 1646/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4721 - val_loss: -4.1575\n",
      "Epoch 1647/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4890 - val_loss: -4.7432\n",
      "Epoch 1648/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5104 - val_loss: -3.7757\n",
      "Epoch 1649/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4938 - val_loss: -4.3941\n",
      "Epoch 1650/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4750 - val_loss: -4.3947\n",
      "Epoch 1651/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3976 - val_loss: -4.3408\n",
      "Epoch 1652/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4909 - val_loss: -3.8448\n",
      "Epoch 1653/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5093 - val_loss: -4.3799\n",
      "Epoch 1654/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4820 - val_loss: -4.2243\n",
      "Epoch 1655/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5247 - val_loss: -4.1091\n",
      "Epoch 1656/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5313 - val_loss: -3.8540\n",
      "Epoch 1657/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4728 - val_loss: -4.2930\n",
      "Epoch 1658/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3207 - val_loss: -4.0591\n",
      "Epoch 1659/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5337 - val_loss: -4.1508\n",
      "Epoch 1660/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3151 - val_loss: -3.9815\n",
      "Epoch 1661/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1643 - val_loss: -4.4127\n",
      "Epoch 1662/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5127 - val_loss: -4.1786\n",
      "Epoch 1663/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3982 - val_loss: -4.7341\n",
      "Epoch 1664/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5032 - val_loss: -4.2574\n",
      "Epoch 1665/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5070 - val_loss: -3.8232\n",
      "Epoch 1666/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5191 - val_loss: -4.3490\n",
      "Epoch 1667/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4936 - val_loss: -3.5425\n",
      "Epoch 1668/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3967 - val_loss: -3.6106\n",
      "Epoch 1669/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5535 - val_loss: -4.0336\n",
      "Epoch 1670/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4161 - val_loss: -4.1315\n",
      "Epoch 1671/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4964 - val_loss: -3.5853\n",
      "Epoch 1672/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4014 - val_loss: -4.6448\n",
      "Epoch 1673/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5226 - val_loss: -3.1352\n",
      "Epoch 1674/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4085 - val_loss: -3.5966\n",
      "Epoch 1675/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4857 - val_loss: -4.0555\n",
      "Epoch 1676/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1291 - val_loss: -3.3236\n",
      "Epoch 1677/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3448 - val_loss: -4.3399\n",
      "Epoch 1678/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4897 - val_loss: -4.4290\n",
      "Epoch 1679/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -5.5052 - val_loss: -4.0641\n",
      "Epoch 1680/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4336 - val_loss: -4.0176\n",
      "Epoch 1681/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.5445 - val_loss: -4.0304\n",
      "Epoch 1682/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5136 - val_loss: -3.9186\n",
      "Epoch 1683/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4468 - val_loss: -3.3527\n",
      "Epoch 1684/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4420 - val_loss: -3.5436\n",
      "Epoch 1685/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5447 - val_loss: -3.7163\n",
      "Epoch 1686/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5252 - val_loss: -4.0268\n",
      "Epoch 1687/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4745 - val_loss: -4.1323\n",
      "Epoch 1688/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.4716"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_150_layer_call_fn, lstm_cell_150_layer_call_and_return_conditional_losses, lstm_cell_151_layer_call_fn, lstm_cell_151_layer_call_and_return_conditional_losses, lstm_cell_152_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -5.4716 - val_loss: -4.9180\n",
      "Epoch 1689/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4934 - val_loss: -4.3299\n",
      "Epoch 1690/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4667 - val_loss: -3.6693\n",
      "Epoch 1691/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5377 - val_loss: -4.5811\n",
      "Epoch 1692/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4595 - val_loss: -4.4002\n",
      "Epoch 1693/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5228 - val_loss: -4.6366\n",
      "Epoch 1694/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.5198 - val_loss: -3.1832\n",
      "Epoch 1695/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5102 - val_loss: -3.2257\n",
      "Epoch 1696/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5124 - val_loss: -4.3064\n",
      "Epoch 1697/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5421 - val_loss: -4.2115\n",
      "Epoch 1698/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3303 - val_loss: -4.5721\n",
      "Epoch 1699/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5218 - val_loss: -4.5379\n",
      "Epoch 1700/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4311 - val_loss: -4.4912\n",
      "Epoch 1701/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5249 - val_loss: -3.9501\n",
      "Epoch 1702/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4875 - val_loss: -4.8022\n",
      "Epoch 1703/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4705 - val_loss: -4.5762\n",
      "Epoch 1704/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9359 - val_loss: -4.0911\n",
      "Epoch 1705/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3034 - val_loss: -4.2296\n",
      "Epoch 1706/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4731 - val_loss: -3.9206\n",
      "Epoch 1707/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4701 - val_loss: -4.0591\n",
      "Epoch 1708/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4776 - val_loss: -3.6749\n",
      "Epoch 1709/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4268 - val_loss: -4.2672\n",
      "Epoch 1710/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5160 - val_loss: -4.8312\n",
      "Epoch 1711/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4827 - val_loss: -4.0626\n",
      "Epoch 1712/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4358 - val_loss: -4.2536\n",
      "Epoch 1713/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4051 - val_loss: -4.1560\n",
      "Epoch 1714/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4470 - val_loss: -4.2830\n",
      "Epoch 1715/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4118 - val_loss: -4.0185\n",
      "Epoch 1716/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5169 - val_loss: -4.0336\n",
      "Epoch 1717/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4539 - val_loss: -4.4438\n",
      "Epoch 1718/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5535 - val_loss: -4.2996\n",
      "Epoch 1719/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4193 - val_loss: -4.2755\n",
      "Epoch 1720/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5451 - val_loss: -4.0089\n",
      "Epoch 1721/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5409 - val_loss: -3.9246\n",
      "Epoch 1722/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5053 - val_loss: -3.7176\n",
      "Epoch 1723/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4681 - val_loss: -4.1261\n",
      "Epoch 1724/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5151 - val_loss: -4.6795\n",
      "Epoch 1725/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3658 - val_loss: -4.3521\n",
      "Epoch 1726/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5408 - val_loss: -3.5660\n",
      "Epoch 1727/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5174 - val_loss: -4.1535\n",
      "Epoch 1728/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4107 - val_loss: -2.7801\n",
      "Epoch 1729/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5408 - val_loss: -4.3858\n",
      "Epoch 1730/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4851 - val_loss: -4.0483\n",
      "Epoch 1731/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5567 - val_loss: -4.4894\n",
      "Epoch 1732/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5572 - val_loss: -3.7433\n",
      "Epoch 1733/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2362 - val_loss: -4.1550\n",
      "Epoch 1734/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4808 - val_loss: -4.3979\n",
      "Epoch 1735/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4835 - val_loss: -1.0361e-04\n",
      "Epoch 1736/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4323 - val_loss: -3.1997\n",
      "Epoch 1737/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5495 - val_loss: -4.2202\n",
      "Epoch 1738/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4470 - val_loss: -4.6746\n",
      "Epoch 1739/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4889 - val_loss: -3.8144\n",
      "Epoch 1740/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5492 - val_loss: -3.4583\n",
      "Epoch 1741/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4516 - val_loss: -3.7691\n",
      "Epoch 1742/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5561 - val_loss: -3.8160\n",
      "Epoch 1743/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4750 - val_loss: -4.2382\n",
      "Epoch 1744/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5010 - val_loss: -4.6316\n",
      "Epoch 1745/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5688 - val_loss: -4.1134\n",
      "Epoch 1746/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4471 - val_loss: -0.6208\n",
      "Epoch 1747/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2100 - val_loss: -4.4499\n",
      "Epoch 1748/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4532 - val_loss: -4.1582\n",
      "Epoch 1749/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5506 - val_loss: -3.2280\n",
      "Epoch 1750/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4933 - val_loss: -3.7614\n",
      "Epoch 1751/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4925 - val_loss: -3.3533\n",
      "Epoch 1752/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4618 - val_loss: -4.0554\n",
      "Epoch 1753/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5141 - val_loss: -4.0117\n",
      "Epoch 1754/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3074 - val_loss: -4.1691\n",
      "Epoch 1755/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5165 - val_loss: -4.1171\n",
      "Epoch 1756/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5682 - val_loss: -3.5012\n",
      "Epoch 1757/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5497 - val_loss: -3.9875\n",
      "Epoch 1758/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4569 - val_loss: -3.9384\n",
      "Epoch 1759/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4694 - val_loss: -3.2312\n",
      "Epoch 1760/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5292 - val_loss: -3.8439\n",
      "Epoch 1761/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0292 - val_loss: -3.8841\n",
      "Epoch 1762/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9881 - val_loss: -4.0471\n",
      "Epoch 1763/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1738 - val_loss: -3.7811\n",
      "Epoch 1764/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3052 - val_loss: -4.3542\n",
      "Epoch 1765/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2266 - val_loss: -3.8531\n",
      "Epoch 1766/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3644 - val_loss: -4.3982\n",
      "Epoch 1767/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4198 - val_loss: -2.8519\n",
      "Epoch 1768/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4141 - val_loss: -4.2750\n",
      "Epoch 1769/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3821 - val_loss: -3.9643\n",
      "Epoch 1770/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3679 - val_loss: -4.7759\n",
      "Epoch 1771/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3478 - val_loss: -4.1973\n",
      "Epoch 1772/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4969 - val_loss: -3.7273\n",
      "Epoch 1773/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4421 - val_loss: -4.6744\n",
      "Epoch 1774/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4383 - val_loss: -4.2399\n",
      "Epoch 1775/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4585 - val_loss: -4.5320\n",
      "Epoch 1776/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5276 - val_loss: -4.2089\n",
      "Epoch 1777/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3668 - val_loss: -4.0288\n",
      "Epoch 1778/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5311 - val_loss: -4.0976\n",
      "Epoch 1779/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4788 - val_loss: -4.0559\n",
      "Epoch 1780/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5048 - val_loss: -3.1755\n",
      "Epoch 1781/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4294 - val_loss: -4.4023\n",
      "Epoch 1782/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2982 - val_loss: -4.2722\n",
      "Epoch 1783/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5260 - val_loss: -4.2777\n",
      "Epoch 1784/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3889 - val_loss: -4.3227\n",
      "Epoch 1785/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4520 - val_loss: -3.9367\n",
      "Epoch 1786/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5153 - val_loss: -4.1888\n",
      "Epoch 1787/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4703 - val_loss: -3.9543\n",
      "Epoch 1788/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4678 - val_loss: -4.4137\n",
      "Epoch 1789/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.5381 - val_loss: -3.3622\n",
      "Epoch 1790/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5399 - val_loss: -3.6227\n",
      "Epoch 1791/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4905 - val_loss: -4.5010\n",
      "Epoch 1792/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5293 - val_loss: -3.4779\n",
      "Epoch 1793/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5437 - val_loss: -4.0920\n",
      "Epoch 1794/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4924 - val_loss: -4.6439\n",
      "Epoch 1795/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4602 - val_loss: -3.7362\n",
      "Epoch 1796/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4745 - val_loss: -4.7733\n",
      "Epoch 1797/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0231 - val_loss: -3.1274\n",
      "Epoch 1798/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.8006 - val_loss: -3.8552\n",
      "Epoch 1799/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9158 - val_loss: -4.1282\n",
      "Epoch 1800/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0249 - val_loss: -2.1283\n",
      "Epoch 1801/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0945 - val_loss: -2.0827\n",
      "Epoch 1802/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2103 - val_loss: -4.3059\n",
      "Epoch 1803/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0828 - val_loss: -4.3511\n",
      "Epoch 1804/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2355 - val_loss: -3.8535\n",
      "Epoch 1805/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2619 - val_loss: -4.7862\n",
      "Epoch 1806/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2549 - val_loss: -4.1063\n",
      "Epoch 1807/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0458 - val_loss: -4.3932\n",
      "Epoch 1808/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2584 - val_loss: -4.2423\n",
      "Epoch 1809/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2540 - val_loss: -2.3662\n",
      "Epoch 1810/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3550 - val_loss: -4.5195\n",
      "Epoch 1811/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3416 - val_loss: -4.6212\n",
      "Epoch 1812/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2988 - val_loss: -4.3004\n",
      "Epoch 1813/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3921 - val_loss: -3.9865\n",
      "Epoch 1814/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3158 - val_loss: -3.5110\n",
      "Epoch 1815/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4187 - val_loss: -3.5917\n",
      "Epoch 1816/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3641 - val_loss: -4.1764\n",
      "Epoch 1817/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4021 - val_loss: -4.1254\n",
      "Epoch 1818/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4225 - val_loss: -4.1843\n",
      "Epoch 1819/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2696 - val_loss: -4.5417\n",
      "Epoch 1820/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2735 - val_loss: -4.3503\n",
      "Epoch 1821/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3840 - val_loss: -4.5225\n",
      "Epoch 1822/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4167 - val_loss: -4.8564\n",
      "Epoch 1823/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3134 - val_loss: -4.1766\n",
      "Epoch 1824/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4492 - val_loss: -3.5192\n",
      "Epoch 1825/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4897 - val_loss: -4.3059\n",
      "Epoch 1826/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4347 - val_loss: -3.7861\n",
      "Epoch 1827/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8402 - val_loss: -4.2281\n",
      "Epoch 1828/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0827 - val_loss: 0.0662\n",
      "Epoch 1829/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3263 - val_loss: -3.6659\n",
      "Epoch 1830/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3803 - val_loss: -0.0453\n",
      "Epoch 1831/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8702 - val_loss: -4.4355\n",
      "Epoch 1832/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3842 - val_loss: -3.8012\n",
      "Epoch 1833/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3019 - val_loss: -4.2787\n",
      "Epoch 1834/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4444 - val_loss: -4.5441\n",
      "Epoch 1835/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3631 - val_loss: -4.4349\n",
      "Epoch 1836/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4413 - val_loss: -4.1649\n",
      "Epoch 1837/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.7529 - val_loss: -4.5021\n",
      "Epoch 1838/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8920 - val_loss: -3.5633\n",
      "Epoch 1839/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2126 - val_loss: -4.5799\n",
      "Epoch 1840/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3022 - val_loss: -4.2722\n",
      "Epoch 1841/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3902 - val_loss: -4.5877\n",
      "Epoch 1842/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2816 - val_loss: -4.6468\n",
      "Epoch 1843/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3178 - val_loss: -4.1900\n",
      "Epoch 1844/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4166 - val_loss: -4.5038\n",
      "Epoch 1845/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4614 - val_loss: -3.9681\n",
      "Epoch 1846/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2630 - val_loss: -4.5350\n",
      "Epoch 1847/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4746 - val_loss: -4.6286\n",
      "Epoch 1848/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5193 - val_loss: -4.4154\n",
      "Epoch 1849/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3811 - val_loss: -4.7109\n",
      "Epoch 1850/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4767 - val_loss: -3.1469\n",
      "Epoch 1851/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4823 - val_loss: -4.2056\n",
      "Epoch 1852/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5036 - val_loss: -4.2203\n",
      "Epoch 1853/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4431 - val_loss: -4.2674\n",
      "Epoch 1854/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2823 - val_loss: -3.5428\n",
      "Epoch 1855/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4972 - val_loss: -4.0520\n",
      "Epoch 1856/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5211 - val_loss: -4.3690\n",
      "Epoch 1857/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0152 - val_loss: -4.4320\n",
      "Epoch 1858/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4003 - val_loss: -4.3545\n",
      "Epoch 1859/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3790 - val_loss: -4.3930\n",
      "Epoch 1860/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4947 - val_loss: -3.6443\n",
      "Epoch 1861/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3882 - val_loss: -4.6367\n",
      "Epoch 1862/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4508 - val_loss: -4.5363\n",
      "Epoch 1863/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3232 - val_loss: -4.1506\n",
      "Epoch 1864/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1715 - val_loss: -2.6973\n",
      "Epoch 1865/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2789 - val_loss: -4.2487\n",
      "Epoch 1866/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4792 - val_loss: -3.8500\n",
      "Epoch 1867/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2879 - val_loss: -3.5666\n",
      "Epoch 1868/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4954 - val_loss: -4.2365\n",
      "Epoch 1869/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4224 - val_loss: -4.4780\n",
      "Epoch 1870/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4868 - val_loss: -2.4804\n",
      "Epoch 1871/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5482 - val_loss: -4.4923\n",
      "Epoch 1872/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5353 - val_loss: -4.4491\n",
      "Epoch 1873/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4667 - val_loss: -4.4481\n",
      "Epoch 1874/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4053 - val_loss: -4.3531\n",
      "Epoch 1875/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5288 - val_loss: -3.9483\n",
      "Epoch 1876/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3482 - val_loss: -3.7827\n",
      "Epoch 1877/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5416 - val_loss: -3.4522\n",
      "Epoch 1878/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3566 - val_loss: -4.0601\n",
      "Epoch 1879/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3866 - val_loss: -4.5487\n",
      "Epoch 1880/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4438 - val_loss: -3.4190\n",
      "Epoch 1881/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4844 - val_loss: -4.0564\n",
      "Epoch 1882/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4680 - val_loss: -4.1184\n",
      "Epoch 1883/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4366 - val_loss: -3.3811\n",
      "Epoch 1884/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5094 - val_loss: -1.0099\n",
      "Epoch 1885/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4563 - val_loss: -3.9343\n",
      "Epoch 1886/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4597 - val_loss: -4.1678\n",
      "Epoch 1887/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5231 - val_loss: -4.4309\n",
      "Epoch 1888/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5284 - val_loss: -4.1310\n",
      "Epoch 1889/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5022 - val_loss: -2.6104\n",
      "Epoch 1890/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3850 - val_loss: 7.1891\n",
      "Epoch 1891/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0846 - val_loss: -4.2031\n",
      "Epoch 1892/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4825 - val_loss: -4.0257\n",
      "Epoch 1893/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4457 - val_loss: -4.2873\n",
      "Epoch 1894/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5199 - val_loss: -4.4896\n",
      "Epoch 1895/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4243 - val_loss: -4.0844\n",
      "Epoch 1896/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.1895 - val_loss: -4.2861\n",
      "Epoch 1897/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5107 - val_loss: -4.4873\n",
      "Epoch 1898/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3421 - val_loss: -3.0768\n",
      "Epoch 1899/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5003 - val_loss: -4.6400\n",
      "Epoch 1900/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4873 - val_loss: -4.6179\n",
      "Epoch 1901/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5227 - val_loss: -4.4872\n",
      "Epoch 1902/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3205 - val_loss: -4.2230\n",
      "Epoch 1903/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3471 - val_loss: -4.7153\n",
      "Epoch 1904/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5305 - val_loss: -4.1972\n",
      "Epoch 1905/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3781 - val_loss: -0.8797\n",
      "Epoch 1906/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2877 - val_loss: -4.2856\n",
      "Epoch 1907/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5484 - val_loss: -4.3968\n",
      "Epoch 1908/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5068 - val_loss: -4.3629\n",
      "Epoch 1909/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5118 - val_loss: -4.5383\n",
      "Epoch 1910/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5269 - val_loss: -2.8298\n",
      "Epoch 1911/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5002 - val_loss: -4.5738\n",
      "Epoch 1912/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5001 - val_loss: -4.5571\n",
      "Epoch 1913/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5251 - val_loss: -4.2101\n",
      "Epoch 1914/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4693 - val_loss: -2.9535\n",
      "Epoch 1915/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5030 - val_loss: -3.9389\n",
      "Epoch 1916/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5229 - val_loss: -4.1580\n",
      "Epoch 1917/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5325 - val_loss: -3.0897\n",
      "Epoch 1918/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5481 - val_loss: -4.1446\n",
      "Epoch 1919/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4839 - val_loss: -4.0630\n",
      "Epoch 1920/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4933 - val_loss: -4.7345\n",
      "Epoch 1921/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4617 - val_loss: -4.0329\n",
      "Epoch 1922/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5203 - val_loss: -4.1173\n",
      "Epoch 1923/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4681 - val_loss: -4.1715\n",
      "Epoch 1924/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5991 - val_loss: -3.8443\n",
      "Epoch 1925/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5249 - val_loss: -2.8792\n",
      "Epoch 1926/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5081 - val_loss: -0.4968\n",
      "Epoch 1927/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5202 - val_loss: -4.1839\n",
      "Epoch 1928/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5613 - val_loss: -3.8520\n",
      "Epoch 1929/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4846 - val_loss: -3.9394\n",
      "Epoch 1930/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5618 - val_loss: -3.6631\n",
      "Epoch 1931/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5292 - val_loss: -3.6105\n",
      "Epoch 1932/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4822 - val_loss: -3.6082\n",
      "Epoch 1933/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5041 - val_loss: -3.9563\n",
      "Epoch 1934/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.6029 - val_loss: -3.1858\n",
      "Epoch 1935/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3336 - val_loss: -4.2926\n",
      "Epoch 1936/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3814 - val_loss: -3.8734\n",
      "Epoch 1937/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5342 - val_loss: -3.3559\n",
      "Epoch 1938/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5203 - val_loss: 0.4709\n",
      "Epoch 1939/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4824 - val_loss: -3.9532\n",
      "Epoch 1940/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5183 - val_loss: -3.8293\n",
      "Epoch 1941/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.5125 - val_loss: -3.3491\n",
      "Epoch 1942/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4753 - val_loss: -4.5169\n",
      "Epoch 1943/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5416 - val_loss: -2.6678\n",
      "Epoch 1944/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4975 - val_loss: -4.3676\n",
      "Epoch 1945/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3813 - val_loss: -3.9594\n",
      "Epoch 1946/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5721 - val_loss: -4.2328\n",
      "Epoch 1947/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5409 - val_loss: -3.6739\n",
      "Epoch 1948/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5684 - val_loss: -4.0940\n",
      "Epoch 1949/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5793 - val_loss: -4.0952\n",
      "Epoch 1950/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2803 - val_loss: -3.8184\n",
      "Epoch 1951/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5030 - val_loss: -4.0926\n",
      "Epoch 1952/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5897 - val_loss: -4.1598\n",
      "Epoch 1953/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4815 - val_loss: -4.5029\n",
      "Epoch 1954/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5843 - val_loss: -4.2908\n",
      "Epoch 1955/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4660 - val_loss: -3.8247\n",
      "Epoch 1956/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9845 - val_loss: -3.9527\n",
      "Epoch 1957/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4405 - val_loss: -4.3624\n",
      "Epoch 1958/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5702 - val_loss: -4.1475\n",
      "Epoch 1959/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5482 - val_loss: -4.1897\n",
      "Epoch 1960/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4896 - val_loss: -4.3389\n",
      "Epoch 1961/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5531 - val_loss: -3.8842\n",
      "Epoch 1962/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5097 - val_loss: -3.5885\n",
      "Epoch 1963/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5119 - val_loss: -3.7385\n",
      "Epoch 1964/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5721 - val_loss: -2.1953\n",
      "Epoch 1965/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5592 - val_loss: -4.2933\n",
      "Epoch 1966/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3820 - val_loss: -3.5813\n",
      "Epoch 1967/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5981 - val_loss: -3.6128\n",
      "Epoch 1968/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5716 - val_loss: -4.5012\n",
      "Epoch 1969/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.4408 - val_loss: -4.5436\n",
      "Epoch 1970/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5558 - val_loss: -4.1722\n",
      "Epoch 1971/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0958 - val_loss: -4.1523\n",
      "Epoch 1972/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4438 - val_loss: -3.2376\n",
      "Epoch 1973/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4299 - val_loss: -4.3376\n",
      "Epoch 1974/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4416 - val_loss: -4.5851\n",
      "Epoch 1975/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5423 - val_loss: -3.5085\n",
      "Epoch 1976/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5220 - val_loss: -4.3014\n",
      "Epoch 1977/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3845 - val_loss: -4.4232\n",
      "Epoch 1978/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5704 - val_loss: -3.5358\n",
      "Epoch 1979/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5643 - val_loss: -2.6384\n",
      "Epoch 1980/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5178 - val_loss: -3.8843\n",
      "Epoch 1981/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5853 - val_loss: -3.8704\n",
      "Epoch 1982/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5279 - val_loss: 1.6200\n",
      "Epoch 1983/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4654 - val_loss: -4.2389\n",
      "Epoch 1984/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5144 - val_loss: -4.4570\n",
      "Epoch 1985/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5990 - val_loss: -3.7651\n",
      "Epoch 1986/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4394 - val_loss: -3.9353\n",
      "Epoch 1987/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5617 - val_loss: -4.1446\n",
      "Epoch 1988/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -5.5962 - val_loss: -4.2977\n",
      "Epoch 1989/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5254 - val_loss: -4.3484\n",
      "Epoch 1990/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5702 - val_loss: -4.5879\n",
      "Epoch 1991/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5624 - val_loss: -4.6773\n",
      "Epoch 1992/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4422 - val_loss: -4.1704\n",
      "Epoch 1993/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.6053 - val_loss: -3.9941\n",
      "Epoch 1994/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4466 - val_loss: -4.5662\n",
      "Epoch 1995/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4982 - val_loss: -4.6234\n",
      "Epoch 1996/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.5542 - val_loss: -4.0669\n",
      "Epoch 1997/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5313 - val_loss: -4.3549\n",
      "Epoch 1998/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5418 - val_loss: -3.3879\n",
      "Epoch 1999/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5726 - val_loss: -4.4231\n",
      "Epoch 2000/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.5020 - val_loss: -4.4030\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABJrElEQVR4nO2dd5gURdrAf7UBlgwSJSigBCUHI4ogqAQFRT3lDKBnzp6JM+Kpn3pnOsyoGFFUVAQRFFCCgOSc05LDssiyC2yaqe+P7tmd0JPzzvt7nnmmp6a66u3q7nqr3qp6S2mtEQRBEFKPtHgLIAiCIMQHUQCCIAgpiigAQRCEFEUUgCAIQooiCkAQBCFFyYi3AMFQr1493bx583iLIQiCkFQsWbLkoNa6vnt4UimA5s2bs3jx4niLIQiCkFQopbZbhYsJSBAEIUURBSAIgpCiiAIQBEFIUZJqDEAQhNhQUlLCrl27KCwsjLcoQhBkZWXRtGlTMjMzA4ovCkAQBA927dpFjRo1aN68OUqpeIsjBIDWmtzcXHbt2kWLFi0COkdMQIIgeFBYWEjdunWl8k8ilFLUrVs3qF6bKABBECyRyj/5CPaeiQIQBEGIJoV5UFocbyksEQUgRI+S45C7Jd5SCElIbm4unTt3pnPnzjRq1IgmTZqU/S4u9l2ZLl68mPvuu89vHueee25EZJ05cyaXXnqp9wiHtkLO+ojkFWlkEFiIHt8Mg02/wFO5kC6PmhA4devWZfny5QCMHDmS6tWr8/DDD5f9X1paSkaG9TPVvXt3unfv7jePefPmRUTWgNC22OUVBNIDEKLHlhnmgew6J4TP8OHDueOOOzjrrLN49NFHWbhwIeeccw5dunTh3HPPZcOGDYBri3zkyJHcfPPN9OrVi5YtWzJq1Kiy9KpXr14Wv1evXlx11VW0bduW6667DsdOiT///DNt27alW7du3Hfffb5b+m589dVXdOjQgfYXXs1jL/wPAJvNxvDhw2nfvj0dOnTg9ddfB2DUqFGcfvrpdOzYkWuvvTb8wgoQaZYJ3tk+Dz7uDw+ugVpN4y2NECeenbSGtXuORDTN0xvX5JnL2gV93q5du5g3bx7p6ekcOXKEOXPmkJGRwfTp03n88cf57rvvPM5Zv349v//+O/n5+bRp04Y777zTY578smXLWLNmDY0bN6ZHjx7MnTuX7t27c/vttzN79mxatGjB0KFDA5Zzz549PPbYYyxZsoQ6x7O5eOhdTJgwgWbNmrF7925Wr14NwOHDhwF46aWX2LZtG5UrVy4LiwXSAxC8s3iM8b09hl1lQfDB1VdfTXp6OgB5eXlcffXVtG/fngcffJA1a9ZYnjNw4EAqV65MvXr1aNCgAfv37/eIc+aZZ9K0aVPS0tLo3Lkz2dnZrF+/npYtW5bNqQ9GASxatIhevXpRv359MjIyuG7IAGbPnk3Lli3ZunUr9957L1OnTqVmzZoAdOzYkeuuu44vvvjCq2krGkgPQBAEn4TSUo8W1apVKzt+6qmn6N27Nz/88APZ2dn06tXL8pzKlSuXHaenp1NaWhpSnEhQp04dVqxYwS+//MJ7773HN998w5gxY5g8eTKzZ89m0qRJvPDCC6xatSomikB6AIIgJCV5eXk0adIEgE8++STi6bdp04atW7eSnZ0NwNdffx3wuWeeeSazZs3i4MGD2Gw2vpowlQsuuICDBw9it9u58soref7551m6dCl2u52dO3fSu3dvXn75ZfLy8igoKIj49VghPQBBEJKSRx99lGHDhvH8888zcODAiKdfpUoV3nnnHfr160e1atU444wzvMadMWMGTZuWj5N9++23vPTSS/Tu3RtdcpyBfc5j8ODBrFixgptuugm73Q7Aiy++iM1m4/rrrycvLw+tNffddx+1a9eO+PVYoRyj3clA9+7dtWwIE0O+uwVWfQtDPoCOfwv+/H/XBXspPHUQ0gNzTiUkBuvWreO0006Ltxhxp6CggOrVq6O15u6776ZVq1Y8+OCDwSWyZ5nx3bhL5AW0wOreKaWWaK095saKCUiIPknUyBAEZz744AM6d+5Mu3btyMvL4/bbb4+3SBFFTECCIAheePDBB4Nv8ScR0gMQoo84FROEhEQUgCAIQooiCkAQBCFFEQUgRB8ZBBaEhCSuCkApVVspNV4ptV4ptU4pdU485REEITHo3bs3v/zyi0vYG2+8wZ133un1nF69euGYJj5gwABLnzojR47klVde8Zn3hAkTWLt2bdnvp59+munTpwchvTV+3UbHgXj3AP4HTNVatwU6AeviLI9ghbTghRgzdOhQxo0b5xI2bty4gP3x/PzzzyEvpnJXAP/+97/p27dvSGklOnFTAEqpWkBP4CMArXWx1vpwvOQRBCFxuOqqq5g8eXLZ5i/Z2dns2bOH888/nzvvvJPu3bvTrl07nnnmGcvzmzdvzsGDBwF44YUXaN26Needd16Zy2gw5vifccYZdOrUiSuvvJJjx44xb948Jk6cyCOPPELnzp3ZsmULw4cPZ/z48YCx4rdLly506NCBm2++maKiorL8nnnmGbp27UqHDh1Yvz7wDWDK3Ea3b89jjz0GxM5tdDzXAbQAcoCPlVKdgCXA/Vrro3GUSbAi7GmcIfYgFoyGgxtg4Kth5i+ExZQRsG9VZNNs1AH6v+T17xNOOIEzzzyTKVOmMHjwYMaNG8ff/vY3lFK88MILnHDCCdhsNvr06cPKlSvp2LGjZTpLlixh3LhxLF++nNLSUrp27Uq3bt0AGDJkCLfeeisATz75JB999BH33nsvgwYN4tJLL+Wqq65ySauwsJDhw4czY8YMWrduzY033si7777LAw88AEC9evVYunQp77zzDq+88goffvih32JwcRtdpw4XX3xxTN1Gx9MElAF0Bd7VWncBjgIj3CMppW5TSi1WSi3OycmJtYxCPJnyCCzy/xIJFRNnM5Cz+eebb76ha9eudOnShTVr1riYa9yZM2cOV1xxBVWrVqVmzZoMGjSo7L/Vq1dz/vnn06FDB8aOHevVnbSDDRs20KJFC1q3bg3AsGHDmD17dtn/Q4YMAaBbt25lDuT84eE2+rrrYuo2Op49gF3ALq31AvP3eCwUgNZ6NDAaDF9AsRNPKEPGAFIbHy31aDJ48GAefPBBli5dyrFjx+jWrRvbtm3jlVdeYdGiRdSpU4fhw4dTWFgYUvrDhw9nwoQJdOrUiU8++YSZM2eGJa/DpXQk3EnHym103HoAWut9wE6lVBszqA/gXZULgpBSVK9end69e3PzzTeXtf6PHDlCtWrVqFWrFvv372fKlCk+0+jZsycTJkzg+PHj5OfnM2nSpLL/8vPzOfHEEykpKWHs2LFl4TVq1CA/P98jrTZt2pCdnc3mzZsB+Pzzz7ngggvCukYPt9FffRVTt9Hx9gV0LzBWKVUJ2ArcFGd5BCvCHQOQHoQQIkOHDuWKK64oMwV16tSJLl260LZtW5o1a0aPHj18nt+1a1euueYaOnXqRIMGDVxcOj/33HOcddZZ1K9fn7POOqus0r/22mu59dZbGTVqVNngL0BWVhYff/wxV199NaWlpZxxxhnccccdQV2PT7fRWjNw4MCYuo0Wd9CCdyLlDvqJ/ZCZFfz5I2uZ33nBnyuEhbiDjiDiDloQBEFINEQBCP5Jol6iIAiBIwpAiAGiQJKRZDIPCwbB3jNRAIJ/xJ9/ypGVlUVubq4ogSRCa01ubi5ZWYGPt8V7FlD8KT4KlarFWwpBSCiaNm3Krl27kMWXEeDwAeM7L/quzrKyslxmGfkjtRXAsi/gx7vhnsVQr1W8pUlcpBWYcmRmZtKiRYt4i1ExGHm2+Z14s9lS2wS0wVxEkhO44yYhBESBCEJCktoKQAgMGQMQhAqJKABBEIQURRSA4B8x4QhChUQUgOCDSJl+RIEIQiIiCkDwQbgVt4wdCClOgveeRQEI/gl5EDixH35BSHVEAQiCIKQoogAE/yR4N1YQhNAQBSBEH1EgQqqS4M++KADBP7IQTBAqJKIABEEQUhRRAIJ/ErwbKwiJS2K/O6IAQCq4qCPlKwiJiCgAwT8hjwHI2IEgJDKiAEAGOaOGtPwFIZGJuwJQSqUrpZYppX6KtyyCF8REJgihkeDvTtwVAHA/EP290oQQiFDPKMFfAkFIVeKqAJRSTYGBwIfxlEPwhjiDE4SKTLx7AG8AjwL2OMsh+EKcwQlChSRuCkApdSlwQGu9xE+825RSi5VSi3NycmIkneCCmHAEIUQS+92JZw+gBzBIKZUNjAMuVEp94R5Jaz1aa91da929fv36sZYxxZENYQShIhM3BaC1/pfWuqnWujlwLfCb1vr6eMkjWCFjAIJQkYn3GICQDMgYgCBUSBJCAWitZ2qtL423HEnH7Fdg78p4SyEIgjcSfPwsIRSAECK/PQfvnx/9fMJ9iBP8JRCEVEUUgOCDcG34MgYgCImMKADBB9JyjxmLP4bsP+IthZBiZMRbACEJkEHg6PPTA8b3yLy4iiFEmsR+B6QHIPhHbPiCUCERBSD4QBaCCUJFRhQASAvXK7IQTBAqMqIABP/IGIAghEaCNy5FAUBy7ggWywcrwR/ioMmeC7lb4i2FEEn+fA+WfxVvKZIOmQUk+KCCbgjzyQDjW2bcVBymPmZ8dx4aXzmSDOkBJCuJVqlakoQ9K0FIIUQBCD4IV8kkg5IShGiS2O+AKADBP8k4RiIIgl9EAUCSmFPckUFgQRDCQwaBBU8O74ANUxBncIJQsREFAGLicOfzIZC7CVr2jrckgpDcJHjvWUxAyUo0H6zCw2YeduNbFoIJQoVEFIDgH9kQRnDn50dl4VUFQExASUsMKtWwTWNiWquwLHzf+JaFV0mN9AAEQRCiRmL3fkUBgJgovBF2uUi5CkIiIwogWUkqpZVMsgpC6hA3BaCUaqaU+l0ptVYptUYpdX+8ZJFpoF6QMQBBCI8Eb6jFcxC4FHhIa71UKVUDWKKUmqa1XhtHmQRBEFKGuPUAtNZ7tdZLzeN8YB3QJE7CxCXb8IiBzDIGIAgVmoQYA1BKNQe6AAss/rtNKbVYKbU4Jycn5rIJCUjORsjfH28pBCHpibsCUEpVB74DHtBaH3H/X2s9WmvdXWvdvX79+tESIjrpJjuRKpdI97DePgNeOy2yaQpCVEjsXnBcFYBSKhOj8h+rtf4+nrIkHUlhtoqiYtW26KUtCClCPGcBKeAjYJ3W+rV4yRERtIaCCmieSgolIwhCqMSzB9ADuAG4UCm13PwMiKM8oTNvFLxyKhzaFsNMk6FyTgYZBQ8OrIc9y+MthRAD4jYNVGv9BxVlovimacZ33k44oUV8ZUlIRBEkFe+cZXyPzIuvHBWBcHrR2+ZA3VOh5omRk8eNuA8CJwRi6rBGFoIJQvz49FJ4v2dUsxAFkKyI0hKEis/RA1FNXhQAJO50x3gjC8EEoUIjCkCIPhVNMQpCwCT2sy8KIJLEdEGZbAgjCEJ4BKQAlFLVlFJp5nFrpdQgcxGXUJGRlrsgVGgC7QHMBrKUUk2AXzHm738SLaGEAJDKWRCEMAlUASit9TFgCPCO1vpqoF30xIoxUplaEzGTlpSvkKIkeN0SsAJQSp0DXAdMNsPSoyOSkDAk+MMrCEJ4BKoAHgD+BfygtV6jlGoJ/B41qWJNUnoDlcpZEITwCMgVhNZ6FjALwBwMPqi1vi+aggkJQFIqRkEQAiXQWUBfKqVqKqWqAauBtUqpR6IrWgyJlKmjoplMArme9ZNh7qjw0xGECkliP/uBmoBONzdruRyYArTAmAkkxItEqVTH/R2mPRVvKQRBCIFAFUCmOe//cmCi1rqERFdtwSCmDmukXAShQhOoAngfyAaqAbOVUicDHts3JjS7l8JbZ0JRvud/idKaDopklFkQhEQiIAWgtR6ltW6itR6gDbYDvaMsW+TQGj69DA5ugF2L4y1NxWTbHCgtcgsUJSWkOAneuAx0ELiWUuo1pdRi8/MqRm8gOdj6OxQXGMfK4pLF1GFNMA/vp5fC1H95Sygi4giCEFkCNQGNAfKBv5mfI8DH0RIq4hzZ4/QjipVRLBVJIrYscta7BYhiTVlsJXA0N95SJD6bpsOST+KWfaBbQp6itb7S6fezSqnlUZAnOhz/q/zYw0xBYlamgpDMfH8rrPlBtpX0x1izWu02PC7ZB9oDOK6UOs/xQynVAzgeHZGiQM6G8mOp7INAykoIkTU/xFsCIQAC7QHcAXymlKpl/v4LGBYdkaLARf+GZZ+bPywqtaTcESwRK2cv5ShKVxASkkBnAa3QWncCOgIdtdZdgAujKlkkqXoC3DbLOJbKSBAEAQhyRzCt9RFzRTDAP8PNXCnVTym1QSm1WSk1Itz0/GRmfGt7VLOJGbFQZKIsDSY/BCNr+Y8neJLqz1CCX384W0KGZTdRSqUDbwP9gdOBoUqp08NJ00+O5ndi3xAhAVn0YbwlCJzcLcZHiD5F+bBharylCItwFEC4NemZwGat9VatdTEwDhgcZpreccz/T3CNnJiEW2ZS5jHjza7GJ1FIhvctfx/sXBj8eRPugq+uiY7CjVG5+VQASql8pdQRi08+0DjMvJsAO51+7zLD3GW4zbEALScnJ/TcfJmAkuEhjSSrxsO8NyOfriyoEzxIgnfrnbPho4uCP+/gJuP7wNrIyhNDfCoArXUNrXVNi08NrXWgM4jCQms9WmvdXWvdvX79+mGkJCagMr77B/z6ZBAnSMXulwPrjZakkHw4rxMKhpx1xvf4m31ECrG+iVGjNCaVuBd2A82cfjc1w6KDLxOQtFytSbWeUTi8cxagYOTheEuSWKTCM5TE1xjOGEC4LAJaKaVaKKUqAdcCE6OWm8wCih/JJGtYpMp1CtGngvcAtNalSql7gF8wNpgfo7VeE70cfbTyU6aCEoRYI+9WIhNPExBa65+Bn2OSmcwCCgEpK0EIi1Drm0SYBVSh8GUCSsoxAKmchSRAGlwJTeoogDIsHkh5SKOMlG/KU1II9giNv42/GUYl0FqHUNi91Mf+GRCrdyZ1FECimoBWfwdTQvCCIa4ghKRAG8/RCw1hctjeYwxWfweHkny180cXw5/vxFuKVFIACWoCGn8zLHg3fvkLkeXQ1nhLkFg4NyKWJPgeUj/cAUs+jXCiid2ISh0FIAvBQkDKKmi+uyXeEgRGUb715kjRIFl6kiu+gkn3xVsKAxkEjjC+TEDJ8oC6EE2ZQ+wRJeVgeqRJkjJ4sSm8f0EMMtJIQ8KKxCiTFFIAFWwhWFRxfzjDfFitFOy+VcGnU3hE3DJHEocrg2iTlA2seCM9gAjjwwQkLdfYsvZHeO88wyldMByJnqeQiCHPkiupUPn7uufO1z/vTT8zf2JP6iiAimYCiqrMjt6Sdv0dKRx7NOesD/LEZKhck0HGWJOE71cwBPou/vpk+cwff+fIGEDkeG3aRoZ9ssj4ISagAAj14Quy8nN+yEfWgg/87DKaDK3rZJAxpujkbGBFip8eiLcEPkkJBVBQWMr23OPmrxiYgHYuBLstsml6EIuXKlJ5BJjO7iURys8Hxw5FOQMF2+fD2gD9Gm6dCa+1i6pEQhzZEKqnG+kBRIwqldI4VmK2/KPdGsmea2wuMfeN6OYTL0bWgh/vjkxaQSveCCjqDVNCPzeQZ0cp+LgffHMDbJvtP/6vT8KRXaHLlOhomQVkTWKUSWoogMx0Sh2Wn2iPATgGKg9EYIbF0dzAW6zHDsEbHWF/DHYnWvZF9POwIiI9tWi/eE4yfnpZcPErKqlsAgoVGQOIHNUrZ5S99gcLCuMqS1D8tyX8p4X1f+4PyKZf4fB2+OP1yOUf7Ycw2PQXvBcdOQIl0B5AMAQS324zZk4lZUUqPYBEJiUUwOVdmnBJe2ML43pznoJx17lGiOQYQLgv6aKPgp8eCUS9JblzkbF6NBQ8yiREWRd9GNp5zoR1f+JUkS38AL650VipmmwkpdKKAX7LpYJvCBNLaletxE3ntYDNZsD6n2KQa4iVnMNhVoerIidKuBQdgY/6wil9IpRgRa4UojCukb/H+C7YH7Q0CUGklEBhHmTJQsBIkhI9AICMjEzvfyZlKyXCq3VdcFs05/AZs3e5n9OiYP6IOG7ldHCT4Zo3oFOjYAKq8ETIBLR5Brx0kjFrSogYqaMAKlWJtwixIRoDpVZplkRgLCUeitc9z7e6wwe9Az054uIEdL+SsoESJHa7b/cgO+Yb3zsXxkaeqCMLwWJKRqXKFOt06z8j2mqL0cvq7QGJ5IPjK61fnwwmIbff8Wwlp0BlmkjoABeCzX/TcA+yY0H0ZYo0SdzrSxkFkJmmKKSS9Z/RqDRj/lBEMj+3tKzK5/D2MNJP0ko4KiagYOIna0UTQLntXWF85+30k1QCPjtRkUl6ABElMz2NInyMA0ScZH1ZoezhyzVHzcs8qfp7KJNgDCDqs4CS+b5Hg0hVZAlcrtIDSHzS0xXa2+Um5Q2MQQuh8LB5EET5lBbBjj8Di2tVGX9zY+B5hUSUyy0pBsJDYO4o+PnR0M6NaAs5AXsA0aAijwEopf6rlFqvlFqplPpBKVU72nlmpCnsidyKiBhhPDh2OxQV4FnhB5HmL4/DmEvggJOnz2DWAaz9MfC8QiGcFyugc+Pg3sKK31+MbHrTnoKF7wd/XqRcQSSyokxEs1SAxKsHMA1or7XuCGwEou4kO035UACJeAM3Tw8ufiRekN9fgBebGPOtnfE2rrFnmWca+9cY38cPeca328FWGr6c0SBe0wsDum8hPJ+zXgr+nGgRzPuViO+iP6KinCpwD0Br/avW2lET/Ak0jXae6WkKe0wuN0I37tA2P9lE4QFZ+bXxbQtgr1i7DUb38h3HXcaJ98JzdYlvN94p701OSvazwcGd641otlRDTTt/H3xxJRw/HFFxQsJWCsu/NBoDLiRwC78CkwhjADcDXl00KqVuU0otVkotzsnJCTmTdF8vj/t/pUWGI7ZwSOQuqzeCaqmFsK/C8jg5kXPG+RrHXhn6uV5JEBOQM3+8bvQofbmS2LUkOu64tYZ1k8p/L3wfJtwJSz8NLb3lX0ZGrlAoLYI8C8+t0WiMJfsYgFJqulJqtcVnsFOcJ4BSYKy3dLTWo7XW3bXW3evXrx+yPGnuYwBbfnfOxDXy2KsMR2wph5eHzkqZbf3dMyxg/FR62XONCikROX7Y8M0TS1PF4k/COz8QWT+80P+GPKEy8Z7y46NmI+54sPsymM9MWNOPw+TzIfC6xd4NydjYM4maLyCtdV9f/yulhgOXAn20js3b5DILaMtv3iMG4sc97gSwEGz/Wvj8CrhjDlRvEECSXlr1gd4e9xch1BfjkwGhnRcI4U4D/Xa4ofxaXAD1W3tGcb/mSfcbPpROH2SdZCBlVByiEz7PzCKUTjAkoU3fG9v/iH4ee1dCyTGo3zb6eRG/WUD9gEeBQVrrY7HKV3t7AWKpwQ9uhoID/uMFXVFZXMP8t6FgH2z8JbAkvOYZ4kucIB4P0Rq2zwu/1a51AC1Qt/uw5BNjc5iIYHGPS4vjW84bf43B7ncJTiTbr++fb8yiq8iDwMBbQA1gmlJquVIqJo7em6t9Tr+iVMD+Hoa3ull3IyOdDzgt4ArUXu+nV5GMMzQA1k2Ej/sblXG4972sLLyUqb/GxN6VcPwv5xMCz3vaU/DLE+W/bSXwfH3/bjmiuTr9y6th3pv+8xYSknjNAjpVa91Ma93Z/NwRByGsj8NhymNOP5Th12TBaM94tmL/aTm/rLYS372GdZPg+1ss0jBvrw6whRbKwG5YBLrCOEwcM6oObYnASmDtdGyFn0r2/fPh4wGQsxGO7AlehPlvwa7FxrHDS+vij4NPJ5L4c99Q0Xi9AxQ4TUiJhmJN9kHglCTHbRvIMRfDlEfCT/enB+CVVt49cM7+r3V4mun8zlvFfniH6++wTUAhrrOIaSsxwLw8pikC638uL0tvMnurDBY4LaI6sBbePgNeOy20yuPDPq55ebu/+9eaazUcskbL1OkrXbdyCvVeJ9JAa94O2DDZdxy7PSl6P6IAID4Pl7852c4Pz9qJxrfL/PwAKuuyHoCXuG908H6uN1kiih9zSqQI9v7uWw3/ruM5drJnqVMReSkT90V0DqY8CnuWWwkXnGyW53qR5d1z/K/V+DgCA+4qlGokAafLRgq7zXh+gvKYGx9SVwHMf6vcNBAPTb3ogwgm5m1w26EAAqxgvbqYjnQF7d4qjKHpKZB7vcv0Ob/hZ89z/fUAHH7rrRh9gWdYWI2PCCjQ7XPDyN/ElwIIuleZ+K1mv9jNNa4LLcy/CUbqKgCAA2uikGgYD/AsJ1OOc8UQsoIKchDYX7xIbV7iHifaCsCR385F4SaE/zGAILFypxEowQ7OR6un67MH4EW2P9+FpZ8ZO7JBuWx5O2FkrfJxDn/pxI1IvJ9OuJt33c2zUSK1FYA/HDf2wDp4sRnk7faM89ODkD3HM9zfy2b1zPz+vB+BvDx0ORu8RDdvb0mgM23DbJUpBTutPIF6q/CDnaXkg+Jj5YOiDg5tNSoTxwrXnX+WL0Sy4sd7oPCI73ycewB/bTcqsnAI+N74kCWY3aWeb+Q6HhEI75zj+39fz/oStxW/jrhHcwzXIO/2cP3fsUBzidvAdsLZ033I8/v/BZ/cmItdf38dqanDvhEFAN4fYMdDt+hDY2N0d5MAwOIx1ueEi2U6XtJ2HhvI2WhUettmlw8Cz/h3gHkGI4sF7g7s/PZiHC1YP7OUXj0Nvr3J+//FR+H/TvSsTDbPML7XTSwP++M17+ks+9xYO1Emnt11ho3L7lYaxl4NU0e4zgiJKaYs9lLjGQ3E3FJ63BiPCIYDa11/uzc4fCmA1eN9px2I3ymIXi8xd0sEEnEr97lvBJ+EY0McByVHQ5YmGEQBgPcX5+XmYSQaprnEZaqoVVffy7n7zX1V1/wQQpc/woPALlNtnV7gIreVrf5e7vw9sOZ77///X2PjO3dTcPJZ4iTzym+MGVgu/znZ3R3XEci03mjgXL6TH7LuiUaDt890/Z2/D0Z1NXpE7oQ0QIzR657/jlNAlHoAb3a1Ds/b7Uc5hGEC2u5jnKgszdiMi0XNFUSFoMic0eF8g6c+DkcPwJUfhp++r0GimRbufLfPhbYDLSL7GQQOFH8PXTi9G+fVon++7fpfzNcf+MD5Gkstpt06293TzdcnZ71nvJjgdj+8TRMOZyFfIKt8V31rfC8fayhD5319g34GTRl3m87pzrnLqIxjbQJ6/XTje6SXWV3h8HE//3FkHUAMCaal/Ofb5Q+8JYHeOG1tUnJg5f9l3N8Nu7Y73uR3fvkOBtA69vfQBePA6+P+5aYdrX2beSL9sG+aZm5sE2GcZwGhIc3cYvSLIZHL4/2ewcnjjN/KNoRydleCvu5VjUaG59Ed85yFCj5PZzbPMCpjZzNeshD17UfDRxQAGC2nSQ/AMYsKztk29/PD/tNyDERGcsKF84N0ZK+xeUlAriDSy4/dzS7WGQUZ7i85u/Wxr3jB4m6PPrDe8Ob62ukh7i6m/ZStLv9Ki0IHumxz9F3GWI7VSnJveH3mwqhMbCWuv8f7GIup0dgzLFQTkIO9y43vgxvDSyfSuDS6ojB9OkYdHlEAYPipX/IxzHzRdStDgE8uCy4th5JYM8F3PPdKJtCdsn64w9i85IjTjCS7l3MDefmcFy6F6w3UG6vH++6BhPOiuNujj5ouM4ryQrOJ+5zV46QcNk6Bg15mX4VL4RGY9oxx7GsleTBbbUJo5ex+zpofgks/3HEof8/etjmuJqdYEZArGekBJAcOO2fBAXjnLLf/SmBnCA9YcZAmiOfqev/PeUZAnjk/uOS4/zStFID7wNYXV5Ufe6sgZoYwrc2ZWS9bL4Lyl28oBFIuvigugPU+lvk7zGDe3G9Egpea+Z89A7D0E9ff3hS+oxUfiiIP5t5Yxd230i2OhQy+KlN/PddPL3WdQpnt5LJZa6MX5ZjZdTQ3+F3RDu80zK7+/DbZbfDVUFdlFPX9p8MnpRSAzdvlOgrbqtIuOQb7V0dGgO+cHLblbg4vrWC8gTqOjx6EaU+7xnGseg00zWgQUQUQAe/iW2ZYhyfaXHT36b3eWtvLve635J9wFYBnJM+gXYso6724jB/ge0ql1SydT5wmSTh6xg4Pqv9tCS+f7F/EzU73/432MKqL59ibuwnoyG5jTG/8za7hoRKxPSB8k1IKYGFWD+s/HA9uuPZKfzgPHq/6Jry0ln3uP87q71x/fzoI1v/kGW+hwy1FnCq4SPqTD7cH4JMEUwDuWG1X6EKQ8i/5NLh7E4gCsFKiod6zz68IMK8gr9tqUN/veE8S+SpyIqUUQNnCKA/MByRYBRDIzJpoYVWRu+M8cPbNjd5dXzjGLeI1HdNdUQXKH294hkVTAXgba0kUCvb7/j/YHszijyLfA/AWp8jPCmwr/G7O43S94a7Y9jX2AU49ggRvJLiRUgpAKS8KwOHAK5gX5Iur4K3u4QsVKwLxLRIPE4fdBtOfCe1cq/OiqcRK47TgK1D8rviOwxhAIHx1LWycGtq5vnCWZ+qI8NJa4L5nlXuLP4JuTdyZ/LAxlhEFUkoBlGZU8R0hmBbe5mkBR20+wsugojcfPvHgw4uIS+tltY8VvqEQTQUQqNuCRMWbOcerKwvle/2GO6E4AoTIjNtY5hXFZ8Gb2+9oNKIi6jnYlZRSAFMb3u47Qqy7+Pl7Y5ufL5wHg2PF1lmRf/mjuT9tSOsKEghvz/dvXnoOe5fHzgQUDea9Fb20f33C9bfDyWDBPs+4CUxKKQBVpY7vCFGrPOLQsg5o4Vec2b3Yx7hMiCSSW4lEw2vZOJkzfnZzFGe1M1rQ6btECjy9cAl3+nIw+JrmHChx2DkvpRTAyfWqcV/xPd4jbP/D+39hcGXaHOy2GFdMPt1VJBCRnnnl3jKrqITk1yeABYML3VxFBzOFNNRZQLFm60xYFcA6i1jjr9EWhcZNSjmDq1+jMhPt5zKKKHYNLXi10nvwnPsgkgBEz/5b0QmlIvXYZCUAfLnPdsexCtsXseihae17BfJng43vRHMv4b4Hgjt2W8R7zCnVA6hXvXK8RRDcmfxQvCVITnaHUJl7W+AWqamz00cGECkGPYD3ewamIGe9HH1ZgmH5l77/j4LyjKsCUEo9pJTSSql6scjPoQBm9k1Cz4KC4MxHF0UurViOF8WiB7BvJTxbO/r5RBp/bsUrkgJQSjUDLgZis/klUK96JQCy1UmxylIQEp8NPnwfRRr3HfSEwAlmSm6AxLMH8DrwKDGcFlCnqqEARk5aS0HHYbHKVhAEIXwqSg9AKTUY2K21XhFA3NuUUouVUotzcsLbezUtrXxg6N3Sy8NKSxAEIaZEYZp61BSAUmq6Umq1xWcw8DjwtL80ALTWo7XW3bXW3evXrx+2XE1qG6uBf9gSesejX5HFdo2CIAjRJJnWAWit+2qt27t/gK1AC2CFUiobaAosVUo1ipYszrx3fTcA9uR52T/VjbMKPaeMbtJNmGHr4hJ2SFcPXzhBEARvVIQxAK31Kq11A611c611c2AX0FVrHZM11B2aljtV6lX0KtcX/4tnS25go70JJZ1ugKauO0zt5wRydY2y3+cV/Q8b6fyj5BF+tJ1bFj7L3in6wgtRY6O9SbxFEBKEd0oHxVsEayrKGECikK1P5A97Bz629efi4v+Se+Er5F/7AyVdhnO0211cXmT4SOlbVL770y5dboZaZG8DwBFdhYlOyiAQRlW/PwJXAAx4xW+UubZ2kckrQbim6KmIpznffrrvCB2vcf1dr3XEZRASg2X2U2OT0ZAPg4tfERWA2RM4GMs8N7/Q3zI8v7CEDs/Pot2ifuSd/zTLtfEg/EVNNtmbsPuCV13izzRb/TcW/4vZ9o78aDuXIUUj6Vb4LpucWpS3FT/oct7jJf/gtYNuW086OKWP0/GF/i/GiyuFG4sfKzu+s+R+bimO4YKry0ZFJ92sWjQv/JJjRH5B33OlN8Cd88sDbpvpGuHkc13vR6YPz7KtLomobEJs2adPiE1G9VoFpwSOhjcJxoq4K4B4kJGexo3neG4Nd6TQ8JVSXGrHZncdcLmo+L+saXCpS9gu3YCchw6wXJ+KjXTuL7mHpbo1udTiouL/UKQNTxvr9UnsvmUl/GM6WmUwzdbNI+8PS/vDnfNY1/MdCmq3gXPvo/Cab9F1W3leQJcbyo8rVbO8xtlOJqlSMphu78b4ypfDsEmekR/Ltkxj5oUTLMP9ckpvy+AptjM8wp4suck14NFt3tOtZIyzqGBnDj/p48UxW/KlZEBDp15AIzeTnkqHoeOMD0D907ynWbNxcPLFkaL0JBi7an5+TLNbpVvGKCcNJ5/jGnT23d6j718bcQlSUgEA/Htwe965rqtL2JXvlu9Humq3p7/vZyd53gC715F5xYDiF/mgdAA7dAOOVzoBmp3Bulu3kUNtj9jPl96Avf7p9H93Ce33PUPBBc/Q9ulfePPUj+Dsu6BP+eYnzef3x+bY3KbD1eWJ3LMYhk3igeK7XNI+RpYhf+HfoUVPo8I/9174xzS6po+n77tuG3ebFNRuw2a7a2W2t2ob10jXfgkd/uZ26Wkwonx934x617Hg8tm8UHqdRx5L7K15uOR2aNQB/jEdqvpofZnXrL1tv9fAixkno5L3NPs+S/NCYwn+nsPHKdKZRnha+atRpDNYW+9iyKgMbfrDTVNhgI9N4bNqev/vsv95BHm9HgtKugw3Dqo1CPgcr2RU4cMeM8NPJxzOvdd/nKs/jb4cbjxf4vSs3hOC2w1nqvjqUbjd+2aejaQyOl0bnhwWpKwCABjQ4USv5qC7xi71CNt92NNnyvwtuV7T36Kb8ELp9YBiyqp9vDF9o0vPYtH5H0H1hgwzzTUlTq53Dx8zdp/6ekUu9HsRzv8nhT0eY5atIwBtj39stGydnUPVawUtejLBfh4A4/vMonOhk3dHx7NWpQ5c/Dw0O5NDR4vZfKAAqjc0/qvZlBnnfcXAohdYnP0XVxc/zRcNHi5L4pxDTrtwnXYZtB0Ig950vfAqdSCrFltOuoonS25iSsPbKanWhF3aqdK69Tf453rW6ZMZb7sA7vjD98MPUNkYjF+lW7iGNzYV+bBJMDIPzjfNXf1e8vryOnpnzqacgwVFdCt6l7/VNlv5I/P452mzaFP0GWtzSspPPvkc60r+lt+MCr6Sj1Z1s7M9gl49Z4Eh9z/X8VmphYuH2uUr19vP7wXXfQcD/Y/9eCXDaBBQtS4Z6U5VwNl3WcePFrWaQZ+R8LfPrP+vWhf+uQ4yDXmP6soUOhR0MJzmZVC3dT/PsK7GAtE0nOztdZp7Tdpe2YuyP9XpPj7kw8VD9YZwcg+48iPo/SS0dbIydPp7+XG1Br4d3IVISisAMMxBK565mP9d29ny/6u7NfV5/gNfLw8on1enbeSN6ZsodarkDzbowW+XzimbQVRiK1cOjo6F8z0v6vEIw0qMre1KyChv2Zot31kbXU0dxzJqc5jyGUw+H5/7V8Kwn+Cehfxe0Iw1ugXr9x3hL2oyt+YA17hn3WF8n3uf8Z2ZBX//FtoNMSoy0yy1pONIvrBd5HEdADTpBjVPLPtZ6uQuu7jT9QD8YOtRHr/vSPTQrzyvRKXBzVONHkc106VUr3/BDRPg7DsNpehM12Fw3j85o+gdXiwZCi17lctg1xRQlY1H0vlx+W6gvIeXFsi717QbdBsO6eZ9adLd6Nk406At3DKjbDzBrhXaYdKq2ZinS51MYgNfhce2wwOroNlZ/LP4DoqoBK36Qk2nWUtWCufCJz3DHBX8LTOMcZqbfibd+cIuCdJ//q2/+zTZ2VQGPHnAqOituPkXSM+A0wd7/nf9d/DoVsOcllkVzr6LG2xP0aXoffIe3A43TQlCUC+9dMfCqpPPKw8zn90053PSvSudw31ftf7jnLvg4c1w3zKj5+hQuu6kZ8BNP0OHq+CCR4y8bp8NlWoYaThwelciScorAIBaVTIZ3LkJCx7vw2knumr0/h0aMWa4/71/A1XO783aUnY8Yflubv6kvIVaUlpeCTp6Cs7plnrbnOP22Qyu/T3Dxiyk2CkN52PnNC3JzIIW50OlamSY5o9SUyFpbQwq31T8iBG3/8tGRd/Macps64vhamt3ts5FM7PKRWDRaipykrWo620AzLJ1gpPOMVpG5z2IrlXeEt7wj41GS3/ETuMFy3LaMzU90+s4BINGQd9nOEJ13rdd5lLAx4uNCuHwsRLuH7ccm13jKLI0Lzd4u7KYPtrE7JGcc5dhIjv1IpdWPE27ww0/MKn9KHoVv4b7bVlkb21U6mfcAlVqG4H/+JXv7T1d02hhbkJypdNAYrX68PAm6PkIXPsVLvR70bhvjdpDt2FQ52TSlOL+4rv44pRXXR+2K96Hu8yeyXn/NML+/o2rQjuhpWGya+PUQLhiNL+1eIieRa/zZo/5xr3xZiat5VR2Nzrttnb7bDi1b/lvpaDfi6zSp3CcLOwZVY1B+cvfgz5+1pOedK6L+dSFOuY4YHOnhkZvYz8J97ttq+5WATc1eqt2lU7PotfLw08bBOfcA817QvX6RhkBXPNFeZwT/UwZP7ETPL7LKGvHM+9+LyNESu0H4I+GNbOYcv/5TF29j0kr9tC4dha92zRAKcV3d57Dle/O5+S6VZl83/ks2f4Xw8aUb6N4f59WvDF9k988flmz3/IYIDv3aNnxqN+MtNKUYsD/5pCZkcboG1wHj/fmHaegsJTL357LUbPyKnZuSbttQnO02Mb4JbsoLrUzqHNjqle2vv2Z6cbjX+J0vvOg8lcLdzD0zNAc6r1f52F63eZpBikutVPNnNxTUu802hV+xFGq8MbNLzJh2W4uKiqlckZ5e2V9bilt/L38zgx+23srzMS9N3e8xFbeA7DqAjywmrs/WEX+ob3898YLKVOHLXrCg2ugltl7vH68saG8vcTl9J31erBDb/AYR7q6eCTZ/x7oVU6tNUopuP572DHPyG/ETkBDWiZUqmpEbDsAnsqF5+p6TSsjXfGj/Txq1nCbFOFsb+77jPEBaH0JTHsa5jqZuoZ+BccOGe6V213B8gPZ7Fi3qbzed1z3nfNgwxRo2M6zZ+TUE7OqIG/+ZFFZD9nmSLjzULCVwox/GxXtVWNgdC/j/FrNYP1PcLOPnkK7Icbn5HONsbQ6LYwWOaAcJqBuwwEoan0ZVZeOLj+3hqkQbKXs0A3pWvgeS1u8a5hW67iVJUCri6BeG+hxPxzeAXtXlJtdfXHHbKNsa1k0NCKAKAAL+rVvRL/2rguTu518Atkvlb+UF7Suz6c3n8mzk9awNecogzo15pJ2jahfozJ9Xp1F3vES92T9csU75YPQ3y81TBDbc8s3TCl1ayqe8+JvXHR6w7LKH2DEd+UDuoVO4Q4e/tZwv/T4D6vY9uIAj/8BMk278P4j1pugj5qxKSAFkG62KAtL7WUtKu2lO15sszNrYw7nn1oPm11zFMM2v2zHXzzw9XKGdG3CS0M6lsVXFi3yBVtzycxIo+tJrlt/FpXayOx0nXUl7kROvuv1bj5Q4NsEVLsZJRnb2K4bcVS5zcaq5WY6zKgEuA5Gl/UqgpzUVGLTVMpQRmXVwuwVeBt4Niu0sgrLDYcMpXbNX0eL8bNpqkHfZ+HCp8vTBqh6AoV9/49KaWlO99qkzQBjs5PaJ0PPhwmF39aXbzZjd34P0jPgjrmGnb5ydaPXUvNEw2xk5ea6UUfDXbQDR+vfzVT4re0CHmm6FtXT6PUe6343yxbNpUf6GsMEWmDIM33NbqANh6hp9Fx8cc9CxwUYvbvqAbi2qVLH+EQJMQGFwQWt6/PbQ73Y/EJ/Wtavzmkn1qRe9cosfeoi5jzam43Plw8wextjCIYeL/3mEbb7L9eB6Z9Wlm80P+q3zT7T++8vG8qOX/llAwu2GgPajoHBfUcMdxnulXagLkkcLbVJK/bw06q9Ps+dsGw3w8Ys5NP52S4tYsfU3P1HCl3CrSrka0b/yRAnJeqgzZNTeWLCarTWLN95GB3gBVw7en7ZlrjeTECVzF7Jp/OzaT5iclCK33EN3meSWVNUGqRLgPuWG61vJzYfyOd4sa1sDMBu13R5bhqvVX/IqFB9oZRr5W+e3/apqTwzcU25JclxXQP+Cw9tNCpoX9yzOKAZPzb38mrUvjztBm0Nc2B6pvWMspunkj9sujEecpJnT9RBDnUouXVOmSIvqdaQ60qeMGaM9X8Z2l8JwPubfMz48kZaWmCVfwwQBRABXGZSAOlpimYnVKVSRhpjbzmLN4d2YXDnJmS/NJDJ951HqwbVaVHPev5+sKzde8RvnKVPXcSLQzp4hL8zs3w84q3fN3PN6D+5e+xSj8rV3VS170ghhSU2Jq7Y41GZNh8xmRs+WsCn87LLxhAAvlxgTAst8bI3skOWHYeOuYxVOMxspTbtojwUit2Hj3Ok0HeF62gtfrVwBxNX7OHyt+cyyUlJ5vs4v7DEbtkDmLv5IO/MNJSrw1w2c4MxAL83L/DdtRxK5ZhFT80XRaXWZeiVE1q4VIYlNjt9X5vN3V8uLeul7Thk9DRHHexmVKhB4jA3frlwB8rsA5TdrvRMqBGAuaNeK2h3ud9oPseyvNH0TKjXml825dPh/QMsOe0Rv9srOitmjyxPu5TmhV+yTUducHb5zsPsPxKYj7JIISagKNPjVNfNzto1rsW0f15Q9rv5CGMzjucvb8/evOO0bliD+8ctj6gMWZlpDD3zJNo0qmHZQnZm8qq9sMp/mm2fmgrAf6au59DRYp4d1I76NQwj/pxNB5mzyXpx99Idh1m64y9qV8kkMz2NhjUrs/9IUVnLWaEsX/AF2w6x+3C5OUyj6fHSb5xYK4v5/+rjEd+B89TaSSuMin9rTkFZWIeRv/q8TocoxTbNvrxCqlZO57oPFwBwV69T2bi/wMfZgTF2wQ4GdWrMWS292+qdFZ27Ath56BgnVKtENS9jOu44JgfM3XyQfu0MU+f8reXTmV/8eR0PX9KmzBQYCEUlRpqhTFScvTGHI4UlXNoxsAV03uZC+OSWaQAsMNfyLNtxmG4n+17xW2yzk5WZbuZZ/kwu2Jrr9V51f346D/RtxfVnW4wD+OHyt+dSo3IGq56N3UpyUQBxZtETfalWOZ2qlcpvxSXtGmHXmoLCUl79dSNLd/zFw5e04fbPl4SURyXzRe56Uh02vdCfLTkF9HtjTkTk32WaoB4Zb72YzApfSmjM3G1c2c16wOs/U8tNVvd8uQyAvX68uh5wGseYvs7oyaggqinHOfd9ZeTXqGb5QLLdrikoKvU4Z+aGA7RtVJNGtYy4zUdM5sK2DejUtDZFpTau7NaUGz5cwKDO5de5ePtfLpWK3a7LxiymrNrLnU7rUvq9MZtVI8srifP/8zsA/7mqI31Pa8gJ1XwsfMO1F2Y1JvP+7K2c3rgmg035vl+6i1MbVKdj09pe0yw0zVJpSpGZYchdbLMzfe1+Smx2+rVvZDluA3Cj2csLVAE4TEB//+BPbjj7ZC46vSHpacoy/RKbneMlNmpmGVM5HT029/E0KwoKS8vOc26UXDP6T5fxwDK57JqDBUU8OWG1iwIY8d1KGteuwn19LFb1u5FfVMofmw6yLfcoN4SgRIJFFECccbSanXG0OqpWyuDlq8oHPtc/149tB4/SvG41qlRKR2vNzI05PPnDat6+riu/rz/A279v9ni4nU1UmelptG1Uk+lmL2Tpjr/ITFc8+LXfvXlCpkZWBvmFnhWlNwaO+sMy/Ne1+y3Dm4+YzNwRF7r8btOwBiMHtWPoB396xD9W7CnL/X1a8b8Z/mdx7XPqoudbVP4OxdqkdhUXmX5bf6BsIPNoUSl78gpdpgS7d/1/WrWXapXS6XNaQ5fKHyC/sJQP52zlph4tXExTj5pKeP1z/cqeISsc5hqlLEwbJo4K77HxK/l68U4Al0rv3q+Wcby4lLev60q6UuU9AAVVzLzfn7WV99kKwItDOnBFlyaMXbCDVg2q07O1bxv4vC0HqZmVSfsmtTz+s9k1drtm3pZc5pkLMW85rwVPXlq+EvzH5btpUCOLMXO3MW3t/jLZHWMepV5Mkc4cKSyhsTkZwZvp0hnnOFtzCmhZ3xiXGLfIKL87e50SUK/q+o+MHqYoAMGFrMx0l3UKSil6t2lQVtF0blabBy9qzapdeWg01SpnsPewdQv51AbVXb6v6GIMdi3ZfogaWZm0bliDUpudA/lF7Dh0jBHfraRto5ps3J/P1oNHLdP0xp//6kO7Z34J+nqDwX2AfMP+fMvKH4wWrjs1q2TStlEN1u8LfIP0Ts96Nx/tPnyc5iMmM/GeHh7/rd7jOW7z2fzt1K5a3nJ39Dg2vdCfjDTlodSfn7yOOlUrMbCjpw2663PTWPvvfhQUlVpO9XWYgOx2Y82DFcdLbIxbuKOs8ndGa82kFXsAY4AdoFcbo0IvKrVbKp9Vu/P41/fltkWrFrQzf/9ggdd4hSU2jynOn83f7qIA3M2ojh5VRppjinN5ec7amMOJtbIYPXury9qZBVsP0bZRTY/4jvTccTbNXfjqLA/Zn5m4hv+7wnMsLp6IAqiAOO95cEr94Jx9OdtFM9LTaFy7Co1rV2HmI64LqwpLbOQeLeaNaRu5sltTvlm0k++X7bZMs0pmetnL8Muaffy8ai8/Lt9T9v+lHU/kgb6t6Pua6zS6QZ0aM3HFHmLBwYIifrynB0eLbKzYeZhGtbLo/7/wzWSD3vKcUbNk+1+WcUdZ9EBaPeF9HvtD367goW89e27Him1lY0sAdatVYkjXJnwwZxvPXHY6H/1hrN4tttl5eaq1m4InfljtEbbn8HEa167CbIvxHccgOMDYBds9/ndMAnCQX1jCql15dGvuOcXRuXU+/OOFfHKT6x4d+YWlHgrAn1XvaHEpNbIyy3rDVhMN3Hlm4hoGdjyRn1ftpZOb+avl4z97xHdvEJSt1zD5csEO6larxEMXt3E/lWd+9CzvWKACnRKXCHTv3l0vXhymYyYh6mit+W39ATLT0zjnlLqW3d6iUhs9XvqNxwecxpCuRu/jQH4hew4X8tj4lWzYn8/cERfSpHYVDh8r5mBBEYPfMha8LX6yL5e8Ppvco8URk/m2ni15fICrh8/pa/dzxxdLePVvnfht/QEXpeVMgxqVeff6rlz57nzL/4XAmPHQBfR5dZZHeL3qlTlY4LpG46QTqpbNXHLmwxu7U1BU6rGob/6/LiS/sJSLXzcaGXdccAoj+rdl/JJdZWtjfPHK1Z14+NsVXHx6Q6+mSCse7Nua16dvdAnb9uIAlFLkHS+h07O/8tzl7XlqgqcCWP70RS69wnBQSi3RWnu4NBAFICQlRaU2Sm2aX9bs4+kf1zBqaGfOO7U+P6/a6/Lyd25Wm+U7D1umseypi3jqx9X8tHIv9/dpxYMX+d7kpbDExrszt/C/GZu478JTy9ZZTLynR9kA6YdztvL85HVBXcv65/rx5ITVjF+yC4D/XNmRR78LfFA9Evx073l8Mi+bnq3rl5mfKhJ39z6FE2tV4UmLijYQmtapwq6/jjNmeHcX9y2hsPH5/lTKSGPzgXyPXq87Hw3rTt7xEi7v3MTvQkZfiAIQUgKtNYuy/+KM5nVQSqG15odlu7mkXSOyc49SOSON9LQ0tuYU0Oe0hhSX2nn7983cccEpVKnke154oPnbtTGvPjv3KOeeUpd0pdhzuJBKGWn8vGova/YcYdbGHA4WFPHtHedwRnPr6Yg/Lt/tYctuUrsKJ9etWjb46aBRzSyXAepgWfxkX+pVr1x2DZe/M48VXhRnoKwceTG3fbaYP7ceCiudROLr286mxKbLBmpjyZxHe9PshKohnSsKQBASiMISG6V27dUfk4O84yUcL7bRqFYWNruxc4BShp3/0NFiqlfOoFaVTNLSDGU3Zm42hSU2tuQU0LBmFo1qZpFfWMLynYfZdKCgzLVIy3rVGDP8DHq9MpPeberz0bAzPFqYecdLyMpMo3JGOst3HuaRb1cw9taz+L/J66iRlUnnZrV56NsVXHR6Q4Z0acJ3S3fz/OXtKS61UykjjUa1sigssZGTX8Qva/Zxzil1mbp6H2/+tplpD/bkotf9uE5wo07VTP57VSeembjG0jV7LHCeYfXUhNW0qFeNyzo15u4vl7JwW3QV3Rf/OIvzWtXzH9ECUQCCICQky3cepkOTWhw6Wswfm3No3bAGp9SvzqGjxUxcsYcL2zagdcMaLuccKSxh/pZclmz/i5W7DqNQzN+ay70Xnkq7xrUY88c2qlZOZ3vuMQZ0aMTbv2+haqV0jhXb+Ob2c7BrzbWjrWeJgeG6pX/7E7lr7BLqVK3Et0t28Vi/ttzZ6xTL+Da74WbEZtd0alaL/03fxDszt/D+Dd04cKSQxrWr8NXCHUxfd4ATa2X5Xb9ixZ//6lO2tiRYRAEIgiD4wGbXpCljkVgwq6BDYc4mY9bUom2HuKv3qSzbcZhF2Ye44eyT+XLhDm7q0Zwpq/bx0R/baN2wOk8MPN1yzVCgiAIQBEFIUbwpAHEGJwiCkKLETQEope5VSq1XSq1RSv0nXnIIgiCkKnFZCayU6g0MBjpprYuUUg38nSMIgiBElnj1AO4EXtJaFwForQ/4iS8IgiBEmHgpgNbA+UqpBUqpWUqpM7xFVErdppRarJRanJOT4y2aIAiCECRRMwEppaYDjSz+esLM9wTgbOAM4BulVEttMSVJaz0aGA3GLKBoySsIgpBqRE0BaK37evtPKXUn8L1Z4S9UStmBeoA08QVBEGJEvExAE4DeAEqp1kAlwHoPQUEQBCEqxGUhmFKqEjAG6AwUAw9rrX/zeZJxXg7g6Ww8MOqRmEpG5AoOkSs4RK7gSFS5IDzZTtZae2zDllQrgcNBKbXYaiVcvBG5gkPkCg6RKzgSVS6IjmyyElgQBCFFEQUgCIKQoqSSAhgdbwG8IHIFh8gVHCJXcCSqXBAF2VJmDEAQBEFwJZV6AIIgCIITogAEQRBSlJRQAEqpfkqpDUqpzUqpETHMt5lS6nel1FrT7fX9ZvhIpdRupdRy8zPA6Zx/mXJuUEpdEmX5spVSq0wZFpthJyilpimlNpnfdcxwpZQaZcq2UinVNUoytXEql+VKqSNKqQfiUWZKqTFKqQNKqdVOYUGXj1JqmBl/k1JqWJTk+q/pXn2lUuoHpVRtM7y5Uuq4U7m953RON/P+bzZlVxbZhStX0Pct0u+rF7m+dpIpWym13AyPZXl5qx9i94xprSv0B0gHtgAtMVYcrwBOj1HeJwJdzeMawEbgdGAkxuI39/inm/JVBlqYcqdHUb5soJ5b2H+AEebxCOBl83gAMAVQGD6cFsTo3u0DTo5HmQE9ga7A6lDLB8Pn1Vbzu455XCcKcl0MZJjHLzvJ1dw5nls6C01ZlSl7/yjIFdR9i8b7aiWX2/+vAk/Hoby81Q8xe8ZSoQdwJrBZa71Va10MjMPYiyDqaK33aq2Xmsf5wDqgiY9TBgPjtNZFWuttwGYM+WPJYOBT8/hT4HKn8M+0wZ9AbaXUiVGWpQ+wRWvta/V31MpMaz0bOGSRXzDlcwkwTWt9SGv9FzAN6BdpubTWv2qtS82ffwJNfaVhylZTa/2nNmqRz5yuJWJy+cDbfYv4++pLLrMV/zfgK19pRKm8vNUPMXvGUkEBNAF2Ov3ehe9KOCoopZoDXYAFZtA9ZjdujKOLR+xl1cCvSqklSqnbzLCGWuu95vE+oGGcZAO4FtcXMxHKLNjyiUe53YzRUnTQQim1TBmu1883w5qYssRCrmDuW6zL63xgv9Z6k1NYzMvLrX6I2TOWCgog7iilqgPfAQ9orY8A7wKnYPhC2ovRBY0H52mtuwL9gbuVUj2d/zRbOnGZJ6wMf1GDgG/NoEQpszLiWT7eUEo9AZQCY82gvcBJWusuwD+BL5VSNWMoUsLdNzeG4trIiHl5WdQPZUT7GUsFBbAbaOb0u6kZFhOUUpkYN3es1vp7AK31fq21TWttBz6g3GQRU1m11rvN7wPAD6Yc+x2mHfPbsVtbrMuxP7BUa73flDEhyozgyydm8imlhgOXAteZFQemiSXXPF6CYV9vbcrgbCaKilwh3LdYllcGMAT42knemJaXVf1ADJ+xVFAAi4BWSqkWZqvyWmBiLDI27YsfAeu01q85hTvbzq8AHLMTJgLXKqUqK6VaAK0wBp6iIVs1pVQNxzHGIOJqUwbHLIJhwI9Ost1ozkQ4G8hz6qZGA5eWWSKUmVN+wZTPL8DFSqk6pvnjYjMsoiil+gGPAoO01secwusrpdLN45YY5bPVlO2IUups8zm90elaIilXsPctlu9rX2C91rrMtBPL8vJWPxDLZyycUexk+WCMnm/E0OZPxDDf8zC6byuB5eZnAPA5sMoMnwic6HTOE6acGwhzloEf2VpizLBYAaxxlAtQF5gBbAKmAyeY4Qp425RtFdA9irJVA3KBWk5hMS8zDAW0FyjBsKv+I5TywbDJbzY/N0VJrs0YdmDHc/aeGfdK8/4uB5YClzml0x2jQt4CvIXpGSDCcgV93yL9vlrJZYZ/AtzhFjeW5eWtfojZMyauIARBEFKUVDABCYIgCBaIAhAEQUhRRAEIgiCkKKIABEEQUhRRAIIgCCmKKABBAJRSNuXqhTRiXmOV4WFytf+YghBbMuItgCAkCMe11p3jLYQgxBLpAQiCD5ThK/4/yvADv1ApdaoZ3lwp9Zvp5GyGUuokM7yhMvzxrzA/55pJpSulPlCG3/dflVJVzPj3KcMf/Eql1Lg4XaaQoogCEASDKm4moGuc/svTWnfAWP35hhn2JvCp1rojhuO1UWb4KGCW1roThg/6NWZ4K+BtrXU74DDGilMw/L13MdO5IzqXJgjWyEpgQQCUUgVa6+oW4dnAhVrrrabjrn1a67pKqYMYbg1KzPC9Wut6SqkcoKnWusgpjeYY/tpbmb8fAzK11s8rpaYCBcAEYILWuiDKlyoIZUgPQBD8o70cB0OR07GN8vG3gRj+XboCi0wPlYIQE0QBCIJ/rnH6nm8ez8PwVAlwHTDHPJ4B3AmglEpXStXylqhSKg1oprX+HXgMqAV49EIEIVpIa0MQDKooc2Nwk6laa8dU0DpKqZUYrfihZti9wMdKqUeAHOAmM/x+YLRS6h8YLf07MTxRWpEOfGEqCQWM0lofjtD1CIJfZAxAEHxgjgF011ofjLcsghBpxAQkCIKQokgPQBAEIUWRHoAgCEKKIgpAEAQhRREFIAiCkKKIAhAEQUhRRAEIgiCkKP8PXfIcUq87GC4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 7s 45ms/step - loss: -5.2499\n",
      "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 8s 45ms/step - loss: -5.3993\n",
      "WARNING:tensorflow:Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.0667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 35s 254ms/step - loss: -1.0667 - val_loss: 0.4095\n",
      "Epoch 2/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -1.2526 - val_loss: 0.4626\n",
      "Epoch 3/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -1.2559 - val_loss: 0.5750\n",
      "Epoch 4/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -1.2580 - val_loss: 0.4866\n",
      "Epoch 5/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -1.2593 - val_loss: 0.5548\n",
      "Epoch 6/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -1.2605 - val_loss: 0.5892\n",
      "Epoch 7/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.2607"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 244ms/step - loss: -1.2607 - val_loss: 0.3824\n",
      "Epoch 8/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -1.2612 - val_loss: 0.5261\n",
      "Epoch 9/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -1.2612 - val_loss: 0.6899\n",
      "Epoch 10/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -1.2618 - val_loss: 0.3896\n",
      "Epoch 11/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.2618"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 258ms/step - loss: -1.2618 - val_loss: 0.3245\n",
      "Epoch 12/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -1.2626 - val_loss: 0.4837\n",
      "Epoch 13/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.2618"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 242ms/step - loss: -1.2618 - val_loss: 0.3110\n",
      "Epoch 14/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -1.2609 - val_loss: 0.7849\n",
      "Epoch 15/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -1.2607 - val_loss: 0.5521\n",
      "Epoch 16/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -1.2620 - val_loss: 0.7148\n",
      "Epoch 17/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -1.2618 - val_loss: 0.5734\n",
      "Epoch 18/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -1.2621 - val_loss: 0.7117\n",
      "Epoch 19/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -1.2615 - val_loss: 0.7297\n",
      "Epoch 20/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -1.2614 - val_loss: 0.3875\n",
      "Epoch 21/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -1.2616 - val_loss: 0.4117\n",
      "Epoch 22/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -1.2619 - val_loss: 0.7301\n",
      "Epoch 23/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -1.2618 - val_loss: 0.6350\n",
      "Epoch 24/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -1.2619 - val_loss: 0.4625\n",
      "Epoch 25/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -1.2619 - val_loss: 0.4506\n",
      "Epoch 26/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -1.2614 - val_loss: 0.5097\n",
      "Epoch 27/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -1.2616 - val_loss: 0.3944\n",
      "Epoch 28/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -1.2620 - val_loss: 0.5846\n",
      "Epoch 29/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -1.2615 - val_loss: 0.6427\n",
      "Epoch 30/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -1.2623 - val_loss: 0.5307\n",
      "Epoch 31/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -1.2623 - val_loss: 0.5467\n",
      "Epoch 32/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.8419"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -1.8419 - val_loss: -0.2093\n",
      "Epoch 33/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.4830"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -2.4830 - val_loss: -2.4194\n",
      "Epoch 34/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.1961"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 242ms/step - loss: -3.1961 - val_loss: -2.9353\n",
      "Epoch 35/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.5474"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -3.5474 - val_loss: -3.0230\n",
      "Epoch 36/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.6765"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -3.6765 - val_loss: -3.3520\n",
      "Epoch 37/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.7890 - val_loss: -3.2432\n",
      "Epoch 38/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.8630 - val_loss: -3.1572\n",
      "Epoch 39/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9279 - val_loss: -2.7931\n",
      "Epoch 40/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.9741"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -3.9741 - val_loss: -3.6397\n",
      "Epoch 41/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -3.9957 - val_loss: -3.3269\n",
      "Epoch 42/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.0170"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 242ms/step - loss: -4.0170 - val_loss: -3.6991\n",
      "Epoch 43/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.0116 - val_loss: -2.6179\n",
      "Epoch 44/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0749 - val_loss: -3.6827\n",
      "Epoch 45/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.0487"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 256ms/step - loss: -4.0487 - val_loss: -3.8260\n",
      "Epoch 46/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0178 - val_loss: -3.3258\n",
      "Epoch 47/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.0982 - val_loss: -3.7517\n",
      "Epoch 48/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1528 - val_loss: -2.0859\n",
      "Epoch 49/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1853 - val_loss: -3.7877\n",
      "Epoch 50/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2519 - val_loss: -3.5193\n",
      "Epoch 51/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1578 - val_loss: -3.0277\n",
      "Epoch 52/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.1101 - val_loss: -3.4449\n",
      "Epoch 53/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.1435 - val_loss: -3.7234\n",
      "Epoch 54/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.2640"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 246ms/step - loss: -4.2640 - val_loss: -3.8924\n",
      "Epoch 55/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2926 - val_loss: -3.6499\n",
      "Epoch 56/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.3316"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.3316 - val_loss: -3.9738\n",
      "Epoch 57/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2890 - val_loss: -3.7972\n",
      "Epoch 58/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.2530"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.2530 - val_loss: -4.0217\n",
      "Epoch 59/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.2682 - val_loss: -3.9261\n",
      "Epoch 60/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9983 - val_loss: -3.8259\n",
      "Epoch 61/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.3122 - val_loss: -3.7232\n",
      "Epoch 62/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3767 - val_loss: -3.9932\n",
      "Epoch 63/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.0777 - val_loss: -3.1531\n",
      "Epoch 64/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.7240 - val_loss: -3.6698\n",
      "Epoch 65/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.2628 - val_loss: -3.9363\n",
      "Epoch 66/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2381 - val_loss: -3.6709\n",
      "Epoch 67/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.3565 - val_loss: -3.4169\n",
      "Epoch 68/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2075 - val_loss: -3.7502\n",
      "Epoch 69/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2625 - val_loss: -3.9905\n",
      "Epoch 70/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3359 - val_loss: -3.2404\n",
      "Epoch 71/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4438 - val_loss: -3.9852\n",
      "Epoch 72/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3801 - val_loss: -2.9785\n",
      "Epoch 73/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3955 - val_loss: -4.0026\n",
      "Epoch 74/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3445 - val_loss: -3.7915\n",
      "Epoch 75/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4304 - val_loss: -3.3865\n",
      "Epoch 76/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4452 - val_loss: -3.9654\n",
      "Epoch 77/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.3413 - val_loss: -3.6836\n",
      "Epoch 78/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5099 - val_loss: -3.9299\n",
      "Epoch 79/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.5205 - val_loss: -3.9864\n",
      "Epoch 80/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2781 - val_loss: -2.5163\n",
      "Epoch 81/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4179 - val_loss: -3.3940\n",
      "Epoch 82/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.4131 - val_loss: -3.9438\n",
      "Epoch 83/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3805 - val_loss: -3.5079\n",
      "Epoch 84/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4783"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 244ms/step - loss: -4.4783 - val_loss: -4.1284\n",
      "Epoch 85/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4373 - val_loss: -4.1245\n",
      "Epoch 86/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.5429 - val_loss: -3.9312\n",
      "Epoch 87/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2953 - val_loss: -2.7424\n",
      "Epoch 88/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4422 - val_loss: -4.0132\n",
      "Epoch 89/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4678 - val_loss: -3.5440\n",
      "Epoch 90/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4341"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -4.4341 - val_loss: -4.1890\n",
      "Epoch 91/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4971"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -4.4971 - val_loss: -4.1921\n",
      "Epoch 92/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4599 - val_loss: -3.9597\n",
      "Epoch 93/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5469 - val_loss: -4.0678\n",
      "Epoch 94/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4391 - val_loss: -3.8319\n",
      "Epoch 95/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5044 - val_loss: -3.4316\n",
      "Epoch 96/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.5025 - val_loss: -3.6037\n",
      "Epoch 97/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5854 - val_loss: -4.0833\n",
      "Epoch 98/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6110 - val_loss: -3.6818\n",
      "Epoch 99/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5056 - val_loss: -3.6819\n",
      "Epoch 100/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.9255 - val_loss: -3.8776\n",
      "Epoch 101/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3653 - val_loss: -4.1088\n",
      "Epoch 102/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5330 - val_loss: -4.1559\n",
      "Epoch 103/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4972 - val_loss: -4.1483\n",
      "Epoch 104/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5835 - val_loss: -4.0018\n",
      "Epoch 105/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5108 - val_loss: -4.0206\n",
      "Epoch 106/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5433"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -4.5433 - val_loss: -4.2366\n",
      "Epoch 107/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.5284 - val_loss: -4.0983\n",
      "Epoch 108/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.3553 - val_loss: -4.2064\n",
      "Epoch 109/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5935 - val_loss: -3.8570\n",
      "Epoch 110/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5645 - val_loss: -3.6561\n",
      "Epoch 111/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6404 - val_loss: -3.3535\n",
      "Epoch 112/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3885 - val_loss: -4.0908\n",
      "Epoch 113/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5458 - val_loss: -2.4516\n",
      "Epoch 114/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5886 - val_loss: -3.8912\n",
      "Epoch 115/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6253 - val_loss: -3.9461\n",
      "Epoch 116/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4735 - val_loss: -4.0651\n",
      "Epoch 117/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6351 - val_loss: -4.0741\n",
      "Epoch 118/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6030 - val_loss: -4.0273\n",
      "Epoch 119/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5273 - val_loss: -3.4433\n",
      "Epoch 120/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6378"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 242ms/step - loss: -4.6378 - val_loss: -4.3063\n",
      "Epoch 121/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5017 - val_loss: -4.1976\n",
      "Epoch 122/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6387 - val_loss: -2.3444\n",
      "Epoch 123/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5756 - val_loss: -3.6642\n",
      "Epoch 124/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5792 - val_loss: -2.9281\n",
      "Epoch 125/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6294 - val_loss: -3.5773\n",
      "Epoch 126/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5731 - val_loss: -4.1198\n",
      "Epoch 127/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6238 - val_loss: -3.3756\n",
      "Epoch 128/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6799 - val_loss: -3.7444\n",
      "Epoch 129/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.7030 - val_loss: -2.0372\n",
      "Epoch 130/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.1104 - val_loss: -3.4800\n",
      "Epoch 131/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.0043 - val_loss: -3.8129\n",
      "Epoch 132/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1593 - val_loss: -3.8890\n",
      "Epoch 133/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2115 - val_loss: -4.0478\n",
      "Epoch 134/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3160 - val_loss: -3.7873\n",
      "Epoch 135/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2871 - val_loss: -3.3194\n",
      "Epoch 136/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3429 - val_loss: -4.0021\n",
      "Epoch 137/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2966 - val_loss: -4.0568\n",
      "Epoch 138/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.3868 - val_loss: -4.1009\n",
      "Epoch 139/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4096 - val_loss: -3.7329\n",
      "Epoch 140/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4050 - val_loss: -3.6905\n",
      "Epoch 141/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4476 - val_loss: -4.0897\n",
      "Epoch 142/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4381 - val_loss: -3.9587\n",
      "Epoch 143/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4072 - val_loss: -4.0035\n",
      "Epoch 144/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5044 - val_loss: -4.1348\n",
      "Epoch 145/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5074 - val_loss: -4.1049\n",
      "Epoch 146/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.3968 - val_loss: -3.6422\n",
      "Epoch 147/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5308 - val_loss: 0.8868\n",
      "Epoch 148/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.1922 - val_loss: -4.0297\n",
      "Epoch 149/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4879 - val_loss: -3.6776\n",
      "Epoch 150/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.4467 - val_loss: -4.1359\n",
      "Epoch 151/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.5115 - val_loss: -4.0437\n",
      "Epoch 152/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4635 - val_loss: -3.7913\n",
      "Epoch 153/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.5400 - val_loss: -4.0872\n",
      "Epoch 154/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -3.3023 - val_loss: -3.2097\n",
      "Epoch 155/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.0220 - val_loss: -3.7147\n",
      "Epoch 156/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.2707 - val_loss: -3.7811\n",
      "Epoch 157/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4429 - val_loss: -3.8153\n",
      "Epoch 158/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4596 - val_loss: -3.6063\n",
      "Epoch 159/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4618 - val_loss: -4.0375\n",
      "Epoch 160/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4743 - val_loss: -2.4444\n",
      "Epoch 161/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4853 - val_loss: -3.2178\n",
      "Epoch 162/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5006 - val_loss: -4.2205\n",
      "Epoch 163/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5430 - val_loss: -2.6394\n",
      "Epoch 164/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5291 - val_loss: -3.8795\n",
      "Epoch 165/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4728 - val_loss: -3.7637\n",
      "Epoch 166/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6161 - val_loss: -3.9338\n",
      "Epoch 167/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.5436 - val_loss: -3.9792\n",
      "Epoch 168/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5179 - val_loss: -4.0748\n",
      "Epoch 169/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6045 - val_loss: -3.9467\n",
      "Epoch 170/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5465 - val_loss: -2.4555\n",
      "Epoch 171/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4981 - val_loss: -4.0729\n",
      "Epoch 172/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5362 - val_loss: -3.6471\n",
      "Epoch 173/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4966 - val_loss: -3.7931\n",
      "Epoch 174/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5089 - val_loss: -3.8730\n",
      "Epoch 175/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5386 - val_loss: -3.3764\n",
      "Epoch 176/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4537 - val_loss: -4.1094\n",
      "Epoch 177/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5912 - val_loss: -2.8546\n",
      "Epoch 178/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5376 - val_loss: -3.4386\n",
      "Epoch 179/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6499 - val_loss: -3.8898\n",
      "Epoch 180/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5730 - val_loss: -3.8328\n",
      "Epoch 181/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6867 - val_loss: -3.8387\n",
      "Epoch 182/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5094 - val_loss: -4.2982\n",
      "Epoch 183/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.9798 - val_loss: -3.4236\n",
      "Epoch 184/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5994 - val_loss: -3.1633\n",
      "Epoch 185/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4728 - val_loss: -2.9269\n",
      "Epoch 186/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6329 - val_loss: -4.2705\n",
      "Epoch 187/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6653 - val_loss: -4.2497\n",
      "Epoch 188/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6090 - val_loss: -3.8345\n",
      "Epoch 189/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6499 - val_loss: -3.6166\n",
      "Epoch 190/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6620 - val_loss: -4.0108\n",
      "Epoch 191/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4611 - val_loss: -4.0492\n",
      "Epoch 192/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6723 - val_loss: -4.0005\n",
      "Epoch 193/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6837 - val_loss: -4.2536\n",
      "Epoch 194/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6014 - val_loss: -4.1422\n",
      "Epoch 195/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6350"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.6350 - val_loss: -4.3069\n",
      "Epoch 196/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6685 - val_loss: -3.8316\n",
      "Epoch 197/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6678 - val_loss: -4.1608\n",
      "Epoch 198/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6509 - val_loss: -4.1554\n",
      "Epoch 199/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4639 - val_loss: -4.2070\n",
      "Epoch 200/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7022"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 243ms/step - loss: -4.7022 - val_loss: -4.3151\n",
      "Epoch 201/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7201 - val_loss: -4.0010\n",
      "Epoch 202/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6216 - val_loss: -3.2105\n",
      "Epoch 203/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6690 - val_loss: -4.2823\n",
      "Epoch 204/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7665 - val_loss: -1.4260\n",
      "Epoch 205/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6008 - val_loss: -3.9452\n",
      "Epoch 206/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6762 - val_loss: -4.1586\n",
      "Epoch 207/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7035 - val_loss: -3.4382\n",
      "Epoch 208/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6760 - val_loss: -4.1030\n",
      "Epoch 209/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7225 - val_loss: -3.5169\n",
      "Epoch 210/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6538 - val_loss: -4.1904\n",
      "Epoch 211/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7878 - val_loss: -4.1353\n",
      "Epoch 212/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6297 - val_loss: -4.2305\n",
      "Epoch 213/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -4.6991 - val_loss: -3.9741\n",
      "Epoch 214/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6311 - val_loss: -4.2062\n",
      "Epoch 215/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.1907 - val_loss: -3.9512\n",
      "Epoch 216/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3337 - val_loss: -3.6254\n",
      "Epoch 217/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5755 - val_loss: -4.0418\n",
      "Epoch 218/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5779 - val_loss: -4.2207\n",
      "Epoch 219/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5627 - val_loss: -3.9891\n",
      "Epoch 220/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6916 - val_loss: -4.0297\n",
      "Epoch 221/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6141 - val_loss: -3.9173\n",
      "Epoch 222/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7005 - val_loss: -4.1732\n",
      "Epoch 223/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.6503 - val_loss: -3.9917\n",
      "Epoch 224/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6907 - val_loss: -4.0833\n",
      "Epoch 225/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6753 - val_loss: -3.9053\n",
      "Epoch 226/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7561"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -4.7561 - val_loss: -4.3612\n",
      "Epoch 227/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.1241 - val_loss: -4.1558\n",
      "Epoch 228/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7411 - val_loss: -3.7801\n",
      "Epoch 229/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7005 - val_loss: -4.0497\n",
      "Epoch 230/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6433 - val_loss: -4.0913\n",
      "Epoch 231/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7474 - val_loss: -3.7598\n",
      "Epoch 232/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7035 - val_loss: -4.2482\n",
      "Epoch 233/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7224 - val_loss: -1.0100\n",
      "Epoch 234/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.1183 - val_loss: -3.9204\n",
      "Epoch 235/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7464 - val_loss: -4.2046\n",
      "Epoch 236/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7472 - val_loss: -3.9808\n",
      "Epoch 237/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7524 - val_loss: -3.8554\n",
      "Epoch 238/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7560 - val_loss: -3.9648\n",
      "Epoch 239/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6481 - val_loss: -3.8842\n",
      "Epoch 240/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7962 - val_loss: -4.1092\n",
      "Epoch 241/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6583 - val_loss: -3.3734\n",
      "Epoch 242/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6867 - val_loss: -4.0798\n",
      "Epoch 243/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7823 - val_loss: -3.9132\n",
      "Epoch 244/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6384 - val_loss: -4.1755\n",
      "Epoch 245/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7495 - val_loss: -3.4459\n",
      "Epoch 246/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8165 - val_loss: -3.3783\n",
      "Epoch 247/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8082 - val_loss: -2.7870\n",
      "Epoch 248/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.1617 - val_loss: -4.2926\n",
      "Epoch 249/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6939 - val_loss: -3.8492\n",
      "Epoch 250/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6945 - val_loss: -4.3371\n",
      "Epoch 251/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7703 - val_loss: -4.0928\n",
      "Epoch 252/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7662 - val_loss: -4.3142\n",
      "Epoch 253/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8092 - val_loss: -4.2484\n",
      "Epoch 254/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6224 - val_loss: -3.9219\n",
      "Epoch 255/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -4.8051 - val_loss: -4.3642\n",
      "Epoch 256/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7824 - val_loss: -4.1180\n",
      "Epoch 257/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7654 - val_loss: -4.3020\n",
      "Epoch 258/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6785 - val_loss: -4.3418\n",
      "Epoch 259/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8283 - val_loss: -3.6862\n",
      "Epoch 260/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7540 - val_loss: -4.2217\n",
      "Epoch 261/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7166 - val_loss: -4.3198\n",
      "Epoch 262/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7943 - val_loss: -3.7636\n",
      "Epoch 263/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8175 - val_loss: -4.1062\n",
      "Epoch 264/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8190 - val_loss: -4.0247\n",
      "Epoch 265/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7974 - val_loss: -4.0915\n",
      "Epoch 266/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8870 - val_loss: -1.3870\n",
      "Epoch 267/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3586 - val_loss: -4.1161\n",
      "Epoch 268/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -4.7390 - val_loss: -3.6052\n",
      "Epoch 269/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8100 - val_loss: -4.1337\n",
      "Epoch 270/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5793 - val_loss: -4.1597\n",
      "Epoch 271/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8655 - val_loss: -4.2847\n",
      "Epoch 272/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4189 - val_loss: -4.0050\n",
      "Epoch 273/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8138 - val_loss: -4.0604\n",
      "Epoch 274/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8117 - val_loss: -4.2722\n",
      "Epoch 275/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8098 - val_loss: -4.0395\n",
      "Epoch 276/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7842 - val_loss: -2.6632\n",
      "Epoch 277/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8495 - val_loss: -2.5602\n",
      "Epoch 278/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5942 - val_loss: -4.2758\n",
      "Epoch 279/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -4.7244 - val_loss: -4.4118\n",
      "Epoch 280/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8538 - val_loss: -4.1551\n",
      "Epoch 281/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5868 - val_loss: -3.9058\n",
      "Epoch 282/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8314 - val_loss: -4.1234\n",
      "Epoch 283/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8815 - val_loss: -2.8854\n",
      "Epoch 284/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6904 - val_loss: -3.8170\n",
      "Epoch 285/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8576 - val_loss: -2.3984\n",
      "Epoch 286/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8393 - val_loss: -3.6526\n",
      "Epoch 287/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8546 - val_loss: -3.8600\n",
      "Epoch 288/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -2.8122 - val_loss: -3.1262\n",
      "Epoch 289/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.8654 - val_loss: -3.3983\n",
      "Epoch 290/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3519 - val_loss: -3.5925\n",
      "Epoch 291/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4906 - val_loss: -2.7966\n",
      "Epoch 292/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5340 - val_loss: -3.2300\n",
      "Epoch 293/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5536 - val_loss: -3.9945\n",
      "Epoch 294/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6759 - val_loss: -3.8581\n",
      "Epoch 295/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6240 - val_loss: -3.1215\n",
      "Epoch 296/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6614 - val_loss: -3.8191\n",
      "Epoch 297/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6942 - val_loss: -4.1863\n",
      "Epoch 298/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7028 - val_loss: -4.0911\n",
      "Epoch 299/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6770 - val_loss: -4.3220\n",
      "Epoch 300/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6964 - val_loss: -3.9208\n",
      "Epoch 301/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.6474 - val_loss: -3.6230\n",
      "Epoch 302/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8150 - val_loss: -2.2917\n",
      "Epoch 303/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4596 - val_loss: -4.1111\n",
      "Epoch 304/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7793 - val_loss: -4.0679\n",
      "Epoch 305/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7020 - val_loss: -4.3134\n",
      "Epoch 306/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7783 - val_loss: -2.5364\n",
      "Epoch 307/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7186 - val_loss: -3.7659\n",
      "Epoch 308/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7502 - val_loss: -3.4418\n",
      "Epoch 309/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7125 - val_loss: -3.3561\n",
      "Epoch 310/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8226 - val_loss: -4.0608\n",
      "Epoch 311/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7432 - val_loss: -4.1284\n",
      "Epoch 312/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7629 - val_loss: -4.2684\n",
      "Epoch 313/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5931 - val_loss: -4.0041\n",
      "Epoch 314/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8320 - val_loss: -4.3149\n",
      "Epoch 315/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3553 - val_loss: -2.6996\n",
      "Epoch 316/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.4715 - val_loss: -3.7627\n",
      "Epoch 317/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7765 - val_loss: -3.8022\n",
      "Epoch 318/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7690 - val_loss: -1.7341\n",
      "Epoch 319/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7712 - val_loss: -3.7747\n",
      "Epoch 320/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7451 - val_loss: -4.4013\n",
      "Epoch 321/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7887 - val_loss: -4.4028\n",
      "Epoch 322/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8049 - val_loss: -4.2753\n",
      "Epoch 323/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7383 - val_loss: -4.2147\n",
      "Epoch 324/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7925 - val_loss: -3.8242\n",
      "Epoch 325/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8398 - val_loss: -4.0992\n",
      "Epoch 326/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7556 - val_loss: -4.2118\n",
      "Epoch 327/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8445 - val_loss: -4.3826\n",
      "Epoch 328/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3658 - val_loss: -4.1842\n",
      "Epoch 329/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7593 - val_loss: -4.0870\n",
      "Epoch 330/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8734 - val_loss: -4.2345\n",
      "Epoch 331/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7844 - val_loss: -4.2732\n",
      "Epoch 332/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8349 - val_loss: -3.7806\n",
      "Epoch 333/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8162 - val_loss: -4.1273\n",
      "Epoch 334/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8529"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 243ms/step - loss: -4.8529 - val_loss: -4.4772\n",
      "Epoch 335/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7331 - val_loss: -4.1530\n",
      "Epoch 336/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6757 - val_loss: -4.2695\n",
      "Epoch 337/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8577 - val_loss: -3.7058\n",
      "Epoch 338/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8482 - val_loss: -4.4159\n",
      "Epoch 339/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7661 - val_loss: -4.2919\n",
      "Epoch 340/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8212 - val_loss: -3.9323\n",
      "Epoch 341/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8157 - val_loss: -3.2319\n",
      "Epoch 342/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9236 - val_loss: -3.8545\n",
      "Epoch 343/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7989 - val_loss: -3.9629\n",
      "Epoch 344/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8725 - val_loss: -4.4261\n",
      "Epoch 345/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7083 - val_loss: -3.4988\n",
      "Epoch 346/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9062 - val_loss: -4.3356\n",
      "Epoch 347/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6758 - val_loss: -4.3514\n",
      "Epoch 348/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8649 - val_loss: -3.2204\n",
      "Epoch 349/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7692 - val_loss: 4.3832\n",
      "Epoch 350/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.6843 - val_loss: -3.4837\n",
      "Epoch 351/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.6273 - val_loss: -4.2077\n",
      "Epoch 352/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7079 - val_loss: -4.2601\n",
      "Epoch 353/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6088 - val_loss: -3.6463\n",
      "Epoch 354/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7394 - val_loss: -4.2711\n",
      "Epoch 355/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8563 - val_loss: -4.3259\n",
      "Epoch 356/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8365 - val_loss: -4.0816\n",
      "Epoch 357/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5598 - val_loss: -4.2765\n",
      "Epoch 358/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8483 - val_loss: -3.8413\n",
      "Epoch 359/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8489 - val_loss: -4.3638\n",
      "Epoch 360/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8720 - val_loss: -3.8617\n",
      "Epoch 361/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8740 - val_loss: -2.5701\n",
      "Epoch 362/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8395 - val_loss: -4.3297\n",
      "Epoch 363/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2890 - val_loss: -4.2915\n",
      "Epoch 364/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7753 - val_loss: -4.2193\n",
      "Epoch 365/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8365 - val_loss: -4.1720\n",
      "Epoch 366/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7709 - val_loss: -3.8560\n",
      "Epoch 367/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9127 - val_loss: -4.3653\n",
      "Epoch 368/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8177"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 257ms/step - loss: -4.8177 - val_loss: -4.5513\n",
      "Epoch 369/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8258 - val_loss: -4.2384\n",
      "Epoch 370/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8856 - val_loss: -3.6233\n",
      "Epoch 371/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8234 - val_loss: -3.4927\n",
      "Epoch 372/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8583 - val_loss: -2.3765\n",
      "Epoch 373/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8304 - val_loss: -2.7794\n",
      "Epoch 374/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8348 - val_loss: -4.0342\n",
      "Epoch 375/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8260 - val_loss: -3.5276\n",
      "Epoch 376/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9041 - val_loss: -3.6373\n",
      "Epoch 377/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8836 - val_loss: -4.4803\n",
      "Epoch 378/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8679 - val_loss: -3.9526\n",
      "Epoch 379/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8349 - val_loss: -4.3695\n",
      "Epoch 380/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9393 - val_loss: -4.2404\n",
      "Epoch 381/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8751 - val_loss: -3.8945\n",
      "Epoch 382/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8931 - val_loss: -4.2192\n",
      "Epoch 383/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.3613 - val_loss: -3.8382\n",
      "Epoch 384/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.4675 - val_loss: -3.9511\n",
      "Epoch 385/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6962 - val_loss: -4.0776\n",
      "Epoch 386/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7465 - val_loss: -3.7574\n",
      "Epoch 387/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8149 - val_loss: -4.2462\n",
      "Epoch 388/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6763 - val_loss: -4.1143\n",
      "Epoch 389/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6258 - val_loss: -3.8959\n",
      "Epoch 390/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8538 - val_loss: -1.1640\n",
      "Epoch 391/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8658 - val_loss: -3.6241\n",
      "Epoch 392/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8380 - val_loss: -4.3746\n",
      "Epoch 393/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.8840 - val_loss: -1.9442\n",
      "Epoch 394/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7650 - val_loss: -3.2875\n",
      "Epoch 395/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5283 - val_loss: -3.4163\n",
      "Epoch 396/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6556 - val_loss: -4.1726\n",
      "Epoch 397/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.6331 - val_loss: -4.0783\n",
      "Epoch 398/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -4.7782 - val_loss: -2.8307\n",
      "Epoch 399/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7135 - val_loss: -4.1083\n",
      "Epoch 400/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8498 - val_loss: -4.3811\n",
      "Epoch 401/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8401 - val_loss: -3.5823\n",
      "Epoch 402/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7321 - val_loss: -3.9845\n",
      "Epoch 403/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.2568 - val_loss: -4.1749\n",
      "Epoch 404/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7171 - val_loss: -4.2785\n",
      "Epoch 405/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7934 - val_loss: -4.1651\n",
      "Epoch 406/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8073 - val_loss: -2.3761\n",
      "Epoch 407/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7511 - val_loss: -4.2501\n",
      "Epoch 408/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8786 - val_loss: -4.4975\n",
      "Epoch 409/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8396 - val_loss: -3.5782\n",
      "Epoch 410/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7324 - val_loss: -3.6677\n",
      "Epoch 411/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9413 - val_loss: -2.9515\n",
      "Epoch 412/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7543 - val_loss: -4.2134\n",
      "Epoch 413/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8038 - val_loss: -4.1661\n",
      "Epoch 414/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9006 - val_loss: -3.5777\n",
      "Epoch 415/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9289 - val_loss: -3.9851\n",
      "Epoch 416/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7515 - val_loss: -4.2025\n",
      "Epoch 417/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8657 - val_loss: -4.2977\n",
      "Epoch 418/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9597 - val_loss: -4.3772\n",
      "Epoch 419/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7461 - val_loss: -4.0259\n",
      "Epoch 420/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9260 - val_loss: -4.1151\n",
      "Epoch 421/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6844 - val_loss: -4.0675\n",
      "Epoch 422/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9581 - val_loss: -4.2655\n",
      "Epoch 423/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8548 - val_loss: -4.2992\n",
      "Epoch 424/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8860 - val_loss: -4.2957\n",
      "Epoch 425/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9471 - val_loss: -3.9691\n",
      "Epoch 426/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9404 - val_loss: -3.3976\n",
      "Epoch 427/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -4.7815 - val_loss: -4.3867\n",
      "Epoch 428/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9225 - val_loss: -3.9343\n",
      "Epoch 429/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8686 - val_loss: -4.0770\n",
      "Epoch 430/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9443 - val_loss: -4.3571\n",
      "Epoch 431/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9628 - val_loss: -2.8488\n",
      "Epoch 432/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9674"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 244ms/step - loss: -4.9674 - val_loss: -4.5831\n",
      "Epoch 433/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8464 - val_loss: -3.9811\n",
      "Epoch 434/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9482 - val_loss: -3.9028\n",
      "Epoch 435/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9169 - val_loss: -4.2005\n",
      "Epoch 436/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7763 - val_loss: -3.8751\n",
      "Epoch 437/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0198 - val_loss: -4.3610\n",
      "Epoch 438/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8649 - val_loss: -4.3703\n",
      "Epoch 439/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9873 - val_loss: -3.9582\n",
      "Epoch 440/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9536 - val_loss: -4.4598\n",
      "Epoch 441/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9369 - val_loss: -4.1377\n",
      "Epoch 442/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9595 - val_loss: -4.3681\n",
      "Epoch 443/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -3.0888 - val_loss: -1.9389\n",
      "Epoch 444/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.2860 - val_loss: -3.1362\n",
      "Epoch 445/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.9664 - val_loss: -3.3452\n",
      "Epoch 446/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.3375 - val_loss: -3.8846\n",
      "Epoch 447/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5068 - val_loss: -3.8748\n",
      "Epoch 448/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.5262 - val_loss: -3.9985\n",
      "Epoch 449/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6237 - val_loss: -3.8939\n",
      "Epoch 450/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6719 - val_loss: -4.2454\n",
      "Epoch 451/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.6412 - val_loss: -4.2233\n",
      "Epoch 452/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6147 - val_loss: -3.9426\n",
      "Epoch 453/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7368 - val_loss: -4.2325\n",
      "Epoch 454/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7120 - val_loss: -4.3038\n",
      "Epoch 455/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7075 - val_loss: -4.1234\n",
      "Epoch 456/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8223 - val_loss: -3.7610\n",
      "Epoch 457/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7116 - val_loss: -3.6418\n",
      "Epoch 458/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6489 - val_loss: -3.9648\n",
      "Epoch 459/2000\n",
      "118/118 [==============================] - 23s 190ms/step - loss: -4.8274 - val_loss: -4.1638\n",
      "Epoch 460/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -4.7770 - val_loss: -3.7589\n",
      "Epoch 461/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7136 - val_loss: -3.9765\n",
      "Epoch 462/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7619 - val_loss: -4.2426\n",
      "Epoch 463/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7753 - val_loss: -4.3284\n",
      "Epoch 464/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7127 - val_loss: -4.2751\n",
      "Epoch 465/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7822 - val_loss: -2.2044\n",
      "Epoch 466/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5751 - val_loss: -4.4840\n",
      "Epoch 467/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8393 - val_loss: -4.3469\n",
      "Epoch 468/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8053 - val_loss: -4.1119\n",
      "Epoch 469/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8744 - val_loss: -3.7887\n",
      "Epoch 470/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7860 - val_loss: -4.2768\n",
      "Epoch 471/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7859 - val_loss: -4.2812\n",
      "Epoch 472/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8379 - val_loss: -4.2805\n",
      "Epoch 473/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8423 - val_loss: -4.2421\n",
      "Epoch 474/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7451 - val_loss: -2.1224\n",
      "Epoch 475/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8504 - val_loss: -2.3541\n",
      "Epoch 476/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8437 - val_loss: -4.3146\n",
      "Epoch 477/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7917 - val_loss: -4.2323\n",
      "Epoch 478/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8633 - val_loss: -4.3323\n",
      "Epoch 479/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4127 - val_loss: -4.2731\n",
      "Epoch 480/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8233 - val_loss: -4.3004\n",
      "Epoch 481/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8497 - val_loss: -4.2779\n",
      "Epoch 482/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7884 - val_loss: -3.4942\n",
      "Epoch 483/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8190 - val_loss: -3.7439\n",
      "Epoch 484/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -4.9152 - val_loss: -3.8856\n",
      "Epoch 485/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8500 - val_loss: -4.0451\n",
      "Epoch 486/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8791 - val_loss: -3.4712\n",
      "Epoch 487/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8214 - val_loss: -3.0227\n",
      "Epoch 488/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8558 - val_loss: -3.7153\n",
      "Epoch 489/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8823 - val_loss: -4.1640\n",
      "Epoch 490/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8480 - val_loss: -4.2578\n",
      "Epoch 491/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8965 - val_loss: -4.3968\n",
      "Epoch 492/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7900 - val_loss: -3.9876\n",
      "Epoch 493/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9124 - val_loss: -4.4752\n",
      "Epoch 494/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8289 - val_loss: -3.7762\n",
      "Epoch 495/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7997 - val_loss: -4.3793\n",
      "Epoch 496/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9441 - val_loss: -3.8656\n",
      "Epoch 497/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8788 - val_loss: -3.9114\n",
      "Epoch 498/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9226 - val_loss: -4.0677\n",
      "Epoch 499/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9068 - val_loss: -4.2842\n",
      "Epoch 500/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9138 - val_loss: -3.8197\n",
      "Epoch 501/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9173 - val_loss: -4.1029\n",
      "Epoch 502/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8597 - val_loss: -4.2266\n",
      "Epoch 503/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7693 - val_loss: -4.2236\n",
      "Epoch 504/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9667 - val_loss: -4.0316\n",
      "Epoch 505/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9001 - val_loss: -2.8646\n",
      "Epoch 506/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.5094 - val_loss: -3.5723\n",
      "Epoch 507/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9398 - val_loss: -4.2725\n",
      "Epoch 508/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7742 - val_loss: -4.3003\n",
      "Epoch 509/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8958 - val_loss: -3.6211\n",
      "Epoch 510/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9838 - val_loss: -4.5352\n",
      "Epoch 511/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8454 - val_loss: -4.2691\n",
      "Epoch 512/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9878 - val_loss: -3.9957\n",
      "Epoch 513/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8930 - val_loss: -3.4300\n",
      "Epoch 514/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8374 - val_loss: -4.1560\n",
      "Epoch 515/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9121 - val_loss: -2.2699\n",
      "Epoch 516/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9041 - val_loss: -3.5688\n",
      "Epoch 517/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8797 - val_loss: -4.2634\n",
      "Epoch 518/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9460 - val_loss: -3.9670\n",
      "Epoch 519/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9692 - val_loss: -4.3110\n",
      "Epoch 520/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7882 - val_loss: -4.4548\n",
      "Epoch 521/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9828 - val_loss: -4.2856\n",
      "Epoch 522/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9052 - val_loss: -4.4795\n",
      "Epoch 523/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9308 - val_loss: -4.4503\n",
      "Epoch 524/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9643 - val_loss: -3.7985\n",
      "Epoch 525/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9337 - val_loss: -3.2714\n",
      "Epoch 526/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9595 - val_loss: -4.3838\n",
      "Epoch 527/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.3501 - val_loss: -3.9904\n",
      "Epoch 528/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7320 - val_loss: -4.0582\n",
      "Epoch 529/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9361 - val_loss: -3.5613\n",
      "Epoch 530/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9258 - val_loss: -0.8128\n",
      "Epoch 531/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8662 - val_loss: -4.1550\n",
      "Epoch 532/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9235 - val_loss: -4.3791\n",
      "Epoch 533/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8733 - val_loss: -4.1529\n",
      "Epoch 534/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0003 - val_loss: -4.4984\n",
      "Epoch 535/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8783 - val_loss: -3.8525\n",
      "Epoch 536/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9733 - val_loss: -4.3065\n",
      "Epoch 537/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9719 - val_loss: -4.4915\n",
      "Epoch 538/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9593 - val_loss: -3.9891\n",
      "Epoch 539/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8738 - val_loss: -4.2314\n",
      "Epoch 540/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9371 - val_loss: -4.3210\n",
      "Epoch 541/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8532 - val_loss: -4.1621\n",
      "Epoch 542/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9877 - val_loss: -4.1295\n",
      "Epoch 543/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8632 - val_loss: -4.4523\n",
      "Epoch 544/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0365 - val_loss: -3.3494\n",
      "Epoch 545/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -4.9370 - val_loss: -4.3410\n",
      "Epoch 546/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0384 - val_loss: -1.3512\n",
      "Epoch 547/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9357 - val_loss: -4.1066\n",
      "Epoch 548/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0236 - val_loss: -4.3672\n",
      "Epoch 549/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.8262 - val_loss: -2.2403\n",
      "Epoch 550/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.8947 - val_loss: -3.9056\n",
      "Epoch 551/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3988 - val_loss: -4.1548\n",
      "Epoch 552/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6502 - val_loss: -4.1078\n",
      "Epoch 553/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6582 - val_loss: -4.2181\n",
      "Epoch 554/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.3779 - val_loss: -3.7414\n",
      "Epoch 555/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6004 - val_loss: -4.2897\n",
      "Epoch 556/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7708 - val_loss: -3.4849\n",
      "Epoch 557/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7767 - val_loss: -4.2828\n",
      "Epoch 558/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8512 - val_loss: -4.4338\n",
      "Epoch 559/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8621 - val_loss: -4.3875\n",
      "Epoch 560/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7540 - val_loss: -4.2055\n",
      "Epoch 561/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8904 - val_loss: -2.8273\n",
      "Epoch 562/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8162 - val_loss: -3.9945\n",
      "Epoch 563/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8910 - val_loss: -3.4159\n",
      "Epoch 564/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5119 - val_loss: -3.7317\n",
      "Epoch 565/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -4.6436 - val_loss: -4.2748\n",
      "Epoch 566/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8601 - val_loss: -3.9915\n",
      "Epoch 567/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -4.8924 - val_loss: -4.0198\n",
      "Epoch 568/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8912 - val_loss: -4.4837\n",
      "Epoch 569/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9255 - val_loss: -4.1944\n",
      "Epoch 570/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8542 - val_loss: -4.1274\n",
      "Epoch 571/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8424 - val_loss: -4.3518\n",
      "Epoch 572/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9757 - val_loss: -3.1214\n",
      "Epoch 573/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8860 - val_loss: -4.3363\n",
      "Epoch 574/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7337 - val_loss: -4.1216\n",
      "Epoch 575/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9466 - val_loss: -4.2073\n",
      "Epoch 576/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9357 - val_loss: -4.4052\n",
      "Epoch 577/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0374 - val_loss: -3.4477\n",
      "Epoch 578/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8989 - val_loss: -4.1853\n",
      "Epoch 579/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9973 - val_loss: -4.1437\n",
      "Epoch 580/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8593 - val_loss: -4.1157\n",
      "Epoch 581/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9971 - val_loss: -3.5884\n",
      "Epoch 582/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9847 - val_loss: -3.8221\n",
      "Epoch 583/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9089 - val_loss: -2.9363\n",
      "Epoch 584/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9307 - val_loss: -3.6426\n",
      "Epoch 585/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9211 - val_loss: -4.0561\n",
      "Epoch 586/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8944 - val_loss: -4.3382\n",
      "Epoch 587/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.2018 - val_loss: -2.7911\n",
      "Epoch 588/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9304 - val_loss: -4.2326\n",
      "Epoch 589/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7671 - val_loss: -3.9385\n",
      "Epoch 590/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.5994 - val_loss: -4.2342\n",
      "Epoch 591/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9139 - val_loss: -4.1856\n",
      "Epoch 592/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8608 - val_loss: -4.2836\n",
      "Epoch 593/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7540 - val_loss: -4.4456\n",
      "Epoch 594/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -4.9695 - val_loss: -4.3076\n",
      "Epoch 595/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0047 - val_loss: -4.4530\n",
      "Epoch 596/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8081 - val_loss: -4.1098\n",
      "Epoch 597/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0339 - val_loss: -4.1418\n",
      "Epoch 598/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9705 - val_loss: -4.2519\n",
      "Epoch 599/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0065 - val_loss: -3.7691\n",
      "Epoch 600/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7640 - val_loss: -4.3332\n",
      "Epoch 601/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9721 - val_loss: -4.2966\n",
      "Epoch 602/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9560 - val_loss: -4.0453\n",
      "Epoch 603/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0060 - val_loss: -4.0356\n",
      "Epoch 604/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9009 - val_loss: -4.2761\n",
      "Epoch 605/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0214 - val_loss: -4.1859\n",
      "Epoch 606/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0221 - val_loss: -2.8368\n",
      "Epoch 607/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9174 - val_loss: -3.7407\n",
      "Epoch 608/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7418 - val_loss: -3.9412\n",
      "Epoch 609/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9288 - val_loss: -4.3762\n",
      "Epoch 610/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8760 - val_loss: -4.3633\n",
      "Epoch 611/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9576 - val_loss: -3.0428\n",
      "Epoch 612/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0150 - val_loss: -4.1879\n",
      "Epoch 613/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0485 - val_loss: -3.8283\n",
      "Epoch 614/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6943 - val_loss: -3.5114\n",
      "Epoch 615/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9954 - val_loss: -4.1167\n",
      "Epoch 616/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9352 - val_loss: -4.4088\n",
      "Epoch 617/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0301 - val_loss: -4.2676\n",
      "Epoch 618/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9590 - val_loss: -3.8298\n",
      "Epoch 619/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0608 - val_loss: -4.4269\n",
      "Epoch 620/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0149 - val_loss: -4.5077\n",
      "Epoch 621/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7316 - val_loss: -4.4539\n",
      "Epoch 622/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0454 - val_loss: -3.5754\n",
      "Epoch 623/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9514 - val_loss: -4.3048\n",
      "Epoch 624/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9034 - val_loss: -3.9613\n",
      "Epoch 625/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8445 - val_loss: -4.1309\n",
      "Epoch 626/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9472 - val_loss: -4.3111\n",
      "Epoch 627/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0141 - val_loss: -4.5172\n",
      "Epoch 628/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9584 - val_loss: -4.3893\n",
      "Epoch 629/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9778 - val_loss: -3.6796\n",
      "Epoch 630/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0877 - val_loss: -4.0320\n",
      "Epoch 631/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8656 - val_loss: -4.4377\n",
      "Epoch 632/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0871 - val_loss: -3.9211\n",
      "Epoch 633/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.0146 - val_loss: -3.7790\n",
      "Epoch 634/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7352 - val_loss: -3.3196\n",
      "Epoch 635/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5685 - val_loss: -3.6993\n",
      "Epoch 636/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8891 - val_loss: -4.1986\n",
      "Epoch 637/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9826 - val_loss: -3.6590\n",
      "Epoch 638/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9109 - val_loss: -4.1922\n",
      "Epoch 639/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0527 - val_loss: -3.9889\n",
      "Epoch 640/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8843 - val_loss: -4.0958\n",
      "Epoch 641/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0535 - val_loss: -4.1503\n",
      "Epoch 642/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9923 - val_loss: -3.8351\n",
      "Epoch 643/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9114 - val_loss: -3.8144\n",
      "Epoch 644/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0502 - val_loss: -4.4361\n",
      "Epoch 645/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0190 - val_loss: -4.1952\n",
      "Epoch 646/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9589 - val_loss: -3.4847\n",
      "Epoch 647/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0262 - val_loss: -4.0064\n",
      "Epoch 648/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9323 - val_loss: -4.3938\n",
      "Epoch 649/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1136 - val_loss: -3.6778\n",
      "Epoch 650/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0280 - val_loss: -3.8316\n",
      "Epoch 651/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9791 - val_loss: -4.3107\n",
      "Epoch 652/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0547 - val_loss: -4.2940\n",
      "Epoch 653/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0164"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -5.0164 - val_loss: -4.5889\n",
      "Epoch 654/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0580 - val_loss: -4.2314\n",
      "Epoch 655/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0280 - val_loss: -4.4283\n",
      "Epoch 656/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9919 - val_loss: -3.7611\n",
      "Epoch 657/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0834"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -5.0834 - val_loss: -4.6201\n",
      "Epoch 658/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9786 - val_loss: -4.0220\n",
      "Epoch 659/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0614 - val_loss: -3.6778\n",
      "Epoch 660/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0393 - val_loss: -3.8923\n",
      "Epoch 661/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9263 - val_loss: -4.0946\n",
      "Epoch 662/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9716 - val_loss: -4.5313\n",
      "Epoch 663/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0415 - val_loss: -4.2304\n",
      "Epoch 664/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9913 - val_loss: -4.1851\n",
      "Epoch 665/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0626 - val_loss: -4.1102\n",
      "Epoch 666/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9876 - val_loss: -4.4403\n",
      "Epoch 667/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0823 - val_loss: -4.5306\n",
      "Epoch 668/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0724 - val_loss: -3.7634\n",
      "Epoch 669/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9968 - val_loss: -4.0698\n",
      "Epoch 670/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1611 - val_loss: -2.5453\n",
      "Epoch 671/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9233 - val_loss: -4.3480\n",
      "Epoch 672/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8980 - val_loss: -4.0571\n",
      "Epoch 673/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0926 - val_loss: -4.3251\n",
      "Epoch 674/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0509 - val_loss: -4.1078\n",
      "Epoch 675/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0289 - val_loss: -4.3951\n",
      "Epoch 676/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1363 - val_loss: -2.9701\n",
      "Epoch 677/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0202 - val_loss: -4.0197\n",
      "Epoch 678/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1169 - val_loss: -3.0173\n",
      "Epoch 679/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8462 - val_loss: -4.3365\n",
      "Epoch 680/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0088 - val_loss: -4.3487\n",
      "Epoch 681/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0861 - val_loss: -4.0858\n",
      "Epoch 682/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0935 - val_loss: -3.3698\n",
      "Epoch 683/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8336 - val_loss: -4.1137\n",
      "Epoch 684/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0626 - val_loss: -4.4848\n",
      "Epoch 685/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0451 - val_loss: -4.3402\n",
      "Epoch 686/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8698 - val_loss: -4.4108\n",
      "Epoch 687/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0073 - val_loss: -3.6213\n",
      "Epoch 688/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0106 - val_loss: -3.4128\n",
      "Epoch 689/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0393 - val_loss: -3.8462\n",
      "Epoch 690/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0697 - val_loss: -4.3925\n",
      "Epoch 691/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8972 - val_loss: -4.4702\n",
      "Epoch 692/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0572 - val_loss: -4.4789\n",
      "Epoch 693/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9325 - val_loss: -4.3687\n",
      "Epoch 694/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1050 - val_loss: -3.1673\n",
      "Epoch 695/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1137 - val_loss: -3.4533\n",
      "Epoch 696/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0889 - val_loss: -4.3536\n",
      "Epoch 697/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0756 - val_loss: -4.0470\n",
      "Epoch 698/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9585 - val_loss: -4.2608\n",
      "Epoch 699/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1639 - val_loss: -4.1538\n",
      "Epoch 700/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -3.1960 - val_loss: -3.8195\n",
      "Epoch 701/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.9627 - val_loss: -4.1188\n",
      "Epoch 702/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4176 - val_loss: -4.0975\n",
      "Epoch 703/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7341 - val_loss: -4.3938\n",
      "Epoch 704/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8337 - val_loss: -3.9441\n",
      "Epoch 705/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9300 - val_loss: -3.7448\n",
      "Epoch 706/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8045 - val_loss: -4.3310\n",
      "Epoch 707/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9161 - val_loss: -4.4032\n",
      "Epoch 708/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9602 - val_loss: -4.4258\n",
      "Epoch 709/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9422 - val_loss: -2.8479\n",
      "Epoch 710/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8491 - val_loss: -4.2224\n",
      "Epoch 711/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9151 - val_loss: -4.0125\n",
      "Epoch 712/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9597 - val_loss: -4.1734\n",
      "Epoch 713/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9979 - val_loss: -4.1087\n",
      "Epoch 714/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0444 - val_loss: -4.1141\n",
      "Epoch 715/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0803 - val_loss: -4.2651\n",
      "Epoch 716/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.9828 - val_loss: -4.2551\n",
      "Epoch 717/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0333 - val_loss: -2.3709\n",
      "Epoch 718/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7072 - val_loss: -3.2110\n",
      "Epoch 719/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0147 - val_loss: -4.0992\n",
      "Epoch 720/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9201 - val_loss: -4.1315\n",
      "Epoch 721/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0251 - val_loss: -3.8093\n",
      "Epoch 722/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0192 - val_loss: -4.2610\n",
      "Epoch 723/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0761 - val_loss: -0.4532\n",
      "Epoch 724/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0303 - val_loss: -4.3013\n",
      "Epoch 725/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9109 - val_loss: -3.9072\n",
      "Epoch 726/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0411 - val_loss: -3.9810\n",
      "Epoch 727/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1216 - val_loss: -4.4194\n",
      "Epoch 728/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9587 - val_loss: -3.5953\n",
      "Epoch 729/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0525 - val_loss: -3.7177\n",
      "Epoch 730/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6001 - val_loss: -3.5331\n",
      "Epoch 731/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -4.9050 - val_loss: -2.6341\n",
      "Epoch 732/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0128 - val_loss: -4.2386\n",
      "Epoch 733/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9765 - val_loss: -4.0120\n",
      "Epoch 734/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0648 - val_loss: -4.5389\n",
      "Epoch 735/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0818 - val_loss: -4.0444\n",
      "Epoch 736/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9815 - val_loss: -3.7284\n",
      "Epoch 737/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1356 - val_loss: -3.5870\n",
      "Epoch 738/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0730 - val_loss: -2.0922\n",
      "Epoch 739/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0294 - val_loss: -4.1346\n",
      "Epoch 740/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0359 - val_loss: -3.7603\n",
      "Epoch 741/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9844 - val_loss: -4.5054\n",
      "Epoch 742/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7914 - val_loss: -4.0738\n",
      "Epoch 743/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1171 - val_loss: -4.2841\n",
      "Epoch 744/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0521 - val_loss: -3.7263\n",
      "Epoch 745/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0908 - val_loss: -4.2614\n",
      "Epoch 746/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8991 - val_loss: -3.4296\n",
      "Epoch 747/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0255 - val_loss: -4.2901\n",
      "Epoch 748/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0780 - val_loss: -3.5402\n",
      "Epoch 749/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0846 - val_loss: -4.1812\n",
      "Epoch 750/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0453 - val_loss: -4.5531\n",
      "Epoch 751/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0879 - val_loss: -3.8101\n",
      "Epoch 752/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0259 - val_loss: -4.4063\n",
      "Epoch 753/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0675 - val_loss: -4.1109\n",
      "Epoch 754/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0058 - val_loss: -4.4323\n",
      "Epoch 755/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0341 - val_loss: -4.4435\n",
      "Epoch 756/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1201 - val_loss: -3.9582\n",
      "Epoch 757/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8630 - val_loss: -4.1311\n",
      "Epoch 758/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0974 - val_loss: -4.3601\n",
      "Epoch 759/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0676 - val_loss: -3.8361\n",
      "Epoch 760/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0788 - val_loss: -4.1218\n",
      "Epoch 761/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0234 - val_loss: -4.3280\n",
      "Epoch 762/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1344 - val_loss: -4.2519\n",
      "Epoch 763/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0855 - val_loss: -2.8994\n",
      "Epoch 764/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7465 - val_loss: -4.4768\n",
      "Epoch 765/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0359 - val_loss: -4.3640\n",
      "Epoch 766/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0945"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 244ms/step - loss: -5.0945 - val_loss: -4.6529\n",
      "Epoch 767/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0179 - val_loss: -4.4095\n",
      "Epoch 768/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0732 - val_loss: -4.1947\n",
      "Epoch 769/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0505 - val_loss: -4.4443\n",
      "Epoch 770/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0439 - val_loss: -3.9955\n",
      "Epoch 771/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1017 - val_loss: -3.9962\n",
      "Epoch 772/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1379 - val_loss: -4.4399\n",
      "Epoch 773/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1156 - val_loss: -4.2913\n",
      "Epoch 774/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 257ms/step - loss: -5.1750 - val_loss: -4.6567\n",
      "Epoch 775/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0590 - val_loss: -4.3122\n",
      "Epoch 776/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0010 - val_loss: -4.3594\n",
      "Epoch 777/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1426 - val_loss: -4.5335\n",
      "Epoch 778/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1209 - val_loss: -3.6184\n",
      "Epoch 779/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0950 - val_loss: -4.4649\n",
      "Epoch 780/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.0618 - val_loss: -4.3203\n",
      "Epoch 781/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0909 - val_loss: -3.9928\n",
      "Epoch 782/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1662 - val_loss: -4.1567\n",
      "Epoch 783/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8534 - val_loss: -4.3692\n",
      "Epoch 784/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -5.1160 - val_loss: -4.6745\n",
      "Epoch 785/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1274 - val_loss: -3.2944\n",
      "Epoch 786/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1209 - val_loss: -4.1803\n",
      "Epoch 787/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1418 - val_loss: -3.9666\n",
      "Epoch 788/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0294 - val_loss: -3.8636\n",
      "Epoch 789/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0517 - val_loss: -4.4813\n",
      "Epoch 790/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1283 - val_loss: -3.7206\n",
      "Epoch 791/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0798 - val_loss: -3.3818\n",
      "Epoch 792/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1231 - val_loss: -4.3613\n",
      "Epoch 793/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0123 - val_loss: -4.1684\n",
      "Epoch 794/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1791 - val_loss: -4.6040\n",
      "Epoch 795/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1214 - val_loss: -4.1743\n",
      "Epoch 796/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0283 - val_loss: -4.3466\n",
      "Epoch 797/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1541 - val_loss: -4.5779\n",
      "Epoch 798/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0620 - val_loss: -2.9866\n",
      "Epoch 799/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1538 - val_loss: -3.4572\n",
      "Epoch 800/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0427 - val_loss: -4.0587\n",
      "Epoch 801/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.2191 - val_loss: -1.6336\n",
      "Epoch 802/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.6513 - val_loss: -3.4149\n",
      "Epoch 803/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5001 - val_loss: -3.9798\n",
      "Epoch 804/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.7325 - val_loss: -3.8498\n",
      "Epoch 805/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8305 - val_loss: -3.9016\n",
      "Epoch 806/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -4.8213 - val_loss: -3.9403\n",
      "Epoch 807/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9160 - val_loss: -4.2612\n",
      "Epoch 808/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9379 - val_loss: -3.7398\n",
      "Epoch 809/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8535 - val_loss: -4.1905\n",
      "Epoch 810/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0174 - val_loss: -3.4445\n",
      "Epoch 811/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9695 - val_loss: -4.1289\n",
      "Epoch 812/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9383 - val_loss: -4.0791\n",
      "Epoch 813/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0440 - val_loss: -4.0620\n",
      "Epoch 814/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0414 - val_loss: -3.9655\n",
      "Epoch 815/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0077 - val_loss: -3.5172\n",
      "Epoch 816/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9249 - val_loss: -4.0733\n",
      "Epoch 817/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0148 - val_loss: -4.1777\n",
      "Epoch 818/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0227 - val_loss: -4.1658\n",
      "Epoch 819/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9801 - val_loss: -2.7977\n",
      "Epoch 820/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0358 - val_loss: -3.7928\n",
      "Epoch 821/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9855 - val_loss: -4.2012\n",
      "Epoch 822/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9949 - val_loss: -3.8488\n",
      "Epoch 823/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1146 - val_loss: -2.8117\n",
      "Epoch 824/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0450 - val_loss: -3.2200\n",
      "Epoch 825/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0064 - val_loss: -3.6436\n",
      "Epoch 826/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1335 - val_loss: -4.3904\n",
      "Epoch 827/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0406 - val_loss: -4.5303\n",
      "Epoch 828/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0845 - val_loss: -4.3968\n",
      "Epoch 829/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1064 - val_loss: -4.1981\n",
      "Epoch 830/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0838 - val_loss: -4.4883\n",
      "Epoch 831/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1728 - val_loss: -3.8197\n",
      "Epoch 832/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0609 - val_loss: -2.1517\n",
      "Epoch 833/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0621 - val_loss: -4.0426\n",
      "Epoch 834/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.0127 - val_loss: -4.5429\n",
      "Epoch 835/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0712 - val_loss: -4.3053\n",
      "Epoch 836/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0680 - val_loss: -4.5390\n",
      "Epoch 837/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1383 - val_loss: -4.3317\n",
      "Epoch 838/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1087 - val_loss: -3.5191\n",
      "Epoch 839/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0139 - val_loss: -4.0930\n",
      "Epoch 840/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1395 - val_loss: -4.4473\n",
      "Epoch 841/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1328 - val_loss: -3.3656\n",
      "Epoch 842/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0608 - val_loss: -4.3450\n",
      "Epoch 843/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0996 - val_loss: -4.2056\n",
      "Epoch 844/2000\n",
      "118/118 [==============================] - 23s 192ms/step - loss: -5.1078 - val_loss: -4.3894\n",
      "Epoch 845/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1642 - val_loss: -4.4015\n",
      "Epoch 846/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1152 - val_loss: -4.4609\n",
      "Epoch 847/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0872 - val_loss: -4.2063\n",
      "Epoch 848/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1732 - val_loss: -4.2860\n",
      "Epoch 849/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9982 - val_loss: -4.5397\n",
      "Epoch 850/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0598 - val_loss: -4.2841\n",
      "Epoch 851/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2202 - val_loss: -4.1103\n",
      "Epoch 852/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0076 - val_loss: -4.3849\n",
      "Epoch 853/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0956 - val_loss: -3.0255\n",
      "Epoch 854/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1520 - val_loss: -3.4831\n",
      "Epoch 855/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0535 - val_loss: -3.8776\n",
      "Epoch 856/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0924 - val_loss: -4.4726\n",
      "Epoch 857/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1144 - val_loss: -3.8941\n",
      "Epoch 858/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1320 - val_loss: -4.2810\n",
      "Epoch 859/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1277 - val_loss: -3.9553\n",
      "Epoch 860/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0923 - val_loss: -4.3509\n",
      "Epoch 861/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0377 - val_loss: -3.9896\n",
      "Epoch 862/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1116 - val_loss: -4.0006\n",
      "Epoch 863/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1737 - val_loss: -3.3452\n",
      "Epoch 864/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2113 - val_loss: -3.9155\n",
      "Epoch 865/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.0977 - val_loss: -4.3183\n",
      "Epoch 866/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7972 - val_loss: -4.1985\n",
      "Epoch 867/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9212 - val_loss: -4.1064\n",
      "Epoch 868/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0340 - val_loss: -4.4214\n",
      "Epoch 869/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0245 - val_loss: -4.3091\n",
      "Epoch 870/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0302 - val_loss: -4.3707\n",
      "Epoch 871/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0878 - val_loss: -4.3038\n",
      "Epoch 872/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1061 - val_loss: -3.0353\n",
      "Epoch 873/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1047 - val_loss: -3.9277\n",
      "Epoch 874/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.1261 - val_loss: -3.9030\n",
      "Epoch 875/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.6283 - val_loss: -4.1213\n",
      "Epoch 876/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9114 - val_loss: -3.9734\n",
      "Epoch 877/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9167 - val_loss: -4.1698\n",
      "Epoch 878/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7086 - val_loss: -4.3195\n",
      "Epoch 879/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.7823 - val_loss: -3.4308\n",
      "Epoch 880/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.8698 - val_loss: -3.0271\n",
      "Epoch 881/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9334 - val_loss: -3.9545\n",
      "Epoch 882/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0275 - val_loss: -3.4781\n",
      "Epoch 883/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0760 - val_loss: -4.3748\n",
      "Epoch 884/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9460 - val_loss: -3.5495\n",
      "Epoch 885/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0702 - val_loss: -4.3933\n",
      "Epoch 886/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0001 - val_loss: -3.4050\n",
      "Epoch 887/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1303 - val_loss: -4.1762\n",
      "Epoch 888/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1237 - val_loss: -4.6319\n",
      "Epoch 889/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8669 - val_loss: -4.5240\n",
      "Epoch 890/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1252 - val_loss: -4.5468\n",
      "Epoch 891/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9889 - val_loss: -4.0866\n",
      "Epoch 892/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0487 - val_loss: -3.3771\n",
      "Epoch 893/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.5855 - val_loss: -4.0671\n",
      "Epoch 894/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7403 - val_loss: -3.8721\n",
      "Epoch 895/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9743 - val_loss: -4.3253\n",
      "Epoch 896/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8944 - val_loss: -4.2638\n",
      "Epoch 897/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -5.0997 - val_loss: -3.5611\n",
      "Epoch 898/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0886 - val_loss: -4.5184\n",
      "Epoch 899/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0225 - val_loss: -4.2612\n",
      "Epoch 900/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0548 - val_loss: -4.0390\n",
      "Epoch 901/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0881 - val_loss: -4.4747\n",
      "Epoch 902/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1019 - val_loss: -3.8476\n",
      "Epoch 903/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1347 - val_loss: -2.2245\n",
      "Epoch 904/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1028 - val_loss: -3.4402\n",
      "Epoch 905/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1484 - val_loss: -4.5489\n",
      "Epoch 906/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7095 - val_loss: -4.1554\n",
      "Epoch 907/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9892 - val_loss: -4.3427\n",
      "Epoch 908/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1337 - val_loss: -3.7234\n",
      "Epoch 909/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0646 - val_loss: -3.3587\n",
      "Epoch 910/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0906 - val_loss: -4.2556\n",
      "Epoch 911/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1124 - val_loss: -4.0642\n",
      "Epoch 912/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1756 - val_loss: -4.1915\n",
      "Epoch 913/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0286 - val_loss: -4.1746\n",
      "Epoch 914/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1312 - val_loss: -4.0262\n",
      "Epoch 915/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2183 - val_loss: -4.4801\n",
      "Epoch 916/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0729 - val_loss: -3.4482\n",
      "Epoch 917/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1134 - val_loss: -4.0857\n",
      "Epoch 918/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2077 - val_loss: -4.4823\n",
      "Epoch 919/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2069 - val_loss: -4.2992\n",
      "Epoch 920/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1581 - val_loss: -4.1450\n",
      "Epoch 921/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0191 - val_loss: -4.6072\n",
      "Epoch 922/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1548 - val_loss: -4.3612\n",
      "Epoch 923/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1667 - val_loss: -2.5905\n",
      "Epoch 924/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0449 - val_loss: -4.5737\n",
      "Epoch 925/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8631 - val_loss: -4.4955\n",
      "Epoch 926/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1276 - val_loss: -4.2833\n",
      "Epoch 927/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1500 - val_loss: -4.6306\n",
      "Epoch 928/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2301 - val_loss: -3.5101\n",
      "Epoch 929/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1675 - val_loss: -4.5745\n",
      "Epoch 930/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1875 - val_loss: -4.2416\n",
      "Epoch 931/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0126 - val_loss: -4.4468\n",
      "Epoch 932/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1637 - val_loss: -4.3344\n",
      "Epoch 933/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 244ms/step - loss: -5.1012 - val_loss: -4.7420\n",
      "Epoch 934/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1419 - val_loss: -4.3673\n",
      "Epoch 935/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1899 - val_loss: -4.4157\n",
      "Epoch 936/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1865 - val_loss: -4.5847\n",
      "Epoch 937/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1638 - val_loss: -4.5943\n",
      "Epoch 938/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1552 - val_loss: -4.4912\n",
      "Epoch 939/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1779 - val_loss: -3.6046\n",
      "Epoch 940/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1131 - val_loss: -4.6843\n",
      "Epoch 941/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1642 - val_loss: -4.5184\n",
      "Epoch 942/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2495 - val_loss: -3.8149\n",
      "Epoch 943/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1335 - val_loss: -4.3259\n",
      "Epoch 944/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1334 - val_loss: -4.3335\n",
      "Epoch 945/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1682 - val_loss: -4.5444\n",
      "Epoch 946/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1684 - val_loss: -4.5357\n",
      "Epoch 947/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1477 - val_loss: -3.0051\n",
      "Epoch 948/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1754 - val_loss: -4.3513\n",
      "Epoch 949/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0604 - val_loss: -2.3140\n",
      "Epoch 950/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0450 - val_loss: -4.4192\n",
      "Epoch 951/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0955 - val_loss: -3.9721\n",
      "Epoch 952/2000\n",
      "118/118 [==============================] - 23s 192ms/step - loss: -5.2197 - val_loss: -3.4755\n",
      "Epoch 953/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1658 - val_loss: -3.9166\n",
      "Epoch 954/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2055 - val_loss: -4.3901\n",
      "Epoch 955/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1918 - val_loss: -3.9287\n",
      "Epoch 956/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2014 - val_loss: -2.9050\n",
      "Epoch 957/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1509 - val_loss: -4.4891\n",
      "Epoch 958/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1867 - val_loss: -4.6316\n",
      "Epoch 959/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2375 - val_loss: -4.5788\n",
      "Epoch 960/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2434 - val_loss: -4.1686\n",
      "Epoch 961/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1381 - val_loss: -4.5452\n",
      "Epoch 962/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1430 - val_loss: -4.6400\n",
      "Epoch 963/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2203 - val_loss: -4.5374\n",
      "Epoch 964/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1724 - val_loss: -4.0825\n",
      "Epoch 965/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2499 - val_loss: -4.1895\n",
      "Epoch 966/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0808 - val_loss: -3.6774\n",
      "Epoch 967/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2369 - val_loss: -3.7692\n",
      "Epoch 968/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1972 - val_loss: -4.4811\n",
      "Epoch 969/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2403 - val_loss: -4.6294\n",
      "Epoch 970/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0967 - val_loss: -3.8951\n",
      "Epoch 971/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1266 - val_loss: -4.5580\n",
      "Epoch 972/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1866 - val_loss: -4.3168\n",
      "Epoch 973/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2110 - val_loss: -4.5422\n",
      "Epoch 974/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1797 - val_loss: -4.0299\n",
      "Epoch 975/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1529 - val_loss: -3.3644\n",
      "Epoch 976/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2554 - val_loss: -4.3355\n",
      "Epoch 977/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1647 - val_loss: -3.7297\n",
      "Epoch 978/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2503 - val_loss: -4.4096\n",
      "Epoch 979/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.1112 - val_loss: -3.8960\n",
      "Epoch 980/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2267 - val_loss: -4.3851\n",
      "Epoch 981/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2371 - val_loss: -3.7711\n",
      "Epoch 982/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1939 - val_loss: -4.4873\n",
      "Epoch 983/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2186 - val_loss: -2.4120\n",
      "Epoch 984/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0435 - val_loss: -4.4365\n",
      "Epoch 985/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1850 - val_loss: -3.7880\n",
      "Epoch 986/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2575 - val_loss: -4.2631\n",
      "Epoch 987/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1973 - val_loss: -2.8185\n",
      "Epoch 988/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2713 - val_loss: -3.9556\n",
      "Epoch 989/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2094 - val_loss: -4.2730\n",
      "Epoch 990/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -4.8526 - val_loss: -4.4111\n",
      "Epoch 991/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9481 - val_loss: -3.6307\n",
      "Epoch 992/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1527 - val_loss: -4.6274\n",
      "Epoch 993/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1536 - val_loss: -4.0410\n",
      "Epoch 994/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2405 - val_loss: -4.4091\n",
      "Epoch 995/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0301 - val_loss: -4.1905\n",
      "Epoch 996/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -5.2105 - val_loss: -3.6773\n",
      "Epoch 997/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1453 - val_loss: -3.3845\n",
      "Epoch 998/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2515 - val_loss: -4.4675\n",
      "Epoch 999/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1477 - val_loss: -4.3784\n",
      "Epoch 1000/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2414 - val_loss: -3.4999\n",
      "Epoch 1001/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2209"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -5.2209 - val_loss: -4.7924\n",
      "Epoch 1002/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1753 - val_loss: -3.5331\n",
      "Epoch 1003/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1717 - val_loss: -4.1009\n",
      "Epoch 1004/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1994 - val_loss: -4.0105\n",
      "Epoch 1005/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2191 - val_loss: -4.0319\n",
      "Epoch 1006/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2326 - val_loss: -4.4754\n",
      "Epoch 1007/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0778 - val_loss: -4.2884\n",
      "Epoch 1008/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2400 - val_loss: -4.1712\n",
      "Epoch 1009/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2182 - val_loss: -3.9318\n",
      "Epoch 1010/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1380 - val_loss: -4.2969\n",
      "Epoch 1011/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2718 - val_loss: -4.3766\n",
      "Epoch 1012/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1820 - val_loss: -4.4289\n",
      "Epoch 1013/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2535 - val_loss: -4.4617\n",
      "Epoch 1014/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1271 - val_loss: -3.9200\n",
      "Epoch 1015/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2318 - val_loss: -4.0470\n",
      "Epoch 1016/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2634 - val_loss: -3.8289\n",
      "Epoch 1017/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2645 - val_loss: -4.6493\n",
      "Epoch 1018/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1810 - val_loss: -4.0229\n",
      "Epoch 1019/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1623 - val_loss: -2.8076\n",
      "Epoch 1020/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1428 - val_loss: -4.4210\n",
      "Epoch 1021/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2451 - val_loss: -4.3030\n",
      "Epoch 1022/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2189 - val_loss: -4.0926\n",
      "Epoch 1023/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1832 - val_loss: -4.2390\n",
      "Epoch 1024/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2353 - val_loss: -3.9591\n",
      "Epoch 1025/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2921 - val_loss: -4.2014\n",
      "Epoch 1026/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2667 - val_loss: -4.6081\n",
      "Epoch 1027/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2499 - val_loss: -4.4010\n",
      "Epoch 1028/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1453 - val_loss: -3.9276\n",
      "Epoch 1029/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2892 - val_loss: -4.6138\n",
      "Epoch 1030/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2181 - val_loss: -2.4464\n",
      "Epoch 1031/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2835 - val_loss: -4.5256\n",
      "Epoch 1032/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2342 - val_loss: -3.6553\n",
      "Epoch 1033/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2810 - val_loss: -4.1651\n",
      "Epoch 1034/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5655 - val_loss: -4.3323\n",
      "Epoch 1035/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9713 - val_loss: -4.0850\n",
      "Epoch 1036/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1171 - val_loss: -3.5181\n",
      "Epoch 1037/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0433 - val_loss: -4.6089\n",
      "Epoch 1038/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0110 - val_loss: -4.2347\n",
      "Epoch 1039/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1807 - val_loss: -4.5355\n",
      "Epoch 1040/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2424"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -5.2424 - val_loss: -4.8160\n",
      "Epoch 1041/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1805 - val_loss: -3.6854\n",
      "Epoch 1042/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.4651 - val_loss: -3.6438\n",
      "Epoch 1043/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1292 - val_loss: -3.8977\n",
      "Epoch 1044/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2040 - val_loss: -4.5436\n",
      "Epoch 1045/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1539 - val_loss: -4.6054\n",
      "Epoch 1046/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2385 - val_loss: -4.5169\n",
      "Epoch 1047/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0910 - val_loss: -3.9487\n",
      "Epoch 1048/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2057 - val_loss: -4.3617\n",
      "Epoch 1049/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1480 - val_loss: -4.1950\n",
      "Epoch 1050/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1395 - val_loss: -4.0253\n",
      "Epoch 1051/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1458 - val_loss: -4.5740\n",
      "Epoch 1052/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2917 - val_loss: -4.5096\n",
      "Epoch 1053/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1878 - val_loss: -3.8955\n",
      "Epoch 1054/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2366 - val_loss: -4.3590\n",
      "Epoch 1055/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2600 - val_loss: 1.7351\n",
      "Epoch 1056/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9309 - val_loss: -4.2117\n",
      "Epoch 1057/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1602 - val_loss: -3.9256\n",
      "Epoch 1058/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2724 - val_loss: -3.5792\n",
      "Epoch 1059/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1919 - val_loss: -4.2226\n",
      "Epoch 1060/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2574 - val_loss: -4.3478\n",
      "Epoch 1061/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2181 - val_loss: -3.8898\n",
      "Epoch 1062/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2849 - val_loss: -3.9703\n",
      "Epoch 1063/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2941 - val_loss: -3.4487\n",
      "Epoch 1064/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2386 - val_loss: -4.1884\n",
      "Epoch 1065/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1985 - val_loss: -3.7372\n",
      "Epoch 1066/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2934 - val_loss: -4.6206\n",
      "Epoch 1067/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2505 - val_loss: -3.6672\n",
      "Epoch 1068/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3061 - val_loss: -4.5745\n",
      "Epoch 1069/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1705 - val_loss: -1.0520\n",
      "Epoch 1070/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2857 - val_loss: -4.4952\n",
      "Epoch 1071/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0021 - val_loss: -4.3221\n",
      "Epoch 1072/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2942 - val_loss: -3.9712\n",
      "Epoch 1073/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2988 - val_loss: -4.1595\n",
      "Epoch 1074/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.2707 - val_loss: -4.5885\n",
      "Epoch 1075/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1944 - val_loss: -4.0884\n",
      "Epoch 1076/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2155 - val_loss: -4.4246\n",
      "Epoch 1077/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2933 - val_loss: -4.5511\n",
      "Epoch 1078/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2873 - val_loss: -3.5219\n",
      "Epoch 1079/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -3.6398 - val_loss: -3.1783\n",
      "Epoch 1080/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.9694 - val_loss: -4.1142\n",
      "Epoch 1081/2000\n",
      "118/118 [==============================] - 23s 190ms/step - loss: -4.3914 - val_loss: -4.2877\n",
      "Epoch 1082/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6067 - val_loss: -2.9446\n",
      "Epoch 1083/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.7221 - val_loss: -3.9769\n",
      "Epoch 1084/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8152 - val_loss: -4.0286\n",
      "Epoch 1085/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8847 - val_loss: -4.1604\n",
      "Epoch 1086/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9002 - val_loss: -3.9255\n",
      "Epoch 1087/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8398 - val_loss: -4.1836\n",
      "Epoch 1088/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8463 - val_loss: -4.2340\n",
      "Epoch 1089/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8742 - val_loss: -4.2035\n",
      "Epoch 1090/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0122 - val_loss: -4.0003\n",
      "Epoch 1091/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9451 - val_loss: -3.9560\n",
      "Epoch 1092/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0195 - val_loss: -3.9449\n",
      "Epoch 1093/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0816 - val_loss: -4.3470\n",
      "Epoch 1094/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0433 - val_loss: -3.7948\n",
      "Epoch 1095/2000\n",
      "118/118 [==============================] - 23s 192ms/step - loss: -4.9476 - val_loss: -3.8135\n",
      "Epoch 1096/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0755 - val_loss: -4.4226\n",
      "Epoch 1097/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0457 - val_loss: -4.1991\n",
      "Epoch 1098/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1098 - val_loss: -3.8294\n",
      "Epoch 1099/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0169 - val_loss: -3.9236\n",
      "Epoch 1100/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9881 - val_loss: -2.8827\n",
      "Epoch 1101/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0058 - val_loss: -4.2473\n",
      "Epoch 1102/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0914 - val_loss: -4.5058\n",
      "Epoch 1103/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0768 - val_loss: -4.5340\n",
      "Epoch 1104/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1583 - val_loss: -4.4537\n",
      "Epoch 1105/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0379 - val_loss: -4.3532\n",
      "Epoch 1106/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1357 - val_loss: -4.2706\n",
      "Epoch 1107/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1367 - val_loss: -3.0092\n",
      "Epoch 1108/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0783 - val_loss: -3.0540\n",
      "Epoch 1109/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1180 - val_loss: -3.9566\n",
      "Epoch 1110/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0714 - val_loss: -3.6886\n",
      "Epoch 1111/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0463 - val_loss: -4.3635\n",
      "Epoch 1112/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1511 - val_loss: -4.0212\n",
      "Epoch 1113/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0664 - val_loss: -3.8493\n",
      "Epoch 1114/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1384 - val_loss: -4.5904\n",
      "Epoch 1115/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0804 - val_loss: -3.8565\n",
      "Epoch 1116/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0336 - val_loss: -4.0275\n",
      "Epoch 1117/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0233 - val_loss: -3.9352\n",
      "Epoch 1118/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1998 - val_loss: -2.8235\n",
      "Epoch 1119/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1265 - val_loss: -4.4338\n",
      "Epoch 1120/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1530 - val_loss: -4.1471\n",
      "Epoch 1121/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.6248 - val_loss: -4.1267\n",
      "Epoch 1122/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.8888 - val_loss: -4.3408\n",
      "Epoch 1123/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0127 - val_loss: -4.2274\n",
      "Epoch 1124/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1217 - val_loss: -3.5975\n",
      "Epoch 1125/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0742 - val_loss: -4.4135\n",
      "Epoch 1126/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1306 - val_loss: -3.1947\n",
      "Epoch 1127/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0847 - val_loss: -4.6907\n",
      "Epoch 1128/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1793 - val_loss: -4.2376\n",
      "Epoch 1129/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1714 - val_loss: -3.9678\n",
      "Epoch 1130/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1821 - val_loss: -4.6105\n",
      "Epoch 1131/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1630 - val_loss: -4.5845\n",
      "Epoch 1132/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9616 - val_loss: -4.6318\n",
      "Epoch 1133/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1126 - val_loss: -4.4990\n",
      "Epoch 1134/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1010 - val_loss: -4.3886\n",
      "Epoch 1135/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1926 - val_loss: -4.7290\n",
      "Epoch 1136/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1889 - val_loss: -3.6234\n",
      "Epoch 1137/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1436 - val_loss: -4.4229\n",
      "Epoch 1138/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1970 - val_loss: -4.4962\n",
      "Epoch 1139/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1448 - val_loss: -3.5025\n",
      "Epoch 1140/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1210 - val_loss: -4.0208\n",
      "Epoch 1141/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1598 - val_loss: -4.4240\n",
      "Epoch 1142/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2467 - val_loss: -4.3742\n",
      "Epoch 1143/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1487 - val_loss: -4.3641\n",
      "Epoch 1144/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0652 - val_loss: -3.5626\n",
      "Epoch 1145/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9775 - val_loss: -4.5175\n",
      "Epoch 1146/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1887 - val_loss: -4.6595\n",
      "Epoch 1147/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.1109 - val_loss: -4.5328\n",
      "Epoch 1148/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2575 - val_loss: -1.6106\n",
      "Epoch 1149/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1205 - val_loss: -4.6930\n",
      "Epoch 1150/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1996 - val_loss: -4.6696\n",
      "Epoch 1151/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1736 - val_loss: -4.4696\n",
      "Epoch 1152/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1466 - val_loss: -3.8664\n",
      "Epoch 1153/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2425 - val_loss: -3.7231\n",
      "Epoch 1154/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2475 - val_loss: -4.6213\n",
      "Epoch 1155/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2700 - val_loss: -3.9920\n",
      "Epoch 1156/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0887 - val_loss: -4.5844\n",
      "Epoch 1157/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2571 - val_loss: -3.1569\n",
      "Epoch 1158/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2719 - val_loss: -2.9976\n",
      "Epoch 1159/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2659 - val_loss: -4.0307\n",
      "Epoch 1160/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2836 - val_loss: -4.6863\n",
      "Epoch 1161/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2980 - val_loss: -2.5858\n",
      "Epoch 1162/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1631 - val_loss: -4.2103\n",
      "Epoch 1163/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2072 - val_loss: -4.6148\n",
      "Epoch 1164/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2970 - val_loss: -4.7337\n",
      "Epoch 1165/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9389 - val_loss: -4.0301\n",
      "Epoch 1166/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1612 - val_loss: -4.3614\n",
      "Epoch 1167/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2326 - val_loss: -3.0670\n",
      "Epoch 1168/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1508 - val_loss: -3.3352\n",
      "Epoch 1169/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2539 - val_loss: -4.7013\n",
      "Epoch 1170/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3300 - val_loss: -4.3913\n",
      "Epoch 1171/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1648 - val_loss: -4.7761\n",
      "Epoch 1172/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2568 - val_loss: -4.4836\n",
      "Epoch 1173/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1231 - val_loss: -4.4281\n",
      "Epoch 1174/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2538 - val_loss: -2.3384\n",
      "Epoch 1175/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2222 - val_loss: -4.7587\n",
      "Epoch 1176/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2259 - val_loss: -3.7318\n",
      "Epoch 1177/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2156 - val_loss: -4.4595\n",
      "Epoch 1178/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2240 - val_loss: -4.7593\n",
      "Epoch 1179/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2714 - val_loss: -4.5981\n",
      "Epoch 1180/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2675 - val_loss: -3.9827\n",
      "Epoch 1181/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1921 - val_loss: -4.5573\n",
      "Epoch 1182/2000\n",
      "118/118 [==============================] - 23s 190ms/step - loss: -5.2329 - val_loss: -4.4504\n",
      "Epoch 1183/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3002 - val_loss: -3.1472\n",
      "Epoch 1184/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2971 - val_loss: -3.7998\n",
      "Epoch 1185/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3388 - val_loss: -2.7407\n",
      "Epoch 1186/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3023 - val_loss: -3.9749\n",
      "Epoch 1187/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.2825 - val_loss: -4.2851\n",
      "Epoch 1188/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0232 - val_loss: -4.6581\n",
      "Epoch 1189/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1829 - val_loss: -3.9426\n",
      "Epoch 1190/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.0874 - val_loss: -4.2854\n",
      "Epoch 1191/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2542 - val_loss: -4.6152\n",
      "Epoch 1192/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2003 - val_loss: -4.0988\n",
      "Epoch 1193/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0956 - val_loss: -4.7324\n",
      "Epoch 1194/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2185 - val_loss: -3.9641\n",
      "Epoch 1195/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2802 - val_loss: -4.5327\n",
      "Epoch 1196/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1511 - val_loss: -4.7950\n",
      "Epoch 1197/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3011 - val_loss: -2.7098\n",
      "Epoch 1198/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.3180 - val_loss: -4.6665\n",
      "Epoch 1199/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0830 - val_loss: -4.4148\n",
      "Epoch 1200/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.0980 - val_loss: -4.6287\n",
      "Epoch 1201/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2198 - val_loss: -4.2135\n",
      "Epoch 1202/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1936 - val_loss: -4.6938\n",
      "Epoch 1203/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2860 - val_loss: -4.6350\n",
      "Epoch 1204/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2399 - val_loss: -4.1262\n",
      "Epoch 1205/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2803 - val_loss: -4.1680\n",
      "Epoch 1206/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2365 - val_loss: -4.2302\n",
      "Epoch 1207/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2608 - val_loss: -4.4037\n",
      "Epoch 1208/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2703 - val_loss: -4.1083\n",
      "Epoch 1209/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1486 - val_loss: -3.1222\n",
      "Epoch 1210/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2090 - val_loss: -4.0550\n",
      "Epoch 1211/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3371 - val_loss: -4.6608\n",
      "Epoch 1212/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2870 - val_loss: -4.6462\n",
      "Epoch 1213/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3223 - val_loss: -3.7555\n",
      "Epoch 1214/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0988 - val_loss: -4.5009\n",
      "Epoch 1215/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2172 - val_loss: -4.5055\n",
      "Epoch 1216/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2675 - val_loss: -4.0791\n",
      "Epoch 1217/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.0738 - val_loss: -3.7914\n",
      "Epoch 1218/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2850 - val_loss: -4.6332\n",
      "Epoch 1219/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1860 - val_loss: -4.4600\n",
      "Epoch 1220/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2679 - val_loss: -4.7274\n",
      "Epoch 1221/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2621 - val_loss: -4.7593\n",
      "Epoch 1222/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2731 - val_loss: -4.8147\n",
      "Epoch 1223/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2654 - val_loss: -4.4552\n",
      "Epoch 1224/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2966 - val_loss: -4.5283\n",
      "Epoch 1225/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2345 - val_loss: -4.1867\n",
      "Epoch 1226/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1882 - val_loss: -4.3845\n",
      "Epoch 1227/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1891 - val_loss: -4.4451\n",
      "Epoch 1228/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3135 - val_loss: -4.0700\n",
      "Epoch 1229/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2368 - val_loss: -4.7198\n",
      "Epoch 1230/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2579 - val_loss: -3.5862\n",
      "Epoch 1231/2000\n",
      "118/118 [==============================] - 22s 191ms/step - loss: -5.2668 - val_loss: -4.6184\n",
      "Epoch 1232/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3214 - val_loss: -2.5799\n",
      "Epoch 1233/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2299 - val_loss: -4.6134\n",
      "Epoch 1234/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3267 - val_loss: -4.6568\n",
      "Epoch 1235/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3115 - val_loss: -4.3741\n",
      "Epoch 1236/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -5.3450 - val_loss: -4.8342\n",
      "Epoch 1237/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2570 - val_loss: -4.6424\n",
      "Epoch 1238/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3475 - val_loss: -4.6041\n",
      "Epoch 1239/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9887 - val_loss: -4.1510\n",
      "Epoch 1240/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1312 - val_loss: -4.5462\n",
      "Epoch 1241/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3259"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 244ms/step - loss: -5.3259 - val_loss: -4.9248\n",
      "Epoch 1242/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5122 - val_loss: -4.0642\n",
      "Epoch 1243/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.0703 - val_loss: -4.3481\n",
      "Epoch 1244/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2188 - val_loss: -4.5917\n",
      "Epoch 1245/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2545 - val_loss: -4.2882\n",
      "Epoch 1246/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3125 - val_loss: -4.6477\n",
      "Epoch 1247/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2252 - val_loss: -4.6455\n",
      "Epoch 1248/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2976 - val_loss: -4.3549\n",
      "Epoch 1249/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2106 - val_loss: -4.1844\n",
      "Epoch 1250/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2447 - val_loss: -4.1763\n",
      "Epoch 1251/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2794 - val_loss: -4.8435\n",
      "Epoch 1252/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3398 - val_loss: -3.6581\n",
      "Epoch 1253/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_160_layer_call_fn, lstm_cell_160_layer_call_and_return_conditional_losses, lstm_cell_161_layer_call_fn, lstm_cell_161_layer_call_and_return_conditional_losses, lstm_cell_162_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 256ms/step - loss: -5.3333 - val_loss: -4.9976\n",
      "Epoch 1254/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2414 - val_loss: -4.2633\n",
      "Epoch 1255/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3347 - val_loss: -3.3459\n",
      "Epoch 1256/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1983 - val_loss: -3.7839\n",
      "Epoch 1257/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3448 - val_loss: -4.7073\n",
      "Epoch 1258/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2230 - val_loss: -3.9094\n",
      "Epoch 1259/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2940 - val_loss: -4.8286\n",
      "Epoch 1260/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2198 - val_loss: -4.4977\n",
      "Epoch 1261/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2289 - val_loss: -3.9076\n",
      "Epoch 1262/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3672 - val_loss: -2.7412\n",
      "Epoch 1263/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3223 - val_loss: -4.6461\n",
      "Epoch 1264/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3915 - val_loss: -1.0398\n",
      "Epoch 1265/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2418 - val_loss: -3.3846\n",
      "Epoch 1266/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3170 - val_loss: -4.6219\n",
      "Epoch 1267/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1785 - val_loss: -4.4592\n",
      "Epoch 1268/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4095 - val_loss: -4.3639\n",
      "Epoch 1269/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3636 - val_loss: -4.5091\n",
      "Epoch 1270/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3556 - val_loss: -4.3339\n",
      "Epoch 1271/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3514 - val_loss: -4.0085\n",
      "Epoch 1272/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1952 - val_loss: -4.4686\n",
      "Epoch 1273/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2643 - val_loss: -4.5067\n",
      "Epoch 1274/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2577 - val_loss: -4.8615\n",
      "Epoch 1275/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3413 - val_loss: -3.0377\n",
      "Epoch 1276/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3473 - val_loss: -4.5638\n",
      "Epoch 1277/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2314 - val_loss: -4.4984\n",
      "Epoch 1278/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2499 - val_loss: -4.0091\n",
      "Epoch 1279/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4287 - val_loss: -1.8578\n",
      "Epoch 1280/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0728 - val_loss: -4.5764\n",
      "Epoch 1281/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3531 - val_loss: -4.4729\n",
      "Epoch 1282/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2632 - val_loss: -4.4406\n",
      "Epoch 1283/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3875 - val_loss: -4.5142\n",
      "Epoch 1284/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2863 - val_loss: -4.4651\n",
      "Epoch 1285/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3556 - val_loss: -3.4734\n",
      "Epoch 1286/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4030 - val_loss: -3.7973\n",
      "Epoch 1287/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2903 - val_loss: -2.8777\n",
      "Epoch 1288/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3775 - val_loss: -4.1677\n",
      "Epoch 1289/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2855 - val_loss: -3.4698\n",
      "Epoch 1290/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3272 - val_loss: -3.9872\n",
      "Epoch 1291/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2778 - val_loss: -2.7964\n",
      "Epoch 1292/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4078 - val_loss: -4.2941\n",
      "Epoch 1293/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2431 - val_loss: -3.9029\n",
      "Epoch 1294/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3220 - val_loss: -4.4902\n",
      "Epoch 1295/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2742 - val_loss: -4.3064\n",
      "Epoch 1296/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3772 - val_loss: -4.6972\n",
      "Epoch 1297/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3315 - val_loss: -4.5662\n",
      "Epoch 1298/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2083 - val_loss: -4.7335\n",
      "Epoch 1299/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3776 - val_loss: -4.0055\n",
      "Epoch 1300/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2299 - val_loss: -4.0376\n",
      "Epoch 1301/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3854 - val_loss: -4.1370\n",
      "Epoch 1302/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3025 - val_loss: -4.6512\n",
      "Epoch 1303/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3419 - val_loss: -4.5974\n",
      "Epoch 1304/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3047 - val_loss: -4.3066\n",
      "Epoch 1305/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3744 - val_loss: -3.3225\n",
      "Epoch 1306/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2813 - val_loss: -4.5786\n",
      "Epoch 1307/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3786 - val_loss: -4.5897\n",
      "Epoch 1308/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1569 - val_loss: -4.5544\n",
      "Epoch 1309/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2632 - val_loss: -4.2074\n",
      "Epoch 1310/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3657 - val_loss: -4.5521\n",
      "Epoch 1311/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3186 - val_loss: -3.6717\n",
      "Epoch 1312/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3443 - val_loss: -4.2789\n",
      "Epoch 1313/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2826 - val_loss: -4.8572\n",
      "Epoch 1314/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3483 - val_loss: -4.7205\n",
      "Epoch 1315/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2813 - val_loss: -4.7152\n",
      "Epoch 1316/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4451 - val_loss: -4.1791\n",
      "Epoch 1317/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3862 - val_loss: -4.2445\n",
      "Epoch 1318/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3273 - val_loss: -4.7595\n",
      "Epoch 1319/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2933 - val_loss: -4.4540\n",
      "Epoch 1320/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0122 - val_loss: -4.0182\n",
      "Epoch 1321/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2116 - val_loss: -4.5282\n",
      "Epoch 1322/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3060 - val_loss: -3.7597\n",
      "Epoch 1323/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4262 - val_loss: -4.6215\n",
      "Epoch 1324/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3920 - val_loss: -4.1846\n",
      "Epoch 1325/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2360 - val_loss: -4.3818\n",
      "Epoch 1326/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3289 - val_loss: -4.0325\n",
      "Epoch 1327/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3233 - val_loss: -4.7923\n",
      "Epoch 1328/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3213 - val_loss: -4.7089\n",
      "Epoch 1329/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3468 - val_loss: -3.6856\n",
      "Epoch 1330/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3087 - val_loss: -4.3986\n",
      "Epoch 1331/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4091 - val_loss: -4.6633\n",
      "Epoch 1332/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3557 - val_loss: -3.0674\n",
      "Epoch 1333/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3521 - val_loss: -3.8597\n",
      "Epoch 1334/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3789 - val_loss: -4.7200\n",
      "Epoch 1335/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2256 - val_loss: -3.8545\n",
      "Epoch 1336/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4202 - val_loss: -4.1666\n",
      "Epoch 1337/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3547 - val_loss: -4.6860\n",
      "Epoch 1338/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2206 - val_loss: -4.5231\n",
      "Epoch 1339/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4166 - val_loss: -4.0369\n",
      "Epoch 1340/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3612 - val_loss: -1.0235\n",
      "Epoch 1341/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3040 - val_loss: -4.1040\n",
      "Epoch 1342/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3293 - val_loss: -4.4009\n",
      "Epoch 1343/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.4116 - val_loss: -4.5467\n",
      "Epoch 1344/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3958 - val_loss: -4.6747\n",
      "Epoch 1345/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3746 - val_loss: -4.6128\n",
      "Epoch 1346/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3357 - val_loss: -4.9159\n",
      "Epoch 1347/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4186 - val_loss: -4.7373\n",
      "Epoch 1348/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3817 - val_loss: -4.3922\n",
      "Epoch 1349/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4051 - val_loss: -3.9914\n",
      "Epoch 1350/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2865 - val_loss: -4.6780\n",
      "Epoch 1351/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4109 - val_loss: -4.1931\n",
      "Epoch 1352/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0293 - val_loss: -3.9126\n",
      "Epoch 1353/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3391 - val_loss: -3.6713\n",
      "Epoch 1354/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3540 - val_loss: -3.7903\n",
      "Epoch 1355/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.1968 - val_loss: -4.8078\n",
      "Epoch 1356/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3030 - val_loss: -4.1377\n",
      "Epoch 1357/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2850 - val_loss: -4.2782\n",
      "Epoch 1358/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3611 - val_loss: -4.0030\n",
      "Epoch 1359/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2765 - val_loss: -3.8044\n",
      "Epoch 1360/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4257 - val_loss: -4.0383\n",
      "Epoch 1361/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3868 - val_loss: -3.2996\n",
      "Epoch 1362/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2382 - val_loss: -3.9103\n",
      "Epoch 1363/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4186 - val_loss: -3.9958\n",
      "Epoch 1364/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3394 - val_loss: -4.4275\n",
      "Epoch 1365/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.3872 - val_loss: -4.1181\n",
      "Epoch 1366/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3825 - val_loss: -4.5076\n",
      "Epoch 1367/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4465 - val_loss: -4.2233\n",
      "Epoch 1368/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3415 - val_loss: -3.3724\n",
      "Epoch 1369/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3772 - val_loss: -4.7526\n",
      "Epoch 1370/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.2794 - val_loss: -4.4019\n",
      "Epoch 1371/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3391 - val_loss: -4.7218\n",
      "Epoch 1372/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -4.9350 - val_loss: -4.3160\n",
      "Epoch 1373/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2218 - val_loss: -3.0428\n",
      "Epoch 1374/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2658 - val_loss: -0.5580\n",
      "Epoch 1375/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2129 - val_loss: -4.6103\n",
      "Epoch 1376/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3702 - val_loss: -4.6298\n",
      "Epoch 1377/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3814 - val_loss: -4.1517\n",
      "Epoch 1378/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3664 - val_loss: -2.2799\n",
      "Epoch 1379/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3270 - val_loss: -4.3059\n",
      "Epoch 1380/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3701 - val_loss: -3.7492\n",
      "Epoch 1381/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3010 - val_loss: -4.8769\n",
      "Epoch 1382/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3984 - val_loss: -4.8753\n",
      "Epoch 1383/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2502 - val_loss: -4.1898\n",
      "Epoch 1384/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3577 - val_loss: -4.4481\n",
      "Epoch 1385/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3452 - val_loss: -4.5075\n",
      "Epoch 1386/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4122 - val_loss: -2.9159\n",
      "Epoch 1387/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3491 - val_loss: -4.7024\n",
      "Epoch 1388/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4615 - val_loss: -3.7039\n",
      "Epoch 1389/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.1707 - val_loss: -4.2389\n",
      "Epoch 1390/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4077 - val_loss: -4.7831\n",
      "Epoch 1391/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3982 - val_loss: -4.5852\n",
      "Epoch 1392/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3205 - val_loss: -3.8967\n",
      "Epoch 1393/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2127 - val_loss: -3.6096\n",
      "Epoch 1394/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2980 - val_loss: -4.0470\n",
      "Epoch 1395/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3429 - val_loss: -4.2988\n",
      "Epoch 1396/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2876 - val_loss: -4.5400\n",
      "Epoch 1397/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2955 - val_loss: -4.2614\n",
      "Epoch 1398/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3836 - val_loss: -4.2577\n",
      "Epoch 1399/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4618 - val_loss: -4.3192\n",
      "Epoch 1400/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3192 - val_loss: -4.0850\n",
      "Epoch 1401/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4146 - val_loss: -4.1934\n",
      "Epoch 1402/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4625 - val_loss: -4.5515\n",
      "Epoch 1403/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3299 - val_loss: -3.2047\n",
      "Epoch 1404/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3564 - val_loss: -4.4795\n",
      "Epoch 1405/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3537 - val_loss: -4.6719\n",
      "Epoch 1406/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4251 - val_loss: -2.6029\n",
      "Epoch 1407/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4548 - val_loss: -4.0580\n",
      "Epoch 1408/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.1710 - val_loss: -4.2457\n",
      "Epoch 1409/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4303 - val_loss: -4.2248\n",
      "Epoch 1410/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3343 - val_loss: -4.6018\n",
      "Epoch 1411/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3882 - val_loss: -3.1479\n",
      "Epoch 1412/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3708 - val_loss: -4.9678\n",
      "Epoch 1413/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4042 - val_loss: -3.2623\n",
      "Epoch 1414/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3844 - val_loss: -1.8149\n",
      "Epoch 1415/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3949 - val_loss: -3.6752\n",
      "Epoch 1416/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4346 - val_loss: -4.2632\n",
      "Epoch 1417/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4191 - val_loss: -4.1290\n",
      "Epoch 1418/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3915 - val_loss: -4.5440\n",
      "Epoch 1419/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3497 - val_loss: -4.6288\n",
      "Epoch 1420/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3905 - val_loss: -4.2133\n",
      "Epoch 1421/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3680 - val_loss: -4.7056\n",
      "Epoch 1422/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3478 - val_loss: -4.5312\n",
      "Epoch 1423/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.9647 - val_loss: -4.4441\n",
      "Epoch 1424/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3879 - val_loss: -3.7573\n",
      "Epoch 1425/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2853 - val_loss: -3.7301\n",
      "Epoch 1426/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3419 - val_loss: -4.6659\n",
      "Epoch 1427/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3966 - val_loss: -3.8519\n",
      "Epoch 1428/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3980 - val_loss: -4.6678\n",
      "Epoch 1429/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4070 - val_loss: -4.6851\n",
      "Epoch 1430/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3710 - val_loss: -4.5001\n",
      "Epoch 1431/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3457 - val_loss: -4.4225\n",
      "Epoch 1432/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2806 - val_loss: -3.7453\n",
      "Epoch 1433/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4701 - val_loss: -4.4343\n",
      "Epoch 1434/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3857 - val_loss: -4.4351\n",
      "Epoch 1435/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.4212 - val_loss: -4.7347\n",
      "Epoch 1436/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4034 - val_loss: -2.7384\n",
      "Epoch 1437/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3905 - val_loss: -4.4962\n",
      "Epoch 1438/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3852 - val_loss: -3.6906\n",
      "Epoch 1439/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4081 - val_loss: -4.4297\n",
      "Epoch 1440/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2142 - val_loss: -4.5358\n",
      "Epoch 1441/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4126 - val_loss: -4.2676\n",
      "Epoch 1442/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4455 - val_loss: -4.6273\n",
      "Epoch 1443/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4310 - val_loss: -4.6874\n",
      "Epoch 1444/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3488 - val_loss: -4.7539\n",
      "Epoch 1445/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2898 - val_loss: -4.1471\n",
      "Epoch 1446/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3825 - val_loss: -4.6957\n",
      "Epoch 1447/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3788 - val_loss: -4.2346\n",
      "Epoch 1448/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -4.5213 - val_loss: -4.2689\n",
      "Epoch 1449/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2851 - val_loss: -4.2324\n",
      "Epoch 1450/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.2746 - val_loss: -3.4574\n",
      "Epoch 1451/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2649 - val_loss: -4.6771\n",
      "Epoch 1452/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3167 - val_loss: -3.9096\n",
      "Epoch 1453/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4684 - val_loss: 0.3263\n",
      "Epoch 1454/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3821 - val_loss: -3.3348\n",
      "Epoch 1455/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1408 - val_loss: -4.9370\n",
      "Epoch 1456/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3809 - val_loss: -3.1888\n",
      "Epoch 1457/2000\n",
      "118/118 [==============================] - 23s 191ms/step - loss: -5.3782 - val_loss: -4.1427\n",
      "Epoch 1458/2000\n",
      "118/118 [==============================] - 22s 190ms/step - loss: -5.3872 - val_loss: -4.7111\n",
      "Epoch 1459/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4194 - val_loss: -4.2926\n",
      "Epoch 1460/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3760 - val_loss: -4.5513\n",
      "Epoch 1461/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3988 - val_loss: -4.6860\n",
      "Epoch 1462/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4466 - val_loss: -3.9113\n",
      "Epoch 1463/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3891 - val_loss: -4.5963\n",
      "Epoch 1464/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4180 - val_loss: -2.8078\n",
      "Epoch 1465/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3093 - val_loss: -2.7776\n",
      "Epoch 1466/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4943 - val_loss: -4.6785\n",
      "Epoch 1467/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4169 - val_loss: -3.5966\n",
      "Epoch 1468/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3599 - val_loss: -3.0813\n",
      "Epoch 1469/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4363 - val_loss: -4.4748\n",
      "Epoch 1470/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3264 - val_loss: -2.0464\n",
      "Epoch 1471/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4039 - val_loss: 4.5847\n",
      "Epoch 1472/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3855 - val_loss: -4.4575\n",
      "Epoch 1473/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3676 - val_loss: -3.8239\n",
      "Epoch 1474/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3405 - val_loss: -4.7915\n",
      "Epoch 1475/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4584 - val_loss: -4.4944\n",
      "Epoch 1476/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3149 - val_loss: -3.8093\n",
      "Epoch 1477/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4352 - val_loss: -4.2234\n",
      "Epoch 1478/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3428 - val_loss: -4.7077\n",
      "Epoch 1479/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4721 - val_loss: -4.8657\n",
      "Epoch 1480/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3735 - val_loss: -3.9561\n",
      "Epoch 1481/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4460 - val_loss: -4.4341\n",
      "Epoch 1482/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4424 - val_loss: -4.6150\n",
      "Epoch 1483/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4395 - val_loss: -3.0236\n",
      "Epoch 1484/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3603 - val_loss: -4.7120\n",
      "Epoch 1485/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4214 - val_loss: -4.6788\n",
      "Epoch 1486/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4180 - val_loss: -4.6977\n",
      "Epoch 1487/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3807 - val_loss: -4.7614\n",
      "Epoch 1488/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4225 - val_loss: -4.4347\n",
      "Epoch 1489/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3498 - val_loss: -4.6334\n",
      "Epoch 1490/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3514 - val_loss: -4.7957\n",
      "Epoch 1491/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3472 - val_loss: -3.9795\n",
      "Epoch 1492/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0727 - val_loss: -4.4281\n",
      "Epoch 1493/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1389 - val_loss: -4.6493\n",
      "Epoch 1494/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0982 - val_loss: -4.5777\n",
      "Epoch 1495/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2002 - val_loss: -4.2906\n",
      "Epoch 1496/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2111 - val_loss: -3.8462\n",
      "Epoch 1497/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1906 - val_loss: -3.9508\n",
      "Epoch 1498/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0384 - val_loss: -0.5717\n",
      "Epoch 1499/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1758 - val_loss: -3.0288\n",
      "Epoch 1500/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3519 - val_loss: -1.6303\n",
      "Epoch 1501/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2724 - val_loss: -4.6990\n",
      "Epoch 1502/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2792 - val_loss: -3.6076\n",
      "Epoch 1503/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.3452 - val_loss: -4.5437\n",
      "Epoch 1504/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2222 - val_loss: -4.3487\n",
      "Epoch 1505/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3254 - val_loss: -3.8630\n",
      "Epoch 1506/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4207 - val_loss: -4.5487\n",
      "Epoch 1507/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3045 - val_loss: -4.3933\n",
      "Epoch 1508/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1728 - val_loss: -3.8888\n",
      "Epoch 1509/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2726 - val_loss: -4.5059\n",
      "Epoch 1510/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4032 - val_loss: -4.0162\n",
      "Epoch 1511/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0024 - val_loss: -4.4236\n",
      "Epoch 1512/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2170 - val_loss: -4.5690\n",
      "Epoch 1513/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3527 - val_loss: -4.3814\n",
      "Epoch 1514/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2911 - val_loss: -4.6795\n",
      "Epoch 1515/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3616 - val_loss: -3.1419\n",
      "Epoch 1516/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4046 - val_loss: -3.4227\n",
      "Epoch 1517/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3552 - val_loss: -1.5219\n",
      "Epoch 1518/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3440 - val_loss: -4.6176\n",
      "Epoch 1519/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1105 - val_loss: -3.9489\n",
      "Epoch 1520/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3354 - val_loss: -4.3593\n",
      "Epoch 1521/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4194 - val_loss: -4.6399\n",
      "Epoch 1522/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3379 - val_loss: -4.6792\n",
      "Epoch 1523/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3074 - val_loss: -3.3090\n",
      "Epoch 1524/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3764 - val_loss: -4.0421\n",
      "Epoch 1525/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4143 - val_loss: -4.1494\n",
      "Epoch 1526/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4054 - val_loss: -4.4890\n",
      "Epoch 1527/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4031 - val_loss: -4.2505\n",
      "Epoch 1528/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4421 - val_loss: -4.4373\n",
      "Epoch 1529/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2550 - val_loss: -2.7796\n",
      "Epoch 1530/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3303 - val_loss: -4.5543\n",
      "Epoch 1531/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4330 - val_loss: -4.1392\n",
      "Epoch 1532/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4656 - val_loss: -3.8148\n",
      "Epoch 1533/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3401 - val_loss: -3.8570\n",
      "Epoch 1534/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3564 - val_loss: -4.5248\n",
      "Epoch 1535/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4756 - val_loss: -4.8584\n",
      "Epoch 1536/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4512 - val_loss: -3.7525\n",
      "Epoch 1537/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3986 - val_loss: -4.1752\n",
      "Epoch 1538/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4931 - val_loss: -4.8924\n",
      "Epoch 1539/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4715 - val_loss: -4.8367\n",
      "Epoch 1540/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3904 - val_loss: -4.5914\n",
      "Epoch 1541/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4332 - val_loss: -4.5231\n",
      "Epoch 1542/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3870 - val_loss: -3.7184\n",
      "Epoch 1543/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3726 - val_loss: -3.4913\n",
      "Epoch 1544/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4639 - val_loss: -4.6314\n",
      "Epoch 1545/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4203 - val_loss: -4.2213\n",
      "Epoch 1546/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3698 - val_loss: -3.9488\n",
      "Epoch 1547/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2537 - val_loss: -3.9206\n",
      "Epoch 1548/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4943 - val_loss: -4.8876\n",
      "Epoch 1549/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4081 - val_loss: -4.5629\n",
      "Epoch 1550/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9878 - val_loss: -4.3379\n",
      "Epoch 1551/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3318 - val_loss: -4.4879\n",
      "Epoch 1552/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3339 - val_loss: -4.0191\n",
      "Epoch 1553/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4156 - val_loss: -4.2097\n",
      "Epoch 1554/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4176 - val_loss: -4.6757\n",
      "Epoch 1555/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0956 - val_loss: -3.9112\n",
      "Epoch 1556/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0864 - val_loss: -3.3847\n",
      "Epoch 1557/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2924 - val_loss: -4.7973\n",
      "Epoch 1558/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4048 - val_loss: -4.2528\n",
      "Epoch 1559/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0901 - val_loss: -4.8404\n",
      "Epoch 1560/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2150 - val_loss: -4.6426\n",
      "Epoch 1561/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3802 - val_loss: -4.6893\n",
      "Epoch 1562/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3054 - val_loss: -4.7078\n",
      "Epoch 1563/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4007 - val_loss: -4.5521\n",
      "Epoch 1564/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3932 - val_loss: -4.2067\n",
      "Epoch 1565/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4047 - val_loss: -3.8415\n",
      "Epoch 1566/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4679 - val_loss: -4.6605\n",
      "Epoch 1567/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2004 - val_loss: -3.9923\n",
      "Epoch 1568/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2060 - val_loss: -4.7592\n",
      "Epoch 1569/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3545 - val_loss: -2.8073\n",
      "Epoch 1570/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2240 - val_loss: -4.5377\n",
      "Epoch 1571/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2866 - val_loss: -4.6114\n",
      "Epoch 1572/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3270 - val_loss: -4.4948\n",
      "Epoch 1573/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3471 - val_loss: -2.7085\n",
      "Epoch 1574/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4830 - val_loss: -4.3138\n",
      "Epoch 1575/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4236 - val_loss: -4.5794\n",
      "Epoch 1576/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3519 - val_loss: -4.4489\n",
      "Epoch 1577/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4059 - val_loss: -4.3722\n",
      "Epoch 1578/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3947 - val_loss: -4.8881\n",
      "Epoch 1579/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4186 - val_loss: -3.7950\n",
      "Epoch 1580/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4163 - val_loss: -3.1823\n",
      "Epoch 1581/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4752 - val_loss: -4.4114\n",
      "Epoch 1582/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0313 - val_loss: -3.8788\n",
      "Epoch 1583/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3756 - val_loss: -4.4519\n",
      "Epoch 1584/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4418 - val_loss: -4.1826\n",
      "Epoch 1585/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3176 - val_loss: -4.5347\n",
      "Epoch 1586/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3388 - val_loss: -2.2960\n",
      "Epoch 1587/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3985 - val_loss: -4.3532\n",
      "Epoch 1588/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4268 - val_loss: -4.1860\n",
      "Epoch 1589/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3823 - val_loss: -3.4980\n",
      "Epoch 1590/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3906 - val_loss: -4.0343\n",
      "Epoch 1591/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4656 - val_loss: -3.4195\n",
      "Epoch 1592/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4270 - val_loss: -4.5766\n",
      "Epoch 1593/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4715 - val_loss: -4.3615\n",
      "Epoch 1594/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3225 - val_loss: -3.6156\n",
      "Epoch 1595/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4892 - val_loss: -0.6838\n",
      "Epoch 1596/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4426 - val_loss: -4.2909\n",
      "Epoch 1597/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3597 - val_loss: -2.9598\n",
      "Epoch 1598/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4441 - val_loss: -4.5483\n",
      "Epoch 1599/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4887 - val_loss: -3.6355\n",
      "Epoch 1600/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3715 - val_loss: -2.9878\n",
      "Epoch 1601/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4996 - val_loss: -4.1865\n",
      "Epoch 1602/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5238 - val_loss: -2.7956\n",
      "Epoch 1603/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2625 - val_loss: -1.4576\n",
      "Epoch 1604/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2386 - val_loss: -4.3939\n",
      "Epoch 1605/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3835 - val_loss: -3.8569\n",
      "Epoch 1606/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3598 - val_loss: -4.0360\n",
      "Epoch 1607/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4270 - val_loss: -4.7194\n",
      "Epoch 1608/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3377 - val_loss: -4.3811\n",
      "Epoch 1609/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4857 - val_loss: -4.5681\n",
      "Epoch 1610/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4012 - val_loss: -4.4029\n",
      "Epoch 1611/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4469 - val_loss: -3.2547\n",
      "Epoch 1612/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4932 - val_loss: -4.5370\n",
      "Epoch 1613/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4388 - val_loss: -4.4350\n",
      "Epoch 1614/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5106 - val_loss: -1.3970\n",
      "Epoch 1615/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2070 - val_loss: -4.2587\n",
      "Epoch 1616/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4307 - val_loss: -4.3438\n",
      "Epoch 1617/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.3998 - val_loss: -4.7567\n",
      "Epoch 1618/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4712 - val_loss: -3.3494\n",
      "Epoch 1619/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4935 - val_loss: -4.0888\n",
      "Epoch 1620/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4981 - val_loss: -4.9083\n",
      "Epoch 1621/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4434 - val_loss: -4.5065\n",
      "Epoch 1622/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4595 - val_loss: -3.5154\n",
      "Epoch 1623/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4333 - val_loss: -4.4048\n",
      "Epoch 1624/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4380 - val_loss: -4.1530\n",
      "Epoch 1625/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4400 - val_loss: -4.5367\n",
      "Epoch 1626/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4140 - val_loss: 3.3782\n",
      "Epoch 1627/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1209 - val_loss: -4.0130\n",
      "Epoch 1628/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3655 - val_loss: -4.2781\n",
      "Epoch 1629/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4127 - val_loss: -4.3517\n",
      "Epoch 1630/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4536 - val_loss: -3.9719\n",
      "Epoch 1631/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4810 - val_loss: -4.3767\n",
      "Epoch 1632/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4846 - val_loss: -4.1621\n",
      "Epoch 1633/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4133 - val_loss: -4.4430\n",
      "Epoch 1634/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4048 - val_loss: -3.8017\n",
      "Epoch 1635/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5078 - val_loss: -1.3082\n",
      "Epoch 1636/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4292 - val_loss: -4.4061\n",
      "Epoch 1637/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4306 - val_loss: -4.4053\n",
      "Epoch 1638/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4132 - val_loss: -4.6800\n",
      "Epoch 1639/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5077 - val_loss: -4.0777\n",
      "Epoch 1640/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4570 - val_loss: -4.3224\n",
      "Epoch 1641/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4398 - val_loss: -3.3849\n",
      "Epoch 1642/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2343 - val_loss: -3.6645\n",
      "Epoch 1643/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3019 - val_loss: -4.5423\n",
      "Epoch 1644/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3596 - val_loss: -4.6561\n",
      "Epoch 1645/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4384 - val_loss: -4.0174\n",
      "Epoch 1646/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4623 - val_loss: -3.8318\n",
      "Epoch 1647/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4244 - val_loss: -4.2839\n",
      "Epoch 1648/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4338 - val_loss: -4.1426\n",
      "Epoch 1649/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4245 - val_loss: -4.8096\n",
      "Epoch 1650/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3754 - val_loss: -4.4879\n",
      "Epoch 1651/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4146 - val_loss: -3.1431\n",
      "Epoch 1652/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4964 - val_loss: -4.5147\n",
      "Epoch 1653/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3126 - val_loss: -4.2834\n",
      "Epoch 1654/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4906 - val_loss: -4.1766\n",
      "Epoch 1655/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4780 - val_loss: -4.6722\n",
      "Epoch 1656/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4446 - val_loss: -1.5726\n",
      "Epoch 1657/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4631 - val_loss: -4.3284\n",
      "Epoch 1658/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3443 - val_loss: -3.0860\n",
      "Epoch 1659/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3670 - val_loss: -4.5484\n",
      "Epoch 1660/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4329 - val_loss: -4.6031\n",
      "Epoch 1661/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2767 - val_loss: -4.5166\n",
      "Epoch 1662/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4577 - val_loss: -4.3431\n",
      "Epoch 1663/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2122 - val_loss: -4.7258\n",
      "Epoch 1664/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4008 - val_loss: -4.2041\n",
      "Epoch 1665/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4634 - val_loss: -4.4079\n",
      "Epoch 1666/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4046 - val_loss: -4.3398\n",
      "Epoch 1667/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2770 - val_loss: -3.8982\n",
      "Epoch 1668/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4768 - val_loss: -4.2382\n",
      "Epoch 1669/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4079 - val_loss: -2.3526\n",
      "Epoch 1670/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4965 - val_loss: -4.6361\n",
      "Epoch 1671/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3849 - val_loss: -4.7874\n",
      "Epoch 1672/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4268 - val_loss: -3.8274\n",
      "Epoch 1673/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4206 - val_loss: -4.2760\n",
      "Epoch 1674/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3936 - val_loss: -3.1077\n",
      "Epoch 1675/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4281 - val_loss: -3.4217\n",
      "Epoch 1676/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4292 - val_loss: -3.4304\n",
      "Epoch 1677/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5509 - val_loss: -4.2111\n",
      "Epoch 1678/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4307 - val_loss: -4.7921\n",
      "Epoch 1679/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5298 - val_loss: -3.9850\n",
      "Epoch 1680/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3820 - val_loss: -4.2424\n",
      "Epoch 1681/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4620 - val_loss: -4.1658\n",
      "Epoch 1682/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3971 - val_loss: -4.4986\n",
      "Epoch 1683/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4399 - val_loss: -4.1838\n",
      "Epoch 1684/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5117 - val_loss: -3.5297\n",
      "Epoch 1685/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5501 - val_loss: -3.6291\n",
      "Epoch 1686/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4151 - val_loss: -4.1154\n",
      "Epoch 1687/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4665 - val_loss: -4.3139\n",
      "Epoch 1688/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4161 - val_loss: -3.5513\n",
      "Epoch 1689/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4432 - val_loss: -3.4834\n",
      "Epoch 1690/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4761 - val_loss: -4.5453\n",
      "Epoch 1691/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3980 - val_loss: -4.3583\n",
      "Epoch 1692/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5548 - val_loss: -4.3185\n",
      "Epoch 1693/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.4501 - val_loss: -4.1490\n",
      "Epoch 1694/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4895 - val_loss: -3.3898\n",
      "Epoch 1695/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2548 - val_loss: -4.3365\n",
      "Epoch 1696/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4311 - val_loss: -4.5604\n",
      "Epoch 1697/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4188 - val_loss: -4.4669\n",
      "Epoch 1698/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4624 - val_loss: -4.6680\n",
      "Epoch 1699/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2847 - val_loss: -3.3392\n",
      "Epoch 1700/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4868 - val_loss: -4.3585\n",
      "Epoch 1701/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4953 - val_loss: -3.3050\n",
      "Epoch 1702/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4619 - val_loss: -3.6332\n",
      "Epoch 1703/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4371 - val_loss: -4.2118\n",
      "Epoch 1704/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4033 - val_loss: -3.8426\n",
      "Epoch 1705/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4106 - val_loss: -4.6156\n",
      "Epoch 1706/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5112 - val_loss: -3.8977\n",
      "Epoch 1707/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5009 - val_loss: -3.6667\n",
      "Epoch 1708/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4083 - val_loss: -4.4415\n",
      "Epoch 1709/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5221 - val_loss: -4.2188\n",
      "Epoch 1710/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4793 - val_loss: -4.9270\n",
      "Epoch 1711/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4627 - val_loss: -4.2005\n",
      "Epoch 1712/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3549 - val_loss: -4.1470\n",
      "Epoch 1713/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4194 - val_loss: -3.2525\n",
      "Epoch 1714/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4838 - val_loss: -3.0485\n",
      "Epoch 1715/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4756 - val_loss: -4.8620\n",
      "Epoch 1716/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2935 - val_loss: -4.5014\n",
      "Epoch 1717/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4403 - val_loss: -3.6645\n",
      "Epoch 1718/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4721 - val_loss: -3.8848\n",
      "Epoch 1719/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4950 - val_loss: -4.4352\n",
      "Epoch 1720/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4405 - val_loss: -4.4723\n",
      "Epoch 1721/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5489 - val_loss: -4.5403\n",
      "Epoch 1722/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4998 - val_loss: -4.1973\n",
      "Epoch 1723/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5226 - val_loss: -4.3870\n",
      "Epoch 1724/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3952 - val_loss: -3.7368\n",
      "Epoch 1725/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3909 - val_loss: -4.3509\n",
      "Epoch 1726/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4946 - val_loss: -4.0151\n",
      "Epoch 1727/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5049 - val_loss: -4.7106\n",
      "Epoch 1728/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4037 - val_loss: -3.2420\n",
      "Epoch 1729/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4903 - val_loss: -3.7861\n",
      "Epoch 1730/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4563 - val_loss: -4.6369\n",
      "Epoch 1731/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3470 - val_loss: -4.7463\n",
      "Epoch 1732/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5001 - val_loss: -4.2910\n",
      "Epoch 1733/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3747 - val_loss: -3.0163\n",
      "Epoch 1734/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4097 - val_loss: -2.0222\n",
      "Epoch 1735/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4955 - val_loss: -4.0192\n",
      "Epoch 1736/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5024 - val_loss: -3.7464\n",
      "Epoch 1737/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5027 - val_loss: -4.9893\n",
      "Epoch 1738/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4736 - val_loss: -4.4497\n",
      "Epoch 1739/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4627 - val_loss: -4.1639\n",
      "Epoch 1740/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7630 - val_loss: -4.3459\n",
      "Epoch 1741/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2221 - val_loss: -4.2010\n",
      "Epoch 1742/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3418 - val_loss: -4.5254\n",
      "Epoch 1743/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3718 - val_loss: -4.8135\n",
      "Epoch 1744/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3215 - val_loss: -4.1282\n",
      "Epoch 1745/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3383 - val_loss: -4.4841\n",
      "Epoch 1746/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4433 - val_loss: -4.5130\n",
      "Epoch 1747/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4030 - val_loss: -4.5356\n",
      "Epoch 1748/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4334 - val_loss: -4.1835\n",
      "Epoch 1749/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4464 - val_loss: -4.7868\n",
      "Epoch 1750/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5150 - val_loss: -3.1390\n",
      "Epoch 1751/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3997 - val_loss: -4.5873\n",
      "Epoch 1752/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3233 - val_loss: -4.5298\n",
      "Epoch 1753/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5379 - val_loss: -4.2823\n",
      "Epoch 1754/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3677 - val_loss: -4.1245\n",
      "Epoch 1755/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -3.5807 - val_loss: -4.4918\n",
      "Epoch 1756/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7257 - val_loss: -4.5713\n",
      "Epoch 1757/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9439 - val_loss: -4.2886\n",
      "Epoch 1758/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0556 - val_loss: -3.8336\n",
      "Epoch 1759/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0480 - val_loss: -3.7300\n",
      "Epoch 1760/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9975 - val_loss: -2.3101\n",
      "Epoch 1761/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1382 - val_loss: -4.0783\n",
      "Epoch 1762/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1763 - val_loss: -2.9470\n",
      "Epoch 1763/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2147 - val_loss: -3.6060\n",
      "Epoch 1764/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2052 - val_loss: -3.7480\n",
      "Epoch 1765/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6619 - val_loss: -2.8556\n",
      "Epoch 1766/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9200 - val_loss: -3.4220\n",
      "Epoch 1767/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1041 - val_loss: -4.6735\n",
      "Epoch 1768/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1355 - val_loss: -3.9039\n",
      "Epoch 1769/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2749 - val_loss: -2.4309\n",
      "Epoch 1770/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2140 - val_loss: -4.3880\n",
      "Epoch 1771/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2859 - val_loss: -1.6901\n",
      "Epoch 1772/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2035 - val_loss: -2.3474\n",
      "Epoch 1773/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0368 - val_loss: -3.5427\n",
      "Epoch 1774/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1778 - val_loss: -4.4483\n",
      "Epoch 1775/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3293 - val_loss: -4.6807\n",
      "Epoch 1776/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2236 - val_loss: -4.1416\n",
      "Epoch 1777/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2218 - val_loss: -3.4230\n",
      "Epoch 1778/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3243 - val_loss: -4.1978\n",
      "Epoch 1779/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3135 - val_loss: -4.4233\n",
      "Epoch 1780/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5306 - val_loss: -4.0325\n",
      "Epoch 1781/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7718 - val_loss: -4.4837\n",
      "Epoch 1782/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9606 - val_loss: -4.3661\n",
      "Epoch 1783/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1455 - val_loss: -3.8124\n",
      "Epoch 1784/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0333 - val_loss: -3.9867\n",
      "Epoch 1785/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0044 - val_loss: -4.6774\n",
      "Epoch 1786/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3035 - val_loss: -4.3083\n",
      "Epoch 1787/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1771 - val_loss: -4.3656\n",
      "Epoch 1788/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2956 - val_loss: -4.2340\n",
      "Epoch 1789/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2009 - val_loss: -3.3764\n",
      "Epoch 1790/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2800 - val_loss: -3.5953\n",
      "Epoch 1791/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3313 - val_loss: -4.3725\n",
      "Epoch 1792/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3642 - val_loss: -4.0632\n",
      "Epoch 1793/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2770 - val_loss: -4.4972\n",
      "Epoch 1794/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2833 - val_loss: -4.3509\n",
      "Epoch 1795/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2490 - val_loss: -4.4987\n",
      "Epoch 1796/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2853 - val_loss: -4.2530\n",
      "Epoch 1797/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3263 - val_loss: -3.6962\n",
      "Epoch 1798/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3241 - val_loss: -4.4310\n",
      "Epoch 1799/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3697 - val_loss: -2.7604\n",
      "Epoch 1800/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3139 - val_loss: -4.2685\n",
      "Epoch 1801/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3662 - val_loss: -3.3998\n",
      "Epoch 1802/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3398 - val_loss: -4.4032\n",
      "Epoch 1803/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3480 - val_loss: -4.5964\n",
      "Epoch 1804/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3848 - val_loss: -4.1358\n",
      "Epoch 1805/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1950 - val_loss: -4.4206\n",
      "Epoch 1806/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1951 - val_loss: -3.9070\n",
      "Epoch 1807/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1817 - val_loss: -4.4938\n",
      "Epoch 1808/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3863 - val_loss: -4.3010\n",
      "Epoch 1809/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3917 - val_loss: -4.3591\n",
      "Epoch 1810/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4260 - val_loss: -4.4152\n",
      "Epoch 1811/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4387 - val_loss: -4.7021\n",
      "Epoch 1812/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2823 - val_loss: -4.6985\n",
      "Epoch 1813/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4208 - val_loss: -4.5308\n",
      "Epoch 1814/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3799 - val_loss: -4.9329\n",
      "Epoch 1815/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3788 - val_loss: -4.8511\n",
      "Epoch 1816/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3000 - val_loss: -4.1381\n",
      "Epoch 1817/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4064 - val_loss: -4.6714\n",
      "Epoch 1818/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3864 - val_loss: -4.2392\n",
      "Epoch 1819/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3359 - val_loss: -4.0882\n",
      "Epoch 1820/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3635 - val_loss: -4.3934\n",
      "Epoch 1821/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3502 - val_loss: -2.7350\n",
      "Epoch 1822/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3269 - val_loss: -0.5563\n",
      "Epoch 1823/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3276 - val_loss: -4.3146\n",
      "Epoch 1824/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3500 - val_loss: -4.6648\n",
      "Epoch 1825/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4203 - val_loss: -4.8269\n",
      "Epoch 1826/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4557 - val_loss: -4.4264\n",
      "Epoch 1827/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4878 - val_loss: -4.4975\n",
      "Epoch 1828/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4021 - val_loss: -3.3712\n",
      "Epoch 1829/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4332 - val_loss: -4.3008\n",
      "Epoch 1830/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3466 - val_loss: -2.8256\n",
      "Epoch 1831/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4183 - val_loss: -4.5455\n",
      "Epoch 1832/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3836 - val_loss: -3.9239\n",
      "Epoch 1833/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2937 - val_loss: -3.7674\n",
      "Epoch 1834/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4901 - val_loss: -4.8252\n",
      "Epoch 1835/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3655 - val_loss: -4.6031\n",
      "Epoch 1836/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5134 - val_loss: -4.2115\n",
      "Epoch 1837/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3789 - val_loss: 0.2702\n",
      "Epoch 1838/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3843 - val_loss: -2.2216\n",
      "Epoch 1839/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4307 - val_loss: -4.3635\n",
      "Epoch 1840/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1101 - val_loss: -4.3581\n",
      "Epoch 1841/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3439 - val_loss: -3.5481\n",
      "Epoch 1842/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3581 - val_loss: -4.1607\n",
      "Epoch 1843/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4386 - val_loss: -1.8438\n",
      "Epoch 1844/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3998 - val_loss: -4.2124\n",
      "Epoch 1845/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4952 - val_loss: -4.5213\n",
      "Epoch 1846/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2668 - val_loss: -4.6235\n",
      "Epoch 1847/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4894 - val_loss: -4.6936\n",
      "Epoch 1848/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4207 - val_loss: -4.1104\n",
      "Epoch 1849/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2899 - val_loss: -3.8933\n",
      "Epoch 1850/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4362 - val_loss: 0.3747\n",
      "Epoch 1851/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2085 - val_loss: -4.0387\n",
      "Epoch 1852/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5057 - val_loss: -3.6642\n",
      "Epoch 1853/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4768 - val_loss: -4.7822\n",
      "Epoch 1854/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5229 - val_loss: -4.0119\n",
      "Epoch 1855/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4974 - val_loss: -2.1948\n",
      "Epoch 1856/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3303 - val_loss: -4.5489\n",
      "Epoch 1857/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4064 - val_loss: -3.1107\n",
      "Epoch 1858/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4950 - val_loss: -4.3737\n",
      "Epoch 1859/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2929 - val_loss: -4.5931\n",
      "Epoch 1860/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4653 - val_loss: -4.2677\n",
      "Epoch 1861/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3669 - val_loss: -4.3873\n",
      "Epoch 1862/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4426 - val_loss: -4.8142\n",
      "Epoch 1863/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5325 - val_loss: -3.1448\n",
      "Epoch 1864/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4865 - val_loss: -4.3799\n",
      "Epoch 1865/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3617 - val_loss: -4.5850\n",
      "Epoch 1866/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4573 - val_loss: -1.8184\n",
      "Epoch 1867/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0242 - val_loss: -4.6894\n",
      "Epoch 1868/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4463 - val_loss: -1.8326\n",
      "Epoch 1869/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4161 - val_loss: -3.7442\n",
      "Epoch 1870/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3363 - val_loss: -4.0416\n",
      "Epoch 1871/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5148 - val_loss: -4.3263\n",
      "Epoch 1872/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0877 - val_loss: -3.6331\n",
      "Epoch 1873/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5674 - val_loss: -4.5438\n",
      "Epoch 1874/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1457 - val_loss: -4.4692\n",
      "Epoch 1875/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2435 - val_loss: -4.3037\n",
      "Epoch 1876/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2757 - val_loss: -3.7672\n",
      "Epoch 1877/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3352 - val_loss: -3.4221\n",
      "Epoch 1878/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3540 - val_loss: -4.5209\n",
      "Epoch 1879/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3642 - val_loss: -4.7125\n",
      "Epoch 1880/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4022 - val_loss: -3.6467\n",
      "Epoch 1881/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4018 - val_loss: -4.5670\n",
      "Epoch 1882/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3560 - val_loss: -4.1367\n",
      "Epoch 1883/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4276 - val_loss: -4.6217\n",
      "Epoch 1884/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3115 - val_loss: -3.0564\n",
      "Epoch 1885/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4009 - val_loss: -3.6419\n",
      "Epoch 1886/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4784 - val_loss: -4.1592\n",
      "Epoch 1887/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4294 - val_loss: -4.6982\n",
      "Epoch 1888/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4271 - val_loss: -4.5352\n",
      "Epoch 1889/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4807 - val_loss: -4.6522\n",
      "Epoch 1890/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4982 - val_loss: -4.8954\n",
      "Epoch 1891/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4813 - val_loss: -3.9390\n",
      "Epoch 1892/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3439 - val_loss: -4.5866\n",
      "Epoch 1893/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4781 - val_loss: -3.4826\n",
      "Epoch 1894/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4840 - val_loss: -4.4091\n",
      "Epoch 1895/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3957 - val_loss: -3.1405\n",
      "Epoch 1896/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3938 - val_loss: -4.1195\n",
      "Epoch 1897/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5066 - val_loss: -3.9117\n",
      "Epoch 1898/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4937 - val_loss: -4.2310\n",
      "Epoch 1899/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4607 - val_loss: -3.3129\n",
      "Epoch 1900/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4758 - val_loss: -4.5033\n",
      "Epoch 1901/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4911 - val_loss: -4.7494\n",
      "Epoch 1902/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4844 - val_loss: -4.0308\n",
      "Epoch 1903/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5144 - val_loss: -4.2083\n",
      "Epoch 1904/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4861 - val_loss: -4.0150\n",
      "Epoch 1905/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3636 - val_loss: -4.6327\n",
      "Epoch 1906/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4529 - val_loss: -3.8821\n",
      "Epoch 1907/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4855 - val_loss: -3.5051\n",
      "Epoch 1908/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4951 - val_loss: -4.2457\n",
      "Epoch 1909/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4313 - val_loss: -4.6363\n",
      "Epoch 1910/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4208 - val_loss: -4.0662\n",
      "Epoch 1911/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4985 - val_loss: -4.0369\n",
      "Epoch 1912/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3583 - val_loss: -4.4283\n",
      "Epoch 1913/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3393 - val_loss: -4.2670\n",
      "Epoch 1914/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.5124 - val_loss: -4.5117\n",
      "Epoch 1915/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5232 - val_loss: -4.4361\n",
      "Epoch 1916/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4234 - val_loss: -3.5251\n",
      "Epoch 1917/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3497 - val_loss: -4.3830\n",
      "Epoch 1918/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7924 - val_loss: -3.8438\n",
      "Epoch 1919/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9349 - val_loss: -3.5858\n",
      "Epoch 1920/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9909 - val_loss: -4.4015\n",
      "Epoch 1921/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0398 - val_loss: -4.4320\n",
      "Epoch 1922/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0596 - val_loss: -3.2964\n",
      "Epoch 1923/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0599 - val_loss: -2.9724\n",
      "Epoch 1924/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1202 - val_loss: -4.4314\n",
      "Epoch 1925/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1263 - val_loss: -4.1660\n",
      "Epoch 1926/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1498 - val_loss: -2.8083\n",
      "Epoch 1927/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0693 - val_loss: -4.5299\n",
      "Epoch 1928/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1924 - val_loss: -4.6101\n",
      "Epoch 1929/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1417 - val_loss: -4.0767\n",
      "Epoch 1930/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1849 - val_loss: -3.0572\n",
      "Epoch 1931/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2079 - val_loss: -2.9445\n",
      "Epoch 1932/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1567 - val_loss: -3.5103\n",
      "Epoch 1933/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2029 - val_loss: -3.6960\n",
      "Epoch 1934/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1363 - val_loss: -4.0678\n",
      "Epoch 1935/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1700 - val_loss: -4.6889\n",
      "Epoch 1936/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2320 - val_loss: -4.1592\n",
      "Epoch 1937/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1950 - val_loss: -3.1439\n",
      "Epoch 1938/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1898 - val_loss: -0.4103\n",
      "Epoch 1939/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1705 - val_loss: -4.6320\n",
      "Epoch 1940/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2391 - val_loss: -4.2331\n",
      "Epoch 1941/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2330 - val_loss: -3.9681\n",
      "Epoch 1942/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2169 - val_loss: -4.6793\n",
      "Epoch 1943/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3062 - val_loss: -4.1533\n",
      "Epoch 1944/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2642 - val_loss: -3.7955\n",
      "Epoch 1945/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2151 - val_loss: -4.1225\n",
      "Epoch 1946/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2880 - val_loss: -4.4614\n",
      "Epoch 1947/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2371 - val_loss: -3.2547\n",
      "Epoch 1948/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2060 - val_loss: -2.9017\n",
      "Epoch 1949/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2587 - val_loss: -4.4437\n",
      "Epoch 1950/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2575 - val_loss: -3.9207\n",
      "Epoch 1951/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3168 - val_loss: -4.4553\n",
      "Epoch 1952/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2406 - val_loss: -4.4859\n",
      "Epoch 1953/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2375 - val_loss: -4.7408\n",
      "Epoch 1954/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3146 - val_loss: -4.0585\n",
      "Epoch 1955/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2476 - val_loss: -4.1034\n",
      "Epoch 1956/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2201 - val_loss: -4.3454\n",
      "Epoch 1957/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3428 - val_loss: -4.0999\n",
      "Epoch 1958/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3193 - val_loss: -4.2346\n",
      "Epoch 1959/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3261 - val_loss: -4.5179\n",
      "Epoch 1960/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2467 - val_loss: -4.6817\n",
      "Epoch 1961/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3253 - val_loss: -3.9381\n",
      "Epoch 1962/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2844 - val_loss: -4.7068\n",
      "Epoch 1963/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3227 - val_loss: -4.0222\n",
      "Epoch 1964/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2921 - val_loss: -3.7425\n",
      "Epoch 1965/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3343 - val_loss: -4.5084\n",
      "Epoch 1966/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3996 - val_loss: -4.5240\n",
      "Epoch 1967/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2662 - val_loss: -4.8148\n",
      "Epoch 1968/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3286 - val_loss: -4.7518\n",
      "Epoch 1969/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3820 - val_loss: -4.4511\n",
      "Epoch 1970/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3088 - val_loss: -4.1152\n",
      "Epoch 1971/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3304 - val_loss: -4.2721\n",
      "Epoch 1972/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3095 - val_loss: -4.6245\n",
      "Epoch 1973/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4008 - val_loss: -4.4678\n",
      "Epoch 1974/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3051 - val_loss: -3.9541\n",
      "Epoch 1975/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2813 - val_loss: -4.4671\n",
      "Epoch 1976/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3517 - val_loss: -4.6844\n",
      "Epoch 1977/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3553 - val_loss: -2.4717\n",
      "Epoch 1978/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1286 - val_loss: -4.4221\n",
      "Epoch 1979/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3400 - val_loss: -4.4982\n",
      "Epoch 1980/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3696 - val_loss: -3.6332\n",
      "Epoch 1981/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3819 - val_loss: -4.7249\n",
      "Epoch 1982/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3368 - val_loss: -4.7188\n",
      "Epoch 1983/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3549 - val_loss: -4.0984\n",
      "Epoch 1984/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2975 - val_loss: -4.2907\n",
      "Epoch 1985/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4113 - val_loss: -4.2019\n",
      "Epoch 1986/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3577 - val_loss: -4.0405\n",
      "Epoch 1987/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3163 - val_loss: -2.3840\n",
      "Epoch 1988/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2703 - val_loss: -4.1517\n",
      "Epoch 1989/2000\n",
      "118/118 [==============================] - 22s 189ms/step - loss: -5.4181 - val_loss: -4.5959\n",
      "Epoch 1990/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2508 - val_loss: -4.4952\n",
      "Epoch 1991/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4429 - val_loss: -4.2411\n",
      "Epoch 1992/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3322 - val_loss: -4.3753\n",
      "Epoch 1993/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3912 - val_loss: -3.6252\n",
      "Epoch 1994/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2927 - val_loss: -4.7583\n",
      "Epoch 1995/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3393 - val_loss: -3.9019\n",
      "Epoch 1996/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2532 - val_loss: -4.4105\n",
      "Epoch 1997/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3932 - val_loss: -3.2704\n",
      "Epoch 1998/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3767 - val_loss: -4.2698\n",
      "Epoch 1999/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3939 - val_loss: -4.2684\n",
      "Epoch 2000/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3257 - val_loss: -3.8072\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 6s 37ms/step - loss: -5.0791\n",
      "WARNING:tensorflow:Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 6s 36ms/step - loss: -5.3411\n",
      "WARNING:tensorflow:Layer lstm_89 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_89 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8881"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 33s 252ms/step - loss: -0.8881 - val_loss: 0.3461\n",
      "Epoch 2/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2498 - val_loss: 0.4381\n",
      "Epoch 3/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.2533 - val_loss: 0.5666\n",
      "Epoch 4/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -1.2558 - val_loss: 0.6400\n",
      "Epoch 5/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2581 - val_loss: 0.4645\n",
      "Epoch 6/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.2597 - val_loss: 0.4965\n",
      "Epoch 7/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.2555 - val_loss: 0.3937\n",
      "Epoch 8/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -1.2595 - val_loss: 0.4896\n",
      "Epoch 9/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.4331"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -1.4331 - val_loss: -0.7150\n",
      "Epoch 10/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.4162"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -2.4162 - val_loss: -2.1447\n",
      "Epoch 11/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.6287"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -2.6287 - val_loss: -2.5469\n",
      "Epoch 12/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.1303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -3.1303 - val_loss: -3.0594\n",
      "Epoch 13/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.4293 - val_loss: -2.8069\n",
      "Epoch 14/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.5753"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -3.5753 - val_loss: -3.4220\n",
      "Epoch 15/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.6599 - val_loss: -3.3913\n",
      "Epoch 16/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.7994 - val_loss: -2.9948\n",
      "Epoch 17/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.8860 - val_loss: -3.2149\n",
      "Epoch 18/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.8929"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -3.8929 - val_loss: -3.5116\n",
      "Epoch 19/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.9937"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -3.9937 - val_loss: -3.5948\n",
      "Epoch 20/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.8923"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -3.8923 - val_loss: -3.6457\n",
      "Epoch 21/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.0406 - val_loss: -3.1571\n",
      "Epoch 22/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.0391 - val_loss: -3.5917\n",
      "Epoch 23/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9851 - val_loss: -3.6388\n",
      "Epoch 24/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.1200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -4.1200 - val_loss: -3.7799\n",
      "Epoch 25/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.1001 - val_loss: -3.1981\n",
      "Epoch 26/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.1276"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -4.1276 - val_loss: -3.7933\n",
      "Epoch 27/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1195 - val_loss: -3.6074\n",
      "Epoch 28/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2168 - val_loss: -3.5005\n",
      "Epoch 29/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1114 - val_loss: -3.3615\n",
      "Epoch 30/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1748 - val_loss: -3.0357\n",
      "Epoch 31/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2674 - val_loss: -3.4940\n",
      "Epoch 32/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1131 - val_loss: -2.0807\n",
      "Epoch 33/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1917 - val_loss: -3.4478\n",
      "Epoch 34/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2816 - val_loss: -3.5449\n",
      "Epoch 35/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2122 - val_loss: -3.7170\n",
      "Epoch 36/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2331 - val_loss: -3.7841\n",
      "Epoch 37/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3179 - val_loss: -3.1479\n",
      "Epoch 38/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2041 - val_loss: -3.5292\n",
      "Epoch 39/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.3114"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -4.3114 - val_loss: -3.8958\n",
      "Epoch 40/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2808 - val_loss: -3.4855\n",
      "Epoch 41/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3061 - val_loss: -3.5283\n",
      "Epoch 42/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2918 - val_loss: -3.8819\n",
      "Epoch 43/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2864 - val_loss: -2.3923\n",
      "Epoch 44/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3465 - val_loss: -0.9658\n",
      "Epoch 45/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.2420"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -4.2420 - val_loss: -3.9116\n",
      "Epoch 46/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3764 - val_loss: -3.2622\n",
      "Epoch 47/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2437 - val_loss: -3.5628\n",
      "Epoch 48/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3029 - val_loss: -3.6085\n",
      "Epoch 49/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4494"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: -4.4494 - val_loss: -3.9389\n",
      "Epoch 50/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3080 - val_loss: -3.8547\n",
      "Epoch 51/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4198"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -4.4198 - val_loss: -3.9817\n",
      "Epoch 52/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4332"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -4.4332 - val_loss: -4.0477\n",
      "Epoch 53/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8108 - val_loss: -3.1786\n",
      "Epoch 54/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1759 - val_loss: -3.6689\n",
      "Epoch 55/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2918 - val_loss: -3.5975\n",
      "Epoch 56/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3493 - val_loss: -3.3225\n",
      "Epoch 57/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3313 - val_loss: -3.7820\n",
      "Epoch 58/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3428 - val_loss: -3.8596\n",
      "Epoch 59/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3243 - val_loss: -3.7609\n",
      "Epoch 60/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4063 - val_loss: -3.7268\n",
      "Epoch 61/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2826 - val_loss: -3.8415\n",
      "Epoch 62/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4588 - val_loss: -3.7028\n",
      "Epoch 63/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3265 - val_loss: -3.6906\n",
      "Epoch 64/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4340 - val_loss: -3.1591\n",
      "Epoch 65/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4442 - val_loss: -3.7176\n",
      "Epoch 66/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4551 - val_loss: -3.7893\n",
      "Epoch 67/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2218 - val_loss: -3.6109\n",
      "Epoch 68/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4944 - val_loss: -3.5896\n",
      "Epoch 69/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5374 - val_loss: 0.7911\n",
      "Epoch 70/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4629 - val_loss: 1.6677\n",
      "Epoch 71/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4621 - val_loss: 2.1855\n",
      "Epoch 72/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3561 - val_loss: -3.7433\n",
      "Epoch 73/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5263 - val_loss: -3.2996\n",
      "Epoch 74/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4129 - val_loss: -3.9611\n",
      "Epoch 75/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5353 - val_loss: -3.8644\n",
      "Epoch 76/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0230 - val_loss: -3.9405\n",
      "Epoch 77/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3089 - val_loss: -3.8665\n",
      "Epoch 78/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4971 - val_loss: -3.8862\n",
      "Epoch 79/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3185 - val_loss: -3.3625\n",
      "Epoch 80/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5235 - val_loss: -3.3835\n",
      "Epoch 81/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5531 - val_loss: -3.4743\n",
      "Epoch 82/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5595 - val_loss: -2.6252\n",
      "Epoch 83/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4454 - val_loss: -3.7296\n",
      "Epoch 84/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5377 - val_loss: -3.9928\n",
      "Epoch 85/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5520 - val_loss: -3.9384\n",
      "Epoch 86/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5939 - val_loss: -3.5835\n",
      "Epoch 87/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2592 - val_loss: -3.6107\n",
      "Epoch 88/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4782 - val_loss: -3.7799\n",
      "Epoch 89/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5803 - val_loss: -3.9396\n",
      "Epoch 90/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5960 - val_loss: -3.9994\n",
      "Epoch 91/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3453 - val_loss: -3.9627\n",
      "Epoch 92/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4495 - val_loss: -3.2093\n",
      "Epoch 93/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6110 - val_loss: -3.9096\n",
      "Epoch 94/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6078 - val_loss: -4.0077\n",
      "Epoch 95/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6026 - val_loss: -3.8448\n",
      "Epoch 96/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6179 - val_loss: -3.6801\n",
      "Epoch 97/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4323 - val_loss: -3.2879\n",
      "Epoch 98/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5823 - val_loss: -2.6741\n",
      "Epoch 99/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6663 - val_loss: -3.9107\n",
      "Epoch 100/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3150 - val_loss: -3.9233\n",
      "Epoch 101/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5859"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -4.5859 - val_loss: -4.0543\n",
      "Epoch 102/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6311 - val_loss: -3.7263\n",
      "Epoch 103/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5800 - val_loss: -3.5708\n",
      "Epoch 104/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6675 - val_loss: -3.6370\n",
      "Epoch 105/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6155"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -4.6155 - val_loss: -4.1150\n",
      "Epoch 106/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6626 - val_loss: -4.0025\n",
      "Epoch 107/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6075"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -4.6075 - val_loss: -4.1435\n",
      "Epoch 108/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5808 - val_loss: -3.8835\n",
      "Epoch 109/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6419 - val_loss: -4.0194\n",
      "Epoch 110/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6144"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -4.6144 - val_loss: -4.1930\n",
      "Epoch 111/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6844 - val_loss: -3.3931\n",
      "Epoch 112/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5411 - val_loss: -2.7004\n",
      "Epoch 113/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5591 - val_loss: -3.9374\n",
      "Epoch 114/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7396 - val_loss: -4.1007\n",
      "Epoch 115/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7411 - val_loss: -2.5581\n",
      "Epoch 116/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6329 - val_loss: -3.8282\n",
      "Epoch 117/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6845 - val_loss: -3.7638\n",
      "Epoch 118/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7235 - val_loss: -4.0156\n",
      "Epoch 119/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7011 - val_loss: -2.2014\n",
      "Epoch 120/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6481 - val_loss: -4.1548\n",
      "Epoch 121/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5429 - val_loss: -3.9627\n",
      "Epoch 122/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5880 - val_loss: -3.8100\n",
      "Epoch 123/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7987 - val_loss: -3.9464\n",
      "Epoch 124/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6516 - val_loss: -3.5156\n",
      "Epoch 125/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7470 - val_loss: -4.0531\n",
      "Epoch 126/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6796"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -4.6796 - val_loss: -4.2301\n",
      "Epoch 127/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2353 - val_loss: -3.8833\n",
      "Epoch 128/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6304 - val_loss: -4.0539\n",
      "Epoch 129/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5592 - val_loss: -3.4738\n",
      "Epoch 130/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7385 - val_loss: -4.1180\n",
      "Epoch 131/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6344 - val_loss: -4.1062\n",
      "Epoch 132/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6097 - val_loss: -4.0180\n",
      "Epoch 133/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7634 - val_loss: -1.6609\n",
      "Epoch 134/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6677 - val_loss: -3.8994\n",
      "Epoch 135/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6344 - val_loss: -2.3189\n",
      "Epoch 136/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6907 - val_loss: -3.6309\n",
      "Epoch 137/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6716 - val_loss: -3.7760\n",
      "Epoch 138/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7907 - val_loss: -3.9550\n",
      "Epoch 139/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6679 - val_loss: -3.8743\n",
      "Epoch 140/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7164 - val_loss: -3.6157\n",
      "Epoch 141/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7760 - val_loss: -4.1296\n",
      "Epoch 142/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6118 - val_loss: -3.7940\n",
      "Epoch 143/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7288 - val_loss: -4.0456\n",
      "Epoch 144/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7520 - val_loss: -4.1474\n",
      "Epoch 145/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.3491 - val_loss: -2.5256\n",
      "Epoch 146/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8842 - val_loss: -3.5520\n",
      "Epoch 147/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1413 - val_loss: -3.4739\n",
      "Epoch 148/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3503 - val_loss: -3.5685\n",
      "Epoch 149/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4254 - val_loss: -3.4919\n",
      "Epoch 150/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4476 - val_loss: -3.9374\n",
      "Epoch 151/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5561 - val_loss: -3.9022\n",
      "Epoch 152/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4815 - val_loss: -1.8608\n",
      "Epoch 153/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5619 - val_loss: -3.5176\n",
      "Epoch 154/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6188 - val_loss: -4.0863\n",
      "Epoch 155/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5808 - val_loss: -3.5199\n",
      "Epoch 156/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4335 - val_loss: -3.9834\n",
      "Epoch 157/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6622 - val_loss: -4.0418\n",
      "Epoch 158/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6765 - val_loss: -3.9333\n",
      "Epoch 159/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5565 - val_loss: -4.0461\n",
      "Epoch 160/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7014 - val_loss: -3.6828\n",
      "Epoch 161/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6413 - val_loss: -3.4132\n",
      "Epoch 162/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6458 - val_loss: -4.0664\n",
      "Epoch 163/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6817 - val_loss: -4.2154\n",
      "Epoch 164/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6309 - val_loss: -4.0878\n",
      "Epoch 165/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3393 - val_loss: -3.1055\n",
      "Epoch 166/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7332 - val_loss: -4.1873\n",
      "Epoch 167/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6324 - val_loss: -3.1111\n",
      "Epoch 168/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6281 - val_loss: -3.7409\n",
      "Epoch 169/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7784 - val_loss: -4.0049\n",
      "Epoch 170/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6482 - val_loss: -4.0964\n",
      "Epoch 171/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6510 - val_loss: -4.0156\n",
      "Epoch 172/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7256 - val_loss: -3.9477\n",
      "Epoch 173/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5750 - val_loss: -0.6967\n",
      "Epoch 174/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6934 - val_loss: -2.4301\n",
      "Epoch 175/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5992 - val_loss: -3.1268\n",
      "Epoch 176/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7964 - val_loss: -3.6686\n",
      "Epoch 177/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6181 - val_loss: -2.9137\n",
      "Epoch 178/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2802 - val_loss: -3.5716\n",
      "Epoch 179/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8018 - val_loss: -4.0397\n",
      "Epoch 180/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7458 - val_loss: -4.0990\n",
      "Epoch 181/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7365 - val_loss: -2.1873\n",
      "Epoch 182/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5876 - val_loss: -3.9299\n",
      "Epoch 183/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2480 - val_loss: -3.6376\n",
      "Epoch 184/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6532 - val_loss: -3.0356\n",
      "Epoch 185/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7401 - val_loss: -3.5965\n",
      "Epoch 186/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5128 - val_loss: -4.0319\n",
      "Epoch 187/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7362 - val_loss: -2.5832\n",
      "Epoch 188/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6756 - val_loss: -3.8292\n",
      "Epoch 189/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6350 - val_loss: -3.7329\n",
      "Epoch 190/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7648 - val_loss: -3.9927\n",
      "Epoch 191/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7753 - val_loss: -3.9653\n",
      "Epoch 192/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8487 - val_loss: -3.6723\n",
      "Epoch 193/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6788 - val_loss: -4.1186\n",
      "Epoch 194/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7805 - val_loss: -4.1747\n",
      "Epoch 195/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7867 - val_loss: -3.9073\n",
      "Epoch 196/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4814 - val_loss: -3.1862\n",
      "Epoch 197/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6002"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -4.6002 - val_loss: -4.2564\n",
      "Epoch 198/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8339 - val_loss: -4.0359\n",
      "Epoch 199/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6506 - val_loss: -4.0227\n",
      "Epoch 200/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7468 - val_loss: -4.0112\n",
      "Epoch 201/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7843 - val_loss: -2.9585\n",
      "Epoch 202/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7608"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 242ms/step - loss: -4.7608 - val_loss: -4.2625\n",
      "Epoch 203/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5682 - val_loss: -4.1866\n",
      "Epoch 204/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8767 - val_loss: -3.8866\n",
      "Epoch 205/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5984 - val_loss: -4.1856\n",
      "Epoch 206/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8170 - val_loss: -3.7598\n",
      "Epoch 207/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7619 - val_loss: -4.0945\n",
      "Epoch 208/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7664"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -4.7664 - val_loss: -4.2829\n",
      "Epoch 209/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8335 - val_loss: -3.0679\n",
      "Epoch 210/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7448 - val_loss: -3.9004\n",
      "Epoch 211/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5015 - val_loss: -3.7100\n",
      "Epoch 212/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7593 - val_loss: -3.7994\n",
      "Epoch 213/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7355 - val_loss: -3.2090\n",
      "Epoch 214/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7157 - val_loss: -4.0902\n",
      "Epoch 215/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8157 - val_loss: -3.4663\n",
      "Epoch 216/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8205 - val_loss: -2.7665\n",
      "Epoch 217/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6970 - val_loss: -4.1613\n",
      "Epoch 218/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7933 - val_loss: -4.2211\n",
      "Epoch 219/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7601 - val_loss: -4.1584\n",
      "Epoch 220/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8175 - val_loss: -4.0548\n",
      "Epoch 221/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8127 - val_loss: -4.1790\n",
      "Epoch 222/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8429"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -4.8429 - val_loss: -4.3050\n",
      "Epoch 223/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8364 - val_loss: -3.6190\n",
      "Epoch 224/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7471 - val_loss: -4.0136\n",
      "Epoch 225/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7203 - val_loss: -3.9669\n",
      "Epoch 226/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8525 - val_loss: -4.0955\n",
      "Epoch 227/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8592 - val_loss: -3.7127\n",
      "Epoch 228/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6100 - val_loss: -4.1136\n",
      "Epoch 229/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8509 - val_loss: -4.1489\n",
      "Epoch 230/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8919 - val_loss: -4.1870\n",
      "Epoch 231/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8028 - val_loss: -3.3319\n",
      "Epoch 232/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8786 - val_loss: -3.6031\n",
      "Epoch 233/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7736 - val_loss: -0.2653\n",
      "Epoch 234/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7368 - val_loss: -2.0580\n",
      "Epoch 235/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6784 - val_loss: 5.0937\n",
      "Epoch 236/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8206 - val_loss: -3.5457\n",
      "Epoch 237/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8705 - val_loss: -4.1685\n",
      "Epoch 238/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7265 - val_loss: -3.1239\n",
      "Epoch 239/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7342 - val_loss: -4.2362\n",
      "Epoch 240/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8934 - val_loss: -4.2155\n",
      "Epoch 241/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8935 - val_loss: -3.8208\n",
      "Epoch 242/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8997 - val_loss: -4.1483\n",
      "Epoch 243/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8258 - val_loss: -4.2539\n",
      "Epoch 244/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4488 - val_loss: -3.7384\n",
      "Epoch 245/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8213 - val_loss: -1.7056\n",
      "Epoch 246/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8121 - val_loss: -3.0544\n",
      "Epoch 247/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7917 - val_loss: -3.5062\n",
      "Epoch 248/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5697 - val_loss: -3.9385\n",
      "Epoch 249/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9074 - val_loss: -4.1912\n",
      "Epoch 250/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9421 - val_loss: -3.8044\n",
      "Epoch 251/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8526 - val_loss: -4.0460\n",
      "Epoch 252/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7437 - val_loss: -4.0887\n",
      "Epoch 253/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6723 - val_loss: -4.2698\n",
      "Epoch 254/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8528 - val_loss: -4.0457\n",
      "Epoch 255/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8921 - val_loss: -3.8953\n",
      "Epoch 256/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.7781 - val_loss: -4.3465\n",
      "Epoch 257/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9559 - val_loss: -2.5735\n",
      "Epoch 258/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8365 - val_loss: -3.5802\n",
      "Epoch 259/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3932 - val_loss: -3.7014\n",
      "Epoch 260/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6023 - val_loss: -3.9940\n",
      "Epoch 261/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6345 - val_loss: -3.9985\n",
      "Epoch 262/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6738 - val_loss: -3.9799\n",
      "Epoch 263/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7527 - val_loss: -3.0250\n",
      "Epoch 264/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6277 - val_loss: -3.7154\n",
      "Epoch 265/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6156 - val_loss: -4.1485\n",
      "Epoch 266/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7896 - val_loss: -4.0500\n",
      "Epoch 267/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7547 - val_loss: -4.0140\n",
      "Epoch 268/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6536 - val_loss: -4.0724\n",
      "Epoch 269/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8251 - val_loss: -3.3753\n",
      "Epoch 270/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7277 - val_loss: -3.8321\n",
      "Epoch 271/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7869 - val_loss: -3.9015\n",
      "Epoch 272/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7728 - val_loss: -4.0026\n",
      "Epoch 273/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6556 - val_loss: -3.9035\n",
      "Epoch 274/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7719 - val_loss: -3.3974\n",
      "Epoch 275/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7480 - val_loss: -4.0969\n",
      "Epoch 276/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8657 - val_loss: -4.1525\n",
      "Epoch 277/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7959 - val_loss: -4.1436\n",
      "Epoch 278/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8616 - val_loss: -4.0791\n",
      "Epoch 279/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8281 - val_loss: -4.2651\n",
      "Epoch 280/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7533 - val_loss: -4.2690\n",
      "Epoch 281/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9449 - val_loss: -3.5544\n",
      "Epoch 282/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7352 - val_loss: -4.0952\n",
      "Epoch 283/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8170 - val_loss: -2.8349\n",
      "Epoch 284/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8270 - val_loss: -4.1431\n",
      "Epoch 285/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7619 - val_loss: -3.3183\n",
      "Epoch 286/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9124 - val_loss: -3.7225\n",
      "Epoch 287/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8959 - val_loss: -4.3105\n",
      "Epoch 288/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9123 - val_loss: -4.2566\n",
      "Epoch 289/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9419 - val_loss: -3.6848\n",
      "Epoch 290/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8315 - val_loss: -3.2415\n",
      "Epoch 291/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9465 - val_loss: -4.0325\n",
      "Epoch 292/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4165 - val_loss: -3.7068\n",
      "Epoch 293/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7155 - val_loss: -3.9978\n",
      "Epoch 294/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8433 - val_loss: -2.6011\n",
      "Epoch 295/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8899 - val_loss: -4.1252\n",
      "Epoch 296/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7807 - val_loss: -4.0867\n",
      "Epoch 297/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9512 - val_loss: -4.0995\n",
      "Epoch 298/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9454 - val_loss: -3.5709\n",
      "Epoch 299/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8698 - val_loss: -3.9100\n",
      "Epoch 300/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9297 - val_loss: -3.9047\n",
      "Epoch 301/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3806 - val_loss: -3.8865\n",
      "Epoch 302/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8554 - val_loss: -4.0097\n",
      "Epoch 303/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7803 - val_loss: -3.1829\n",
      "Epoch 304/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6854 - val_loss: -3.8687\n",
      "Epoch 305/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8922 - val_loss: -4.1366\n",
      "Epoch 306/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8000 - val_loss: -4.1138\n",
      "Epoch 307/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8959 - val_loss: -4.2289\n",
      "Epoch 308/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8756 - val_loss: -3.8136\n",
      "Epoch 309/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9478 - val_loss: -3.9126\n",
      "Epoch 310/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9038 - val_loss: -3.8896\n",
      "Epoch 311/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8050 - val_loss: -3.7775\n",
      "Epoch 312/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8808 - val_loss: -1.6358\n",
      "Epoch 313/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9549 - val_loss: -3.7737\n",
      "Epoch 314/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9088 - val_loss: -3.8461\n",
      "Epoch 315/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8936 - val_loss: -3.7820\n",
      "Epoch 316/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0002 - val_loss: -2.4036\n",
      "Epoch 317/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8797 - val_loss: -3.8356\n",
      "Epoch 318/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0012 - val_loss: -3.5270\n",
      "Epoch 319/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9871 - val_loss: -3.5014\n",
      "Epoch 320/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8705 - val_loss: -4.1933\n",
      "Epoch 321/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8326 - val_loss: -2.9283\n",
      "Epoch 322/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9061 - val_loss: -4.2691\n",
      "Epoch 323/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0162 - val_loss: -3.9126\n",
      "Epoch 324/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5401 - val_loss: -3.2105\n",
      "Epoch 325/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6007 - val_loss: -3.9243\n",
      "Epoch 326/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2741 - val_loss: -3.7430\n",
      "Epoch 327/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7823 - val_loss: -4.1788\n",
      "Epoch 328/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9605 - val_loss: -3.9971\n",
      "Epoch 329/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9323 - val_loss: -4.2213\n",
      "Epoch 330/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9266 - val_loss: -3.6558\n",
      "Epoch 331/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3860 - val_loss: -3.7490\n",
      "Epoch 332/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9785 - val_loss: -3.6430\n",
      "Epoch 333/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8709 - val_loss: -4.0155\n",
      "Epoch 334/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0127 - val_loss: -4.0439\n",
      "Epoch 335/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7915 - val_loss: -3.8255\n",
      "Epoch 336/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8923 - val_loss: -3.7355\n",
      "Epoch 337/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7736 - val_loss: -4.1388\n",
      "Epoch 338/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9034 - val_loss: -4.1547\n",
      "Epoch 339/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9824 - val_loss: -4.0874\n",
      "Epoch 340/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8600 - val_loss: -3.1677\n",
      "Epoch 341/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0212 - val_loss: -4.1130\n",
      "Epoch 342/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9499 - val_loss: -4.1111\n",
      "Epoch 343/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7464 - val_loss: -4.1344\n",
      "Epoch 344/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9126 - val_loss: -2.5433\n",
      "Epoch 345/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8689 - val_loss: -4.2122\n",
      "Epoch 346/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9182 - val_loss: -3.9744\n",
      "Epoch 347/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9487 - val_loss: -4.0516\n",
      "Epoch 348/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0012 - val_loss: -4.3399\n",
      "Epoch 349/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9391 - val_loss: -4.0153\n",
      "Epoch 350/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9834 - val_loss: -4.0933\n",
      "Epoch 351/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7998 - val_loss: -4.0987\n",
      "Epoch 352/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0481 - val_loss: -3.2364\n",
      "Epoch 353/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4453 - val_loss: -3.6569\n",
      "Epoch 354/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8617 - val_loss: -4.2760\n",
      "Epoch 355/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0018 - val_loss: -1.6004\n",
      "Epoch 356/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.3566 - val_loss: -3.6266\n",
      "Epoch 357/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2820 - val_loss: -3.5979\n",
      "Epoch 358/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5274 - val_loss: -3.1237\n",
      "Epoch 359/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6302 - val_loss: -3.9043\n",
      "Epoch 360/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7406 - val_loss: -3.3248\n",
      "Epoch 361/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6652 - val_loss: -3.9641\n",
      "Epoch 362/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7578 - val_loss: -3.4502\n",
      "Epoch 363/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7309 - val_loss: -2.9265\n",
      "Epoch 364/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8474 - val_loss: -3.2642\n",
      "Epoch 365/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7306 - val_loss: -2.7968\n",
      "Epoch 366/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6825 - val_loss: -3.4959\n",
      "Epoch 367/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8769 - val_loss: -3.8111\n",
      "Epoch 368/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.8454"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -3.8454 - val_loss: -4.4146\n",
      "Epoch 369/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3516 - val_loss: -3.7462\n",
      "Epoch 370/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5899 - val_loss: -3.1767\n",
      "Epoch 371/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7016 - val_loss: -1.2521\n",
      "Epoch 372/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7131 - val_loss: -1.8139\n",
      "Epoch 373/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7068 - val_loss: -2.1549\n",
      "Epoch 374/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5174 - val_loss: -2.0263\n",
      "Epoch 375/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7745 - val_loss: -4.0838\n",
      "Epoch 376/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8086 - val_loss: -2.0769\n",
      "Epoch 377/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7826 - val_loss: -3.6364\n",
      "Epoch 378/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8167 - val_loss: -3.9077\n",
      "Epoch 379/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7771 - val_loss: -2.7946\n",
      "Epoch 380/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8507 - val_loss: -3.3761\n",
      "Epoch 381/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7998 - val_loss: -2.9731\n",
      "Epoch 382/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8271 - val_loss: -4.2251\n",
      "Epoch 383/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9214 - val_loss: -3.1834\n",
      "Epoch 384/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2518 - val_loss: -3.9561\n",
      "Epoch 385/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5730 - val_loss: -3.6622\n",
      "Epoch 386/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8183 - val_loss: -4.2526\n",
      "Epoch 387/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8502 - val_loss: -3.1477\n",
      "Epoch 388/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8811 - val_loss: -2.9782\n",
      "Epoch 389/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9143 - val_loss: -3.3255\n",
      "Epoch 390/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8595 - val_loss: -2.5370\n",
      "Epoch 391/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9115 - val_loss: -4.3284\n",
      "Epoch 392/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8753 - val_loss: -3.1768\n",
      "Epoch 393/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9533 - val_loss: -3.5420\n",
      "Epoch 394/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8791 - val_loss: -3.5646\n",
      "Epoch 395/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6779 - val_loss: -3.7882\n",
      "Epoch 396/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8694 - val_loss: -3.2279\n",
      "Epoch 397/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9441 - val_loss: -3.6753\n",
      "Epoch 398/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8297 - val_loss: -4.0084\n",
      "Epoch 399/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7142 - val_loss: -4.1410\n",
      "Epoch 400/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7911 - val_loss: -3.8593\n",
      "Epoch 401/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7831 - val_loss: -3.9442\n",
      "Epoch 402/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8133 - val_loss: -3.7320\n",
      "Epoch 403/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9464 - val_loss: -4.0214\n",
      "Epoch 404/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6936 - val_loss: -4.2642\n",
      "Epoch 405/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0051 - val_loss: -0.6716\n",
      "Epoch 406/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8153 - val_loss: -3.8532\n",
      "Epoch 407/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8467 - val_loss: -4.2081\n",
      "Epoch 408/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9129 - val_loss: -3.7243\n",
      "Epoch 409/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2786 - val_loss: -3.9186\n",
      "Epoch 410/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9069 - val_loss: -2.6449\n",
      "Epoch 411/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9191 - val_loss: 0.3885\n",
      "Epoch 412/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9473 - val_loss: -4.0194\n",
      "Epoch 413/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8403 - val_loss: -4.1462\n",
      "Epoch 414/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9307 - val_loss: -3.5428\n",
      "Epoch 415/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0010 - val_loss: -3.7184\n",
      "Epoch 416/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8786 - val_loss: -2.4636\n",
      "Epoch 417/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9314 - val_loss: -4.3152\n",
      "Epoch 418/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0560 - val_loss: -4.0326\n",
      "Epoch 419/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7746 - val_loss: -4.0137\n",
      "Epoch 420/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9516 - val_loss: -3.3362\n",
      "Epoch 421/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0440 - val_loss: -4.1741\n",
      "Epoch 422/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9204 - val_loss: -4.0887\n",
      "Epoch 423/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0098 - val_loss: -3.4881\n",
      "Epoch 424/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9912 - val_loss: -4.3217\n",
      "Epoch 425/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.5218 - val_loss: -4.0646\n",
      "Epoch 426/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9979 - val_loss: -3.9949\n",
      "Epoch 427/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9871 - val_loss: 1.0348\n",
      "Epoch 428/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9906 - val_loss: -3.6708\n",
      "Epoch 429/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8912 - val_loss: -3.4816\n",
      "Epoch 430/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0588 - val_loss: -4.0409\n",
      "Epoch 431/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8060 - val_loss: -3.9542\n",
      "Epoch 432/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0130 - val_loss: -3.9811\n",
      "Epoch 433/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9161 - val_loss: -3.8063\n",
      "Epoch 434/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9628 - val_loss: -4.3533\n",
      "Epoch 435/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9263 - val_loss: -4.2787\n",
      "Epoch 436/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0168 - val_loss: -4.0721\n",
      "Epoch 437/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0220 - val_loss: -3.9381\n",
      "Epoch 438/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0210 - val_loss: -3.6603\n",
      "Epoch 439/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0203 - val_loss: -3.4912\n",
      "Epoch 440/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.2990 - val_loss: -2.4362\n",
      "Epoch 441/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8084 - val_loss: -3.1187\n",
      "Epoch 442/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1147 - val_loss: -3.4508\n",
      "Epoch 443/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1759 - val_loss: -3.7691\n",
      "Epoch 444/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.4146 - val_loss: -3.8209\n",
      "Epoch 445/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5907 - val_loss: -3.5936\n",
      "Epoch 446/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6318 - val_loss: -4.0996\n",
      "Epoch 447/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6928 - val_loss: -3.8670\n",
      "Epoch 448/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7140 - val_loss: -2.7712\n",
      "Epoch 449/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6631 - val_loss: -3.4081\n",
      "Epoch 450/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7620 - val_loss: -4.1488\n",
      "Epoch 451/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7731 - val_loss: -4.1375\n",
      "Epoch 452/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2717 - val_loss: -4.2797\n",
      "Epoch 453/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6616 - val_loss: -4.2378\n",
      "Epoch 454/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7946 - val_loss: -3.4283\n",
      "Epoch 455/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7454 - val_loss: -2.9205\n",
      "Epoch 456/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7884 - val_loss: -3.9054\n",
      "Epoch 457/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8501 - val_loss: -3.8195\n",
      "Epoch 458/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9115 - val_loss: -2.9433\n",
      "Epoch 459/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.6734 - val_loss: -3.8760\n",
      "Epoch 460/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2749 - val_loss: -3.6915\n",
      "Epoch 461/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5011 - val_loss: -4.0632\n",
      "Epoch 462/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8160 - val_loss: -3.3858\n",
      "Epoch 463/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6929 - val_loss: -3.0387\n",
      "Epoch 464/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7234 - val_loss: -4.1704\n",
      "Epoch 465/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8964 - val_loss: -3.9138\n",
      "Epoch 466/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8681 - val_loss: -4.2333\n",
      "Epoch 467/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7909 - val_loss: -4.1584\n",
      "Epoch 468/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7490 - val_loss: -3.7633\n",
      "Epoch 469/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8558 - val_loss: -4.1113\n",
      "Epoch 470/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8946 - val_loss: -3.7305\n",
      "Epoch 471/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9523 - val_loss: -3.7131\n",
      "Epoch 472/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9008 - val_loss: 0.4108\n",
      "Epoch 473/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.7032 - val_loss: -3.4147\n",
      "Epoch 474/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8042 - val_loss: -3.8609\n",
      "Epoch 475/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1635 - val_loss: -3.8675\n",
      "Epoch 476/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4533 - val_loss: -4.0567\n",
      "Epoch 477/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5401 - val_loss: -3.1232\n",
      "Epoch 478/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6741 - val_loss: -3.6931\n",
      "Epoch 479/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8151 - val_loss: -4.0507\n",
      "Epoch 480/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6962 - val_loss: -3.7213\n",
      "Epoch 481/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7678 - val_loss: -3.8987\n",
      "Epoch 482/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7569 - val_loss: -3.6836\n",
      "Epoch 483/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8113 - val_loss: -3.1671\n",
      "Epoch 484/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8525 - val_loss: -2.3855\n",
      "Epoch 485/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8745 - val_loss: -4.0164\n",
      "Epoch 486/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9333 - val_loss: -3.8392\n",
      "Epoch 487/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7352 - val_loss: -3.7608\n",
      "Epoch 488/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9599 - val_loss: -2.9195\n",
      "Epoch 489/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8439 - val_loss: -4.1771\n",
      "Epoch 490/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9119 - val_loss: -3.9871\n",
      "Epoch 491/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8549 - val_loss: -4.3818\n",
      "Epoch 492/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9288 - val_loss: -3.7760\n",
      "Epoch 493/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0035 - val_loss: -4.2425\n",
      "Epoch 494/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6069 - val_loss: -4.0344\n",
      "Epoch 495/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8552 - val_loss: -4.2955\n",
      "Epoch 496/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8808 - val_loss: -4.1966\n",
      "Epoch 497/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9693 - val_loss: -4.0464\n",
      "Epoch 498/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7270 - val_loss: -3.1478\n",
      "Epoch 499/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5777 - val_loss: -3.4152\n",
      "Epoch 500/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8560 - val_loss: -4.4015\n",
      "Epoch 501/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9238 - val_loss: -4.2329\n",
      "Epoch 502/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8800"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -4.8800 - val_loss: -4.4314\n",
      "Epoch 503/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9467"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -4.9467 - val_loss: -4.4387\n",
      "Epoch 504/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8832 - val_loss: -4.0773\n",
      "Epoch 505/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9393 - val_loss: -3.4040\n",
      "Epoch 506/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8554 - val_loss: -3.7200\n",
      "Epoch 507/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9827 - val_loss: -3.9310\n",
      "Epoch 508/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0469 - val_loss: -4.2368\n",
      "Epoch 509/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9162 - val_loss: -3.5352\n",
      "Epoch 510/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0020 - val_loss: -4.2045\n",
      "Epoch 511/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9415 - val_loss: -4.2723\n",
      "Epoch 512/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9416 - val_loss: -2.7774\n",
      "Epoch 513/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9196 - val_loss: -3.8176\n",
      "Epoch 514/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8971 - val_loss: -4.1209\n",
      "Epoch 515/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0457 - val_loss: -1.9681\n",
      "Epoch 516/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8041 - val_loss: -3.7542\n",
      "Epoch 517/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1912 - val_loss: -3.4709\n",
      "Epoch 518/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8590 - val_loss: -3.8690\n",
      "Epoch 519/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9335 - val_loss: -3.4276\n",
      "Epoch 520/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8048 - val_loss: -4.1819\n",
      "Epoch 521/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0225 - val_loss: -4.0532\n",
      "Epoch 522/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9569 - val_loss: -3.9800\n",
      "Epoch 523/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0414 - val_loss: -4.1770\n",
      "Epoch 524/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8109 - val_loss: -3.2032\n",
      "Epoch 525/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0803 - val_loss: -3.2187\n",
      "Epoch 526/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8152 - val_loss: -3.9134\n",
      "Epoch 527/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0473 - val_loss: -3.8008\n",
      "Epoch 528/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7582 - val_loss: -3.7853\n",
      "Epoch 529/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0887 - val_loss: -4.3609\n",
      "Epoch 530/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9787 - val_loss: -3.1384\n",
      "Epoch 531/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.0339 - val_loss: -3.8692\n",
      "Epoch 532/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0669 - val_loss: -3.7494\n",
      "Epoch 533/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0431 - val_loss: -2.7114\n",
      "Epoch 534/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0633 - val_loss: 0.2863\n",
      "Epoch 535/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5897 - val_loss: -3.1792\n",
      "Epoch 536/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0254 - val_loss: -3.7216\n",
      "Epoch 537/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0625 - val_loss: -4.1331\n",
      "Epoch 538/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0229 - val_loss: -4.0178\n",
      "Epoch 539/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9477 - val_loss: -3.5804\n",
      "Epoch 540/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0453 - val_loss: -3.6722\n",
      "Epoch 541/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8478 - val_loss: -4.2317\n",
      "Epoch 542/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0126 - val_loss: -4.0993\n",
      "Epoch 543/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9663 - val_loss: -4.3031\n",
      "Epoch 544/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9556 - val_loss: -4.3108\n",
      "Epoch 545/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0765 - val_loss: -4.2754\n",
      "Epoch 546/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0857 - val_loss: -3.5126\n",
      "Epoch 547/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0157 - val_loss: -4.2442\n",
      "Epoch 548/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1126 - val_loss: -4.4048\n",
      "Epoch 549/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9240 - val_loss: -3.4440\n",
      "Epoch 550/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9826 - val_loss: -3.8225\n",
      "Epoch 551/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1022"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -5.1022 - val_loss: -4.4478\n",
      "Epoch 552/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6908 - val_loss: -4.2065\n",
      "Epoch 553/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9635 - val_loss: -3.7953\n",
      "Epoch 554/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9980 - val_loss: -4.1816\n",
      "Epoch 555/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0827 - val_loss: -3.8836\n",
      "Epoch 556/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4995 - val_loss: -3.7661\n",
      "Epoch 557/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9103 - val_loss: -3.9217\n",
      "Epoch 558/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9755 - val_loss: -4.2976\n",
      "Epoch 559/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0961 - val_loss: -3.9922\n",
      "Epoch 560/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1424 - val_loss: -3.0921\n",
      "Epoch 561/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0376 - val_loss: -4.2802\n",
      "Epoch 562/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9743 - val_loss: -3.7337\n",
      "Epoch 563/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0533 - val_loss: -3.5555\n",
      "Epoch 564/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0488 - val_loss: -3.8302\n",
      "Epoch 565/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0864 - val_loss: -3.4581\n",
      "Epoch 566/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0604 - val_loss: -3.4287\n",
      "Epoch 567/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0965 - val_loss: -4.0778\n",
      "Epoch 568/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1357 - val_loss: -4.0415\n",
      "Epoch 569/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0848 - val_loss: -4.3045\n",
      "Epoch 570/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0164 - val_loss: -4.3387\n",
      "Epoch 571/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9978 - val_loss: -3.9301\n",
      "Epoch 572/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0877 - val_loss: -2.8413\n",
      "Epoch 573/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0602 - val_loss: -4.3616\n",
      "Epoch 574/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0172 - val_loss: -3.6412\n",
      "Epoch 575/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0744 - val_loss: -4.0954\n",
      "Epoch 576/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0601 - val_loss: -4.2030\n",
      "Epoch 577/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1048 - val_loss: -4.2074\n",
      "Epoch 578/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0589 - val_loss: -3.6561\n",
      "Epoch 579/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0664 - val_loss: -3.7929\n",
      "Epoch 580/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0425 - val_loss: -3.8071\n",
      "Epoch 581/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4530 - val_loss: -4.0391\n",
      "Epoch 582/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7863 - val_loss: -3.5064\n",
      "Epoch 583/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0373 - val_loss: -4.0616\n",
      "Epoch 584/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0649 - val_loss: -3.9502\n",
      "Epoch 585/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0413 - val_loss: -4.0496\n",
      "Epoch 586/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9592 - val_loss: -4.1080\n",
      "Epoch 587/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0741 - val_loss: -2.4389\n",
      "Epoch 588/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0835 - val_loss: -4.2022\n",
      "Epoch 589/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1143 - val_loss: -3.6879\n",
      "Epoch 590/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0008 - val_loss: -3.9006\n",
      "Epoch 591/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0376 - val_loss: -3.4615\n",
      "Epoch 592/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1979 - val_loss: -3.1434\n",
      "Epoch 593/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0594 - val_loss: -4.0052\n",
      "Epoch 594/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0622 - val_loss: -3.9449\n",
      "Epoch 595/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1323 - val_loss: -2.6809\n",
      "Epoch 596/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0135 - val_loss: -4.1387\n",
      "Epoch 597/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1160 - val_loss: -3.2486\n",
      "Epoch 598/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1313 - val_loss: -3.6722\n",
      "Epoch 599/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0352 - val_loss: -4.1830\n",
      "Epoch 600/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1162 - val_loss: -3.9919\n",
      "Epoch 601/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1530 - val_loss: -4.2914\n",
      "Epoch 602/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1094 - val_loss: -4.0841\n",
      "Epoch 603/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9807 - val_loss: -4.1810\n",
      "Epoch 604/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1819 - val_loss: -4.1497\n",
      "Epoch 605/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1977 - val_loss: -4.3623\n",
      "Epoch 606/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9848 - val_loss: -3.0923\n",
      "Epoch 607/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1102 - val_loss: -3.9720\n",
      "Epoch 608/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0259 - val_loss: -4.2070\n",
      "Epoch 609/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1495 - val_loss: -2.5026\n",
      "Epoch 610/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0177 - val_loss: -3.3102\n",
      "Epoch 611/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8857 - val_loss: -3.6921\n",
      "Epoch 612/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0390 - val_loss: -2.5037\n",
      "Epoch 613/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9583 - val_loss: -4.3950\n",
      "Epoch 614/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0149 - val_loss: -4.2221\n",
      "Epoch 615/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1360 - val_loss: -4.0079\n",
      "Epoch 616/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0663 - val_loss: -4.3686\n",
      "Epoch 617/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9944 - val_loss: -4.3733\n",
      "Epoch 618/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0877 - val_loss: -3.9387\n",
      "Epoch 619/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1210 - val_loss: -0.0766\n",
      "Epoch 620/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0076 - val_loss: -3.9380\n",
      "Epoch 621/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0155 - val_loss: -4.2379\n",
      "Epoch 622/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0923 - val_loss: -4.2190\n",
      "Epoch 623/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0439 - val_loss: -3.9723\n",
      "Epoch 624/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0141 - val_loss: -4.0938\n",
      "Epoch 625/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0886 - val_loss: -4.0076\n",
      "Epoch 626/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1485 - val_loss: -2.7661\n",
      "Epoch 627/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0846 - val_loss: -3.2766\n",
      "Epoch 628/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7343 - val_loss: -4.0872\n",
      "Epoch 629/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1017 - val_loss: -4.2747\n",
      "Epoch 630/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9545 - val_loss: -4.1562\n",
      "Epoch 631/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0406 - val_loss: -3.4525\n",
      "Epoch 632/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0342 - val_loss: -4.2146\n",
      "Epoch 633/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9352 - val_loss: -4.0885\n",
      "Epoch 634/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0981 - val_loss: -3.4045\n",
      "Epoch 635/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1474 - val_loss: -4.2571\n",
      "Epoch 636/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1425 - val_loss: -3.1554\n",
      "Epoch 637/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0370 - val_loss: -4.2530\n",
      "Epoch 638/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1862 - val_loss: -4.0219\n",
      "Epoch 639/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1526 - val_loss: -3.1875\n",
      "Epoch 640/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.0610 - val_loss: -3.0124\n",
      "Epoch 641/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1134 - val_loss: -3.7687\n",
      "Epoch 642/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1269 - val_loss: -3.2157\n",
      "Epoch 643/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1574 - val_loss: -4.1613\n",
      "Epoch 644/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1885 - val_loss: -3.9557\n",
      "Epoch 645/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9040 - val_loss: -3.9259\n",
      "Epoch 646/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0914 - val_loss: -2.0830\n",
      "Epoch 647/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1812 - val_loss: -4.0566\n",
      "Epoch 648/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6989 - val_loss: -4.2653\n",
      "Epoch 649/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9786 - val_loss: -4.3015\n",
      "Epoch 650/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0086 - val_loss: -4.3698\n",
      "Epoch 651/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0147 - val_loss: -4.2261\n",
      "Epoch 652/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0540 - val_loss: -3.4619\n",
      "Epoch 653/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0963 - val_loss: -2.5517\n",
      "Epoch 654/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0843"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -5.0843 - val_loss: -4.4995\n",
      "Epoch 655/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1376 - val_loss: -3.6139\n",
      "Epoch 656/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0767 - val_loss: -4.3136\n",
      "Epoch 657/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9610 - val_loss: -4.4139\n",
      "Epoch 658/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1477 - val_loss: -3.6777\n",
      "Epoch 659/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1637 - val_loss: -4.3899\n",
      "Epoch 660/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0964 - val_loss: -3.8313\n",
      "Epoch 661/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1163 - val_loss: -2.5201\n",
      "Epoch 662/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1518 - val_loss: -4.2176\n",
      "Epoch 663/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0360 - val_loss: -4.2746\n",
      "Epoch 664/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1624 - val_loss: -3.0053\n",
      "Epoch 665/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1578 - val_loss: -3.8018\n",
      "Epoch 666/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1795 - val_loss: -4.1603\n",
      "Epoch 667/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0145 - val_loss: -4.4374\n",
      "Epoch 668/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1852 - val_loss: -4.0840\n",
      "Epoch 669/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1396 - val_loss: -3.8816\n",
      "Epoch 670/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1922 - val_loss: -3.9127\n",
      "Epoch 671/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1216 - val_loss: -3.9414\n",
      "Epoch 672/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1547 - val_loss: -3.4758\n",
      "Epoch 673/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1879 - val_loss: -3.2219\n",
      "Epoch 674/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1032 - val_loss: -3.9345\n",
      "Epoch 675/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9563 - val_loss: -4.0688\n",
      "Epoch 676/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2414 - val_loss: -0.1382\n",
      "Epoch 677/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8515 - val_loss: -3.2151\n",
      "Epoch 678/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1419 - val_loss: -4.3178\n",
      "Epoch 679/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1041 - val_loss: -4.3068\n",
      "Epoch 680/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1327 - val_loss: -3.6790\n",
      "Epoch 681/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1979 - val_loss: -3.8516\n",
      "Epoch 682/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2123 - val_loss: -4.1871\n",
      "Epoch 683/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1983 - val_loss: -2.3545\n",
      "Epoch 684/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1770"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -5.1770 - val_loss: -4.5091\n",
      "Epoch 685/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1876 - val_loss: -4.3279\n",
      "Epoch 686/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2220 - val_loss: -3.5912\n",
      "Epoch 687/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2491 - val_loss: -3.8341\n",
      "Epoch 688/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8147 - val_loss: -1.6072\n",
      "Epoch 689/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1528 - val_loss: -3.8556\n",
      "Epoch 690/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1981 - val_loss: -2.8073\n",
      "Epoch 691/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1933 - val_loss: -3.5954\n",
      "Epoch 692/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2074 - val_loss: -4.1912\n",
      "Epoch 693/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0198 - val_loss: -4.1847\n",
      "Epoch 694/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1394 - val_loss: -0.7210\n",
      "Epoch 695/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2134 - val_loss: -3.8495\n",
      "Epoch 696/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2169 - val_loss: -3.3473\n",
      "Epoch 697/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1162 - val_loss: -4.2615\n",
      "Epoch 698/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1872 - val_loss: -2.4595\n",
      "Epoch 699/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1948 - val_loss: -3.7943\n",
      "Epoch 700/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1821 - val_loss: -4.1348\n",
      "Epoch 701/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1348 - val_loss: -3.9461\n",
      "Epoch 702/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9365 - val_loss: -4.4271\n",
      "Epoch 703/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1809 - val_loss: -4.3339\n",
      "Epoch 704/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1606 - val_loss: -4.0821\n",
      "Epoch 705/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2592 - val_loss: -2.4525\n",
      "Epoch 706/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1354 - val_loss: -2.5700\n",
      "Epoch 707/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1838 - val_loss: -2.3402\n",
      "Epoch 708/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -5.1562 - val_loss: -4.5227\n",
      "Epoch 709/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2105 - val_loss: -1.7894\n",
      "Epoch 710/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2197 - val_loss: -4.3488\n",
      "Epoch 711/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1823 - val_loss: -4.1699\n",
      "Epoch 712/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2373 - val_loss: -2.1556\n",
      "Epoch 713/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0860 - val_loss: -3.8758\n",
      "Epoch 714/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2594 - val_loss: -4.3556\n",
      "Epoch 715/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0545 - val_loss: -4.1168\n",
      "Epoch 716/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2769 - val_loss: -4.3730\n",
      "Epoch 717/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2266 - val_loss: -2.5008\n",
      "Epoch 718/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1807 - val_loss: -2.5798\n",
      "Epoch 719/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2081 - val_loss: -3.7444\n",
      "Epoch 720/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0882 - val_loss: -4.2653\n",
      "Epoch 721/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9471 - val_loss: -4.1012\n",
      "Epoch 722/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1617 - val_loss: -3.5384\n",
      "Epoch 723/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1230 - val_loss: -3.7871\n",
      "Epoch 724/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2665 - val_loss: -3.1813\n",
      "Epoch 725/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7575 - val_loss: -4.1166\n",
      "Epoch 726/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9655 - val_loss: -3.6533\n",
      "Epoch 727/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1844 - val_loss: -4.2940\n",
      "Epoch 728/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0926"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 256ms/step - loss: -5.0926 - val_loss: -4.6828\n",
      "Epoch 729/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0840 - val_loss: -3.3433\n",
      "Epoch 730/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1704 - val_loss: -3.9291\n",
      "Epoch 731/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1644 - val_loss: -4.1835\n",
      "Epoch 732/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1686 - val_loss: -4.2428\n",
      "Epoch 733/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2098 - val_loss: -3.8779\n",
      "Epoch 734/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1864 - val_loss: -3.5653\n",
      "Epoch 735/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2021 - val_loss: -3.8332\n",
      "Epoch 736/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2347 - val_loss: -3.8864\n",
      "Epoch 737/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1736 - val_loss: -4.0868\n",
      "Epoch 738/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1184 - val_loss: -1.7745\n",
      "Epoch 739/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2377 - val_loss: -3.2281\n",
      "Epoch 740/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1500 - val_loss: -4.1538\n",
      "Epoch 741/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1680 - val_loss: -4.4328\n",
      "Epoch 742/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1212 - val_loss: -3.2247\n",
      "Epoch 743/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2799 - val_loss: -4.4322\n",
      "Epoch 744/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1256 - val_loss: -3.8662\n",
      "Epoch 745/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2838 - val_loss: -4.2289\n",
      "Epoch 746/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2033 - val_loss: -4.3501\n",
      "Epoch 747/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1729 - val_loss: -1.7019\n",
      "Epoch 748/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1786 - val_loss: -3.6911\n",
      "Epoch 749/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1778 - val_loss: -4.1082\n",
      "Epoch 750/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2199 - val_loss: -4.2158\n",
      "Epoch 751/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1593 - val_loss: -3.9736\n",
      "Epoch 752/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2188 - val_loss: -4.1575\n",
      "Epoch 753/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2460 - val_loss: -1.1237\n",
      "Epoch 754/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9129 - val_loss: -3.2612\n",
      "Epoch 755/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1263 - val_loss: -4.5467\n",
      "Epoch 756/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2571 - val_loss: -3.8714\n",
      "Epoch 757/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1319 - val_loss: -4.1744\n",
      "Epoch 758/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2550 - val_loss: -4.1505\n",
      "Epoch 759/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2515 - val_loss: -4.4771\n",
      "Epoch 760/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1953 - val_loss: -3.7973\n",
      "Epoch 761/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2447 - val_loss: -1.0587\n",
      "Epoch 762/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9826 - val_loss: -3.9651\n",
      "Epoch 763/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1925 - val_loss: -3.1471\n",
      "Epoch 764/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2587 - val_loss: -4.0479\n",
      "Epoch 765/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1950 - val_loss: -2.4399\n",
      "Epoch 766/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1184 - val_loss: -4.1968\n",
      "Epoch 767/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1882 - val_loss: -4.3772\n",
      "Epoch 768/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2207 - val_loss: -4.2110\n",
      "Epoch 769/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2800 - val_loss: -4.4398\n",
      "Epoch 770/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8343 - val_loss: -3.8851\n",
      "Epoch 771/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1825 - val_loss: -4.4630\n",
      "Epoch 772/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3303 - val_loss: -4.2917\n",
      "Epoch 773/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2346 - val_loss: -4.0162\n",
      "Epoch 774/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2177 - val_loss: -4.0547\n",
      "Epoch 775/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7511 - val_loss: -3.8944\n",
      "Epoch 776/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0649 - val_loss: -4.1088\n",
      "Epoch 777/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1818 - val_loss: -2.7143\n",
      "Epoch 778/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2372 - val_loss: -4.5351\n",
      "Epoch 779/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7475 - val_loss: -4.0188\n",
      "Epoch 780/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1602 - val_loss: -4.3053\n",
      "Epoch 781/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1092 - val_loss: -4.3103\n",
      "Epoch 782/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2962 - val_loss: -4.0271\n",
      "Epoch 783/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0634 - val_loss: -3.7288\n",
      "Epoch 784/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1865 - val_loss: -4.4607\n",
      "Epoch 785/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2650 - val_loss: -4.2641\n",
      "Epoch 786/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2164 - val_loss: -2.8893\n",
      "Epoch 787/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2047 - val_loss: -3.7624\n",
      "Epoch 788/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2542 - val_loss: -4.3678\n",
      "Epoch 789/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3155 - val_loss: -3.1549\n",
      "Epoch 790/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1350 - val_loss: -4.5803\n",
      "Epoch 791/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3188 - val_loss: -4.0423\n",
      "Epoch 792/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0638 - val_loss: -4.0855\n",
      "Epoch 793/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1205 - val_loss: -3.4080\n",
      "Epoch 794/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1156 - val_loss: -4.1443\n",
      "Epoch 795/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1845 - val_loss: -2.1814\n",
      "Epoch 796/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2174 - val_loss: -2.8722\n",
      "Epoch 797/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0895"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -5.0895 - val_loss: -4.6892\n",
      "Epoch 798/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2390 - val_loss: -4.4548\n",
      "Epoch 799/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9553 - val_loss: -3.9211\n",
      "Epoch 800/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1381 - val_loss: -3.9440\n",
      "Epoch 801/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1616 - val_loss: -4.3724\n",
      "Epoch 802/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1592 - val_loss: -3.8160\n",
      "Epoch 803/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2915 - val_loss: -4.3925\n",
      "Epoch 804/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2831 - val_loss: -3.4887\n",
      "Epoch 805/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2434 - val_loss: -2.7689\n",
      "Epoch 806/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3221 - val_loss: -3.6314\n",
      "Epoch 807/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1647 - val_loss: -4.0813\n",
      "Epoch 808/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1908 - val_loss: -3.9258\n",
      "Epoch 809/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1901 - val_loss: -4.2737\n",
      "Epoch 810/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3404 - val_loss: -1.1885\n",
      "Epoch 811/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2084 - val_loss: -4.3194\n",
      "Epoch 812/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5764 - val_loss: -3.1605\n",
      "Epoch 813/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1967 - val_loss: -4.6839\n",
      "Epoch 814/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2647 - val_loss: -4.1797\n",
      "Epoch 815/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0872 - val_loss: -4.3064\n",
      "Epoch 816/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2484 - val_loss: -4.2161\n",
      "Epoch 817/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0809 - val_loss: -4.4413\n",
      "Epoch 818/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1387 - val_loss: -4.2492\n",
      "Epoch 819/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0383 - val_loss: -3.9231\n",
      "Epoch 820/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2758 - val_loss: -4.4602\n",
      "Epoch 821/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2834 - val_loss: -4.0577\n",
      "Epoch 822/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1884 - val_loss: -2.0574\n",
      "Epoch 823/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2474 - val_loss: -4.2082\n",
      "Epoch 824/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2585 - val_loss: -4.2275\n",
      "Epoch 825/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2124 - val_loss: -3.3548\n",
      "Epoch 826/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9954 - val_loss: -4.0800\n",
      "Epoch 827/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1203 - val_loss: -4.2331\n",
      "Epoch 828/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1408 - val_loss: -3.3317\n",
      "Epoch 829/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9894 - val_loss: -3.8914\n",
      "Epoch 830/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1281 - val_loss: -3.7551\n",
      "Epoch 831/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1670 - val_loss: -3.9923\n",
      "Epoch 832/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1688 - val_loss: -3.9827\n",
      "Epoch 833/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2808 - val_loss: -3.7921\n",
      "Epoch 834/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1101 - val_loss: -3.9685\n",
      "Epoch 835/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1930 - val_loss: -3.8418\n",
      "Epoch 836/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1469 - val_loss: -4.2701\n",
      "Epoch 837/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2646 - val_loss: -4.1791\n",
      "Epoch 838/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2448 - val_loss: -2.3494\n",
      "Epoch 839/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2113 - val_loss: -1.8207\n",
      "Epoch 840/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0449 - val_loss: -4.3583\n",
      "Epoch 841/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0886 - val_loss: -1.8073\n",
      "Epoch 842/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1988 - val_loss: -4.1000\n",
      "Epoch 843/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1867 - val_loss: -4.3955\n",
      "Epoch 844/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2599 - val_loss: -4.6082\n",
      "Epoch 845/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1016 - val_loss: -4.3538\n",
      "Epoch 846/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2728 - val_loss: -3.6586\n",
      "Epoch 847/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1238 - val_loss: -2.2174\n",
      "Epoch 848/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2701 - val_loss: -3.1965\n",
      "Epoch 849/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2586 - val_loss: -2.0631\n",
      "Epoch 850/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2914 - val_loss: -3.5130\n",
      "Epoch 851/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2383 - val_loss: -4.2829\n",
      "Epoch 852/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3082 - val_loss: -4.3412\n",
      "Epoch 853/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1710 - val_loss: -1.3580\n",
      "Epoch 854/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1062 - val_loss: -4.4537\n",
      "Epoch 855/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1615 - val_loss: -3.6627\n",
      "Epoch 856/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1951 - val_loss: -4.0585\n",
      "Epoch 857/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2100 - val_loss: -4.4507\n",
      "Epoch 858/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3015 - val_loss: -3.9635\n",
      "Epoch 859/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2945 - val_loss: -2.7149\n",
      "Epoch 860/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2341 - val_loss: -4.5216\n",
      "Epoch 861/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2688 - val_loss: -3.5294\n",
      "Epoch 862/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2660 - val_loss: -4.6603\n",
      "Epoch 863/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3064 - val_loss: -3.7505\n",
      "Epoch 864/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0678 - val_loss: -3.3804\n",
      "Epoch 865/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0608 - val_loss: -4.1627\n",
      "Epoch 866/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2020 - val_loss: -4.4873\n",
      "Epoch 867/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2237 - val_loss: -4.5761\n",
      "Epoch 868/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2122 - val_loss: -3.5022\n",
      "Epoch 869/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1915 - val_loss: -4.5323\n",
      "Epoch 870/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3138 - val_loss: -3.8867\n",
      "Epoch 871/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1981 - val_loss: -4.1121\n",
      "Epoch 872/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2575 - val_loss: -0.6049\n",
      "Epoch 873/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.6884 - val_loss: -3.8628\n",
      "Epoch 874/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0551 - val_loss: -3.9213\n",
      "Epoch 875/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2993 - val_loss: -3.9117\n",
      "Epoch 876/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3958 - val_loss: -3.6961\n",
      "Epoch 877/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5296 - val_loss: -3.9715\n",
      "Epoch 878/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6220 - val_loss: -3.1713\n",
      "Epoch 879/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6556 - val_loss: -4.2088\n",
      "Epoch 880/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6821 - val_loss: -3.9602\n",
      "Epoch 881/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7220 - val_loss: -4.0987\n",
      "Epoch 882/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7860 - val_loss: -3.2667\n",
      "Epoch 883/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8651 - val_loss: -4.2680\n",
      "Epoch 884/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7792 - val_loss: -4.2527\n",
      "Epoch 885/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8910 - val_loss: -4.2237\n",
      "Epoch 886/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9341 - val_loss: -3.7842\n",
      "Epoch 887/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9180 - val_loss: -1.4886\n",
      "Epoch 888/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8769 - val_loss: -3.6022\n",
      "Epoch 889/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9138 - val_loss: -3.4479\n",
      "Epoch 890/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0154 - val_loss: -4.4600\n",
      "Epoch 891/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0361 - val_loss: -4.1868\n",
      "Epoch 892/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8936 - val_loss: -4.4260\n",
      "Epoch 893/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8676 - val_loss: -3.8274\n",
      "Epoch 894/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1147 - val_loss: -3.8331\n",
      "Epoch 895/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0827 - val_loss: -1.6844\n",
      "Epoch 896/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0704 - val_loss: -3.7325\n",
      "Epoch 897/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9435 - val_loss: -3.6688\n",
      "Epoch 898/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0687 - val_loss: -4.6148\n",
      "Epoch 899/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0485 - val_loss: -4.4486\n",
      "Epoch 900/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1552 - val_loss: -4.1898\n",
      "Epoch 901/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0826 - val_loss: -4.2456\n",
      "Epoch 902/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1468 - val_loss: -4.5731\n",
      "Epoch 903/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9391 - val_loss: -4.2514\n",
      "Epoch 904/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1503 - val_loss: -3.7256\n",
      "Epoch 905/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0170 - val_loss: -4.3053\n",
      "Epoch 906/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1290 - val_loss: -3.7234\n",
      "Epoch 907/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2869 - val_loss: -3.6937\n",
      "Epoch 908/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6528 - val_loss: -4.5545\n",
      "Epoch 909/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.1880 - val_loss: -3.0153\n",
      "Epoch 910/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9382 - val_loss: -3.4623\n",
      "Epoch 911/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4310 - val_loss: -3.3611\n",
      "Epoch 912/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5563 - val_loss: -3.3231\n",
      "Epoch 913/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6446 - val_loss: -3.6833\n",
      "Epoch 914/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8464 - val_loss: -3.8416\n",
      "Epoch 915/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7823 - val_loss: -4.1621\n",
      "Epoch 916/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8611 - val_loss: -3.8356\n",
      "Epoch 917/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8238 - val_loss: -3.0659\n",
      "Epoch 918/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8827 - val_loss: -4.0573\n",
      "Epoch 919/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7817 - val_loss: -4.1393\n",
      "Epoch 920/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9839 - val_loss: -4.1319\n",
      "Epoch 921/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9084 - val_loss: -4.2503\n",
      "Epoch 922/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9052 - val_loss: -3.9095\n",
      "Epoch 923/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8989 - val_loss: -4.0688\n",
      "Epoch 924/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9700 - val_loss: -4.3552\n",
      "Epoch 925/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9445 - val_loss: -3.8916\n",
      "Epoch 926/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9375 - val_loss: -4.1962\n",
      "Epoch 927/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8782 - val_loss: -4.1304\n",
      "Epoch 928/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9226 - val_loss: -4.1824\n",
      "Epoch 929/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9325 - val_loss: -4.2118\n",
      "Epoch 930/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0983 - val_loss: -3.7664\n",
      "Epoch 931/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9263 - val_loss: -4.2060\n",
      "Epoch 932/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9425 - val_loss: -4.3827\n",
      "Epoch 933/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9381 - val_loss: -4.5072\n",
      "Epoch 934/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9695 - val_loss: -4.1229\n",
      "Epoch 935/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9571 - val_loss: -4.2611\n",
      "Epoch 936/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9741 - val_loss: -3.4747\n",
      "Epoch 937/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0527 - val_loss: -4.3075\n",
      "Epoch 938/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9450 - val_loss: -4.0198\n",
      "Epoch 939/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0195 - val_loss: -4.0959\n",
      "Epoch 940/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9838 - val_loss: -4.4310\n",
      "Epoch 941/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0253 - val_loss: -4.2448\n",
      "Epoch 942/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1319 - val_loss: -3.8756\n",
      "Epoch 943/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9355 - val_loss: -4.0780\n",
      "Epoch 944/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0666 - val_loss: -1.9393\n",
      "Epoch 945/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5878 - val_loss: -4.2022\n",
      "Epoch 946/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1206 - val_loss: -3.8586\n",
      "Epoch 947/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9939 - val_loss: -3.8863\n",
      "Epoch 948/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9926 - val_loss: -3.9830\n",
      "Epoch 949/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0672 - val_loss: -3.7658\n",
      "Epoch 950/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1169 - val_loss: -2.8086\n",
      "Epoch 951/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1052 - val_loss: -4.0261\n",
      "Epoch 952/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0772 - val_loss: -3.3511\n",
      "Epoch 953/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0826 - val_loss: -3.6197\n",
      "Epoch 954/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0508 - val_loss: -3.9122\n",
      "Epoch 955/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1463 - val_loss: -3.7373\n",
      "Epoch 956/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7683 - val_loss: -2.7296\n",
      "Epoch 957/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5634 - val_loss: -3.8800\n",
      "Epoch 958/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0208 - val_loss: -4.2622\n",
      "Epoch 959/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9583 - val_loss: -4.1555\n",
      "Epoch 960/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6159 - val_loss: -4.1333\n",
      "Epoch 961/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0101 - val_loss: -4.0840\n",
      "Epoch 962/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0700 - val_loss: -4.4893\n",
      "Epoch 963/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0190 - val_loss: -4.2809\n",
      "Epoch 964/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9938 - val_loss: -4.1052\n",
      "Epoch 965/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0288 - val_loss: -4.2663\n",
      "Epoch 966/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1522 - val_loss: -1.9841\n",
      "Epoch 967/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0467 - val_loss: -3.5927\n",
      "Epoch 968/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1202 - val_loss: -4.2505\n",
      "Epoch 969/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1603 - val_loss: -4.3993\n",
      "Epoch 970/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8330 - val_loss: -4.2404\n",
      "Epoch 971/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0055 - val_loss: -4.2218\n",
      "Epoch 972/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0189 - val_loss: -4.3831\n",
      "Epoch 973/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0459 - val_loss: -4.3747\n",
      "Epoch 974/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0276 - val_loss: -4.2368\n",
      "Epoch 975/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0592 - val_loss: -4.1334\n",
      "Epoch 976/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0984 - val_loss: -4.2676\n",
      "Epoch 977/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0513 - val_loss: -2.7969\n",
      "Epoch 978/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0682 - val_loss: -3.9550\n",
      "Epoch 979/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0245 - val_loss: -3.3242\n",
      "Epoch 980/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1779 - val_loss: -4.1708\n",
      "Epoch 981/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2003 - val_loss: -3.5299\n",
      "Epoch 982/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1693 - val_loss: -4.4082\n",
      "Epoch 983/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2037 - val_loss: -2.4926\n",
      "Epoch 984/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9047 - val_loss: -4.4119\n",
      "Epoch 985/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0787 - val_loss: -4.1094\n",
      "Epoch 986/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0457 - val_loss: -3.9746\n",
      "Epoch 987/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1895 - val_loss: -3.9142\n",
      "Epoch 988/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0699 - val_loss: -3.8560\n",
      "Epoch 989/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2303 - val_loss: -4.3784\n",
      "Epoch 990/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1643 - val_loss: -4.0749\n",
      "Epoch 991/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2155 - val_loss: -4.3997\n",
      "Epoch 992/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1035 - val_loss: -3.0900\n",
      "Epoch 993/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1786 - val_loss: -4.2246\n",
      "Epoch 994/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9512 - val_loss: -4.2032\n",
      "Epoch 995/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1185 - val_loss: -4.0415\n",
      "Epoch 996/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1702 - val_loss: -4.0481\n",
      "Epoch 997/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1900 - val_loss: -2.0614\n",
      "Epoch 998/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1315 - val_loss: -4.1415\n",
      "Epoch 999/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9702 - val_loss: -4.5233\n",
      "Epoch 1000/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1547 - val_loss: -4.3961\n",
      "Epoch 1001/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1589 - val_loss: -4.3666\n",
      "Epoch 1002/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1654 - val_loss: -4.2062\n",
      "Epoch 1003/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2285 - val_loss: -4.2282\n",
      "Epoch 1004/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1488 - val_loss: -2.9344\n",
      "Epoch 1005/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2141 - val_loss: -3.7529\n",
      "Epoch 1006/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2265 - val_loss: -4.2537\n",
      "Epoch 1007/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1230 - val_loss: -4.3172\n",
      "Epoch 1008/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2319 - val_loss: -3.6717\n",
      "Epoch 1009/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1506 - val_loss: -4.4136\n",
      "Epoch 1010/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1410 - val_loss: -4.4705\n",
      "Epoch 1011/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1338 - val_loss: -2.6244\n",
      "Epoch 1012/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9713 - val_loss: -4.1679\n",
      "Epoch 1013/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1564 - val_loss: -4.5823\n",
      "Epoch 1014/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2255 - val_loss: -4.1480\n",
      "Epoch 1015/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0139 - val_loss: -3.4321\n",
      "Epoch 1016/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2357 - val_loss: -4.0542\n",
      "Epoch 1017/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2575 - val_loss: -3.6693\n",
      "Epoch 1018/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2009 - val_loss: -4.0752\n",
      "Epoch 1019/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0243 - val_loss: -1.4005\n",
      "Epoch 1020/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1638 - val_loss: -4.0637\n",
      "Epoch 1021/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1905 - val_loss: -4.3986\n",
      "Epoch 1022/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1863 - val_loss: -3.9928\n",
      "Epoch 1023/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6697 - val_loss: -3.2033\n",
      "Epoch 1024/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9150 - val_loss: -4.1865\n",
      "Epoch 1025/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1706 - val_loss: -3.8800\n",
      "Epoch 1026/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0817 - val_loss: -4.3214\n",
      "Epoch 1027/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1184 - val_loss: -3.9550\n",
      "Epoch 1028/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1846 - val_loss: -3.2106\n",
      "Epoch 1029/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2355 - val_loss: -4.3395\n",
      "Epoch 1030/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1253 - val_loss: -4.1325\n",
      "Epoch 1031/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2066 - val_loss: -4.5486\n",
      "Epoch 1032/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1160 - val_loss: -4.2742\n",
      "Epoch 1033/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2772 - val_loss: -4.4148\n",
      "Epoch 1034/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2609 - val_loss: -4.0343\n",
      "Epoch 1035/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1992 - val_loss: -4.2833\n",
      "Epoch 1036/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1043 - val_loss: -4.2451\n",
      "Epoch 1037/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2947 - val_loss: -3.3680\n",
      "Epoch 1038/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1343 - val_loss: -4.4458\n",
      "Epoch 1039/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2199 - val_loss: -4.1866\n",
      "Epoch 1040/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2351 - val_loss: -2.7733\n",
      "Epoch 1041/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2646 - val_loss: -3.8179\n",
      "Epoch 1042/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0044 - val_loss: -4.2498\n",
      "Epoch 1043/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1151 - val_loss: -4.5979\n",
      "Epoch 1044/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8216 - val_loss: -4.3164\n",
      "Epoch 1045/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1356 - val_loss: -3.6590\n",
      "Epoch 1046/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0943 - val_loss: -4.4606\n",
      "Epoch 1047/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1398 - val_loss: -4.1082\n",
      "Epoch 1048/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0942 - val_loss: -4.2618\n",
      "Epoch 1049/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1534 - val_loss: -3.7695\n",
      "Epoch 1050/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1763 - val_loss: -3.8387\n",
      "Epoch 1051/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1771 - val_loss: -3.9654\n",
      "Epoch 1052/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0613 - val_loss: -4.2973\n",
      "Epoch 1053/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1753 - val_loss: -3.6560\n",
      "Epoch 1054/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1854 - val_loss: -3.9553\n",
      "Epoch 1055/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1588 - val_loss: -3.1275\n",
      "Epoch 1056/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0639 - val_loss: -4.4316\n",
      "Epoch 1057/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1795 - val_loss: -3.2017\n",
      "Epoch 1058/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1605 - val_loss: -4.0380\n",
      "Epoch 1059/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1143 - val_loss: -3.9991\n",
      "Epoch 1060/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2108 - val_loss: -3.8828\n",
      "Epoch 1061/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1736 - val_loss: -4.1436\n",
      "Epoch 1062/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2431 - val_loss: -3.4582\n",
      "Epoch 1063/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1909 - val_loss: -3.4467\n",
      "Epoch 1064/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2283 - val_loss: -3.3522\n",
      "Epoch 1065/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2147 - val_loss: -3.9913\n",
      "Epoch 1066/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1703 - val_loss: -4.0249\n",
      "Epoch 1067/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2158 - val_loss: -4.0324\n",
      "Epoch 1068/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3367 - val_loss: -3.0991\n",
      "Epoch 1069/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.5109 - val_loss: -2.4925\n",
      "Epoch 1070/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9869 - val_loss: -4.3385\n",
      "Epoch 1071/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0868 - val_loss: -4.0478\n",
      "Epoch 1072/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2178 - val_loss: -3.4274\n",
      "Epoch 1073/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2547 - val_loss: -3.0761\n",
      "Epoch 1074/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1661 - val_loss: -4.4026\n",
      "Epoch 1075/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8456 - val_loss: -4.2906\n",
      "Epoch 1076/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9622 - val_loss: -4.2125\n",
      "Epoch 1077/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0475 - val_loss: -3.6422\n",
      "Epoch 1078/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1933 - val_loss: -4.4403\n",
      "Epoch 1079/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1772 - val_loss: -3.6080\n",
      "Epoch 1080/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8843 - val_loss: -2.4711\n",
      "Epoch 1081/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.5908 - val_loss: -3.5874\n",
      "Epoch 1082/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7036 - val_loss: -4.1611\n",
      "Epoch 1083/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9218 - val_loss: -3.0174\n",
      "Epoch 1084/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0501 - val_loss: -3.7840\n",
      "Epoch 1085/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0980 - val_loss: -3.9640\n",
      "Epoch 1086/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0669 - val_loss: -3.5755\n",
      "Epoch 1087/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9817 - val_loss: -4.3446\n",
      "Epoch 1088/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0020 - val_loss: -4.3114\n",
      "Epoch 1089/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0261 - val_loss: -4.0333\n",
      "Epoch 1090/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1793 - val_loss: -3.7121\n",
      "Epoch 1091/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1976 - val_loss: -4.3675\n",
      "Epoch 1092/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9860 - val_loss: -4.5524\n",
      "Epoch 1093/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1982 - val_loss: -4.5286\n",
      "Epoch 1094/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1535 - val_loss: -4.4461\n",
      "Epoch 1095/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1491 - val_loss: -4.3268\n",
      "Epoch 1096/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1548 - val_loss: -3.9223\n",
      "Epoch 1097/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1954 - val_loss: -4.1671\n",
      "Epoch 1098/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2085 - val_loss: -4.3151\n",
      "Epoch 1099/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1669 - val_loss: -3.7903\n",
      "Epoch 1100/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0950 - val_loss: -4.1073\n",
      "Epoch 1101/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2102 - val_loss: -4.6349\n",
      "Epoch 1102/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0828 - val_loss: -4.3489\n",
      "Epoch 1103/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1567 - val_loss: -4.2368\n",
      "Epoch 1104/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2124"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -5.2124 - val_loss: -4.7069\n",
      "Epoch 1105/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1642 - val_loss: -2.8769\n",
      "Epoch 1106/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2857 - val_loss: -4.2427\n",
      "Epoch 1107/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2477 - val_loss: -3.1605\n",
      "Epoch 1108/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2526 - val_loss: -4.3641\n",
      "Epoch 1109/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0909 - val_loss: -3.9097\n",
      "Epoch 1110/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2315 - val_loss: -3.1771\n",
      "Epoch 1111/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1964 - val_loss: -4.0554\n",
      "Epoch 1112/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2060 - val_loss: -4.0355\n",
      "Epoch 1113/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2719 - val_loss: -3.4045\n",
      "Epoch 1114/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1506 - val_loss: -3.9083\n",
      "Epoch 1115/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1065 - val_loss: -3.8443\n",
      "Epoch 1116/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2004 - val_loss: -4.0135\n",
      "Epoch 1117/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2020 - val_loss: -4.5825\n",
      "Epoch 1118/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3058 - val_loss: -4.4200\n",
      "Epoch 1119/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2456 - val_loss: -4.5014\n",
      "Epoch 1120/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1385 - val_loss: -4.3566\n",
      "Epoch 1121/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2124 - val_loss: -4.3922\n",
      "Epoch 1122/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1907 - val_loss: -3.0604\n",
      "Epoch 1123/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2255 - val_loss: -3.5255\n",
      "Epoch 1124/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2900 - val_loss: -0.9401\n",
      "Epoch 1125/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3417 - val_loss: -1.7815\n",
      "Epoch 1126/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1596 - val_loss: -4.1778\n",
      "Epoch 1127/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2442 - val_loss: -4.2995\n",
      "Epoch 1128/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2514 - val_loss: -2.2596\n",
      "Epoch 1129/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1956 - val_loss: -3.5014\n",
      "Epoch 1130/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2573"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -5.2573 - val_loss: -4.7560\n",
      "Epoch 1131/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9321 - val_loss: -3.9578\n",
      "Epoch 1132/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1535 - val_loss: -4.3475\n",
      "Epoch 1133/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1874 - val_loss: -4.5599\n",
      "Epoch 1134/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2499 - val_loss: -4.4332\n",
      "Epoch 1135/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2136 - val_loss: -4.4465\n",
      "Epoch 1136/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3099 - val_loss: -1.4903\n",
      "Epoch 1137/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2335 - val_loss: -3.9490\n",
      "Epoch 1138/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2330 - val_loss: -4.0917\n",
      "Epoch 1139/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2236 - val_loss: -3.5443\n",
      "Epoch 1140/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2458 - val_loss: -4.5755\n",
      "Epoch 1141/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2802 - val_loss: -3.9092\n",
      "Epoch 1142/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1942 - val_loss: -4.3456\n",
      "Epoch 1143/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2725 - val_loss: -4.0878\n",
      "Epoch 1144/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3134 - val_loss: -4.1259\n",
      "Epoch 1145/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2629 - val_loss: -4.5361\n",
      "Epoch 1146/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0798 - val_loss: -3.7048\n",
      "Epoch 1147/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2928 - val_loss: -3.6359\n",
      "Epoch 1148/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3336 - val_loss: -2.7650\n",
      "Epoch 1149/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2029 - val_loss: -4.0488\n",
      "Epoch 1150/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2624 - val_loss: -3.6966\n",
      "Epoch 1151/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3225 - val_loss: -4.5752\n",
      "Epoch 1152/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2024 - val_loss: -3.4653\n",
      "Epoch 1153/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2598 - val_loss: -4.2340\n",
      "Epoch 1154/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2410 - val_loss: -3.9194\n",
      "Epoch 1155/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2556 - val_loss: -3.3237\n",
      "Epoch 1156/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3036 - val_loss: -2.9468\n",
      "Epoch 1157/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1873 - val_loss: -3.6299\n",
      "Epoch 1158/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2102 - val_loss: -3.3882\n",
      "Epoch 1159/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2881 - val_loss: -4.6107\n",
      "Epoch 1160/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1977 - val_loss: -4.4971\n",
      "Epoch 1161/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3298 - val_loss: -4.2574\n",
      "Epoch 1162/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1625 - val_loss: -4.2268\n",
      "Epoch 1163/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3688 - val_loss: -4.3062\n",
      "Epoch 1164/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2714 - val_loss: -3.9767\n",
      "Epoch 1165/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2513 - val_loss: -4.2268\n",
      "Epoch 1166/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3163 - val_loss: -3.9585\n",
      "Epoch 1167/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2397 - val_loss: -3.3318\n",
      "Epoch 1168/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2868 - val_loss: -3.1450\n",
      "Epoch 1169/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1306 - val_loss: -2.8276\n",
      "Epoch 1170/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1020 - val_loss: -4.2722\n",
      "Epoch 1171/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8318 - val_loss: -3.5348\n",
      "Epoch 1172/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9086 - val_loss: -2.9841\n",
      "Epoch 1173/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0710 - val_loss: -3.3380\n",
      "Epoch 1174/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1642 - val_loss: -4.1097\n",
      "Epoch 1175/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1250 - val_loss: -4.4258\n",
      "Epoch 1176/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1318 - val_loss: -1.3954\n",
      "Epoch 1177/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2237 - val_loss: -2.0685\n",
      "Epoch 1178/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1954 - val_loss: -4.2431\n",
      "Epoch 1179/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1948 - val_loss: -4.3177\n",
      "Epoch 1180/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1871 - val_loss: -3.8107\n",
      "Epoch 1181/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0957 - val_loss: -4.1149\n",
      "Epoch 1182/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2164 - val_loss: -4.2840\n",
      "Epoch 1183/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2550 - val_loss: -3.9432\n",
      "Epoch 1184/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2581 - val_loss: -3.5473\n",
      "Epoch 1185/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2812 - val_loss: -3.7787\n",
      "Epoch 1186/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2466 - val_loss: -2.2083\n",
      "Epoch 1187/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1745 - val_loss: -4.3854\n",
      "Epoch 1188/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2719 - val_loss: -4.3485\n",
      "Epoch 1189/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2357 - val_loss: -4.2738\n",
      "Epoch 1190/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2421 - val_loss: -4.0332\n",
      "Epoch 1191/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3154 - val_loss: -4.5417\n",
      "Epoch 1192/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2355 - val_loss: -3.8611\n",
      "Epoch 1193/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5377 - val_loss: -4.4548\n",
      "Epoch 1194/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8813 - val_loss: -3.9695\n",
      "Epoch 1195/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9567 - val_loss: -4.4454\n",
      "Epoch 1196/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0954 - val_loss: -3.9420\n",
      "Epoch 1197/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0936 - val_loss: -2.1645\n",
      "Epoch 1198/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1944 - val_loss: -4.4525\n",
      "Epoch 1199/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1910 - val_loss: -4.6356\n",
      "Epoch 1200/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1456 - val_loss: -2.2607\n",
      "Epoch 1201/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2460 - val_loss: -3.9126\n",
      "Epoch 1202/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1602 - val_loss: -4.2723\n",
      "Epoch 1203/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2770 - val_loss: -4.0231\n",
      "Epoch 1204/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1022 - val_loss: -1.9236\n",
      "Epoch 1205/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1149 - val_loss: -4.1104\n",
      "Epoch 1206/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1681 - val_loss: -4.2433\n",
      "Epoch 1207/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1947 - val_loss: -4.2500\n",
      "Epoch 1208/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3056 - val_loss: -3.9844\n",
      "Epoch 1209/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1489 - val_loss: -3.1641\n",
      "Epoch 1210/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9828 - val_loss: -3.2356\n",
      "Epoch 1211/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2501 - val_loss: -3.8956\n",
      "Epoch 1212/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0331 - val_loss: -3.7628\n",
      "Epoch 1213/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2586 - val_loss: -4.6352\n",
      "Epoch 1214/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1333 - val_loss: -4.2857\n",
      "Epoch 1215/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3004 - val_loss: -4.2125\n",
      "Epoch 1216/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1777 - val_loss: -2.7583\n",
      "Epoch 1217/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2476 - val_loss: -3.8380\n",
      "Epoch 1218/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2860 - val_loss: -3.9716\n",
      "Epoch 1219/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2446 - val_loss: -4.4003\n",
      "Epoch 1220/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3032 - val_loss: -3.0519\n",
      "Epoch 1221/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2411 - val_loss: -4.7227\n",
      "Epoch 1222/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3237 - val_loss: -4.3172\n",
      "Epoch 1223/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2352 - val_loss: -3.5684\n",
      "Epoch 1224/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2699 - val_loss: -3.8494\n",
      "Epoch 1225/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2365 - val_loss: -4.0766\n",
      "Epoch 1226/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2795 - val_loss: -3.4111\n",
      "Epoch 1227/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2465 - val_loss: -4.2149\n",
      "Epoch 1228/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9140 - val_loss: -2.6613\n",
      "Epoch 1229/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4724 - val_loss: -3.7093\n",
      "Epoch 1230/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1903 - val_loss: -4.4573\n",
      "Epoch 1231/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2977 - val_loss: -3.5714\n",
      "Epoch 1232/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2327 - val_loss: -4.2189\n",
      "Epoch 1233/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3073 - val_loss: -1.4652\n",
      "Epoch 1234/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3045 - val_loss: -4.2648\n",
      "Epoch 1235/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2994 - val_loss: 1.3856\n",
      "Epoch 1236/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2564 - val_loss: -3.4814\n",
      "Epoch 1237/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2067 - val_loss: -2.7686\n",
      "Epoch 1238/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2070 - val_loss: -3.8014\n",
      "Epoch 1239/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3142 - val_loss: -4.0646\n",
      "Epoch 1240/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3376 - val_loss: -3.8974\n",
      "Epoch 1241/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2416 - val_loss: -4.3719\n",
      "Epoch 1242/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2657 - val_loss: -3.8740\n",
      "Epoch 1243/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3160 - val_loss: -3.0762\n",
      "Epoch 1244/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0957 - val_loss: -4.4193\n",
      "Epoch 1245/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3316 - val_loss: -2.7290\n",
      "Epoch 1246/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1339 - val_loss: -4.1260\n",
      "Epoch 1247/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2986 - val_loss: -4.1928\n",
      "Epoch 1248/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3219 - val_loss: -2.0316\n",
      "Epoch 1249/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2926 - val_loss: -4.2093\n",
      "Epoch 1250/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2951 - val_loss: -4.1712\n",
      "Epoch 1251/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3172 - val_loss: -4.5084\n",
      "Epoch 1252/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3072 - val_loss: -3.4925\n",
      "Epoch 1253/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1608 - val_loss: -4.0042\n",
      "Epoch 1254/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3046 - val_loss: -3.6959\n",
      "Epoch 1255/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3103 - val_loss: -4.3266\n",
      "Epoch 1256/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2855 - val_loss: -3.9278\n",
      "Epoch 1257/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3317 - val_loss: -4.3962\n",
      "Epoch 1258/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2649 - val_loss: -4.2912\n",
      "Epoch 1259/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8920 - val_loss: -2.4886\n",
      "Epoch 1260/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3520 - val_loss: -3.4267\n",
      "Epoch 1261/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0346 - val_loss: -4.5325\n",
      "Epoch 1262/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0920 - val_loss: -4.4775\n",
      "Epoch 1263/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2178 - val_loss: -4.1104\n",
      "Epoch 1264/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2523 - val_loss: -3.2305\n",
      "Epoch 1265/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1458 - val_loss: -3.8555\n",
      "Epoch 1266/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2404 - val_loss: -4.0143\n",
      "Epoch 1267/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1232 - val_loss: -4.4087\n",
      "Epoch 1268/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2783 - val_loss: -4.2320\n",
      "Epoch 1269/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2822 - val_loss: -4.3079\n",
      "Epoch 1270/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2687 - val_loss: -3.8273\n",
      "Epoch 1271/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2796 - val_loss: -3.8200\n",
      "Epoch 1272/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7467 - val_loss: -4.3343\n",
      "Epoch 1273/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1986 - val_loss: -4.5648\n",
      "Epoch 1274/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2949 - val_loss: -4.5330\n",
      "Epoch 1275/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2972 - val_loss: -3.9939\n",
      "Epoch 1276/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8463 - val_loss: -3.5900\n",
      "Epoch 1277/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1857 - val_loss: -4.1764\n",
      "Epoch 1278/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2199 - val_loss: -4.1274\n",
      "Epoch 1279/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1938 - val_loss: -4.0834\n",
      "Epoch 1280/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2843 - val_loss: -4.2431\n",
      "Epoch 1281/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2418 - val_loss: -4.0199\n",
      "Epoch 1282/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0277 - val_loss: -4.4792\n",
      "Epoch 1283/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3466 - val_loss: -4.1800\n",
      "Epoch 1284/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1032 - val_loss: -3.7328\n",
      "Epoch 1285/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1876 - val_loss: -4.4134\n",
      "Epoch 1286/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2948 - val_loss: -3.7378\n",
      "Epoch 1287/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2786 - val_loss: -1.2768\n",
      "Epoch 1288/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2950 - val_loss: -4.1623\n",
      "Epoch 1289/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2476 - val_loss: -3.4099\n",
      "Epoch 1290/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1248 - val_loss: -3.5447\n",
      "Epoch 1291/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3196 - val_loss: -2.3570\n",
      "Epoch 1292/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2719 - val_loss: -4.0865\n",
      "Epoch 1293/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2510 - val_loss: -0.8520\n",
      "Epoch 1294/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3233 - val_loss: -3.8260\n",
      "Epoch 1295/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3090 - val_loss: -4.3013\n",
      "Epoch 1296/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2871 - val_loss: -3.9928\n",
      "Epoch 1297/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2929 - val_loss: -2.0089\n",
      "Epoch 1298/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9873 - val_loss: -3.8732\n",
      "Epoch 1299/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3491 - val_loss: -4.3376\n",
      "Epoch 1300/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3074 - val_loss: -2.0819\n",
      "Epoch 1301/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1516 - val_loss: -3.9663\n",
      "Epoch 1302/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2286 - val_loss: -3.7217\n",
      "Epoch 1303/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3215 - val_loss: -4.3641\n",
      "Epoch 1304/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2416 - val_loss: -4.1782\n",
      "Epoch 1305/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3752 - val_loss: -4.1760\n",
      "Epoch 1306/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2802 - val_loss: -4.3091\n",
      "Epoch 1307/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3386 - val_loss: -4.2467\n",
      "Epoch 1308/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3078 - val_loss: -4.2032\n",
      "Epoch 1309/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3387 - val_loss: -4.1635\n",
      "Epoch 1310/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0621 - val_loss: -4.3366\n",
      "Epoch 1311/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2880 - val_loss: -2.1578\n",
      "Epoch 1312/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3487 - val_loss: -4.0813\n",
      "Epoch 1313/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3149 - val_loss: -4.2058\n",
      "Epoch 1314/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2928 - val_loss: -3.4195\n",
      "Epoch 1315/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2766 - val_loss: -4.3917\n",
      "Epoch 1316/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4065 - val_loss: -3.4115\n",
      "Epoch 1317/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2067 - val_loss: -4.6117\n",
      "Epoch 1318/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3445 - val_loss: -4.1311\n",
      "Epoch 1319/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3406 - val_loss: -3.9983\n",
      "Epoch 1320/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2983 - val_loss: -4.5830\n",
      "Epoch 1321/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4240 - val_loss: -3.6894\n",
      "Epoch 1322/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3376 - val_loss: -4.6628\n",
      "Epoch 1323/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3898 - val_loss: -4.2804\n",
      "Epoch 1324/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3969 - val_loss: -4.2511\n",
      "Epoch 1325/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1928 - val_loss: -3.9565\n",
      "Epoch 1326/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4195 - val_loss: -3.7582\n",
      "Epoch 1327/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3287 - val_loss: -4.1107\n",
      "Epoch 1328/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2902 - val_loss: -4.1558\n",
      "Epoch 1329/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3421 - val_loss: -4.4347\n",
      "Epoch 1330/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1417 - val_loss: -3.6884\n",
      "Epoch 1331/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2857 - val_loss: -3.2137\n",
      "Epoch 1332/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3359 - val_loss: -4.1859\n",
      "Epoch 1333/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3209 - val_loss: -3.7623\n",
      "Epoch 1334/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2966 - val_loss: -1.8062\n",
      "Epoch 1335/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.6614 - val_loss: -3.9529\n",
      "Epoch 1336/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6177 - val_loss: -4.5580\n",
      "Epoch 1337/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1105 - val_loss: -4.1463\n",
      "Epoch 1338/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0601 - val_loss: -3.6436\n",
      "Epoch 1339/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9533 - val_loss: -3.5717\n",
      "Epoch 1340/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1563 - val_loss: -2.1996\n",
      "Epoch 1341/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2597 - val_loss: -4.1269\n",
      "Epoch 1342/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2229 - val_loss: -3.5625\n",
      "Epoch 1343/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2230 - val_loss: -4.4764\n",
      "Epoch 1344/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3118 - val_loss: -4.4338\n",
      "Epoch 1345/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2122 - val_loss: -4.6341\n",
      "Epoch 1346/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2961 - val_loss: -4.0828\n",
      "Epoch 1347/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2733 - val_loss: -4.5249\n",
      "Epoch 1348/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2823 - val_loss: -4.2349\n",
      "Epoch 1349/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3433 - val_loss: -4.0141\n",
      "Epoch 1350/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2145 - val_loss: -4.3110\n",
      "Epoch 1351/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3833 - val_loss: -4.0158\n",
      "Epoch 1352/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0915 - val_loss: -3.4037\n",
      "Epoch 1353/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2653 - val_loss: -4.4598\n",
      "Epoch 1354/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3232 - val_loss: -4.1609\n",
      "Epoch 1355/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2225 - val_loss: -4.4519\n",
      "Epoch 1356/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2834 - val_loss: -4.2009\n",
      "Epoch 1357/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9695 - val_loss: -4.3695\n",
      "Epoch 1358/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2704 - val_loss: -4.1528\n",
      "Epoch 1359/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3161 - val_loss: -4.1690\n",
      "Epoch 1360/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3073 - val_loss: -4.2457\n",
      "Epoch 1361/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2864 - val_loss: -3.8417\n",
      "Epoch 1362/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2469 - val_loss: -4.3858\n",
      "Epoch 1363/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3708 - val_loss: -3.9607\n",
      "Epoch 1364/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3290 - val_loss: -3.4287\n",
      "Epoch 1365/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3576 - val_loss: -4.0420\n",
      "Epoch 1366/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3267 - val_loss: -4.0182\n",
      "Epoch 1367/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3328 - val_loss: -4.2890\n",
      "Epoch 1368/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3915 - val_loss: -3.2611\n",
      "Epoch 1369/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3537 - val_loss: -4.3671\n",
      "Epoch 1370/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3360 - val_loss: -4.0156\n",
      "Epoch 1371/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3026 - val_loss: -4.4209\n",
      "Epoch 1372/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3508 - val_loss: -4.5917\n",
      "Epoch 1373/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4152 - val_loss: -3.8817\n",
      "Epoch 1374/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3769 - val_loss: -1.2481\n",
      "Epoch 1375/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.0450 - val_loss: -4.4631\n",
      "Epoch 1376/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3190 - val_loss: -4.0507\n",
      "Epoch 1377/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3800 - val_loss: -4.1446\n",
      "Epoch 1378/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2959 - val_loss: 0.1173\n",
      "Epoch 1379/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2140 - val_loss: -4.3534\n",
      "Epoch 1380/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2787 - val_loss: -3.2170\n",
      "Epoch 1381/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3680 - val_loss: -3.8829\n",
      "Epoch 1382/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3424"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -5.3424 - val_loss: -4.8014\n",
      "Epoch 1383/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3307 - val_loss: -4.4504\n",
      "Epoch 1384/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3808 - val_loss: -4.0312\n",
      "Epoch 1385/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2161 - val_loss: -4.1530\n",
      "Epoch 1386/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3372 - val_loss: -2.1303\n",
      "Epoch 1387/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3849 - val_loss: -2.4226\n",
      "Epoch 1388/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3226 - val_loss: -4.5958\n",
      "Epoch 1389/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3877 - val_loss: -3.5269\n",
      "Epoch 1390/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4169 - val_loss: -4.2535\n",
      "Epoch 1391/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2950 - val_loss: -4.2534\n",
      "Epoch 1392/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1960 - val_loss: -4.3826\n",
      "Epoch 1393/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1686 - val_loss: -3.8306\n",
      "Epoch 1394/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2232 - val_loss: -3.4190\n",
      "Epoch 1395/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3518 - val_loss: -4.1280\n",
      "Epoch 1396/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3525 - val_loss: -3.9205\n",
      "Epoch 1397/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0964 - val_loss: -4.6298\n",
      "Epoch 1398/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3920 - val_loss: -2.1640\n",
      "Epoch 1399/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3596 - val_loss: -3.3652\n",
      "Epoch 1400/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2907 - val_loss: -4.6797\n",
      "Epoch 1401/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3729 - val_loss: -4.0567\n",
      "Epoch 1402/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3734 - val_loss: -4.2947\n",
      "Epoch 1403/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1278 - val_loss: 1.4614\n",
      "Epoch 1404/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3430 - val_loss: -4.5272\n",
      "Epoch 1405/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3378 - val_loss: -4.6474\n",
      "Epoch 1406/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4402 - val_loss: -4.2342\n",
      "Epoch 1407/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2810 - val_loss: -4.6101\n",
      "Epoch 1408/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4621 - val_loss: -4.5489\n",
      "Epoch 1409/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2192 - val_loss: -4.2194\n",
      "Epoch 1410/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2543 - val_loss: -4.4752\n",
      "Epoch 1411/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3458 - val_loss: -4.3269\n",
      "Epoch 1412/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.4053"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -5.4053 - val_loss: -4.8369\n",
      "Epoch 1413/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3269 - val_loss: -4.4678\n",
      "Epoch 1414/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3511 - val_loss: -3.9022\n",
      "Epoch 1415/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4062 - val_loss: -3.9772\n",
      "Epoch 1416/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2788 - val_loss: -4.4635\n",
      "Epoch 1417/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3597 - val_loss: -3.8295\n",
      "Epoch 1418/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3138 - val_loss: -4.3735\n",
      "Epoch 1419/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3100 - val_loss: -4.1426\n",
      "Epoch 1420/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3829 - val_loss: -4.5540\n",
      "Epoch 1421/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4040 - val_loss: -4.5350\n",
      "Epoch 1422/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4190 - val_loss: -4.6025\n",
      "Epoch 1423/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2805 - val_loss: -3.6330\n",
      "Epoch 1424/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3236 - val_loss: -4.0488\n",
      "Epoch 1425/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4675 - val_loss: -4.1581\n",
      "Epoch 1426/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1829 - val_loss: -4.4375\n",
      "Epoch 1427/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3677 - val_loss: -4.0590\n",
      "Epoch 1428/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4206 - val_loss: -4.7137\n",
      "Epoch 1429/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3560 - val_loss: -3.9450\n",
      "Epoch 1430/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3147 - val_loss: -3.9023\n",
      "Epoch 1431/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3181 - val_loss: -3.4102\n",
      "Epoch 1432/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3515 - val_loss: -4.3818\n",
      "Epoch 1433/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4198 - val_loss: -4.5869\n",
      "Epoch 1434/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3876 - val_loss: -3.4072\n",
      "Epoch 1435/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2094 - val_loss: -3.3821\n",
      "Epoch 1436/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3745 - val_loss: -4.1086\n",
      "Epoch 1437/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4154 - val_loss: -2.9284\n",
      "Epoch 1438/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2602 - val_loss: -4.1443\n",
      "Epoch 1439/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3823 - val_loss: -3.7145\n",
      "Epoch 1440/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4629 - val_loss: -4.6169\n",
      "Epoch 1441/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3563 - val_loss: -3.1738\n",
      "Epoch 1442/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3815 - val_loss: -4.5079\n",
      "Epoch 1443/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4055 - val_loss: -4.0908\n",
      "Epoch 1444/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4762 - val_loss: -4.1546\n",
      "Epoch 1445/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4073 - val_loss: -3.9180\n",
      "Epoch 1446/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3825 - val_loss: -4.3533\n",
      "Epoch 1447/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3516 - val_loss: -4.4698\n",
      "Epoch 1448/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2123 - val_loss: -4.5890\n",
      "Epoch 1449/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7628 - val_loss: -4.4838\n",
      "Epoch 1450/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2137 - val_loss: -4.2011\n",
      "Epoch 1451/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4070 - val_loss: -2.9454\n",
      "Epoch 1452/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3231 - val_loss: -4.2631\n",
      "Epoch 1453/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2909 - val_loss: -4.6343\n",
      "Epoch 1454/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4263 - val_loss: -4.1193\n",
      "Epoch 1455/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3968"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_170_layer_call_fn, lstm_cell_170_layer_call_and_return_conditional_losses, lstm_cell_171_layer_call_fn, lstm_cell_171_layer_call_and_return_conditional_losses, lstm_cell_172_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -5.3968 - val_loss: -4.8581\n",
      "Epoch 1456/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3633 - val_loss: -3.8779\n",
      "Epoch 1457/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3893 - val_loss: -4.3859\n",
      "Epoch 1458/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.2653 - val_loss: -2.0191\n",
      "Epoch 1459/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.2073 - val_loss: -3.4887\n",
      "Epoch 1460/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0674 - val_loss: -3.9540\n",
      "Epoch 1461/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3227 - val_loss: -4.0924\n",
      "Epoch 1462/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5129 - val_loss: -3.9946\n",
      "Epoch 1463/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5951 - val_loss: -3.5122\n",
      "Epoch 1464/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5807 - val_loss: -4.0832\n",
      "Epoch 1465/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7289 - val_loss: 0.4141\n",
      "Epoch 1466/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7386 - val_loss: -4.3660\n",
      "Epoch 1467/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8404 - val_loss: -3.6631\n",
      "Epoch 1468/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6849 - val_loss: -3.2535\n",
      "Epoch 1469/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8829 - val_loss: -4.3955\n",
      "Epoch 1470/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8328 - val_loss: -3.9910\n",
      "Epoch 1471/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7201 - val_loss: -4.0913\n",
      "Epoch 1472/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8431 - val_loss: -3.0978\n",
      "Epoch 1473/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8273 - val_loss: -4.0598\n",
      "Epoch 1474/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8744 - val_loss: -4.1760\n",
      "Epoch 1475/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7636 - val_loss: -3.3323\n",
      "Epoch 1476/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8168 - val_loss: -4.1948\n",
      "Epoch 1477/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9268 - val_loss: -4.1676\n",
      "Epoch 1478/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8365 - val_loss: -3.8390\n",
      "Epoch 1479/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9742 - val_loss: -4.3887\n",
      "Epoch 1480/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9839 - val_loss: -3.8286\n",
      "Epoch 1481/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0264 - val_loss: -3.8362\n",
      "Epoch 1482/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9693 - val_loss: -4.0550\n",
      "Epoch 1483/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9317 - val_loss: -4.1125\n",
      "Epoch 1484/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0960 - val_loss: -4.3832\n",
      "Epoch 1485/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9726 - val_loss: -4.5894\n",
      "Epoch 1486/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1004 - val_loss: -2.4117\n",
      "Epoch 1487/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9852 - val_loss: -4.1588\n",
      "Epoch 1488/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1236 - val_loss: -4.0375\n",
      "Epoch 1489/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9977 - val_loss: -4.5597\n",
      "Epoch 1490/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1279 - val_loss: -4.2333\n",
      "Epoch 1491/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1854 - val_loss: -4.3651\n",
      "Epoch 1492/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9729 - val_loss: -4.1220\n",
      "Epoch 1493/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1896 - val_loss: -4.1999\n",
      "Epoch 1494/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1481 - val_loss: -4.3207\n",
      "Epoch 1495/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0321 - val_loss: -4.0735\n",
      "Epoch 1496/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1363 - val_loss: -4.5456\n",
      "Epoch 1497/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1472 - val_loss: -4.3654\n",
      "Epoch 1498/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0141 - val_loss: -4.5545\n",
      "Epoch 1499/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9357 - val_loss: -3.9393\n",
      "Epoch 1500/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1550 - val_loss: -1.7616\n",
      "Epoch 1501/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1619 - val_loss: -4.4957\n",
      "Epoch 1502/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0823 - val_loss: -3.4333\n",
      "Epoch 1503/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2317 - val_loss: -4.4937\n",
      "Epoch 1504/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1945 - val_loss: -3.3548\n",
      "Epoch 1505/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2126 - val_loss: -4.1563\n",
      "Epoch 1506/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2607 - val_loss: -3.1960\n",
      "Epoch 1507/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1852 - val_loss: -4.4340\n",
      "Epoch 1508/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2607 - val_loss: -4.2606\n",
      "Epoch 1509/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2620 - val_loss: -3.3891\n",
      "Epoch 1510/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1842 - val_loss: -4.3506\n",
      "Epoch 1511/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2038 - val_loss: -3.4872\n",
      "Epoch 1512/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1978 - val_loss: -3.9803\n",
      "Epoch 1513/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1995 - val_loss: -2.8964\n",
      "Epoch 1514/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2061 - val_loss: -4.2848\n",
      "Epoch 1515/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2286 - val_loss: -4.4111\n",
      "Epoch 1516/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1862 - val_loss: -2.4714\n",
      "Epoch 1517/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3024 - val_loss: -4.1064\n",
      "Epoch 1518/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2248 - val_loss: -4.3410\n",
      "Epoch 1519/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2200 - val_loss: -3.3727\n",
      "Epoch 1520/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1912 - val_loss: -3.7352\n",
      "Epoch 1521/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2966 - val_loss: -4.1704\n",
      "Epoch 1522/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9851 - val_loss: -4.3367\n",
      "Epoch 1523/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2432 - val_loss: -2.2755\n",
      "Epoch 1524/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5130 - val_loss: -2.8542\n",
      "Epoch 1525/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6986 - val_loss: -4.1354\n",
      "Epoch 1526/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0767 - val_loss: -4.2009\n",
      "Epoch 1527/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1292 - val_loss: -3.7547\n",
      "Epoch 1528/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0784 - val_loss: -4.3328\n",
      "Epoch 1529/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1482 - val_loss: -1.3665\n",
      "Epoch 1530/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1222 - val_loss: -4.1797\n",
      "Epoch 1531/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1016 - val_loss: -3.2858\n",
      "Epoch 1532/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1146 - val_loss: -4.3608\n",
      "Epoch 1533/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1950 - val_loss: -2.0770\n",
      "Epoch 1534/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1405 - val_loss: -4.1142\n",
      "Epoch 1535/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2364 - val_loss: -4.1011\n",
      "Epoch 1536/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1946 - val_loss: -3.5945\n",
      "Epoch 1537/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1916 - val_loss: -3.8483\n",
      "Epoch 1538/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2680 - val_loss: -4.3292\n",
      "Epoch 1539/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2926 - val_loss: -3.6346\n",
      "Epoch 1540/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1872 - val_loss: -4.3555\n",
      "Epoch 1541/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0442 - val_loss: -4.1795\n",
      "Epoch 1542/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2729 - val_loss: -4.5848\n",
      "Epoch 1543/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1169 - val_loss: -3.8343\n",
      "Epoch 1544/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2408 - val_loss: -4.4097\n",
      "Epoch 1545/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1363 - val_loss: -3.9199\n",
      "Epoch 1546/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1322 - val_loss: -4.0658\n",
      "Epoch 1547/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2089 - val_loss: -4.1754\n",
      "Epoch 1548/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2207 - val_loss: -4.3573\n",
      "Epoch 1549/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2912 - val_loss: -4.1595\n",
      "Epoch 1550/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2023 - val_loss: -4.3346\n",
      "Epoch 1551/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2835 - val_loss: -4.6214\n",
      "Epoch 1552/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2499 - val_loss: -3.4339\n",
      "Epoch 1553/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2205 - val_loss: -4.2457\n",
      "Epoch 1554/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3260 - val_loss: -4.2156\n",
      "Epoch 1555/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2300 - val_loss: -3.3743\n",
      "Epoch 1556/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3093 - val_loss: -4.2048\n",
      "Epoch 1557/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3267 - val_loss: -3.5255\n",
      "Epoch 1558/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1752 - val_loss: -3.8209\n",
      "Epoch 1559/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2948 - val_loss: -3.6868\n",
      "Epoch 1560/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2858 - val_loss: -3.9326\n",
      "Epoch 1561/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1219 - val_loss: -4.1415\n",
      "Epoch 1562/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1592 - val_loss: -2.7298\n",
      "Epoch 1563/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7749 - val_loss: -4.3257\n",
      "Epoch 1564/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0493 - val_loss: -4.5414\n",
      "Epoch 1565/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1981 - val_loss: -1.7845\n",
      "Epoch 1566/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2464 - val_loss: -4.0858\n",
      "Epoch 1567/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2775 - val_loss: -4.4871\n",
      "Epoch 1568/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0316 - val_loss: -3.1371\n",
      "Epoch 1569/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2213 - val_loss: -4.2688\n",
      "Epoch 1570/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1946 - val_loss: -4.1421\n",
      "Epoch 1571/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2767 - val_loss: -2.9675\n",
      "Epoch 1572/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2850 - val_loss: -2.7715\n",
      "Epoch 1573/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3063 - val_loss: -3.0602\n",
      "Epoch 1574/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1936 - val_loss: -1.9309\n",
      "Epoch 1575/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3029 - val_loss: -3.6907\n",
      "Epoch 1576/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3370 - val_loss: -4.2510\n",
      "Epoch 1577/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1952 - val_loss: -4.2655\n",
      "Epoch 1578/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2962 - val_loss: -3.8242\n",
      "Epoch 1579/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2975 - val_loss: -4.4943\n",
      "Epoch 1580/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2788 - val_loss: -3.9334\n",
      "Epoch 1581/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3163 - val_loss: -4.3677\n",
      "Epoch 1582/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2682 - val_loss: -4.4866\n",
      "Epoch 1583/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3446 - val_loss: -4.3170\n",
      "Epoch 1584/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2085 - val_loss: -4.3583\n",
      "Epoch 1585/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2932 - val_loss: -4.1517\n",
      "Epoch 1586/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3434 - val_loss: -2.9700\n",
      "Epoch 1587/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2914 - val_loss: -3.6932\n",
      "Epoch 1588/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3691 - val_loss: -4.4387\n",
      "Epoch 1589/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2462 - val_loss: -4.3824\n",
      "Epoch 1590/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2682 - val_loss: -3.7473\n",
      "Epoch 1591/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0120 - val_loss: -3.4106\n",
      "Epoch 1592/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0746 - val_loss: -4.5175\n",
      "Epoch 1593/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2642 - val_loss: -4.2349\n",
      "Epoch 1594/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2393 - val_loss: -4.5244\n",
      "Epoch 1595/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1550 - val_loss: -3.9644\n",
      "Epoch 1596/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3288 - val_loss: -4.5208\n",
      "Epoch 1597/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3655 - val_loss: -4.4334\n",
      "Epoch 1598/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1723 - val_loss: -4.3293\n",
      "Epoch 1599/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2441 - val_loss: -1.5754\n",
      "Epoch 1600/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2265 - val_loss: -2.8583\n",
      "Epoch 1601/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2976 - val_loss: -4.5780\n",
      "Epoch 1602/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3284 - val_loss: -4.3475\n",
      "Epoch 1603/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1120 - val_loss: -1.4440\n",
      "Epoch 1604/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0744 - val_loss: -3.7662\n",
      "Epoch 1605/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2032 - val_loss: -3.1054\n",
      "Epoch 1606/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3464 - val_loss: -3.8051\n",
      "Epoch 1607/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2612 - val_loss: -4.6460\n",
      "Epoch 1608/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2044 - val_loss: -4.2605\n",
      "Epoch 1609/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1928 - val_loss: -3.6581\n",
      "Epoch 1610/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3356 - val_loss: -4.4456\n",
      "Epoch 1611/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3091 - val_loss: -3.8094\n",
      "Epoch 1612/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3591 - val_loss: -4.0244\n",
      "Epoch 1613/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3345 - val_loss: -4.4109\n",
      "Epoch 1614/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3257 - val_loss: -1.9038\n",
      "Epoch 1615/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2528 - val_loss: -3.5263\n",
      "Epoch 1616/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3728 - val_loss: -4.4638\n",
      "Epoch 1617/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3290 - val_loss: -4.1231\n",
      "Epoch 1618/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2664 - val_loss: -3.3966\n",
      "Epoch 1619/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3472 - val_loss: -4.1105\n",
      "Epoch 1620/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3691 - val_loss: -4.3546\n",
      "Epoch 1621/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1469 - val_loss: -4.1417\n",
      "Epoch 1622/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2745 - val_loss: -3.4545\n",
      "Epoch 1623/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3477 - val_loss: -3.5193\n",
      "Epoch 1624/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3749 - val_loss: -4.6558\n",
      "Epoch 1625/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2303 - val_loss: -3.8989\n",
      "Epoch 1626/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4012 - val_loss: -0.3888\n",
      "Epoch 1627/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3130 - val_loss: -2.3974\n",
      "Epoch 1628/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0883 - val_loss: -2.8115\n",
      "Epoch 1629/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3263 - val_loss: -4.6914\n",
      "Epoch 1630/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3779 - val_loss: -4.0831\n",
      "Epoch 1631/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3236 - val_loss: -4.5965\n",
      "Epoch 1632/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3988 - val_loss: -2.8198\n",
      "Epoch 1633/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2155 - val_loss: -4.2126\n",
      "Epoch 1634/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3501 - val_loss: -4.0949\n",
      "Epoch 1635/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1825 - val_loss: -3.8910\n",
      "Epoch 1636/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2544 - val_loss: -4.5425\n",
      "Epoch 1637/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3696 - val_loss: -4.1616\n",
      "Epoch 1638/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5221 - val_loss: -3.9673\n",
      "Epoch 1639/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1043 - val_loss: -4.5725\n",
      "Epoch 1640/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1046 - val_loss: -4.4816\n",
      "Epoch 1641/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2716 - val_loss: -3.5519\n",
      "Epoch 1642/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3146 - val_loss: -3.7666\n",
      "Epoch 1643/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1207 - val_loss: -4.2922\n",
      "Epoch 1644/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2299 - val_loss: -4.6199\n",
      "Epoch 1645/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3578 - val_loss: -4.1045\n",
      "Epoch 1646/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3098 - val_loss: -4.0410\n",
      "Epoch 1647/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3152 - val_loss: -4.5619\n",
      "Epoch 1648/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2601 - val_loss: -4.2410\n",
      "Epoch 1649/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3883 - val_loss: -4.0847\n",
      "Epoch 1650/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3316 - val_loss: -4.0392\n",
      "Epoch 1651/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3335 - val_loss: -3.5538\n",
      "Epoch 1652/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3232 - val_loss: -3.9022\n",
      "Epoch 1653/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4151 - val_loss: -3.8491\n",
      "Epoch 1654/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3205 - val_loss: -4.0533\n",
      "Epoch 1655/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4164 - val_loss: -4.2520\n",
      "Epoch 1656/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3246 - val_loss: -4.5607\n",
      "Epoch 1657/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3388 - val_loss: -4.0331\n",
      "Epoch 1658/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3791 - val_loss: -1.1911\n",
      "Epoch 1659/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3597 - val_loss: -4.2640\n",
      "Epoch 1660/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3800 - val_loss: -4.5521\n",
      "Epoch 1661/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3793 - val_loss: -3.1554\n",
      "Epoch 1662/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3009 - val_loss: -4.4764\n",
      "Epoch 1663/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4139 - val_loss: -4.4483\n",
      "Epoch 1664/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2919 - val_loss: -3.9757\n",
      "Epoch 1665/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2028 - val_loss: -3.8810\n",
      "Epoch 1666/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3885 - val_loss: -4.1335\n",
      "Epoch 1667/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4455 - val_loss: -2.7409\n",
      "Epoch 1668/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3296 - val_loss: -3.1770\n",
      "Epoch 1669/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4459 - val_loss: -4.5123\n",
      "Epoch 1670/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3780 - val_loss: -4.3105\n",
      "Epoch 1671/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2978 - val_loss: -3.7659\n",
      "Epoch 1672/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3054 - val_loss: -4.1285\n",
      "Epoch 1673/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3938 - val_loss: -4.7335\n",
      "Epoch 1674/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3577 - val_loss: -3.1298\n",
      "Epoch 1675/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2475 - val_loss: -2.1474\n",
      "Epoch 1676/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7155 - val_loss: -3.6389\n",
      "Epoch 1677/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9529 - val_loss: -4.3061\n",
      "Epoch 1678/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2030 - val_loss: -4.4860\n",
      "Epoch 1679/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2626 - val_loss: -3.6947\n",
      "Epoch 1680/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3721 - val_loss: -4.0353\n",
      "Epoch 1681/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3808 - val_loss: -4.5025\n",
      "Epoch 1682/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3436 - val_loss: -4.5927\n",
      "Epoch 1683/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3459 - val_loss: -4.4002\n",
      "Epoch 1684/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4188 - val_loss: -2.9864\n",
      "Epoch 1685/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4549 - val_loss: -4.0553\n",
      "Epoch 1686/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3486 - val_loss: -4.0831\n",
      "Epoch 1687/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3768 - val_loss: -3.8381\n",
      "Epoch 1688/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4119 - val_loss: -4.3003\n",
      "Epoch 1689/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3380 - val_loss: -3.7170\n",
      "Epoch 1690/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3008 - val_loss: -3.2706\n",
      "Epoch 1691/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4288 - val_loss: -3.7955\n",
      "Epoch 1692/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4113 - val_loss: -4.5910\n",
      "Epoch 1693/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2504 - val_loss: -3.6807\n",
      "Epoch 1694/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4713 - val_loss: -4.1053\n",
      "Epoch 1695/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4534 - val_loss: -1.8906\n",
      "Epoch 1696/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3894 - val_loss: -2.5764\n",
      "Epoch 1697/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4591 - val_loss: -4.1264\n",
      "Epoch 1698/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3776 - val_loss: -4.2598\n",
      "Epoch 1699/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3561 - val_loss: -4.1041\n",
      "Epoch 1700/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2708 - val_loss: -4.4396\n",
      "Epoch 1701/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2547 - val_loss: -4.5497\n",
      "Epoch 1702/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3687 - val_loss: -4.4109\n",
      "Epoch 1703/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3936 - val_loss: -4.7978\n",
      "Epoch 1704/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3487 - val_loss: -4.0842\n",
      "Epoch 1705/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4050 - val_loss: -2.5318\n",
      "Epoch 1706/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3942 - val_loss: -4.2504\n",
      "Epoch 1707/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4560 - val_loss: -3.0904\n",
      "Epoch 1708/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3068 - val_loss: -3.3633\n",
      "Epoch 1709/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4515 - val_loss: 0.2035\n",
      "Epoch 1710/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2818 - val_loss: -4.1730\n",
      "Epoch 1711/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1058 - val_loss: -3.2215\n",
      "Epoch 1712/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1496 - val_loss: -4.3809\n",
      "Epoch 1713/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4436 - val_loss: -3.6124\n",
      "Epoch 1714/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4449 - val_loss: -3.0221\n",
      "Epoch 1715/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2901 - val_loss: -4.5203\n",
      "Epoch 1716/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2618 - val_loss: -4.4112\n",
      "Epoch 1717/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3160 - val_loss: -3.4926\n",
      "Epoch 1718/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4376 - val_loss: -4.0831\n",
      "Epoch 1719/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3629 - val_loss: -3.7951\n",
      "Epoch 1720/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4020 - val_loss: -4.1236\n",
      "Epoch 1721/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4727 - val_loss: -4.0485\n",
      "Epoch 1722/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3964 - val_loss: -4.5015\n",
      "Epoch 1723/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8366 - val_loss: -4.1412\n",
      "Epoch 1724/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9914 - val_loss: -3.9032\n",
      "Epoch 1725/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2871 - val_loss: -4.0818\n",
      "Epoch 1726/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2447 - val_loss: -0.8858\n",
      "Epoch 1727/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3286 - val_loss: -4.1324\n",
      "Epoch 1728/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3968 - val_loss: -4.0586\n",
      "Epoch 1729/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2847 - val_loss: -4.5825\n",
      "Epoch 1730/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4175 - val_loss: -4.0900\n",
      "Epoch 1731/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2592 - val_loss: -4.2613\n",
      "Epoch 1732/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3324 - val_loss: -3.9603\n",
      "Epoch 1733/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3146 - val_loss: -4.5023\n",
      "Epoch 1734/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3700 - val_loss: -2.2417\n",
      "Epoch 1735/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4262 - val_loss: -3.5550\n",
      "Epoch 1736/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1794 - val_loss: -3.8588\n",
      "Epoch 1737/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3915 - val_loss: -4.4847\n",
      "Epoch 1738/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3679 - val_loss: -4.1648\n",
      "Epoch 1739/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3731 - val_loss: -4.1046\n",
      "Epoch 1740/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4885 - val_loss: -3.9861\n",
      "Epoch 1741/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3722 - val_loss: -4.1084\n",
      "Epoch 1742/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4420 - val_loss: -4.2950\n",
      "Epoch 1743/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4635 - val_loss: -4.2986\n",
      "Epoch 1744/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3100 - val_loss: -4.0333\n",
      "Epoch 1745/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4699 - val_loss: -3.5942\n",
      "Epoch 1746/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4387 - val_loss: -4.5597\n",
      "Epoch 1747/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3304 - val_loss: -4.6604\n",
      "Epoch 1748/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4680 - val_loss: -2.8214\n",
      "Epoch 1749/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3934 - val_loss: -4.1000\n",
      "Epoch 1750/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4832 - val_loss: -3.9497\n",
      "Epoch 1751/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4306 - val_loss: -4.5961\n",
      "Epoch 1752/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1026 - val_loss: -4.6549\n",
      "Epoch 1753/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4336 - val_loss: -3.6744\n",
      "Epoch 1754/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2405 - val_loss: -3.4633\n",
      "Epoch 1755/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3817 - val_loss: -3.7694\n",
      "Epoch 1756/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.4350 - val_loss: -3.8440\n",
      "Epoch 1757/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4138 - val_loss: -3.8468\n",
      "Epoch 1758/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.5037 - val_loss: -4.4062\n",
      "Epoch 1759/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1178 - val_loss: -3.8231\n",
      "Epoch 1760/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3786 - val_loss: -4.4624\n",
      "Epoch 1761/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4861 - val_loss: -1.7832\n",
      "Epoch 1762/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2390 - val_loss: -3.3271\n",
      "Epoch 1763/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4067 - val_loss: -3.8826\n",
      "Epoch 1764/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4290 - val_loss: -3.5003\n",
      "Epoch 1765/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3683 - val_loss: -4.3449\n",
      "Epoch 1766/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4856 - val_loss: -3.3253\n",
      "Epoch 1767/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3529 - val_loss: -3.2930\n",
      "Epoch 1768/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4693 - val_loss: -4.2749\n",
      "Epoch 1769/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4454 - val_loss: -3.6580\n",
      "Epoch 1770/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4558 - val_loss: -3.8213\n",
      "Epoch 1771/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3315 - val_loss: -3.6711\n",
      "Epoch 1772/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4886 - val_loss: -3.4019\n",
      "Epoch 1773/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4057 - val_loss: -3.4100\n",
      "Epoch 1774/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4623 - val_loss: -3.9985\n",
      "Epoch 1775/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5113 - val_loss: -2.8325\n",
      "Epoch 1776/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6961 - val_loss: -1.8814\n",
      "Epoch 1777/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7406 - val_loss: -3.7899\n",
      "Epoch 1778/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6634 - val_loss: -3.9895\n",
      "Epoch 1779/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8669 - val_loss: -4.4255\n",
      "Epoch 1780/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9189 - val_loss: -3.7082\n",
      "Epoch 1781/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0353 - val_loss: -4.4234\n",
      "Epoch 1782/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0227 - val_loss: -3.8035\n",
      "Epoch 1783/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1002 - val_loss: -3.9046\n",
      "Epoch 1784/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1574 - val_loss: -4.2641\n",
      "Epoch 1785/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1156 - val_loss: -4.6782\n",
      "Epoch 1786/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0728 - val_loss: -3.9912\n",
      "Epoch 1787/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2450 - val_loss: -4.5840\n",
      "Epoch 1788/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1306 - val_loss: -3.9888\n",
      "Epoch 1789/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2188 - val_loss: -3.3583\n",
      "Epoch 1790/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0829 - val_loss: -4.2238\n",
      "Epoch 1791/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1707 - val_loss: -4.2240\n",
      "Epoch 1792/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1017 - val_loss: -3.9388\n",
      "Epoch 1793/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8961 - val_loss: -4.2977\n",
      "Epoch 1794/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1999 - val_loss: -3.2925\n",
      "Epoch 1795/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1337 - val_loss: -1.1504\n",
      "Epoch 1796/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2127 - val_loss: -3.7749\n",
      "Epoch 1797/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2595 - val_loss: -4.2413\n",
      "Epoch 1798/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0125 - val_loss: -3.3940\n",
      "Epoch 1799/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3123 - val_loss: -3.9922\n",
      "Epoch 1800/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3343 - val_loss: -4.0275\n",
      "Epoch 1801/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1941 - val_loss: -3.1172\n",
      "Epoch 1802/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2720 - val_loss: -4.3675\n",
      "Epoch 1803/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2838 - val_loss: -2.8761\n",
      "Epoch 1804/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2520 - val_loss: -4.2131\n",
      "Epoch 1805/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2617 - val_loss: -4.4926\n",
      "Epoch 1806/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1769 - val_loss: -3.8234\n",
      "Epoch 1807/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2433 - val_loss: -3.9516\n",
      "Epoch 1808/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2763 - val_loss: -3.4208\n",
      "Epoch 1809/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2370 - val_loss: -3.6960\n",
      "Epoch 1810/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2943 - val_loss: -4.3532\n",
      "Epoch 1811/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2950 - val_loss: -4.1726\n",
      "Epoch 1812/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2256 - val_loss: -4.2926\n",
      "Epoch 1813/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3250 - val_loss: -4.4368\n",
      "Epoch 1814/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2984 - val_loss: -3.8855\n",
      "Epoch 1815/2000\n",
      "118/118 [==============================] - 22s 188ms/step - loss: -5.2690 - val_loss: -4.2559\n",
      "Epoch 1816/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2683 - val_loss: -3.1227\n",
      "Epoch 1817/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3537 - val_loss: -3.7456\n",
      "Epoch 1818/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2127 - val_loss: -3.9496\n",
      "Epoch 1819/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0064 - val_loss: -4.2565\n",
      "Epoch 1820/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2675 - val_loss: -4.1672\n",
      "Epoch 1821/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2727 - val_loss: -4.5115\n",
      "Epoch 1822/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0757 - val_loss: -3.5554\n",
      "Epoch 1823/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3774 - val_loss: -3.4661\n",
      "Epoch 1824/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3108 - val_loss: -3.8408\n",
      "Epoch 1825/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3280 - val_loss: -3.5386\n",
      "Epoch 1826/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3282 - val_loss: -3.2277\n",
      "Epoch 1827/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3267 - val_loss: -4.2799\n",
      "Epoch 1828/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3040 - val_loss: -1.6538\n",
      "Epoch 1829/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2640 - val_loss: -4.3656\n",
      "Epoch 1830/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3122 - val_loss: -4.5597\n",
      "Epoch 1831/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3139 - val_loss: -4.2183\n",
      "Epoch 1832/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3600 - val_loss: -3.6342\n",
      "Epoch 1833/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2561 - val_loss: -3.1055\n",
      "Epoch 1834/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3108 - val_loss: -4.1731\n",
      "Epoch 1835/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3481 - val_loss: -4.4944\n",
      "Epoch 1836/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3101 - val_loss: -3.5440\n",
      "Epoch 1837/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3445 - val_loss: -0.3475\n",
      "Epoch 1838/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3070 - val_loss: -3.1882\n",
      "Epoch 1839/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2213 - val_loss: -4.1310\n",
      "Epoch 1840/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3322 - val_loss: -3.5871\n",
      "Epoch 1841/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4455 - val_loss: -4.2173\n",
      "Epoch 1842/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4212 - val_loss: -3.1741\n",
      "Epoch 1843/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2318 - val_loss: -2.7737\n",
      "Epoch 1844/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3589 - val_loss: -4.5406\n",
      "Epoch 1845/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3244 - val_loss: -1.9966\n",
      "Epoch 1846/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2801 - val_loss: -4.4870\n",
      "Epoch 1847/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3540 - val_loss: -4.1403\n",
      "Epoch 1848/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3497 - val_loss: -4.4505\n",
      "Epoch 1849/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2976 - val_loss: -2.5590\n",
      "Epoch 1850/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3648 - val_loss: -2.2558\n",
      "Epoch 1851/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3579 - val_loss: -4.3342\n",
      "Epoch 1852/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0765 - val_loss: -4.0105\n",
      "Epoch 1853/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2345 - val_loss: -3.6178\n",
      "Epoch 1854/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3372 - val_loss: -4.5749\n",
      "Epoch 1855/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3034 - val_loss: -3.4428\n",
      "Epoch 1856/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2995 - val_loss: -4.2165\n",
      "Epoch 1857/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1952 - val_loss: -3.8163\n",
      "Epoch 1858/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4155 - val_loss: -3.5468\n",
      "Epoch 1859/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2206 - val_loss: -3.9700\n",
      "Epoch 1860/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4323 - val_loss: -3.8641\n",
      "Epoch 1861/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3225 - val_loss: -4.1521\n",
      "Epoch 1862/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4032 - val_loss: -4.5133\n",
      "Epoch 1863/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4305 - val_loss: -3.0146\n",
      "Epoch 1864/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3265 - val_loss: -3.9934\n",
      "Epoch 1865/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2929 - val_loss: -4.5450\n",
      "Epoch 1866/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3804 - val_loss: -4.1426\n",
      "Epoch 1867/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3412 - val_loss: -4.4370\n",
      "Epoch 1868/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4445 - val_loss: -4.2335\n",
      "Epoch 1869/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3081 - val_loss: -4.4278\n",
      "Epoch 1870/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4591 - val_loss: -2.3452\n",
      "Epoch 1871/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4094 - val_loss: -1.1208\n",
      "Epoch 1872/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3498 - val_loss: -4.6073\n",
      "Epoch 1873/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4066 - val_loss: -4.3362\n",
      "Epoch 1874/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3888 - val_loss: -4.4065\n",
      "Epoch 1875/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3816 - val_loss: -3.1655\n",
      "Epoch 1876/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3957 - val_loss: -3.8103\n",
      "Epoch 1877/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2829 - val_loss: -0.9940\n",
      "Epoch 1878/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2268 - val_loss: -3.4975\n",
      "Epoch 1879/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3807 - val_loss: -4.4118\n",
      "Epoch 1880/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3555 - val_loss: -4.3084\n",
      "Epoch 1881/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3058 - val_loss: -3.0849\n",
      "Epoch 1882/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3884 - val_loss: 0.1009\n",
      "Epoch 1883/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3166 - val_loss: -4.1961\n",
      "Epoch 1884/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4211 - val_loss: -1.5282\n",
      "Epoch 1885/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3086 - val_loss: -3.7131\n",
      "Epoch 1886/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3831 - val_loss: -2.2117\n",
      "Epoch 1887/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3889 - val_loss: -4.2643\n",
      "Epoch 1888/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4428 - val_loss: -4.2896\n",
      "Epoch 1889/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3301 - val_loss: -4.5719\n",
      "Epoch 1890/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4093 - val_loss: -4.0018\n",
      "Epoch 1891/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4059 - val_loss: -3.1254\n",
      "Epoch 1892/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1195 - val_loss: -3.3722\n",
      "Epoch 1893/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4242 - val_loss: -4.2616\n",
      "Epoch 1894/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4045 - val_loss: -3.3333\n",
      "Epoch 1895/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3421 - val_loss: -4.2967\n",
      "Epoch 1896/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3477 - val_loss: -3.7347\n",
      "Epoch 1897/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3610 - val_loss: -3.4682\n",
      "Epoch 1898/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3834 - val_loss: -2.7196\n",
      "Epoch 1899/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4896 - val_loss: -1.3450\n",
      "Epoch 1900/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3508 - val_loss: -4.4217\n",
      "Epoch 1901/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3634 - val_loss: -3.7736\n",
      "Epoch 1902/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2910 - val_loss: -3.2324\n",
      "Epoch 1903/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3228 - val_loss: -4.4515\n",
      "Epoch 1904/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4276 - val_loss: -4.3337\n",
      "Epoch 1905/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3955 - val_loss: -3.9076\n",
      "Epoch 1906/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5039 - val_loss: -4.7023\n",
      "Epoch 1907/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4402 - val_loss: -3.9193\n",
      "Epoch 1908/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3720 - val_loss: -4.2003\n",
      "Epoch 1909/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3797 - val_loss: -4.2170\n",
      "Epoch 1910/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3413 - val_loss: -3.3227\n",
      "Epoch 1911/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8198 - val_loss: -4.0717\n",
      "Epoch 1912/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2881 - val_loss: -4.0551\n",
      "Epoch 1913/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3692 - val_loss: -2.8165\n",
      "Epoch 1914/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3858 - val_loss: -4.3627\n",
      "Epoch 1915/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3900 - val_loss: -3.6318\n",
      "Epoch 1916/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3626 - val_loss: -4.7470\n",
      "Epoch 1917/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3692 - val_loss: -4.5086\n",
      "Epoch 1918/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4602 - val_loss: -4.2954\n",
      "Epoch 1919/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3367 - val_loss: -3.8283\n",
      "Epoch 1920/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4722 - val_loss: -4.4022\n",
      "Epoch 1921/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3647 - val_loss: -4.3994\n",
      "Epoch 1922/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3802 - val_loss: -4.4115\n",
      "Epoch 1923/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4083 - val_loss: -3.8584\n",
      "Epoch 1924/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3843 - val_loss: -3.6578\n",
      "Epoch 1925/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4760 - val_loss: -2.7806\n",
      "Epoch 1926/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2660 - val_loss: -4.2474\n",
      "Epoch 1927/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4960 - val_loss: -3.2299\n",
      "Epoch 1928/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2765 - val_loss: -4.1469\n",
      "Epoch 1929/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4362 - val_loss: -4.0310\n",
      "Epoch 1930/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.2971 - val_loss: -1.6629\n",
      "Epoch 1931/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4354 - val_loss: -3.8196\n",
      "Epoch 1932/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4276 - val_loss: -3.9597\n",
      "Epoch 1933/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4293 - val_loss: -4.5816\n",
      "Epoch 1934/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4509 - val_loss: -3.5414\n",
      "Epoch 1935/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3177 - val_loss: -4.2804\n",
      "Epoch 1936/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4097 - val_loss: -3.3957\n",
      "Epoch 1937/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3611 - val_loss: -3.5499\n",
      "Epoch 1938/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4242 - val_loss: -3.3303\n",
      "Epoch 1939/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4042 - val_loss: -4.0737\n",
      "Epoch 1940/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4414 - val_loss: -4.2493\n",
      "Epoch 1941/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4998 - val_loss: -3.7833\n",
      "Epoch 1942/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5045 - val_loss: -3.8628\n",
      "Epoch 1943/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4216 - val_loss: -3.9903\n",
      "Epoch 1944/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3336 - val_loss: -3.4318\n",
      "Epoch 1945/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3315 - val_loss: -3.4226\n",
      "Epoch 1946/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4504 - val_loss: -4.4552\n",
      "Epoch 1947/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4369 - val_loss: -4.4307\n",
      "Epoch 1948/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4381 - val_loss: -4.2774\n",
      "Epoch 1949/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5023 - val_loss: -3.5544\n",
      "Epoch 1950/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4243 - val_loss: -1.3549\n",
      "Epoch 1951/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3653 - val_loss: -3.5335\n",
      "Epoch 1952/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4144 - val_loss: -4.4772\n",
      "Epoch 1953/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4038 - val_loss: -4.3651\n",
      "Epoch 1954/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4721 - val_loss: -4.2466\n",
      "Epoch 1955/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3814 - val_loss: -4.1774\n",
      "Epoch 1956/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4128 - val_loss: -4.2873\n",
      "Epoch 1957/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4464 - val_loss: -4.2574\n",
      "Epoch 1958/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4932 - val_loss: -3.7611\n",
      "Epoch 1959/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4355 - val_loss: -2.8871\n",
      "Epoch 1960/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4268 - val_loss: -4.2037\n",
      "Epoch 1961/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3987 - val_loss: -3.7530\n",
      "Epoch 1962/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4973 - val_loss: -3.5207\n",
      "Epoch 1963/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7105 - val_loss: -4.5708\n",
      "Epoch 1964/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2570 - val_loss: -4.0540\n",
      "Epoch 1965/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2774 - val_loss: -4.5540\n",
      "Epoch 1966/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3276 - val_loss: -3.4294\n",
      "Epoch 1967/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2371 - val_loss: -4.1841\n",
      "Epoch 1968/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3886 - val_loss: -3.9228\n",
      "Epoch 1969/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3941 - val_loss: -4.2208\n",
      "Epoch 1970/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3734 - val_loss: -4.7241\n",
      "Epoch 1971/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3768 - val_loss: -3.5210\n",
      "Epoch 1972/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4527 - val_loss: -3.3908\n",
      "Epoch 1973/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4536 - val_loss: -4.0381\n",
      "Epoch 1974/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5028 - val_loss: -3.4369\n",
      "Epoch 1975/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4149 - val_loss: -4.0846\n",
      "Epoch 1976/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3756 - val_loss: -3.6523\n",
      "Epoch 1977/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2614 - val_loss: -1.8885\n",
      "Epoch 1978/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3286 - val_loss: -3.6226\n",
      "Epoch 1979/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3442 - val_loss: -3.6705\n",
      "Epoch 1980/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4419 - val_loss: -4.3652\n",
      "Epoch 1981/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4045 - val_loss: -3.8691\n",
      "Epoch 1982/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5222 - val_loss: -4.2734\n",
      "Epoch 1983/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3308 - val_loss: -4.0414\n",
      "Epoch 1984/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4285 - val_loss: -4.4200\n",
      "Epoch 1985/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4483 - val_loss: -4.6064\n",
      "Epoch 1986/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4192 - val_loss: -4.3069\n",
      "Epoch 1987/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4538 - val_loss: -4.1036\n",
      "Epoch 1988/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4800 - val_loss: -4.2587\n",
      "Epoch 1989/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4489 - val_loss: -4.0582\n",
      "Epoch 1990/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4365 - val_loss: -4.4207\n",
      "Epoch 1991/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5030 - val_loss: -4.4042\n",
      "Epoch 1992/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3973 - val_loss: -2.4781\n",
      "Epoch 1993/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4409 - val_loss: -4.1051\n",
      "Epoch 1994/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2859 - val_loss: -4.1591\n",
      "Epoch 1995/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3563 - val_loss: -4.3279\n",
      "Epoch 1996/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4306 - val_loss: -3.2928\n",
      "Epoch 1997/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4769 - val_loss: -1.5836\n",
      "Epoch 1998/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4568 - val_loss: -3.6093\n",
      "Epoch 1999/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4851 - val_loss: -3.9502\n",
      "Epoch 2000/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3659 - val_loss: -4.4069\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 6s 36ms/step - loss: -5.0330\n",
      "WARNING:tensorflow:Layer lstm_89 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_89 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 6s 36ms/step - loss: -5.0714\n",
      "WARNING:tensorflow:Layer lstm_94 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_94 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 177293.7656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 33s 258ms/step - loss: 177293.7656 - val_loss: 0.6907\n",
      "Epoch 2/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.6598"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: 0.6598 - val_loss: 0.6904\n",
      "Epoch 3/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.6561"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: 0.6561 - val_loss: 0.6891\n",
      "Epoch 4/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: 0.6469 - val_loss: 0.6894\n",
      "Epoch 5/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.6503"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: 0.6503 - val_loss: 0.6836\n",
      "Epoch 6/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.6018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: 0.6018 - val_loss: 0.6809\n",
      "Epoch 7/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: 0.6369 - val_loss: 0.6913\n",
      "Epoch 8/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: 0.6578 - val_loss: 0.6854\n",
      "Epoch 9/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: 0.6347 - val_loss: 0.6858\n",
      "Epoch 10/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: 0.6320 - val_loss: 0.6854\n",
      "Epoch 11/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.6130"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: 0.6130 - val_loss: 0.6803\n",
      "Epoch 12/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.5982"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: 0.5982 - val_loss: 0.6801\n",
      "Epoch 13/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.5938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: 0.5938 - val_loss: 0.6778\n",
      "Epoch 14/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: 0.5908 - val_loss: 0.6800\n",
      "Epoch 15/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: 0.6322 - val_loss: 0.6962\n",
      "Epoch 16/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: 0.7978 - val_loss: 0.7077\n",
      "Epoch 17/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: 0.8008 - val_loss: 0.7028\n",
      "Epoch 18/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: 0.8048 - val_loss: 0.7046\n",
      "Epoch 19/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: 0.8000 - val_loss: 0.7047\n",
      "Epoch 20/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: 0.8038 - val_loss: 0.7030\n",
      "Epoch 21/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: 0.8057 - val_loss: 0.7030\n",
      "Epoch 22/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: 0.8062 - val_loss: 0.7031\n",
      "Epoch 23/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: 0.8050 - val_loss: 0.7033\n",
      "Epoch 24/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: 0.8054 - val_loss: 0.7028\n",
      "Epoch 25/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: 0.8049 - val_loss: 0.7027\n",
      "Epoch 26/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: 0.8042 - val_loss: 0.7031\n",
      "Epoch 27/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: 0.8046 - val_loss: 0.7036\n",
      "Epoch 28/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: 0.8011 - val_loss: 0.7051\n",
      "Epoch 29/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: 0.7968 - val_loss: 0.7058\n",
      "Epoch 30/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: 0.7965 - val_loss: 0.7057\n",
      "Epoch 31/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: 0.7966 - val_loss: 0.7068\n",
      "Epoch 32/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: 0.7874 - val_loss: 0.7069\n",
      "Epoch 33/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: 0.7853 - val_loss: 0.7084\n",
      "Epoch 34/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.7073"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: 0.7073 - val_loss: 0.4768\n",
      "Epoch 35/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.2193"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: 0.2193 - val_loss: 0.3625\n",
      "Epoch 36/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.2081"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 256ms/step - loss: 0.2081 - val_loss: 0.3508\n",
      "Epoch 37/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.2044"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: 0.2044 - val_loss: 0.3380\n",
      "Epoch 38/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1991"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: 0.1991 - val_loss: 0.3189\n",
      "Epoch 39/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1934"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: 0.1934 - val_loss: 0.2963\n",
      "Epoch 40/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1859"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 251ms/step - loss: 0.1859 - val_loss: 0.2598\n",
      "Epoch 41/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1783"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: 0.1783 - val_loss: 0.2136\n",
      "Epoch 42/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1722"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: 0.1722 - val_loss: 0.1723\n",
      "Epoch 43/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1684"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: 0.1684 - val_loss: 0.1449\n",
      "Epoch 44/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: 0.1657 - val_loss: 0.1264\n",
      "Epoch 45/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1638"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: 0.1638 - val_loss: 0.1143\n",
      "Epoch 46/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1622"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: 0.1622 - val_loss: 0.1062\n",
      "Epoch 47/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1609"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: 0.1609 - val_loss: 0.0997\n",
      "Epoch 48/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1598"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: 0.1598 - val_loss: 0.0946\n",
      "Epoch 49/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1588"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: 0.1588 - val_loss: 0.0898\n",
      "Epoch 50/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: 0.1578 - val_loss: 0.0856\n",
      "Epoch 51/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1568"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 237ms/step - loss: 0.1568 - val_loss: 0.0818\n",
      "Epoch 52/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1560"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: 0.1560 - val_loss: 0.0782\n",
      "Epoch 53/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1551"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: 0.1551 - val_loss: 0.0748\n",
      "Epoch 54/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: 0.1542 - val_loss: 0.0714\n",
      "Epoch 55/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1532"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: 0.1532 - val_loss: 0.0682\n",
      "Epoch 56/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1524"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: 0.1524 - val_loss: 0.0652\n",
      "Epoch 57/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1514"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 247ms/step - loss: 0.1514 - val_loss: 0.0621\n",
      "Epoch 58/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1505"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: 0.1505 - val_loss: 0.0590\n",
      "Epoch 59/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1496"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: 0.1496 - val_loss: 0.0559\n",
      "Epoch 60/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1487"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: 0.1487 - val_loss: 0.0530\n",
      "Epoch 61/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: 0.1477 - val_loss: 0.0502\n",
      "Epoch 62/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1468"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: 0.1468 - val_loss: 0.0473\n",
      "Epoch 63/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1459"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: 0.1459 - val_loss: 0.0444\n",
      "Epoch 64/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: 0.1449 - val_loss: 0.0414\n",
      "Epoch 65/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1439"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: 0.1439 - val_loss: 0.0385\n",
      "Epoch 66/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1429"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: 0.1429 - val_loss: 0.0356\n",
      "Epoch 67/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1419"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: 0.1419 - val_loss: 0.0326\n",
      "Epoch 68/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1408"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: 0.1408 - val_loss: 0.0296\n",
      "Epoch 69/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1397"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: 0.1397 - val_loss: 0.0266\n",
      "Epoch 70/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1386"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: 0.1386 - val_loss: 0.0235\n",
      "Epoch 71/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: 0.1375 - val_loss: 0.0205\n",
      "Epoch 72/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1363"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: 0.1363 - val_loss: 0.0173\n",
      "Epoch 73/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1352"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: 0.1352 - val_loss: 0.0142\n",
      "Epoch 74/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1340"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: 0.1340 - val_loss: 0.0109\n",
      "Epoch 75/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1327"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: 0.1327 - val_loss: 0.0076\n",
      "Epoch 76/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1314"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: 0.1314 - val_loss: 0.0042\n",
      "Epoch 77/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1301"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: 0.1301 - val_loss: 7.2501e-04\n",
      "Epoch 78/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1288"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: 0.1288 - val_loss: -0.0028\n",
      "Epoch 79/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1273"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: 0.1273 - val_loss: -0.0065\n",
      "Epoch 80/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1259"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: 0.1259 - val_loss: -0.0103\n",
      "Epoch 81/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: 0.1244 - val_loss: -0.0141\n",
      "Epoch 82/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1228"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: 0.1228 - val_loss: -0.0182\n",
      "Epoch 83/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1212"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: 0.1212 - val_loss: -0.0221\n",
      "Epoch 84/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1196"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: 0.1196 - val_loss: -0.0264\n",
      "Epoch 85/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1178"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: 0.1178 - val_loss: -0.0308\n",
      "Epoch 86/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1161"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: 0.1161 - val_loss: -0.0352\n",
      "Epoch 87/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1142"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: 0.1142 - val_loss: -0.0399\n",
      "Epoch 88/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1122"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: 0.1122 - val_loss: -0.0447\n",
      "Epoch 89/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1102"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: 0.1102 - val_loss: -0.0498\n",
      "Epoch 90/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1081"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: 0.1081 - val_loss: -0.0551\n",
      "Epoch 91/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: 0.1059 - val_loss: -0.0606\n",
      "Epoch 92/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1035"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: 0.1035 - val_loss: -0.0664\n",
      "Epoch 93/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: 0.1011 - val_loss: -0.0725\n",
      "Epoch 94/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0985"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: 0.0985 - val_loss: -0.0790\n",
      "Epoch 95/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0957"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: 0.0957 - val_loss: -0.0861\n",
      "Epoch 96/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 237ms/step - loss: 0.0927 - val_loss: -0.0937\n",
      "Epoch 97/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0895"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: 0.0895 - val_loss: -0.1021\n",
      "Epoch 98/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0859"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: 0.0859 - val_loss: -0.1114\n",
      "Epoch 99/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0820"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: 0.0820 - val_loss: -0.1221\n",
      "Epoch 100/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: 0.0775 - val_loss: -0.1349\n",
      "Epoch 101/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0722"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: 0.0722 - val_loss: -0.1511\n",
      "Epoch 102/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: 0.0656 - val_loss: -0.1747\n",
      "Epoch 103/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0557"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: 0.0557 - val_loss: -0.2202\n",
      "Epoch 104/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0330"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: 0.0330 - val_loss: -0.3204\n",
      "Epoch 105/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.0287"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -0.0287 - val_loss: -0.3890\n",
      "Epoch 106/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.1450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -0.1450 - val_loss: -0.4587\n",
      "Epoch 107/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.2785"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -0.2785 - val_loss: -0.4906\n",
      "Epoch 108/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.3416"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -0.3416 - val_loss: -0.4968\n",
      "Epoch 109/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.3915"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -0.3915 - val_loss: -0.5057\n",
      "Epoch 110/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.4291"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -0.4291 - val_loss: -0.5268\n",
      "Epoch 111/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.4603"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -0.4603 - val_loss: -0.5408\n",
      "Epoch 112/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.4848"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -0.4848 - val_loss: -0.5492\n",
      "Epoch 113/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.5043"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -0.5043 - val_loss: -0.5560\n",
      "Epoch 114/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.5207"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -0.5207 - val_loss: -0.5618\n",
      "Epoch 115/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.5348"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -0.5348 - val_loss: -0.5664\n",
      "Epoch 116/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.5470"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -0.5470 - val_loss: -0.5705\n",
      "Epoch 117/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.5578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 237ms/step - loss: -0.5578 - val_loss: -0.5743\n",
      "Epoch 118/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.5675"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -0.5675 - val_loss: -0.5777\n",
      "Epoch 119/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.5764"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -0.5764 - val_loss: -0.5815\n",
      "Epoch 120/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.5847"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -0.5847 - val_loss: -0.5863\n",
      "Epoch 121/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.5927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -0.5927 - val_loss: -0.5933\n",
      "Epoch 122/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.6005"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -0.6005 - val_loss: -0.6015\n",
      "Epoch 123/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.6082"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -0.6082 - val_loss: -0.6101\n",
      "Epoch 124/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.6158"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -0.6158 - val_loss: -0.6195\n",
      "Epoch 125/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.6234"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -0.6234 - val_loss: -0.6287\n",
      "Epoch 126/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.6308"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -0.6308 - val_loss: -0.6383\n",
      "Epoch 127/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.6382"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -0.6382 - val_loss: -0.6474\n",
      "Epoch 128/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.6453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -0.6453 - val_loss: -0.6563\n",
      "Epoch 129/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.6522"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -0.6522 - val_loss: -0.6651\n",
      "Epoch 130/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.6590"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 248ms/step - loss: -0.6590 - val_loss: -0.6750\n",
      "Epoch 131/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.6661"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -0.6661 - val_loss: -0.6845\n",
      "Epoch 132/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.6730"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -0.6730 - val_loss: -0.6953\n",
      "Epoch 133/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.6801"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 237ms/step - loss: -0.6801 - val_loss: -0.7060\n",
      "Epoch 134/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.6877"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -0.6877 - val_loss: -0.7184\n",
      "Epoch 135/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.6967"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -0.6967 - val_loss: -0.7328\n",
      "Epoch 136/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.7066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -0.7066 - val_loss: -0.7430\n",
      "Epoch 137/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -0.7164 - val_loss: -0.7421\n",
      "Epoch 138/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.7376"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -0.7376 - val_loss: -0.7984\n",
      "Epoch 139/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.7657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -0.7657 - val_loss: -0.8490\n",
      "Epoch 140/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.7804"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -0.7804 - val_loss: -0.8826\n",
      "Epoch 141/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.7946"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -0.7946 - val_loss: -0.9081\n",
      "Epoch 142/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8052"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -0.8052 - val_loss: -0.9279\n",
      "Epoch 143/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8144"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -0.8144 - val_loss: -0.9436\n",
      "Epoch 144/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8226"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -0.8226 - val_loss: -0.9551\n",
      "Epoch 145/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -0.8304 - val_loss: -0.9670\n",
      "Epoch 146/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8378"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -0.8378 - val_loss: -0.9774\n",
      "Epoch 147/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8448"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -0.8448 - val_loss: -0.9868\n",
      "Epoch 148/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8516"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -0.8516 - val_loss: -0.9946\n",
      "Epoch 149/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8579"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -0.8579 - val_loss: -1.0032\n",
      "Epoch 150/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8651"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -0.8651 - val_loss: -1.0132\n",
      "Epoch 151/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8721"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -0.8721 - val_loss: -1.0318\n",
      "Epoch 152/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8804"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -0.8804 - val_loss: -1.0690\n",
      "Epoch 153/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8903"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -0.8903 - val_loss: -1.1158\n",
      "Epoch 154/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.8992"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -0.8992 - val_loss: -1.1345\n",
      "Epoch 155/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.9076"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -0.9076 - val_loss: -1.1438\n",
      "Epoch 156/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.9155"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -0.9155 - val_loss: -1.1489\n",
      "Epoch 157/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.9246"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -0.9246 - val_loss: -1.1593\n",
      "Epoch 158/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.9331"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -0.9331 - val_loss: -1.1671\n",
      "Epoch 159/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.9428"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -0.9428 - val_loss: -1.1710\n",
      "Epoch 160/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.9524"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -0.9524 - val_loss: -1.1809\n",
      "Epoch 161/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.9629"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -0.9629 - val_loss: -1.1973\n",
      "Epoch 162/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.9743"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -0.9743 - val_loss: -1.2025\n",
      "Epoch 163/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -0.9849 - val_loss: -1.0874\n",
      "Epoch 164/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -0.9951"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -0.9951 - val_loss: -1.2286\n",
      "Epoch 165/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.0109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -1.0109 - val_loss: -1.2378\n",
      "Epoch 166/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.0248"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -1.0248 - val_loss: -1.2552\n",
      "Epoch 167/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.0396"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -1.0396 - val_loss: -1.2693\n",
      "Epoch 168/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.0552"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 249ms/step - loss: -1.0552 - val_loss: -1.2788\n",
      "Epoch 169/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.0716"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -1.0716 - val_loss: -1.3038\n",
      "Epoch 170/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.0892"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -1.0892 - val_loss: -1.3164\n",
      "Epoch 171/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.1080"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -1.1080 - val_loss: -1.3407\n",
      "Epoch 172/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.1280"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -1.1280 - val_loss: -1.3608\n",
      "Epoch 173/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.1495"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -1.1495 - val_loss: -1.3780\n",
      "Epoch 174/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.1722"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -1.1722 - val_loss: -1.3986\n",
      "Epoch 175/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.1965"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -1.1965 - val_loss: -1.4145\n",
      "Epoch 176/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.2226"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -1.2226 - val_loss: -1.4385\n",
      "Epoch 177/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.2510"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -1.2510 - val_loss: -1.4644\n",
      "Epoch 178/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.2840"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -1.2840 - val_loss: -1.4828\n",
      "Epoch 179/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -1.2824 - val_loss: -0.3120\n",
      "Epoch 180/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.1776 - val_loss: -0.2724\n",
      "Epoch 181/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -1.1995 - val_loss: -0.3996\n",
      "Epoch 182/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.3132 - val_loss: -1.4259\n",
      "Epoch 183/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.4176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -1.4176 - val_loss: -1.5298\n",
      "Epoch 184/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.4757"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -1.4757 - val_loss: -1.5964\n",
      "Epoch 185/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.5322"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -1.5322 - val_loss: -1.6389\n",
      "Epoch 186/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.5927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -1.5927 - val_loss: -1.6912\n",
      "Epoch 187/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.6553"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -1.6553 - val_loss: -1.7184\n",
      "Epoch 188/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.7182"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -1.7182 - val_loss: -1.7722\n",
      "Epoch 189/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.7936 - val_loss: -1.7661\n",
      "Epoch 190/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.8672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -1.8672 - val_loss: -1.9333\n",
      "Epoch 191/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.9429 - val_loss: -1.9124\n",
      "Epoch 192/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.0292"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -2.0292 - val_loss: -2.0714\n",
      "Epoch 193/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.1131"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -2.1131 - val_loss: -2.1511\n",
      "Epoch 194/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.2029"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -2.2029 - val_loss: -2.1588\n",
      "Epoch 195/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.3017"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 251ms/step - loss: -2.3017 - val_loss: -2.1759\n",
      "Epoch 196/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.3908"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -2.3908 - val_loss: -2.2960\n",
      "Epoch 197/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.4646"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -2.4646 - val_loss: -2.4220\n",
      "Epoch 198/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -2.5925 - val_loss: -2.4213\n",
      "Epoch 199/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.6949"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -2.6949 - val_loss: -2.6154\n",
      "Epoch 200/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -2.7566 - val_loss: -2.5031\n",
      "Epoch 201/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.8884"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -2.8884 - val_loss: -2.8517\n",
      "Epoch 202/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.9807"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 250ms/step - loss: -2.9807 - val_loss: -2.8688\n",
      "Epoch 203/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.0836"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -3.0836 - val_loss: -2.9717\n",
      "Epoch 204/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.1705 - val_loss: -2.9521\n",
      "Epoch 205/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.2598"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -3.2598 - val_loss: -3.0001\n",
      "Epoch 206/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.3545 - val_loss: -2.8662\n",
      "Epoch 207/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.4330"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -3.4330 - val_loss: -3.2572\n",
      "Epoch 208/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.4677"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 251ms/step - loss: -3.4677 - val_loss: -3.3538\n",
      "Epoch 209/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.5247 - val_loss: -3.0320\n",
      "Epoch 210/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.6013 - val_loss: -3.3169\n",
      "Epoch 211/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.6841"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -3.6841 - val_loss: -3.4978\n",
      "Epoch 212/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -3.6973 - val_loss: -3.0000\n",
      "Epoch 213/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.7145 - val_loss: -3.1171\n",
      "Epoch 214/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.7742"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -3.7742 - val_loss: -3.5439\n",
      "Epoch 215/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8526 - val_loss: -3.0866\n",
      "Epoch 216/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.7976 - val_loss: -3.3147\n",
      "Epoch 217/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -3.8771 - val_loss: -2.9639\n",
      "Epoch 218/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9890 - val_loss: -3.3221\n",
      "Epoch 219/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9115 - val_loss: -3.3829\n",
      "Epoch 220/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9897 - val_loss: -3.1564\n",
      "Epoch 221/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.0511"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.0511 - val_loss: -3.6568\n",
      "Epoch 222/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.9332"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -3.9332 - val_loss: -3.6804\n",
      "Epoch 223/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9962 - val_loss: -1.8154\n",
      "Epoch 224/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.0406 - val_loss: -3.6361\n",
      "Epoch 225/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1740 - val_loss: -3.5558\n",
      "Epoch 226/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1234 - val_loss: -3.2667\n",
      "Epoch 227/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0095 - val_loss: -3.4919\n",
      "Epoch 228/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1643 - val_loss: -3.6071\n",
      "Epoch 229/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1471 - val_loss: -1.1460\n",
      "Epoch 230/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.1473"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -4.1473 - val_loss: -3.7642\n",
      "Epoch 231/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1485 - val_loss: -2.6674\n",
      "Epoch 232/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2299 - val_loss: -3.6563\n",
      "Epoch 233/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1996 - val_loss: -2.7237\n",
      "Epoch 234/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3121 - val_loss: -3.6387\n",
      "Epoch 235/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2687 - val_loss: -3.3183\n",
      "Epoch 236/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.2769 - val_loss: -3.7179\n",
      "Epoch 237/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2707 - val_loss: -3.7390\n",
      "Epoch 238/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2260 - val_loss: -2.7469\n",
      "Epoch 239/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3237 - val_loss: -3.6476\n",
      "Epoch 240/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.3485 - val_loss: -3.7174\n",
      "Epoch 241/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3440 - val_loss: -3.6986\n",
      "Epoch 242/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4246 - val_loss: -3.6272\n",
      "Epoch 243/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.3718"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -4.3718 - val_loss: -3.8230\n",
      "Epoch 244/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2869 - val_loss: -3.1452\n",
      "Epoch 245/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4248 - val_loss: -2.8306\n",
      "Epoch 246/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3888 - val_loss: -2.0610\n",
      "Epoch 247/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3965 - val_loss: -3.7507\n",
      "Epoch 248/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4098 - val_loss: -3.5864\n",
      "Epoch 249/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4917"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.4917 - val_loss: -3.9469\n",
      "Epoch 250/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4825 - val_loss: -3.5264\n",
      "Epoch 251/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4494 - val_loss: -3.4749\n",
      "Epoch 252/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3325 - val_loss: -3.3351\n",
      "Epoch 253/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4750 - val_loss: -0.6376\n",
      "Epoch 254/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4460 - val_loss: -3.1256\n",
      "Epoch 255/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5083 - val_loss: -3.9450\n",
      "Epoch 256/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5089 - val_loss: -3.7442\n",
      "Epoch 257/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4520 - val_loss: -3.4727\n",
      "Epoch 258/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5827 - val_loss: -3.2946\n",
      "Epoch 259/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4576 - val_loss: -3.7221\n",
      "Epoch 260/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -4.5176 - val_loss: -4.0491\n",
      "Epoch 261/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5094 - val_loss: -2.9912\n",
      "Epoch 262/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5211 - val_loss: -3.8231\n",
      "Epoch 263/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5947"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -4.5947 - val_loss: -4.1813\n",
      "Epoch 264/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.5962 - val_loss: -4.0501\n",
      "Epoch 265/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4110 - val_loss: -4.0514\n",
      "Epoch 266/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5566 - val_loss: -3.9959\n",
      "Epoch 267/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5352 - val_loss: -4.0474\n",
      "Epoch 268/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6052 - val_loss: -4.1752\n",
      "Epoch 269/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5285 - val_loss: -3.6307\n",
      "Epoch 270/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5019 - val_loss: -4.0619\n",
      "Epoch 271/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6256 - val_loss: -3.7707\n",
      "Epoch 272/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5877 - val_loss: -4.0505\n",
      "Epoch 273/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6240 - val_loss: -3.8475\n",
      "Epoch 274/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6565 - val_loss: -3.8027\n",
      "Epoch 275/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6753 - val_loss: -3.9636\n",
      "Epoch 276/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1687 - val_loss: -3.6840\n",
      "Epoch 277/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6546"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -4.6546 - val_loss: -4.2086\n",
      "Epoch 278/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4703 - val_loss: -4.0731\n",
      "Epoch 279/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6722 - val_loss: -3.1585\n",
      "Epoch 280/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6747 - val_loss: -3.8387\n",
      "Epoch 281/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7344 - val_loss: -1.9964\n",
      "Epoch 282/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6103 - val_loss: -3.9166\n",
      "Epoch 283/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6307 - val_loss: -4.0409\n",
      "Epoch 284/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7166 - val_loss: -2.3016\n",
      "Epoch 285/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6752 - val_loss: -3.5944\n",
      "Epoch 286/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6380 - val_loss: -3.0106\n",
      "Epoch 287/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6941 - val_loss: -3.7622\n",
      "Epoch 288/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7659 - val_loss: -2.8581\n",
      "Epoch 289/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6303 - val_loss: -3.2245\n",
      "Epoch 290/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6854 - val_loss: -4.0110\n",
      "Epoch 291/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6606 - val_loss: -3.9568\n",
      "Epoch 292/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6304 - val_loss: -2.9229\n",
      "Epoch 293/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7114 - val_loss: -3.5580\n",
      "Epoch 294/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6155 - val_loss: -1.1105\n",
      "Epoch 295/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7381 - val_loss: -4.0246\n",
      "Epoch 296/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6869 - val_loss: -2.7669\n",
      "Epoch 297/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7217"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -4.7217 - val_loss: -4.2191\n",
      "Epoch 298/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7494 - val_loss: -4.1250\n",
      "Epoch 299/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7248 - val_loss: -4.2077\n",
      "Epoch 300/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7126 - val_loss: -4.1185\n",
      "Epoch 301/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7435 - val_loss: -3.6146\n",
      "Epoch 302/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6222 - val_loss: -3.5867\n",
      "Epoch 303/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6226 - val_loss: -3.5234\n",
      "Epoch 304/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7666 - val_loss: -3.7199\n",
      "Epoch 305/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7928 - val_loss: -3.9035\n",
      "Epoch 306/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7379 - val_loss: -3.8511\n",
      "Epoch 307/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3949 - val_loss: -3.2558\n",
      "Epoch 308/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9422 - val_loss: -3.6807\n",
      "Epoch 309/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4294 - val_loss: -4.1534\n",
      "Epoch 310/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5536 - val_loss: -3.4448\n",
      "Epoch 311/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6444 - val_loss: -4.0542\n",
      "Epoch 312/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6991"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -4.6991 - val_loss: -4.2360\n",
      "Epoch 313/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6548 - val_loss: -4.0500\n",
      "Epoch 314/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7418 - val_loss: -4.1840\n",
      "Epoch 315/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7790"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -4.7790 - val_loss: -4.3720\n",
      "Epoch 316/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7355 - val_loss: -3.4973\n",
      "Epoch 317/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7330 - val_loss: -4.1556\n",
      "Epoch 318/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3423 - val_loss: -3.4258\n",
      "Epoch 319/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6715 - val_loss: -3.3200\n",
      "Epoch 320/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7202 - val_loss: -4.0191\n",
      "Epoch 321/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7333 - val_loss: -4.2974\n",
      "Epoch 322/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8332 - val_loss: -4.2654\n",
      "Epoch 323/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8484 - val_loss: -4.1276\n",
      "Epoch 324/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7089 - val_loss: -4.2006\n",
      "Epoch 325/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8489"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -4.8489 - val_loss: -4.3857\n",
      "Epoch 326/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6648 - val_loss: -3.8879\n",
      "Epoch 327/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8587 - val_loss: -4.2985\n",
      "Epoch 328/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7472 - val_loss: -3.8899\n",
      "Epoch 329/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9222 - val_loss: -3.6199\n",
      "Epoch 330/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8183 - val_loss: -3.4288\n",
      "Epoch 331/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6885 - val_loss: -3.7474\n",
      "Epoch 332/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8177 - val_loss: -3.8922\n",
      "Epoch 333/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7764 - val_loss: -4.2050\n",
      "Epoch 334/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9091 - val_loss: -4.1108\n",
      "Epoch 335/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8668 - val_loss: -3.3807\n",
      "Epoch 336/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6507 - val_loss: -3.5345\n",
      "Epoch 337/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9341 - val_loss: -3.9448\n",
      "Epoch 338/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1396 - val_loss: -1.0469\n",
      "Epoch 339/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.7204 - val_loss: -2.3760\n",
      "Epoch 340/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.9386 - val_loss: -2.7950\n",
      "Epoch 341/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.4863 - val_loss: -3.2313\n",
      "Epoch 342/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.9506 - val_loss: -2.6824\n",
      "Epoch 343/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -2.5194 - val_loss: -3.4301\n",
      "Epoch 344/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.0173 - val_loss: -3.5754\n",
      "Epoch 345/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3577 - val_loss: -3.7439\n",
      "Epoch 346/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4591 - val_loss: -3.8156\n",
      "Epoch 347/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4304 - val_loss: -3.8810\n",
      "Epoch 348/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5992 - val_loss: -4.2423\n",
      "Epoch 349/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5912 - val_loss: -3.6798\n",
      "Epoch 350/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.2816 - val_loss: -2.4762\n",
      "Epoch 351/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.7865 - val_loss: 0.4593\n",
      "Epoch 352/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.2496 - val_loss: 0.4499\n",
      "Epoch 353/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.3444 - val_loss: -1.0964\n",
      "Epoch 354/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -2.0719 - val_loss: -1.6414\n",
      "Epoch 355/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.2982 - val_loss: -1.7726\n",
      "Epoch 356/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -2.4958 - val_loss: -2.1765\n",
      "Epoch 357/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.9603 - val_loss: -2.0793\n",
      "Epoch 358/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.1566 - val_loss: -1.7383\n",
      "Epoch 359/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.1428 - val_loss: -3.1512\n",
      "Epoch 360/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.3558 - val_loss: -2.4996\n",
      "Epoch 361/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.3350 - val_loss: -3.0540\n",
      "Epoch 362/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.3253 - val_loss: 24412.6230\n",
      "Epoch 363/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -3.5698 - val_loss: 34744.3125\n",
      "Epoch 364/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -2.8957 - val_loss: 9592.7471\n",
      "Epoch 365/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.5665 - val_loss: 11605.0156\n",
      "Epoch 366/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.2773 - val_loss: -2.6629\n",
      "Epoch 367/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.6143 - val_loss: -3.3815\n",
      "Epoch 368/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7211 - val_loss: -2.9782\n",
      "Epoch 369/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.5944 - val_loss: -3.2828\n",
      "Epoch 370/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -3.7731 - val_loss: -1.6377\n",
      "Epoch 371/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.7248 - val_loss: -2.9890\n",
      "Epoch 372/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.7872 - val_loss: -0.2348\n",
      "Epoch 373/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.7311 - val_loss: -3.4562\n",
      "Epoch 374/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9294 - val_loss: -3.0859\n",
      "Epoch 375/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8263 - val_loss: -2.6299\n",
      "Epoch 376/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9015 - val_loss: -3.3422\n",
      "Epoch 377/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9545 - val_loss: -3.8010\n",
      "Epoch 378/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8293 - val_loss: -3.7140\n",
      "Epoch 379/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9916 - val_loss: -3.2325\n",
      "Epoch 380/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7947 - val_loss: -3.6556\n",
      "Epoch 381/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9308 - val_loss: -3.2755\n",
      "Epoch 382/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.0649 - val_loss: -2.9373\n",
      "Epoch 383/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0308 - val_loss: -3.2735\n",
      "Epoch 384/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0238 - val_loss: -3.9166\n",
      "Epoch 385/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0484 - val_loss: -3.8243\n",
      "Epoch 386/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0532 - val_loss: -3.5982\n",
      "Epoch 387/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1578 - val_loss: -3.7020\n",
      "Epoch 388/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0572 - val_loss: -3.4891\n",
      "Epoch 389/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1633 - val_loss: -3.5530\n",
      "Epoch 390/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0682 - val_loss: -2.4677\n",
      "Epoch 391/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1268 - val_loss: -3.2188\n",
      "Epoch 392/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1747 - val_loss: -3.9957\n",
      "Epoch 393/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1796 - val_loss: -3.4450\n",
      "Epoch 394/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0854 - val_loss: -3.9032\n",
      "Epoch 395/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2315 - val_loss: -1.9452\n",
      "Epoch 396/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0670 - val_loss: -3.8286\n",
      "Epoch 397/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2677 - val_loss: -3.6625\n",
      "Epoch 398/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.0249 - val_loss: -3.9216\n",
      "Epoch 399/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2257 - val_loss: -3.9525\n",
      "Epoch 400/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1123 - val_loss: -3.0439\n",
      "Epoch 401/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2081 - val_loss: -3.5014\n",
      "Epoch 402/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2218 - val_loss: -3.9476\n",
      "Epoch 403/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1507 - val_loss: -3.4185\n",
      "Epoch 404/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2902 - val_loss: -3.6305\n",
      "Epoch 405/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2560 - val_loss: -3.8334\n",
      "Epoch 406/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2567 - val_loss: -3.6408\n",
      "Epoch 407/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2078 - val_loss: -3.9722\n",
      "Epoch 408/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1455 - val_loss: -3.1559\n",
      "Epoch 409/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2884 - val_loss: -3.7903\n",
      "Epoch 410/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.2551 - val_loss: -3.4039\n",
      "Epoch 411/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3084 - val_loss: -3.9541\n",
      "Epoch 412/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3230 - val_loss: -3.9449\n",
      "Epoch 413/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.3419 - val_loss: -3.9580\n",
      "Epoch 414/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2022 - val_loss: -2.9909\n",
      "Epoch 415/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3247 - val_loss: -4.0338\n",
      "Epoch 416/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3240 - val_loss: -2.1287\n",
      "Epoch 417/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2326 - val_loss: -3.9692\n",
      "Epoch 418/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3324 - val_loss: -3.7504\n",
      "Epoch 419/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3369 - val_loss: -3.9608\n",
      "Epoch 420/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3555 - val_loss: -4.1044\n",
      "Epoch 421/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3382 - val_loss: -4.1192\n",
      "Epoch 422/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.0733 - val_loss: -3.5209\n",
      "Epoch 423/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2991 - val_loss: -3.7097\n",
      "Epoch 424/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3732 - val_loss: -4.0096\n",
      "Epoch 425/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3771 - val_loss: -1.1652\n",
      "Epoch 426/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.6525 - val_loss: -1.4549\n",
      "Epoch 427/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: 8.8591 - val_loss: 0.9169\n",
      "Epoch 428/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -0.8631 - val_loss: 0.6847\n",
      "Epoch 429/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -0.9673 - val_loss: 1.0563\n",
      "Epoch 430/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.1672 - val_loss: 0.6368\n",
      "Epoch 431/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.3425 - val_loss: 0.1653\n",
      "Epoch 432/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.5884 - val_loss: -0.3838\n",
      "Epoch 433/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -1.5655 - val_loss: -0.1201\n",
      "Epoch 434/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.6202 - val_loss: -2.4497\n",
      "Epoch 435/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.1371 - val_loss: -2.6593\n",
      "Epoch 436/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.4114 - val_loss: -2.8238\n",
      "Epoch 437/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.5150 - val_loss: -3.0420\n",
      "Epoch 438/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.6481 - val_loss: -2.0578\n",
      "Epoch 439/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7060 - val_loss: -3.2846\n",
      "Epoch 440/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.7667 - val_loss: -3.1927\n",
      "Epoch 441/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.7923 - val_loss: -3.3856\n",
      "Epoch 442/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.7760 - val_loss: -3.4367\n",
      "Epoch 443/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.8040 - val_loss: -3.4233\n",
      "Epoch 444/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.8577 - val_loss: -2.4055\n",
      "Epoch 445/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9153 - val_loss: -2.0291\n",
      "Epoch 446/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.8615 - val_loss: -3.4833\n",
      "Epoch 447/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9502 - val_loss: -3.4968\n",
      "Epoch 448/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9544 - val_loss: -2.9322\n",
      "Epoch 449/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9749 - val_loss: -3.4291\n",
      "Epoch 450/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0834 - val_loss: -3.4682\n",
      "Epoch 451/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0845 - val_loss: -3.6363\n",
      "Epoch 452/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -3.8159 - val_loss: -3.6218\n",
      "Epoch 453/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9664 - val_loss: -3.4405\n",
      "Epoch 454/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9978 - val_loss: -3.5805\n",
      "Epoch 455/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9934 - val_loss: -3.6554\n",
      "Epoch 456/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9301 - val_loss: -3.2524\n",
      "Epoch 457/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9461 - val_loss: -3.2802\n",
      "Epoch 458/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1560 - val_loss: -3.3068\n",
      "Epoch 459/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0406 - val_loss: -3.3097\n",
      "Epoch 460/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1126 - val_loss: -3.5880\n",
      "Epoch 461/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1487 - val_loss: -3.3455\n",
      "Epoch 462/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1516 - val_loss: -3.8226\n",
      "Epoch 463/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0912 - val_loss: -3.1984\n",
      "Epoch 464/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1782 - val_loss: -3.8134\n",
      "Epoch 465/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1602 - val_loss: -3.3909\n",
      "Epoch 466/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9969 - val_loss: -3.6941\n",
      "Epoch 467/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1108 - val_loss: -3.7472\n",
      "Epoch 468/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2047 - val_loss: -3.7203\n",
      "Epoch 469/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2602 - val_loss: -3.4857\n",
      "Epoch 470/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2101 - val_loss: -3.9243\n",
      "Epoch 471/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.0377 - val_loss: -2.5953\n",
      "Epoch 472/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.6207 - val_loss: -3.5438\n",
      "Epoch 473/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0493 - val_loss: -3.2334\n",
      "Epoch 474/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9262 - val_loss: -2.8448\n",
      "Epoch 475/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 21s 181ms/step - loss: -4.0412 - val_loss: -2.7066\n",
      "Epoch 476/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.1513 - val_loss: -3.7917\n",
      "Epoch 477/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.8959 - val_loss: -2.9430\n",
      "Epoch 478/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1331 - val_loss: -3.7965\n",
      "Epoch 479/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1872 - val_loss: -3.9501\n",
      "Epoch 480/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1099 - val_loss: -3.3606\n",
      "Epoch 481/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9147 - val_loss: -3.8701\n",
      "Epoch 482/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.2041 - val_loss: -3.8677\n",
      "Epoch 483/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2281 - val_loss: -3.4198\n",
      "Epoch 484/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2198 - val_loss: -3.1338\n",
      "Epoch 485/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2261 - val_loss: -3.7057\n",
      "Epoch 486/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2362 - val_loss: -3.6012\n",
      "Epoch 487/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3483 - val_loss: -3.1905\n",
      "Epoch 488/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2402 - val_loss: -2.8382\n",
      "Epoch 489/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2280 - val_loss: -3.9670\n",
      "Epoch 490/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9928 - val_loss: -2.8649\n",
      "Epoch 491/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3588 - val_loss: -3.9123\n",
      "Epoch 492/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3696 - val_loss: -3.2365\n",
      "Epoch 493/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2522 - val_loss: -4.0179\n",
      "Epoch 494/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2373 - val_loss: -3.7409\n",
      "Epoch 495/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3819 - val_loss: -3.8496\n",
      "Epoch 496/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3178 - val_loss: -3.8611\n",
      "Epoch 497/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3602 - val_loss: -3.9728\n",
      "Epoch 498/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.3500 - val_loss: -3.3824\n",
      "Epoch 499/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.2945 - val_loss: -3.3404\n",
      "Epoch 500/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4427 - val_loss: -3.8996\n",
      "Epoch 501/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3312 - val_loss: -3.7290\n",
      "Epoch 502/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4381 - val_loss: -3.3324\n",
      "Epoch 503/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0400 - val_loss: -3.6875\n",
      "Epoch 504/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4576 - val_loss: -3.9712\n",
      "Epoch 505/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4279 - val_loss: -3.9422\n",
      "Epoch 506/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3198 - val_loss: -3.7765\n",
      "Epoch 507/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3713 - val_loss: -4.0482\n",
      "Epoch 508/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5137 - val_loss: -3.9086\n",
      "Epoch 509/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4255 - val_loss: -3.6894\n",
      "Epoch 510/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3877 - val_loss: -3.9633\n",
      "Epoch 511/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4135 - val_loss: -4.0732\n",
      "Epoch 512/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3876 - val_loss: -1.3724\n",
      "Epoch 513/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4570 - val_loss: -3.6127\n",
      "Epoch 514/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3853 - val_loss: -3.9875\n",
      "Epoch 515/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2252 - val_loss: -3.4831\n",
      "Epoch 516/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3609 - val_loss: -4.0651\n",
      "Epoch 517/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4055 - val_loss: -4.1047\n",
      "Epoch 518/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4559 - val_loss: -3.4498\n",
      "Epoch 519/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4398 - val_loss: -4.0195\n",
      "Epoch 520/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.4495 - val_loss: -3.6309\n",
      "Epoch 521/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.4898 - val_loss: -3.9891\n",
      "Epoch 522/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4291 - val_loss: -3.9228\n",
      "Epoch 523/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5471 - val_loss: -3.6675\n",
      "Epoch 524/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3860 - val_loss: -2.6757\n",
      "Epoch 525/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5784 - val_loss: -2.5045\n",
      "Epoch 526/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3685 - val_loss: -3.6542\n",
      "Epoch 527/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4730 - val_loss: -3.5539\n",
      "Epoch 528/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4144 - val_loss: 3.5045\n",
      "Epoch 529/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9954 - val_loss: -3.9590\n",
      "Epoch 530/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4748 - val_loss: -3.9187\n",
      "Epoch 531/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4108 - val_loss: -3.8366\n",
      "Epoch 532/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5635 - val_loss: -4.2413\n",
      "Epoch 533/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0002 - val_loss: -4.0485\n",
      "Epoch 534/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5541 - val_loss: -4.1417\n",
      "Epoch 535/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4929 - val_loss: -3.8679\n",
      "Epoch 536/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4735 - val_loss: -3.3974\n",
      "Epoch 537/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3145 - val_loss: -3.8699\n",
      "Epoch 538/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5164 - val_loss: -2.9955\n",
      "Epoch 539/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4067 - val_loss: -4.1570\n",
      "Epoch 540/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5536 - val_loss: -3.7633\n",
      "Epoch 541/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5623 - val_loss: -4.1886\n",
      "Epoch 542/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5288 - val_loss: -4.1702\n",
      "Epoch 543/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3983 - val_loss: -3.9476\n",
      "Epoch 544/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.5884 - val_loss: -2.9463\n",
      "Epoch 545/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.0848 - val_loss: -3.6042\n",
      "Epoch 546/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4022 - val_loss: -3.3826\n",
      "Epoch 547/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4512 - val_loss: -3.9395\n",
      "Epoch 548/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4380 - val_loss: -4.0985\n",
      "Epoch 549/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5411 - val_loss: -4.2184\n",
      "Epoch 550/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5726 - val_loss: -3.7897\n",
      "Epoch 551/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5690 - val_loss: -4.0390\n",
      "Epoch 552/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5742 - val_loss: -4.2220\n",
      "Epoch 553/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5528 - val_loss: -3.4992\n",
      "Epoch 554/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5845 - val_loss: -4.0350\n",
      "Epoch 555/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4745 - val_loss: -3.9186\n",
      "Epoch 556/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.6147 - val_loss: -3.4609\n",
      "Epoch 557/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4653 - val_loss: -3.8937\n",
      "Epoch 558/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5441 - val_loss: -3.7518\n",
      "Epoch 559/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5627 - val_loss: -3.2825\n",
      "Epoch 560/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5763 - val_loss: -3.4322\n",
      "Epoch 561/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5721 - val_loss: -3.9509\n",
      "Epoch 562/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6067 - val_loss: -4.0686\n",
      "Epoch 563/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6467 - val_loss: -4.0476\n",
      "Epoch 564/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6506 - val_loss: -2.5731\n",
      "Epoch 565/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5457 - val_loss: -3.4052\n",
      "Epoch 566/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6159 - val_loss: -3.7699\n",
      "Epoch 567/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5817 - val_loss: -4.2428\n",
      "Epoch 568/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1532 - val_loss: -4.2157\n",
      "Epoch 569/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6858 - val_loss: -3.7964\n",
      "Epoch 570/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6327 - val_loss: -4.2818\n",
      "Epoch 571/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.6346 - val_loss: -3.7721\n",
      "Epoch 572/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5260 - val_loss: -3.9644\n",
      "Epoch 573/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6252 - val_loss: -4.1132\n",
      "Epoch 574/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6831 - val_loss: -3.5092\n",
      "Epoch 575/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3257 - val_loss: -3.7740\n",
      "Epoch 576/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6957 - val_loss: -3.9498\n",
      "Epoch 577/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.5678 - val_loss: -2.4677\n",
      "Epoch 578/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6640 - val_loss: -3.7732\n",
      "Epoch 579/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4447 - val_loss: -3.1531\n",
      "Epoch 580/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4520 - val_loss: -4.0839\n",
      "Epoch 581/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6825 - val_loss: -2.9103\n",
      "Epoch 582/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7016 - val_loss: -3.8438\n",
      "Epoch 583/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2879 - val_loss: -4.1698\n",
      "Epoch 584/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6303 - val_loss: -2.4187\n",
      "Epoch 585/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4628 - val_loss: -4.0202\n",
      "Epoch 586/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6360 - val_loss: -2.2169\n",
      "Epoch 587/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6818 - val_loss: -3.7319\n",
      "Epoch 588/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6871 - val_loss: -3.6437\n",
      "Epoch 589/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -2.9051 - val_loss: -4.0074\n",
      "Epoch 590/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2078 - val_loss: -4.0907\n",
      "Epoch 591/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4787 - val_loss: -3.9571\n",
      "Epoch 592/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.5521 - val_loss: -4.3207\n",
      "Epoch 593/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6275 - val_loss: -3.8805\n",
      "Epoch 594/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4949 - val_loss: -4.1671\n",
      "Epoch 595/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5894 - val_loss: -4.2840\n",
      "Epoch 596/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5774 - val_loss: -4.3014\n",
      "Epoch 597/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6729 - val_loss: -3.8861\n",
      "Epoch 598/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5207 - val_loss: -3.9340\n",
      "Epoch 599/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7029 - val_loss: -3.8805\n",
      "Epoch 600/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.8425 - val_loss: -3.5728\n",
      "Epoch 601/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4265 - val_loss: -4.1575\n",
      "Epoch 602/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6194 - val_loss: -4.2113\n",
      "Epoch 603/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6465 - val_loss: -2.4410\n",
      "Epoch 604/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6103 - val_loss: -3.7882\n",
      "Epoch 605/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6455 - val_loss: -4.2853\n",
      "Epoch 606/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6826 - val_loss: -3.9105\n",
      "Epoch 607/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3744 - val_loss: -4.1306\n",
      "Epoch 608/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6154 - val_loss: -3.1947\n",
      "Epoch 609/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7177 - val_loss: -3.6599\n",
      "Epoch 610/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0563 - val_loss: -3.6370\n",
      "Epoch 611/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5320 - val_loss: -3.6568\n",
      "Epoch 612/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7540 - val_loss: -3.7538\n",
      "Epoch 613/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6859 - val_loss: -4.2326\n",
      "Epoch 614/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7038 - val_loss: -3.0614\n",
      "Epoch 615/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7029 - val_loss: -3.1412\n",
      "Epoch 616/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5407 - val_loss: -4.1549\n",
      "Epoch 617/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8054 - val_loss: -3.9609\n",
      "Epoch 618/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3879 - val_loss: -3.7990\n",
      "Epoch 619/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7206 - val_loss: -4.2825\n",
      "Epoch 620/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6628 - val_loss: 1.8744\n",
      "Epoch 621/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9166 - val_loss: -4.0389\n",
      "Epoch 622/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7091 - val_loss: -4.1153\n",
      "Epoch 623/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5977 - val_loss: -4.0758\n",
      "Epoch 624/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7530 - val_loss: -4.2534\n",
      "Epoch 625/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7583 - val_loss: -4.2065\n",
      "Epoch 626/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0738 - val_loss: -3.4541\n",
      "Epoch 627/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6880 - val_loss: -4.0244\n",
      "Epoch 628/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6365 - val_loss: -2.2999\n",
      "Epoch 629/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4057 - val_loss: -4.0497\n",
      "Epoch 630/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7192 - val_loss: -4.1372\n",
      "Epoch 631/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7494 - val_loss: -3.5651\n",
      "Epoch 632/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3756 - val_loss: -3.4781\n",
      "Epoch 633/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4912 - val_loss: -3.8236\n",
      "Epoch 634/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6662 - val_loss: -3.4732\n",
      "Epoch 635/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6991 - val_loss: -3.5808\n",
      "Epoch 636/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7439 - val_loss: -4.3253\n",
      "Epoch 637/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5820 - val_loss: -3.4643\n",
      "Epoch 638/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4875 - val_loss: -0.3909\n",
      "Epoch 639/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5171 - val_loss: -3.9963\n",
      "Epoch 640/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4931 - val_loss: -4.2437\n",
      "Epoch 641/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8210 - val_loss: -3.5994\n",
      "Epoch 642/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7965 - val_loss: -3.9496\n",
      "Epoch 643/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7661 - val_loss: -4.1907\n",
      "Epoch 644/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7064 - val_loss: -4.0311\n",
      "Epoch 645/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7990 - val_loss: -2.0655\n",
      "Epoch 646/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7177 - val_loss: -3.9584\n",
      "Epoch 647/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7325 - val_loss: -3.8929\n",
      "Epoch 648/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7216 - val_loss: -3.2832\n",
      "Epoch 649/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8210 - val_loss: -4.3240\n",
      "Epoch 650/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7669 - val_loss: -3.9403\n",
      "Epoch 651/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8251 - val_loss: -4.3367\n",
      "Epoch 652/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6425 - val_loss: -2.9377\n",
      "Epoch 653/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6919 - val_loss: -3.5859\n",
      "Epoch 654/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7734 - val_loss: -3.7910\n",
      "Epoch 655/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8259 - val_loss: -4.3034\n",
      "Epoch 656/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7050 - val_loss: -4.2808\n",
      "Epoch 657/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5158 - val_loss: -4.3030\n",
      "Epoch 658/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8009 - val_loss: -3.8950\n",
      "Epoch 659/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7580 - val_loss: -3.7161\n",
      "Epoch 660/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7912 - val_loss: -4.1086\n",
      "Epoch 661/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6216 - val_loss: -4.0293\n",
      "Epoch 662/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7840 - val_loss: -4.0846\n",
      "Epoch 663/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8620 - val_loss: -4.0465\n",
      "Epoch 664/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7427 - val_loss: -3.8905\n",
      "Epoch 665/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8666 - val_loss: -3.9246\n",
      "Epoch 666/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8538"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -4.8538 - val_loss: -4.4206\n",
      "Epoch 667/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8760 - val_loss: -4.0878\n",
      "Epoch 668/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8507 - val_loss: -3.9251\n",
      "Epoch 669/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7004 - val_loss: -4.1777\n",
      "Epoch 670/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6319 - val_loss: -4.3858\n",
      "Epoch 671/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7439 - val_loss: -4.2242\n",
      "Epoch 672/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9057 - val_loss: -2.8914\n",
      "Epoch 673/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8294 - val_loss: -4.2689\n",
      "Epoch 674/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6412 - val_loss: -3.6938\n",
      "Epoch 675/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6905 - val_loss: -3.4788\n",
      "Epoch 676/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3124 - val_loss: -4.1980\n",
      "Epoch 677/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8602 - val_loss: -3.9255\n",
      "Epoch 678/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8480 - val_loss: -2.9155\n",
      "Epoch 679/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4190 - val_loss: -3.4999\n",
      "Epoch 680/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8829 - val_loss: -4.1210\n",
      "Epoch 681/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8464 - val_loss: -4.3679\n",
      "Epoch 682/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7863 - val_loss: -3.3939\n",
      "Epoch 683/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9362 - val_loss: -4.1115\n",
      "Epoch 684/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5723 - val_loss: -4.2379\n",
      "Epoch 685/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7128 - val_loss: -2.2953\n",
      "Epoch 686/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8066 - val_loss: -4.2253\n",
      "Epoch 687/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8058 - val_loss: -3.6687\n",
      "Epoch 688/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8303 - val_loss: -3.1596\n",
      "Epoch 689/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8161 - val_loss: -3.3663\n",
      "Epoch 690/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8215"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -4.8215 - val_loss: -4.4408\n",
      "Epoch 691/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8894 - val_loss: -4.2889\n",
      "Epoch 692/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6759 - val_loss: -3.9802\n",
      "Epoch 693/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9224 - val_loss: -4.3132\n",
      "Epoch 694/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7641 - val_loss: -4.4299\n",
      "Epoch 695/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9573 - val_loss: -3.2793\n",
      "Epoch 696/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0742 - val_loss: -3.4319\n",
      "Epoch 697/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4168 - val_loss: -4.4276\n",
      "Epoch 698/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8251 - val_loss: -4.0949\n",
      "Epoch 699/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6015 - val_loss: -3.5522\n",
      "Epoch 700/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2938 - val_loss: -4.2222\n",
      "Epoch 701/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8869 - val_loss: -4.1409\n",
      "Epoch 702/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7702"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.7702 - val_loss: -4.4835\n",
      "Epoch 703/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7898 - val_loss: -4.3890\n",
      "Epoch 704/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8760 - val_loss: -4.0991\n",
      "Epoch 705/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4449 - val_loss: 0.1791\n",
      "Epoch 706/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2533 - val_loss: -4.3444\n",
      "Epoch 707/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8567 - val_loss: -4.2318\n",
      "Epoch 708/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8839"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 29s 242ms/step - loss: -4.8839 - val_loss: -4.5687\n",
      "Epoch 709/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8904 - val_loss: -2.7053\n",
      "Epoch 710/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8791 - val_loss: -3.9330\n",
      "Epoch 711/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7413 - val_loss: -4.2715\n",
      "Epoch 712/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8044 - val_loss: -4.4409\n",
      "Epoch 713/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8687 - val_loss: -4.2885\n",
      "Epoch 714/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9673 - val_loss: -4.2850\n",
      "Epoch 715/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.8269 - val_loss: -3.9441\n",
      "Epoch 716/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7024 - val_loss: -4.3825\n",
      "Epoch 717/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8423 - val_loss: -3.9031\n",
      "Epoch 718/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8525 - val_loss: -3.5542\n",
      "Epoch 719/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8324 - val_loss: -4.1944\n",
      "Epoch 720/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8388 - val_loss: -4.2494\n",
      "Epoch 721/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8784 - val_loss: -4.0058\n",
      "Epoch 722/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0651 - val_loss: -2.9468\n",
      "Epoch 723/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1703 - val_loss: -3.7776\n",
      "Epoch 724/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6921 - val_loss: -3.9060\n",
      "Epoch 725/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8095 - val_loss: -0.0427\n",
      "Epoch 726/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7203 - val_loss: -1.9947\n",
      "Epoch 727/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8724 - val_loss: -4.0774\n",
      "Epoch 728/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3159 - val_loss: -3.9431\n",
      "Epoch 729/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9271 - val_loss: -4.1161\n",
      "Epoch 730/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9006 - val_loss: -4.4188\n",
      "Epoch 731/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8847 - val_loss: -4.1490\n",
      "Epoch 732/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7365 - val_loss: -4.4423\n",
      "Epoch 733/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7837 - val_loss: -3.7080\n",
      "Epoch 734/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9738 - val_loss: -3.9775\n",
      "Epoch 735/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8227 - val_loss: -4.3983\n",
      "Epoch 736/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9620 - val_loss: -4.0963\n",
      "Epoch 737/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9150 - val_loss: -4.4163\n",
      "Epoch 738/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7038 - val_loss: -4.1462\n",
      "Epoch 739/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8931 - val_loss: -3.6516\n",
      "Epoch 740/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.5739 - val_loss: -3.6279\n",
      "Epoch 741/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4731 - val_loss: -4.0695\n",
      "Epoch 742/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8235 - val_loss: -4.2083\n",
      "Epoch 743/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8673 - val_loss: -4.3069\n",
      "Epoch 744/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8939 - val_loss: -3.8162\n",
      "Epoch 745/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9436 - val_loss: -4.3712\n",
      "Epoch 746/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8328 - val_loss: -4.3572\n",
      "Epoch 747/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7638 - val_loss: -4.3367\n",
      "Epoch 748/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9130 - val_loss: -4.2349\n",
      "Epoch 749/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9228 - val_loss: -3.3480\n",
      "Epoch 750/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9545 - val_loss: -4.2734\n",
      "Epoch 751/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.4606 - val_loss: -3.6429\n",
      "Epoch 752/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8877 - val_loss: -4.1370\n",
      "Epoch 753/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9603 - val_loss: -3.0564\n",
      "Epoch 754/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8531 - val_loss: -3.0983\n",
      "Epoch 755/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8775 - val_loss: -4.0828\n",
      "Epoch 756/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9874 - val_loss: -3.7604\n",
      "Epoch 757/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7499 - val_loss: -3.6517\n",
      "Epoch 758/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9466 - val_loss: -4.2028\n",
      "Epoch 759/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6535 - val_loss: -4.1435\n",
      "Epoch 760/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0398 - val_loss: -3.8514\n",
      "Epoch 761/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9686 - val_loss: -4.1129\n",
      "Epoch 762/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9393 - val_loss: -4.0921\n",
      "Epoch 763/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9492 - val_loss: -3.4897\n",
      "Epoch 764/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0137 - val_loss: -4.1554\n",
      "Epoch 765/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8002 - val_loss: -4.3176\n",
      "Epoch 766/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9849 - val_loss: -3.5696\n",
      "Epoch 767/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0339 - val_loss: -3.9552\n",
      "Epoch 768/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9266 - val_loss: -4.1252\n",
      "Epoch 769/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0602 - val_loss: -4.2562\n",
      "Epoch 770/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8005 - val_loss: -4.3254\n",
      "Epoch 771/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9423 - val_loss: -4.2350\n",
      "Epoch 772/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9885 - val_loss: -4.2252\n",
      "Epoch 773/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4474 - val_loss: -4.2688\n",
      "Epoch 774/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8986 - val_loss: -4.0483\n",
      "Epoch 775/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9795 - val_loss: -4.3210\n",
      "Epoch 776/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8177 - val_loss: -4.1957\n",
      "Epoch 777/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8497 - val_loss: -4.3867\n",
      "Epoch 778/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0338 - val_loss: -4.3776\n",
      "Epoch 779/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9249 - val_loss: -4.3499\n",
      "Epoch 780/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9911 - val_loss: -4.3411\n",
      "Epoch 781/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9928 - val_loss: -2.8283\n",
      "Epoch 782/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0399 - val_loss: -4.1258\n",
      "Epoch 783/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9926 - val_loss: -2.8877\n",
      "Epoch 784/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0074 - val_loss: -4.5144\n",
      "Epoch 785/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0092 - val_loss: -0.6095\n",
      "Epoch 786/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9492 - val_loss: -4.1278\n",
      "Epoch 787/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9982 - val_loss: -4.0248\n",
      "Epoch 788/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7155 - val_loss: -2.7471\n",
      "Epoch 789/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.7971 - val_loss: -3.2817\n",
      "Epoch 790/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1751 - val_loss: -3.3991\n",
      "Epoch 791/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4453 - val_loss: -4.0320\n",
      "Epoch 792/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6965 - val_loss: -4.4087\n",
      "Epoch 793/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8022 - val_loss: -3.6667\n",
      "Epoch 794/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8326 - val_loss: -2.7817\n",
      "Epoch 795/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8786 - val_loss: -2.7583\n",
      "Epoch 796/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5906 - val_loss: -4.2758\n",
      "Epoch 797/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7260 - val_loss: -4.1737\n",
      "Epoch 798/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8422 - val_loss: -4.1026\n",
      "Epoch 799/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9480 - val_loss: -3.3959\n",
      "Epoch 800/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7782 - val_loss: -3.4243\n",
      "Epoch 801/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8618 - val_loss: -3.9144\n",
      "Epoch 802/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3358 - val_loss: -3.7711\n",
      "Epoch 803/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5485"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 256ms/step - loss: -4.5485 - val_loss: -4.6529\n",
      "Epoch 804/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5598 - val_loss: -3.9370\n",
      "Epoch 805/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8327 - val_loss: -4.2592\n",
      "Epoch 806/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8721 - val_loss: -3.8442\n",
      "Epoch 807/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8108 - val_loss: -3.6870\n",
      "Epoch 808/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8916 - val_loss: -4.4499\n",
      "Epoch 809/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8037 - val_loss: -3.9205\n",
      "Epoch 810/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8965 - val_loss: -2.1657\n",
      "Epoch 811/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9375 - val_loss: -4.0149\n",
      "Epoch 812/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8252 - val_loss: -3.5453\n",
      "Epoch 813/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9826 - val_loss: -4.2248\n",
      "Epoch 814/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9511 - val_loss: -4.3483\n",
      "Epoch 815/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0176 - val_loss: -4.2008\n",
      "Epoch 816/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6209 - val_loss: -3.7963\n",
      "Epoch 817/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8635 - val_loss: -4.0523\n",
      "Epoch 818/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9481 - val_loss: -3.9195\n",
      "Epoch 819/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9924 - val_loss: -4.4507\n",
      "Epoch 820/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9957 - val_loss: -4.4611\n",
      "Epoch 821/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9910 - val_loss: -2.8172\n",
      "Epoch 822/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.5917 - val_loss: -3.0153\n",
      "Epoch 823/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.3538 - val_loss: -3.3945\n",
      "Epoch 824/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.5888 - val_loss: -3.5784\n",
      "Epoch 825/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7019 - val_loss: -3.4178\n",
      "Epoch 826/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7401 - val_loss: -4.0010\n",
      "Epoch 827/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8465 - val_loss: -4.1505\n",
      "Epoch 828/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9059 - val_loss: -3.6756\n",
      "Epoch 829/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7927 - val_loss: -3.5304\n",
      "Epoch 830/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8841 - val_loss: -3.4204\n",
      "Epoch 831/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9943 - val_loss: -3.7001\n",
      "Epoch 832/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9509 - val_loss: -4.0695\n",
      "Epoch 833/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9405 - val_loss: -3.2402\n",
      "Epoch 834/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9494 - val_loss: -4.0974\n",
      "Epoch 835/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9144 - val_loss: -4.2414\n",
      "Epoch 836/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7609 - val_loss: -4.1105\n",
      "Epoch 837/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0557 - val_loss: -3.4868\n",
      "Epoch 838/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9474 - val_loss: -3.8179\n",
      "Epoch 839/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9168 - val_loss: -3.7709\n",
      "Epoch 840/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7947 - val_loss: -4.1953\n",
      "Epoch 841/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0473 - val_loss: -3.5843\n",
      "Epoch 842/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0547 - val_loss: -4.3497\n",
      "Epoch 843/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5180 - val_loss: -3.9580\n",
      "Epoch 844/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0139 - val_loss: -4.0710\n",
      "Epoch 845/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9487 - val_loss: -4.4841\n",
      "Epoch 846/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7509 - val_loss: 2.9377\n",
      "Epoch 847/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3747 - val_loss: -3.8087\n",
      "Epoch 848/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9237 - val_loss: -3.8631\n",
      "Epoch 849/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0047 - val_loss: -4.3029\n",
      "Epoch 850/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9539 - val_loss: -3.9532\n",
      "Epoch 851/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0789 - val_loss: -4.3028\n",
      "Epoch 852/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8674 - val_loss: -4.2587\n",
      "Epoch 853/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0926 - val_loss: -3.2201\n",
      "Epoch 854/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0766 - val_loss: -4.0457\n",
      "Epoch 855/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8327 - val_loss: 11.6365\n",
      "Epoch 856/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6540 - val_loss: -4.1915\n",
      "Epoch 857/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1055 - val_loss: -3.8886\n",
      "Epoch 858/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4117 - val_loss: -3.2354\n",
      "Epoch 859/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2669 - val_loss: -3.9017\n",
      "Epoch 860/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8860 - val_loss: -3.5684\n",
      "Epoch 861/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9007 - val_loss: -3.9288\n",
      "Epoch 862/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9444 - val_loss: -3.7957\n",
      "Epoch 863/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0775 - val_loss: -4.2844\n",
      "Epoch 864/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0209 - val_loss: -3.4367\n",
      "Epoch 865/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9243 - val_loss: -3.3739\n",
      "Epoch 866/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0584 - val_loss: -4.0534\n",
      "Epoch 867/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8881 - val_loss: -3.9847\n",
      "Epoch 868/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0350 - val_loss: -4.3624\n",
      "Epoch 869/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0446 - val_loss: -4.6199\n",
      "Epoch 870/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9840 - val_loss: -3.9408\n",
      "Epoch 871/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0434 - val_loss: -4.0099\n",
      "Epoch 872/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0431 - val_loss: -2.7136\n",
      "Epoch 873/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0166 - val_loss: -3.0494\n",
      "Epoch 874/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9395 - val_loss: -4.0390\n",
      "Epoch 875/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1056 - val_loss: -3.2358\n",
      "Epoch 876/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9158 - val_loss: -4.4464\n",
      "Epoch 877/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0838 - val_loss: -3.6354\n",
      "Epoch 878/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1226"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -5.1226 - val_loss: -4.6879\n",
      "Epoch 879/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9910 - val_loss: -4.2863\n",
      "Epoch 880/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9969 - val_loss: -4.2959\n",
      "Epoch 881/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1555 - val_loss: -3.7019\n",
      "Epoch 882/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8219 - val_loss: -3.8954\n",
      "Epoch 883/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0565 - val_loss: -4.4545\n",
      "Epoch 884/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0519 - val_loss: -3.6242\n",
      "Epoch 885/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0912 - val_loss: -4.3403\n",
      "Epoch 886/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0833 - val_loss: -4.1496\n",
      "Epoch 887/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1075 - val_loss: -3.3649\n",
      "Epoch 888/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0140 - val_loss: -4.5526\n",
      "Epoch 889/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0398 - val_loss: -4.3332\n",
      "Epoch 890/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9827 - val_loss: -4.2232\n",
      "Epoch 891/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1726 - val_loss: -4.0479\n",
      "Epoch 892/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7459 - val_loss: -3.0138\n",
      "Epoch 893/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6050 - val_loss: -3.3303\n",
      "Epoch 894/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1201 - val_loss: -4.2490\n",
      "Epoch 895/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1080 - val_loss: -3.6030\n",
      "Epoch 896/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6644 - val_loss: -4.0279\n",
      "Epoch 897/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0110 - val_loss: -4.1287\n",
      "Epoch 898/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7787 - val_loss: -3.4115\n",
      "Epoch 899/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1162 - val_loss: -3.7842\n",
      "Epoch 900/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0890 - val_loss: -4.0243\n",
      "Epoch 901/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7607 - val_loss: -4.0662\n",
      "Epoch 902/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0963 - val_loss: -3.7960\n",
      "Epoch 903/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1046 - val_loss: -3.9158\n",
      "Epoch 904/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0871 - val_loss: -3.7952\n",
      "Epoch 905/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0773 - val_loss: -4.3528\n",
      "Epoch 906/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0847 - val_loss: -3.9074\n",
      "Epoch 907/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9264 - val_loss: -3.9638\n",
      "Epoch 908/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6036 - val_loss: -4.4628\n",
      "Epoch 909/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9117 - val_loss: -3.8195\n",
      "Epoch 910/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0672 - val_loss: -4.3772\n",
      "Epoch 911/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7023 - val_loss: -3.7017\n",
      "Epoch 912/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0268 - val_loss: -4.5155\n",
      "Epoch 913/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0536 - val_loss: -2.3058\n",
      "Epoch 914/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0861 - val_loss: -4.6468\n",
      "Epoch 915/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2572 - val_loss: -4.3052\n",
      "Epoch 916/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0002 - val_loss: -4.0097\n",
      "Epoch 917/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1138 - val_loss: -4.1089\n",
      "Epoch 918/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0659 - val_loss: -3.5529\n",
      "Epoch 919/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0564 - val_loss: -4.1515\n",
      "Epoch 920/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0623 - val_loss: -4.1294\n",
      "Epoch 921/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5962 - val_loss: -2.2007\n",
      "Epoch 922/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0008 - val_loss: -3.9549\n",
      "Epoch 923/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1379 - val_loss: -4.3483\n",
      "Epoch 924/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0515 - val_loss: -3.2728\n",
      "Epoch 925/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3789 - val_loss: -4.0408\n",
      "Epoch 926/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0128 - val_loss: -3.1692\n",
      "Epoch 927/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0443 - val_loss: -3.9404\n",
      "Epoch 928/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1546 - val_loss: -3.9084\n",
      "Epoch 929/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0227 - val_loss: -4.3909\n",
      "Epoch 930/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1720 - val_loss: -4.1526\n",
      "Epoch 931/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1132 - val_loss: -4.1129\n",
      "Epoch 932/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1469 - val_loss: -4.2857\n",
      "Epoch 933/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1172 - val_loss: -4.5146\n",
      "Epoch 934/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1399 - val_loss: -4.4873\n",
      "Epoch 935/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6009 - val_loss: -3.2199\n",
      "Epoch 936/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6065 - val_loss: -4.1631\n",
      "Epoch 937/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1192 - val_loss: -4.5839\n",
      "Epoch 938/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0719 - val_loss: -4.0456\n",
      "Epoch 939/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1327 - val_loss: -3.8148\n",
      "Epoch 940/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0527 - val_loss: -4.2675\n",
      "Epoch 941/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9940 - val_loss: -4.0769\n",
      "Epoch 942/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4362 - val_loss: -2.9927\n",
      "Epoch 943/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9209 - val_loss: -3.9155\n",
      "Epoch 944/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6173 - val_loss: -3.8914\n",
      "Epoch 945/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8498 - val_loss: -3.7540\n",
      "Epoch 946/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9752 - val_loss: -4.5937\n",
      "Epoch 947/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0445 - val_loss: -4.0799\n",
      "Epoch 948/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2605 - val_loss: -3.4322\n",
      "Epoch 949/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7364 - val_loss: -2.9760\n",
      "Epoch 950/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0516 - val_loss: -4.3786\n",
      "Epoch 951/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9424 - val_loss: -3.8253\n",
      "Epoch 952/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1644 - val_loss: -4.2671\n",
      "Epoch 953/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7828 - val_loss: -2.4709\n",
      "Epoch 954/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0559 - val_loss: -4.2320\n",
      "Epoch 955/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1381 - val_loss: -3.4639\n",
      "Epoch 956/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0074 - val_loss: -3.8203\n",
      "Epoch 957/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8607 - val_loss: -4.0961\n",
      "Epoch 958/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2093 - val_loss: -4.4692\n",
      "Epoch 959/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1848 - val_loss: -4.4541\n",
      "Epoch 960/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8393 - val_loss: -3.5389\n",
      "Epoch 961/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0057 - val_loss: -4.5457\n",
      "Epoch 962/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1140 - val_loss: -4.5745\n",
      "Epoch 963/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9732 - val_loss: -4.3474\n",
      "Epoch 964/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1179 - val_loss: -4.4752\n",
      "Epoch 965/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1453 - val_loss: -4.2485\n",
      "Epoch 966/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1348 - val_loss: -3.6931\n",
      "Epoch 967/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2883 - val_loss: -3.0536\n",
      "Epoch 968/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1262 - val_loss: -3.8456\n",
      "Epoch 969/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8357 - val_loss: -3.7312\n",
      "Epoch 970/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0018 - val_loss: -4.2866\n",
      "Epoch 971/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0353 - val_loss: -4.5583\n",
      "Epoch 972/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5924 - val_loss: -4.2230\n",
      "Epoch 973/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0981 - val_loss: -4.5561\n",
      "Epoch 974/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0556 - val_loss: -4.2920\n",
      "Epoch 975/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1923 - val_loss: -4.3090\n",
      "Epoch 976/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0767 - val_loss: -4.1397\n",
      "Epoch 977/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9769 - val_loss: -3.9902\n",
      "Epoch 978/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1742 - val_loss: -3.7398\n",
      "Epoch 979/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1484 - val_loss: -4.4535\n",
      "Epoch 980/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0445 - val_loss: -4.3478\n",
      "Epoch 981/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2205 - val_loss: -3.9343\n",
      "Epoch 982/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0224 - val_loss: -4.5018\n",
      "Epoch 983/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2312 - val_loss: 1.2446\n",
      "Epoch 984/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5836 - val_loss: -2.9471\n",
      "Epoch 985/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5161 - val_loss: -3.8244\n",
      "Epoch 986/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0975 - val_loss: -4.0209\n",
      "Epoch 987/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1689 - val_loss: -4.4274\n",
      "Epoch 988/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1886 - val_loss: -4.3616\n",
      "Epoch 989/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1362 - val_loss: -3.0720\n",
      "Epoch 990/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1410 - val_loss: -3.6521\n",
      "Epoch 991/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2099 - val_loss: -4.0992\n",
      "Epoch 992/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1720 - val_loss: -4.5031\n",
      "Epoch 993/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1948 - val_loss: -4.4733\n",
      "Epoch 994/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6775 - val_loss: -4.0133\n",
      "Epoch 995/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1209 - val_loss: -3.4358\n",
      "Epoch 996/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0808 - val_loss: -4.1225\n",
      "Epoch 997/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2013 - val_loss: -1.7287\n",
      "Epoch 998/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -2.4543 - val_loss: -1.6220\n",
      "Epoch 999/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.2672 - val_loss: -2.2302\n",
      "Epoch 1000/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9180 - val_loss: -3.0382\n",
      "Epoch 1001/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1788 - val_loss: -3.4106\n",
      "Epoch 1002/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3228 - val_loss: -3.6059\n",
      "Epoch 1003/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4854 - val_loss: -3.4099\n",
      "Epoch 1004/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5556 - val_loss: -3.3393\n",
      "Epoch 1005/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6321 - val_loss: -3.6047\n",
      "Epoch 1006/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5821 - val_loss: -3.7061\n",
      "Epoch 1007/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7151 - val_loss: -3.6667\n",
      "Epoch 1008/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4503 - val_loss: -2.9691\n",
      "Epoch 1009/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9991 - val_loss: -3.8161\n",
      "Epoch 1010/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5151 - val_loss: -3.9782\n",
      "Epoch 1011/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6025 - val_loss: -3.8892\n",
      "Epoch 1012/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7693 - val_loss: -3.7607\n",
      "Epoch 1013/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8189 - val_loss: -4.1798\n",
      "Epoch 1014/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8596 - val_loss: -3.8463\n",
      "Epoch 1015/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8546 - val_loss: -3.8966\n",
      "Epoch 1016/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.8397 - val_loss: -4.0396\n",
      "Epoch 1017/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9783 - val_loss: -3.6807\n",
      "Epoch 1018/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8552 - val_loss: -4.0543\n",
      "Epoch 1019/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9189 - val_loss: -3.4662\n",
      "Epoch 1020/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6535 - val_loss: -3.6576\n",
      "Epoch 1021/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9548 - val_loss: -4.0688\n",
      "Epoch 1022/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9728 - val_loss: -4.1991\n",
      "Epoch 1023/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9769 - val_loss: -4.3751\n",
      "Epoch 1024/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9200 - val_loss: -3.9836\n",
      "Epoch 1025/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0315 - val_loss: -3.8474\n",
      "Epoch 1026/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9487 - val_loss: -3.4391\n",
      "Epoch 1027/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9872 - val_loss: -3.8107\n",
      "Epoch 1028/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8767 - val_loss: -2.7086\n",
      "Epoch 1029/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.6056 - val_loss: -3.2180\n",
      "Epoch 1030/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9789 - val_loss: -3.3269\n",
      "Epoch 1031/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1851 - val_loss: -3.2207\n",
      "Epoch 1032/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2783 - val_loss: -3.5002\n",
      "Epoch 1033/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4872 - val_loss: -3.5049\n",
      "Epoch 1034/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5676 - val_loss: -3.6304\n",
      "Epoch 1035/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5848 - val_loss: -3.7070\n",
      "Epoch 1036/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6753 - val_loss: -3.5382\n",
      "Epoch 1037/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6995 - val_loss: -4.1135\n",
      "Epoch 1038/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8071 - val_loss: -4.2241\n",
      "Epoch 1039/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8375 - val_loss: -4.1972\n",
      "Epoch 1040/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8772 - val_loss: 1.6546\n",
      "Epoch 1041/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6575 - val_loss: -4.3290\n",
      "Epoch 1042/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8364 - val_loss: -4.4226\n",
      "Epoch 1043/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8607 - val_loss: -3.2952\n",
      "Epoch 1044/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8486 - val_loss: -4.0982\n",
      "Epoch 1045/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8661 - val_loss: -3.5272\n",
      "Epoch 1046/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9174 - val_loss: -4.1431\n",
      "Epoch 1047/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9505 - val_loss: -3.5394\n",
      "Epoch 1048/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9296 - val_loss: -4.0790\n",
      "Epoch 1049/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9125 - val_loss: -4.1515\n",
      "Epoch 1050/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8210 - val_loss: -3.9253\n",
      "Epoch 1051/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9577 - val_loss: -3.7677\n",
      "Epoch 1052/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8094 - val_loss: -4.2215\n",
      "Epoch 1053/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9285 - val_loss: -4.3058\n",
      "Epoch 1054/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9838 - val_loss: -4.3280\n",
      "Epoch 1055/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0403 - val_loss: -0.5115\n",
      "Epoch 1056/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.0117 - val_loss: -4.1827\n",
      "Epoch 1057/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9706 - val_loss: -2.7085\n",
      "Epoch 1058/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9811 - val_loss: -3.8086\n",
      "Epoch 1059/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8647 - val_loss: -4.2963\n",
      "Epoch 1060/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8058 - val_loss: -3.4930\n",
      "Epoch 1061/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9722 - val_loss: -4.1160\n",
      "Epoch 1062/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0284 - val_loss: -3.9707\n",
      "Epoch 1063/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0661 - val_loss: -1.7031\n",
      "Epoch 1064/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9607 - val_loss: -4.1237\n",
      "Epoch 1065/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0523 - val_loss: -4.3988\n",
      "Epoch 1066/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0309 - val_loss: -3.9048\n",
      "Epoch 1067/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0685 - val_loss: -3.9711\n",
      "Epoch 1068/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9914 - val_loss: -1.1810\n",
      "Epoch 1069/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8337 - val_loss: -3.0612\n",
      "Epoch 1070/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6829 - val_loss: -4.1335\n",
      "Epoch 1071/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0901 - val_loss: -4.3086\n",
      "Epoch 1072/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0784 - val_loss: -3.6803\n",
      "Epoch 1073/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0662 - val_loss: -2.7457\n",
      "Epoch 1074/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3537 - val_loss: -2.6801\n",
      "Epoch 1075/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.2711 - val_loss: -3.4340\n",
      "Epoch 1076/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9867 - val_loss: -3.4208\n",
      "Epoch 1077/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4348 - val_loss: -3.9419\n",
      "Epoch 1078/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6381 - val_loss: -3.8393\n",
      "Epoch 1079/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7371 - val_loss: -3.9771\n",
      "Epoch 1080/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7696 - val_loss: -3.8339\n",
      "Epoch 1081/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8256 - val_loss: -3.9936\n",
      "Epoch 1082/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8739 - val_loss: -3.6920\n",
      "Epoch 1083/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9422 - val_loss: -4.0639\n",
      "Epoch 1084/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8466 - val_loss: -2.9647\n",
      "Epoch 1085/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9545 - val_loss: -4.0850\n",
      "Epoch 1086/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9066 - val_loss: -3.9973\n",
      "Epoch 1087/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6827 - val_loss: -3.8117\n",
      "Epoch 1088/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4853 - val_loss: -2.9914\n",
      "Epoch 1089/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9108 - val_loss: -3.9312\n",
      "Epoch 1090/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9216 - val_loss: -4.3798\n",
      "Epoch 1091/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8911 - val_loss: -4.3819\n",
      "Epoch 1092/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8598 - val_loss: -4.3409\n",
      "Epoch 1093/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9667 - val_loss: -4.2658\n",
      "Epoch 1094/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9081 - val_loss: -3.9601\n",
      "Epoch 1095/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0346 - val_loss: -4.1781\n",
      "Epoch 1096/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9845 - val_loss: -3.6371\n",
      "Epoch 1097/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8846 - val_loss: -3.9424\n",
      "Epoch 1098/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0619 - val_loss: -4.1527\n",
      "Epoch 1099/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8251 - val_loss: -4.4374\n",
      "Epoch 1100/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9532 - val_loss: -4.1183\n",
      "Epoch 1101/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0697 - val_loss: -4.0190\n",
      "Epoch 1102/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9349 - val_loss: -4.1936\n",
      "Epoch 1103/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8075 - val_loss: 9.0472\n",
      "Epoch 1104/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.6192 - val_loss: -3.3154\n",
      "Epoch 1105/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4949 - val_loss: -3.7664\n",
      "Epoch 1106/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7235 - val_loss: -3.9884\n",
      "Epoch 1107/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8004 - val_loss: -4.0858\n",
      "Epoch 1108/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8725 - val_loss: -3.9522\n",
      "Epoch 1109/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8949 - val_loss: -3.2323\n",
      "Epoch 1110/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8659 - val_loss: -2.8092\n",
      "Epoch 1111/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9718 - val_loss: -4.2743\n",
      "Epoch 1112/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6658 - val_loss: -4.1195\n",
      "Epoch 1113/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8445 - val_loss: -4.5388\n",
      "Epoch 1114/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8906 - val_loss: -4.5205\n",
      "Epoch 1115/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8349 - val_loss: -4.2590\n",
      "Epoch 1116/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0395 - val_loss: -0.7877\n",
      "Epoch 1117/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9188 - val_loss: -4.5789\n",
      "Epoch 1118/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.0247 - val_loss: -4.3316\n",
      "Epoch 1119/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0346 - val_loss: -4.4909\n",
      "Epoch 1120/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9364 - val_loss: -4.3951\n",
      "Epoch 1121/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9799 - val_loss: -4.2134\n",
      "Epoch 1122/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8871 - val_loss: -4.0490\n",
      "Epoch 1123/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0690 - val_loss: -4.5569\n",
      "Epoch 1124/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0282 - val_loss: -4.3055\n",
      "Epoch 1125/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9976 - val_loss: -4.3724\n",
      "Epoch 1126/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0071 - val_loss: -4.0348\n",
      "Epoch 1127/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0724 - val_loss: -4.3736\n",
      "Epoch 1128/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0840 - val_loss: -4.5366\n",
      "Epoch 1129/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0292 - val_loss: -3.8221\n",
      "Epoch 1130/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0994 - val_loss: -4.4616\n",
      "Epoch 1131/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0547 - val_loss: -4.5873\n",
      "Epoch 1132/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7885 - val_loss: -4.1702\n",
      "Epoch 1133/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0431 - val_loss: -4.0039\n",
      "Epoch 1134/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0896 - val_loss: -4.1342\n",
      "Epoch 1135/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1125 - val_loss: -4.4742\n",
      "Epoch 1136/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0770 - val_loss: -4.1332\n",
      "Epoch 1137/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0866 - val_loss: -4.5822\n",
      "Epoch 1138/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1152 - val_loss: -2.4520\n",
      "Epoch 1139/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0850 - val_loss: -4.3872\n",
      "Epoch 1140/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9506 - val_loss: -4.4617\n",
      "Epoch 1141/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1237 - val_loss: -4.1209\n",
      "Epoch 1142/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8418 - val_loss: -4.2780\n",
      "Epoch 1143/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9735 - val_loss: -4.6328\n",
      "Epoch 1144/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0871 - val_loss: -4.4983\n",
      "Epoch 1145/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8340 - val_loss: -4.4074\n",
      "Epoch 1146/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1377 - val_loss: -4.6222\n",
      "Epoch 1147/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9323 - val_loss: -4.5855\n",
      "Epoch 1148/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1542 - val_loss: -3.0960\n",
      "Epoch 1149/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9258 - val_loss: -4.5863\n",
      "Epoch 1150/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0550 - val_loss: -4.5765\n",
      "Epoch 1151/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0170 - val_loss: -4.5969\n",
      "Epoch 1152/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1238 - val_loss: -4.4474\n",
      "Epoch 1153/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0787 - val_loss: -4.2757\n",
      "Epoch 1154/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0818 - val_loss: -4.5898\n",
      "Epoch 1155/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1403 - val_loss: -3.5457\n",
      "Epoch 1156/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0752 - val_loss: -3.5652\n",
      "Epoch 1157/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1349 - val_loss: -3.4146\n",
      "Epoch 1158/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1299 - val_loss: -1.8312\n",
      "Epoch 1159/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9934 - val_loss: -3.6192\n",
      "Epoch 1160/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0744 - val_loss: -4.4067\n",
      "Epoch 1161/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9108 - val_loss: -4.4550\n",
      "Epoch 1162/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0930 - val_loss: -4.5647\n",
      "Epoch 1163/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1765 - val_loss: -4.6082\n",
      "Epoch 1164/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1909 - val_loss: -2.6133\n",
      "Epoch 1165/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.6875 - val_loss: -3.8285\n",
      "Epoch 1166/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3153 - val_loss: -3.9877\n",
      "Epoch 1167/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4224 - val_loss: -4.1376\n",
      "Epoch 1168/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4343 - val_loss: -3.9669\n",
      "Epoch 1169/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8540 - val_loss: -4.1002\n",
      "Epoch 1170/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9499 - val_loss: -3.7208\n",
      "Epoch 1171/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8580 - val_loss: -4.2195\n",
      "Epoch 1172/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0334 - val_loss: -4.2410\n",
      "Epoch 1173/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0160 - val_loss: -3.4945\n",
      "Epoch 1174/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2928 - val_loss: -4.0813\n",
      "Epoch 1175/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8593 - val_loss: -4.4433\n",
      "Epoch 1176/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9834 - val_loss: -4.3429\n",
      "Epoch 1177/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0301 - val_loss: -3.6732\n",
      "Epoch 1178/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0538 - val_loss: -4.2270\n",
      "Epoch 1179/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0194 - val_loss: -4.4403\n",
      "Epoch 1180/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0516 - val_loss: -4.4484\n",
      "Epoch 1181/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0310 - val_loss: -4.2214\n",
      "Epoch 1182/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0445 - val_loss: -4.2653\n",
      "Epoch 1183/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1454 - val_loss: -4.2784\n",
      "Epoch 1184/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8701 - val_loss: -3.3970\n",
      "Epoch 1185/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6142 - val_loss: -3.9454\n",
      "Epoch 1186/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8914 - val_loss: -3.9681\n",
      "Epoch 1187/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5883 - val_loss: -3.8742\n",
      "Epoch 1188/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9976 - val_loss: -4.0157\n",
      "Epoch 1189/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0601 - val_loss: -4.4853\n",
      "Epoch 1190/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9819 - val_loss: -4.2844\n",
      "Epoch 1191/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0504 - val_loss: -4.4928\n",
      "Epoch 1192/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0619 - val_loss: -4.5362\n",
      "Epoch 1193/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0208 - val_loss: -4.2658\n",
      "Epoch 1194/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5468 - val_loss: -4.3715\n",
      "Epoch 1195/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0644 - val_loss: -4.3926\n",
      "Epoch 1196/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1206 - val_loss: -3.4935\n",
      "Epoch 1197/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0412 - val_loss: -4.4479\n",
      "Epoch 1198/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0837 - val_loss: -4.6534\n",
      "Epoch 1199/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1165 - val_loss: -3.8194\n",
      "Epoch 1200/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1389 - val_loss: -3.6835\n",
      "Epoch 1201/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1777 - val_loss: -4.3609\n",
      "Epoch 1202/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0553 - val_loss: -2.2734\n",
      "Epoch 1203/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1106 - val_loss: -4.5606\n",
      "Epoch 1204/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1633 - val_loss: -3.8899\n",
      "Epoch 1205/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.2241"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -5.2241 - val_loss: -4.6899\n",
      "Epoch 1206/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0402 - val_loss: -4.4292\n",
      "Epoch 1207/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1428 - val_loss: -3.9538\n",
      "Epoch 1208/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0417 - val_loss: -4.6153\n",
      "Epoch 1209/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8989 - val_loss: -3.1368\n",
      "Epoch 1210/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5823 - val_loss: -4.3562\n",
      "Epoch 1211/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0988 - val_loss: -4.0976\n",
      "Epoch 1212/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1737 - val_loss: -4.1047\n",
      "Epoch 1213/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1201 - val_loss: -4.4830\n",
      "Epoch 1214/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0840 - val_loss: -4.4526\n",
      "Epoch 1215/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0515 - val_loss: -4.3335\n",
      "Epoch 1216/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1972 - val_loss: -4.6623\n",
      "Epoch 1217/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1596 - val_loss: -4.0283\n",
      "Epoch 1218/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2225 - val_loss: -4.6174\n",
      "Epoch 1219/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0534 - val_loss: -4.4372\n",
      "Epoch 1220/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2172 - val_loss: -2.4545\n",
      "Epoch 1221/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4725 - val_loss: -4.5359\n",
      "Epoch 1222/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1483 - val_loss: -4.6219\n",
      "Epoch 1223/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1977 - val_loss: -4.3749\n",
      "Epoch 1224/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1694 - val_loss: -3.5734\n",
      "Epoch 1225/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0652 - val_loss: -3.9493\n",
      "Epoch 1226/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1588 - val_loss: -4.3900\n",
      "Epoch 1227/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2132 - val_loss: -4.2865\n",
      "Epoch 1228/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1944 - val_loss: -4.1405\n",
      "Epoch 1229/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1973 - val_loss: 0.1297\n",
      "Epoch 1230/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0373 - val_loss: -4.5716\n",
      "Epoch 1231/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2417 - val_loss: -4.4409\n",
      "Epoch 1232/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1171 - val_loss: -4.3307\n",
      "Epoch 1233/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2015 - val_loss: -4.2213\n",
      "Epoch 1234/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.5645 - val_loss: -4.1887\n",
      "Epoch 1235/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6161 - val_loss: -3.4868\n",
      "Epoch 1236/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9021 - val_loss: -4.5062\n",
      "Epoch 1237/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0521 - val_loss: -4.4336\n",
      "Epoch 1238/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0155 - val_loss: -4.4455\n",
      "Epoch 1239/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9738 - val_loss: -4.6014\n",
      "Epoch 1240/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1260 - val_loss: -4.0211\n",
      "Epoch 1241/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8849 - val_loss: -4.6691\n",
      "Epoch 1242/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1621 - val_loss: -4.6040\n",
      "Epoch 1243/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1957 - val_loss: -4.4015\n",
      "Epoch 1244/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1400 - val_loss: -4.6461\n",
      "Epoch 1245/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2389 - val_loss: -4.5224\n",
      "Epoch 1246/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1741 - val_loss: -4.1222\n",
      "Epoch 1247/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9160 - val_loss: -1.1520\n",
      "Epoch 1248/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4157 - val_loss: -4.5390\n",
      "Epoch 1249/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1357 - val_loss: -4.0538\n",
      "Epoch 1250/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0688 - val_loss: -4.6208\n",
      "Epoch 1251/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2576 - val_loss: -4.6664\n",
      "Epoch 1252/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1302 - val_loss: -4.3008\n",
      "Epoch 1253/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1854 - val_loss: -3.4360\n",
      "Epoch 1254/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1227 - val_loss: -3.9856\n",
      "Epoch 1255/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8079 - val_loss: -4.4647\n",
      "Epoch 1256/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1754 - val_loss: -3.5866\n",
      "Epoch 1257/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1347 - val_loss: -4.5697\n",
      "Epoch 1258/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2747 - val_loss: -4.2232\n",
      "Epoch 1259/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9818 - val_loss: -4.1575\n",
      "Epoch 1260/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9450 - val_loss: -4.5551\n",
      "Epoch 1261/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1613 - val_loss: -3.8778\n",
      "Epoch 1262/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2677 - val_loss: -4.3444\n",
      "Epoch 1263/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0577 - val_loss: -3.2600\n",
      "Epoch 1264/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1824 - val_loss: -4.2230\n",
      "Epoch 1265/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2335 - val_loss: -4.3357\n",
      "Epoch 1266/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2591 - val_loss: -3.9837\n",
      "Epoch 1267/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1773 - val_loss: -4.2894\n",
      "Epoch 1268/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2754 - val_loss: -4.5921\n",
      "Epoch 1269/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4405 - val_loss: -4.4947\n",
      "Epoch 1270/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0829 - val_loss: -4.4645\n",
      "Epoch 1271/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2567 - val_loss: -4.0188\n",
      "Epoch 1272/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2718 - val_loss: -2.8153\n",
      "Epoch 1273/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2723 - val_loss: -4.6312\n",
      "Epoch 1274/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2275 - val_loss: -4.6482\n",
      "Epoch 1275/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1834 - val_loss: -2.7053\n",
      "Epoch 1276/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1654 - val_loss: -4.4647\n",
      "Epoch 1277/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2868 - val_loss: -3.8279\n",
      "Epoch 1278/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1600 - val_loss: -4.6267\n",
      "Epoch 1279/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2307 - val_loss: -1.2266\n",
      "Epoch 1280/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0801 - val_loss: -4.2337\n",
      "Epoch 1281/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2410 - val_loss: -4.2676\n",
      "Epoch 1282/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0898 - val_loss: -3.6346\n",
      "Epoch 1283/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3022 - val_loss: -4.2639\n",
      "Epoch 1284/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2176 - val_loss: -4.3165\n",
      "Epoch 1285/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2682 - val_loss: -4.5069\n",
      "Epoch 1286/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3199 - val_loss: -4.3517\n",
      "Epoch 1287/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2106 - val_loss: -1.1208\n",
      "Epoch 1288/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2825 - val_loss: -3.6960\n",
      "Epoch 1289/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2155 - val_loss: -3.7369\n",
      "Epoch 1290/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3201 - val_loss: -4.4047\n",
      "Epoch 1291/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2388 - val_loss: -3.2931\n",
      "Epoch 1292/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2854 - val_loss: -4.6320\n",
      "Epoch 1293/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2414 - val_loss: -4.0317\n",
      "Epoch 1294/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2576 - val_loss: -4.4124\n",
      "Epoch 1295/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2533 - val_loss: -4.4201\n",
      "Epoch 1296/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.3606 - val_loss: -3.3859\n",
      "Epoch 1297/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.3180 - val_loss: -4.3860\n",
      "Epoch 1298/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6930 - val_loss: -4.5251\n",
      "Epoch 1299/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7738 - val_loss: -3.5398\n",
      "Epoch 1300/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8342"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 237ms/step - loss: -4.8342 - val_loss: -4.7505\n",
      "Epoch 1301/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6123 - val_loss: -4.1219\n",
      "Epoch 1302/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8733 - val_loss: -4.1990\n",
      "Epoch 1303/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0258 - val_loss: -4.0916\n",
      "Epoch 1304/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9119"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -4.9119 - val_loss: -4.7723\n",
      "Epoch 1305/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8723 - val_loss: -4.6608\n",
      "Epoch 1306/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0918 - val_loss: -4.5279\n",
      "Epoch 1307/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0947 - val_loss: -3.8090\n",
      "Epoch 1308/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0267 - val_loss: -4.5090\n",
      "Epoch 1309/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9467 - val_loss: -4.4722\n",
      "Epoch 1310/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1357 - val_loss: -4.0052\n",
      "Epoch 1311/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0613 - val_loss: -4.7355\n",
      "Epoch 1312/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0647 - val_loss: -4.2880\n",
      "Epoch 1313/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1526 - val_loss: -3.8000\n",
      "Epoch 1314/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1679 - val_loss: -4.6019\n",
      "Epoch 1315/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0227 - val_loss: -4.3784\n",
      "Epoch 1316/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2428 - val_loss: -4.1393\n",
      "Epoch 1317/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1919 - val_loss: -4.5963\n",
      "Epoch 1318/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1034 - val_loss: -4.5081\n",
      "Epoch 1319/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1045 - val_loss: -4.4828\n",
      "Epoch 1320/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2218 - val_loss: -4.1262\n",
      "Epoch 1321/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2845 - val_loss: -4.3608\n",
      "Epoch 1322/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2492 - val_loss: -4.6678\n",
      "Epoch 1323/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1869 - val_loss: -4.0754\n",
      "Epoch 1324/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2944 - val_loss: -4.3271\n",
      "Epoch 1325/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1237 - val_loss: -4.5131\n",
      "Epoch 1326/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1138 - val_loss: -4.0305\n",
      "Epoch 1327/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3003 - val_loss: -4.1963\n",
      "Epoch 1328/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3042 - val_loss: -4.4361\n",
      "Epoch 1329/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -1.6711 - val_loss: -3.1297\n",
      "Epoch 1330/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.4039 - val_loss: -4.0325\n",
      "Epoch 1331/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9251 - val_loss: -4.2154\n",
      "Epoch 1332/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1817 - val_loss: -4.3547\n",
      "Epoch 1333/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3999 - val_loss: -4.2197\n",
      "Epoch 1334/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4484 - val_loss: -3.3569\n",
      "Epoch 1335/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5749 - val_loss: -3.9616\n",
      "Epoch 1336/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6464 - val_loss: -3.0177\n",
      "Epoch 1337/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6293 - val_loss: -4.3648\n",
      "Epoch 1338/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6181 - val_loss: -3.9908\n",
      "Epoch 1339/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7131 - val_loss: -3.8577\n",
      "Epoch 1340/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8014 - val_loss: -3.9457\n",
      "Epoch 1341/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7257 - val_loss: -4.2169\n",
      "Epoch 1342/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8658 - val_loss: -3.9196\n",
      "Epoch 1343/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8308 - val_loss: -4.3229\n",
      "Epoch 1344/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7275 - val_loss: -2.6830\n",
      "Epoch 1345/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8300 - val_loss: -4.4183\n",
      "Epoch 1346/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2129 - val_loss: -3.1591\n",
      "Epoch 1347/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3881 - val_loss: -3.0332\n",
      "Epoch 1348/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7494 - val_loss: -3.8531\n",
      "Epoch 1349/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9041 - val_loss: -3.9557\n",
      "Epoch 1350/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7646 - val_loss: -4.5849\n",
      "Epoch 1351/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8944 - val_loss: -3.2646\n",
      "Epoch 1352/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8893 - val_loss: -4.1432\n",
      "Epoch 1353/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6371 - val_loss: -2.2346\n",
      "Epoch 1354/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7634 - val_loss: -4.4746\n",
      "Epoch 1355/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9474 - val_loss: -3.9967\n",
      "Epoch 1356/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9207 - val_loss: -4.4529\n",
      "Epoch 1357/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9318 - val_loss: -4.4526\n",
      "Epoch 1358/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7498 - val_loss: -4.1982\n",
      "Epoch 1359/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9338 - val_loss: -3.7586\n",
      "Epoch 1360/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8373 - val_loss: -4.0715\n",
      "Epoch 1361/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0528 - val_loss: -3.7316\n",
      "Epoch 1362/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0020 - val_loss: -4.5964\n",
      "Epoch 1363/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9496 - val_loss: -3.1026\n",
      "Epoch 1364/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0351 - val_loss: -4.1186\n",
      "Epoch 1365/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0825 - val_loss: -3.6763\n",
      "Epoch 1366/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6875 - val_loss: -4.0406\n",
      "Epoch 1367/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0959 - val_loss: -4.5460\n",
      "Epoch 1368/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1095 - val_loss: -3.9000\n",
      "Epoch 1369/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0113 - val_loss: -4.5353\n",
      "Epoch 1370/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9839 - val_loss: -4.5899\n",
      "Epoch 1371/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9889 - val_loss: -2.6544\n",
      "Epoch 1372/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1947 - val_loss: -4.2121\n",
      "Epoch 1373/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0944 - val_loss: -3.8103\n",
      "Epoch 1374/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7462 - val_loss: -4.0389\n",
      "Epoch 1375/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1452 - val_loss: -3.7945\n",
      "Epoch 1376/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2758 - val_loss: -4.6148\n",
      "Epoch 1377/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1699 - val_loss: -4.2144\n",
      "Epoch 1378/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1441 - val_loss: -4.0838\n",
      "Epoch 1379/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3192 - val_loss: -3.7794\n",
      "Epoch 1380/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1862 - val_loss: -3.8953\n",
      "Epoch 1381/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1352 - val_loss: -4.7654\n",
      "Epoch 1382/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2774 - val_loss: -4.7426\n",
      "Epoch 1383/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3152 - val_loss: -4.1703\n",
      "Epoch 1384/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2913 - val_loss: -3.6921\n",
      "Epoch 1385/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1703 - val_loss: -4.5680\n",
      "Epoch 1386/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2897 - val_loss: -4.5568\n",
      "Epoch 1387/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2840 - val_loss: -1.9362\n",
      "Epoch 1388/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2744 - val_loss: -4.3702\n",
      "Epoch 1389/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9952 - val_loss: -4.0990\n",
      "Epoch 1390/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1755 - val_loss: -3.7599\n",
      "Epoch 1391/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3011 - val_loss: -4.5687\n",
      "Epoch 1392/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0841 - val_loss: -4.1408\n",
      "Epoch 1393/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3213 - val_loss: -2.2900\n",
      "Epoch 1394/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8544 - val_loss: -4.1284\n",
      "Epoch 1395/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1896 - val_loss: -3.9334\n",
      "Epoch 1396/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1744 - val_loss: -3.6983\n",
      "Epoch 1397/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2906 - val_loss: -4.0844\n",
      "Epoch 1398/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2885 - val_loss: -1.9144\n",
      "Epoch 1399/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1342 - val_loss: -4.4616\n",
      "Epoch 1400/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8127 - val_loss: -3.8012\n",
      "Epoch 1401/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9653 - val_loss: -3.8754\n",
      "Epoch 1402/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9500 - val_loss: -4.5694\n",
      "Epoch 1403/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1270 - val_loss: -4.3520\n",
      "Epoch 1404/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0961 - val_loss: -4.4630\n",
      "Epoch 1405/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1740 - val_loss: -4.2666\n",
      "Epoch 1406/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1797 - val_loss: -3.8822\n",
      "Epoch 1407/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2717 - val_loss: -3.6396\n",
      "Epoch 1408/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2113 - val_loss: -3.9311\n",
      "Epoch 1409/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2608 - val_loss: -4.1857\n",
      "Epoch 1410/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2328 - val_loss: -4.7233\n",
      "Epoch 1411/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2534 - val_loss: -4.2172\n",
      "Epoch 1412/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8568 - val_loss: -2.7788\n",
      "Epoch 1413/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9328 - val_loss: -4.3179\n",
      "Epoch 1414/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2720 - val_loss: -4.1832\n",
      "Epoch 1415/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2787 - val_loss: -4.2963\n",
      "Epoch 1416/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2435 - val_loss: -3.9309\n",
      "Epoch 1417/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2248 - val_loss: -3.8575\n",
      "Epoch 1418/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2871 - val_loss: -4.7533\n",
      "Epoch 1419/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2929 - val_loss: -4.3083\n",
      "Epoch 1420/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2632 - val_loss: -4.4929\n",
      "Epoch 1421/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2512 - val_loss: -4.1827\n",
      "Epoch 1422/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3318 - val_loss: -3.2708\n",
      "Epoch 1423/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3076 - val_loss: -4.3512\n",
      "Epoch 1424/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2371 - val_loss: -2.8465\n",
      "Epoch 1425/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3373 - val_loss: -3.0543\n",
      "Epoch 1426/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2737 - val_loss: -3.6732\n",
      "Epoch 1427/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3113 - val_loss: -4.5232\n",
      "Epoch 1428/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2750 - val_loss: -4.5782\n",
      "Epoch 1429/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2349 - val_loss: -4.4285\n",
      "Epoch 1430/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3431 - val_loss: -4.5609\n",
      "Epoch 1431/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3661 - val_loss: -0.9516\n",
      "Epoch 1432/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -5.3000 - val_loss: -4.9253\n",
      "Epoch 1433/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2408 - val_loss: -4.4186\n",
      "Epoch 1434/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8421 - val_loss: -4.1992\n",
      "Epoch 1435/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9874 - val_loss: -4.2786\n",
      "Epoch 1436/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2509 - val_loss: -2.6716\n",
      "Epoch 1437/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1737 - val_loss: -4.1782\n",
      "Epoch 1438/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2531 - val_loss: -3.8276\n",
      "Epoch 1439/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3505 - val_loss: -4.5530\n",
      "Epoch 1440/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2871 - val_loss: -4.4261\n",
      "Epoch 1441/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1649 - val_loss: -2.6912\n",
      "Epoch 1442/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3100 - val_loss: -4.5000\n",
      "Epoch 1443/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3179 - val_loss: -4.0265\n",
      "Epoch 1444/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3214 - val_loss: -4.6374\n",
      "Epoch 1445/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3221 - val_loss: -4.1722\n",
      "Epoch 1446/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3023 - val_loss: 2.3253\n",
      "Epoch 1447/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2825 - val_loss: -3.6291\n",
      "Epoch 1448/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2203 - val_loss: -4.3020\n",
      "Epoch 1449/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3352 - val_loss: -3.5704\n",
      "Epoch 1450/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0529 - val_loss: -4.2080\n",
      "Epoch 1451/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3113 - val_loss: -3.7882\n",
      "Epoch 1452/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2430 - val_loss: -4.1768\n",
      "Epoch 1453/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4103 - val_loss: 3.5849\n",
      "Epoch 1454/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2577 - val_loss: -4.6110\n",
      "Epoch 1455/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3615 - val_loss: -4.6562\n",
      "Epoch 1456/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0292 - val_loss: -3.4559\n",
      "Epoch 1457/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2047 - val_loss: -4.5852\n",
      "Epoch 1458/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2934 - val_loss: -4.7185\n",
      "Epoch 1459/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3265 - val_loss: -4.3669\n",
      "Epoch 1460/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3411 - val_loss: -4.4445\n",
      "Epoch 1461/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3731 - val_loss: -2.8843\n",
      "Epoch 1462/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3425 - val_loss: -4.4206\n",
      "Epoch 1463/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2098 - val_loss: -3.6635\n",
      "Epoch 1464/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3812 - val_loss: -4.5156\n",
      "Epoch 1465/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2916 - val_loss: -4.6842\n",
      "Epoch 1466/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3946 - val_loss: -3.2620\n",
      "Epoch 1467/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1210 - val_loss: -3.7424\n",
      "Epoch 1468/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3990 - val_loss: -2.1614\n",
      "Epoch 1469/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2871 - val_loss: -3.6108\n",
      "Epoch 1470/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3574 - val_loss: -2.9037\n",
      "Epoch 1471/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3801 - val_loss: -3.5133\n",
      "Epoch 1472/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4066 - val_loss: -4.6963\n",
      "Epoch 1473/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1137 - val_loss: -4.4877\n",
      "Epoch 1474/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3015 - val_loss: -4.6779\n",
      "Epoch 1475/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3844 - val_loss: -4.5075\n",
      "Epoch 1476/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4210 - val_loss: -3.0447\n",
      "Epoch 1477/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1414 - val_loss: -3.8954\n",
      "Epoch 1478/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2805 - val_loss: -4.5342\n",
      "Epoch 1479/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3802 - val_loss: -4.9083\n",
      "Epoch 1480/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3931 - val_loss: -4.6068\n",
      "Epoch 1481/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3961 - val_loss: -3.8679\n",
      "Epoch 1482/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4256 - val_loss: -4.2966\n",
      "Epoch 1483/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2705 - val_loss: -2.2586\n",
      "Epoch 1484/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4157 - val_loss: -4.4222\n",
      "Epoch 1485/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3772 - val_loss: -4.3023\n",
      "Epoch 1486/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3684 - val_loss: -4.3460\n",
      "Epoch 1487/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2902 - val_loss: -3.4585\n",
      "Epoch 1488/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4024 - val_loss: -4.3768\n",
      "Epoch 1489/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3276 - val_loss: -3.8613\n",
      "Epoch 1490/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0371 - val_loss: -4.3055\n",
      "Epoch 1491/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4022 - val_loss: -3.8342\n",
      "Epoch 1492/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3775 - val_loss: -4.1202\n",
      "Epoch 1493/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2893 - val_loss: -3.6503\n",
      "Epoch 1494/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3788 - val_loss: -4.4113\n",
      "Epoch 1495/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0424 - val_loss: -3.9845\n",
      "Epoch 1496/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2145 - val_loss: -3.9648\n",
      "Epoch 1497/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2738 - val_loss: -3.5529\n",
      "Epoch 1498/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3012 - val_loss: -4.4978\n",
      "Epoch 1499/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3502 - val_loss: -4.2344\n",
      "Epoch 1500/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3953 - val_loss: -0.0552\n",
      "Epoch 1501/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2877 - val_loss: -4.4247\n",
      "Epoch 1502/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2988 - val_loss: -4.2309\n",
      "Epoch 1503/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4012 - val_loss: -4.4453\n",
      "Epoch 1504/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4044 - val_loss: -4.4422\n",
      "Epoch 1505/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4278 - val_loss: -4.4167\n",
      "Epoch 1506/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3943 - val_loss: -4.9077\n",
      "Epoch 1507/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2202 - val_loss: -3.3571\n",
      "Epoch 1508/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3506 - val_loss: -4.2391\n",
      "Epoch 1509/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4262 - val_loss: -4.3284\n",
      "Epoch 1510/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4570 - val_loss: -2.6040\n",
      "Epoch 1511/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3348 - val_loss: -4.0822\n",
      "Epoch 1512/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3367 - val_loss: -3.7819\n",
      "Epoch 1513/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3907 - val_loss: -4.2999\n",
      "Epoch 1514/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3787 - val_loss: -4.5460\n",
      "Epoch 1515/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2956 - val_loss: -4.0791\n",
      "Epoch 1516/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3920 - val_loss: -3.0352\n",
      "Epoch 1517/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3703 - val_loss: -4.4852\n",
      "Epoch 1518/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3884 - val_loss: -3.7137\n",
      "Epoch 1519/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6972 - val_loss: -3.4907\n",
      "Epoch 1520/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2104 - val_loss: -3.8201\n",
      "Epoch 1521/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3435 - val_loss: -4.5388\n",
      "Epoch 1522/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2561 - val_loss: -4.0840\n",
      "Epoch 1523/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2881 - val_loss: -4.5699\n",
      "Epoch 1524/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3537 - val_loss: -3.6344\n",
      "Epoch 1525/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2848 - val_loss: -3.6657\n",
      "Epoch 1526/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4331 - val_loss: -4.3362\n",
      "Epoch 1527/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2534 - val_loss: -4.6378\n",
      "Epoch 1528/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3332 - val_loss: -4.0859\n",
      "Epoch 1529/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3833 - val_loss: -3.7781\n",
      "Epoch 1530/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3686 - val_loss: -4.6917\n",
      "Epoch 1531/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4283 - val_loss: -3.8930\n",
      "Epoch 1532/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4298 - val_loss: -4.0495\n",
      "Epoch 1533/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3218 - val_loss: -3.9425\n",
      "Epoch 1534/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4016 - val_loss: -3.9580\n",
      "Epoch 1535/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4617 - val_loss: -4.0579\n",
      "Epoch 1536/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7176 - val_loss: -4.3443\n",
      "Epoch 1537/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2274 - val_loss: -3.8200\n",
      "Epoch 1538/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3660 - val_loss: -4.5730\n",
      "Epoch 1539/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3531 - val_loss: -4.3258\n",
      "Epoch 1540/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1443 - val_loss: -4.3012\n",
      "Epoch 1541/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2502 - val_loss: -4.8583\n",
      "Epoch 1542/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3867 - val_loss: -3.2966\n",
      "Epoch 1543/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4155 - val_loss: -4.1567\n",
      "Epoch 1544/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3802 - val_loss: -3.9459\n",
      "Epoch 1545/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2355 - val_loss: -4.5928\n",
      "Epoch 1546/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2672 - val_loss: -4.3675\n",
      "Epoch 1547/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3057 - val_loss: -3.7771\n",
      "Epoch 1548/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4260 - val_loss: -4.4070\n",
      "Epoch 1549/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4408 - val_loss: -4.2171\n",
      "Epoch 1550/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4657 - val_loss: -3.8934\n",
      "Epoch 1551/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3839 - val_loss: -3.6498\n",
      "Epoch 1552/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3471 - val_loss: -2.5082\n",
      "Epoch 1553/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3126 - val_loss: -3.2548\n",
      "Epoch 1554/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4629 - val_loss: -4.1626\n",
      "Epoch 1555/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2920 - val_loss: -4.2733\n",
      "Epoch 1556/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2358 - val_loss: -4.5017\n",
      "Epoch 1557/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4282 - val_loss: -3.9637\n",
      "Epoch 1558/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3455 - val_loss: -4.6895\n",
      "Epoch 1559/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2991 - val_loss: -4.6514\n",
      "Epoch 1560/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3533 - val_loss: -4.1733\n",
      "Epoch 1561/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4175 - val_loss: -3.2119\n",
      "Epoch 1562/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3686 - val_loss: -4.1826\n",
      "Epoch 1563/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4310 - val_loss: -4.0223\n",
      "Epoch 1564/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4109 - val_loss: -4.3179\n",
      "Epoch 1565/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3389 - val_loss: -4.5516\n",
      "Epoch 1566/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4471 - val_loss: -4.4719\n",
      "Epoch 1567/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4252 - val_loss: -4.7446\n",
      "Epoch 1568/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2830 - val_loss: -4.5765\n",
      "Epoch 1569/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3900 - val_loss: -4.2325\n",
      "Epoch 1570/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3176 - val_loss: -4.0204\n",
      "Epoch 1571/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4693 - val_loss: -4.2033\n",
      "Epoch 1572/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4575 - val_loss: -4.2789\n",
      "Epoch 1573/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2933 - val_loss: -3.4492\n",
      "Epoch 1574/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3735 - val_loss: -3.8852\n",
      "Epoch 1575/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3159 - val_loss: -4.3119\n",
      "Epoch 1576/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4178 - val_loss: -4.6762\n",
      "Epoch 1577/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3666 - val_loss: -4.6295\n",
      "Epoch 1578/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3890 - val_loss: -3.0719\n",
      "Epoch 1579/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4037 - val_loss: -4.1686\n",
      "Epoch 1580/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4390 - val_loss: -4.7786\n",
      "Epoch 1581/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2413 - val_loss: -4.0079\n",
      "Epoch 1582/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3497 - val_loss: -4.1742\n",
      "Epoch 1583/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4320 - val_loss: -4.2494\n",
      "Epoch 1584/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3855 - val_loss: -3.5466\n",
      "Epoch 1585/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1639 - val_loss: -4.3854\n",
      "Epoch 1586/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3350 - val_loss: -2.4776\n",
      "Epoch 1587/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3411 - val_loss: -3.6664\n",
      "Epoch 1588/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4620 - val_loss: -1.0937\n",
      "Epoch 1589/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4757 - val_loss: -4.5390\n",
      "Epoch 1590/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3405 - val_loss: -4.3957\n",
      "Epoch 1591/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1054 - val_loss: -3.9053\n",
      "Epoch 1592/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1328 - val_loss: -3.9244\n",
      "Epoch 1593/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3341 - val_loss: -3.9606\n",
      "Epoch 1594/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3576 - val_loss: -3.5598\n",
      "Epoch 1595/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3532 - val_loss: -3.4773\n",
      "Epoch 1596/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4037 - val_loss: -3.8617\n",
      "Epoch 1597/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4327 - val_loss: -1.9595\n",
      "Epoch 1598/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3887 - val_loss: -3.9464\n",
      "Epoch 1599/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4474 - val_loss: -3.8079\n",
      "Epoch 1600/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3781 - val_loss: -3.8400\n",
      "Epoch 1601/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2872 - val_loss: -3.9920\n",
      "Epoch 1602/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4412 - val_loss: -2.9709\n",
      "Epoch 1603/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2414 - val_loss: -4.2516\n",
      "Epoch 1604/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4088 - val_loss: -4.1940\n",
      "Epoch 1605/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3419 - val_loss: -4.2073\n",
      "Epoch 1606/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4628 - val_loss: -3.5519\n",
      "Epoch 1607/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4263 - val_loss: -2.1608\n",
      "Epoch 1608/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4514 - val_loss: -4.2327\n",
      "Epoch 1609/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4424 - val_loss: -3.8849\n",
      "Epoch 1610/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4014 - val_loss: -4.5375\n",
      "Epoch 1611/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4604 - val_loss: -2.9606\n",
      "Epoch 1612/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4815 - val_loss: -2.6975\n",
      "Epoch 1613/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4292 - val_loss: -4.2750\n",
      "Epoch 1614/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3739 - val_loss: -4.3856\n",
      "Epoch 1615/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4523 - val_loss: -3.7617\n",
      "Epoch 1616/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4470 - val_loss: -4.5099\n",
      "Epoch 1617/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4117 - val_loss: -4.8074\n",
      "Epoch 1618/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1604 - val_loss: -4.0167\n",
      "Epoch 1619/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3226 - val_loss: -4.4648\n",
      "Epoch 1620/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4598 - val_loss: -1.8525\n",
      "Epoch 1621/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3807 - val_loss: -4.3946\n",
      "Epoch 1622/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4432 - val_loss: -3.8045\n",
      "Epoch 1623/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3584 - val_loss: -3.5874\n",
      "Epoch 1624/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4481 - val_loss: -3.4894\n",
      "Epoch 1625/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5149 - val_loss: -4.0118\n",
      "Epoch 1626/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4607 - val_loss: -2.0024\n",
      "Epoch 1627/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2613 - val_loss: -4.2400\n",
      "Epoch 1628/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4236 - val_loss: -4.4049\n",
      "Epoch 1629/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4905 - val_loss: -4.8525\n",
      "Epoch 1630/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2620 - val_loss: -4.7945\n",
      "Epoch 1631/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3713 - val_loss: -4.3813\n",
      "Epoch 1632/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3448 - val_loss: -3.6175\n",
      "Epoch 1633/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3754 - val_loss: -4.1417\n",
      "Epoch 1634/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4697 - val_loss: -3.8936\n",
      "Epoch 1635/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4691 - val_loss: -3.7757\n",
      "Epoch 1636/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3990 - val_loss: -2.1802\n",
      "Epoch 1637/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3484 - val_loss: -4.2033\n",
      "Epoch 1638/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3300 - val_loss: -4.3033\n",
      "Epoch 1639/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4985 - val_loss: -3.6301\n",
      "Epoch 1640/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4654 - val_loss: -4.7175\n",
      "Epoch 1641/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4948 - val_loss: -3.9920\n",
      "Epoch 1642/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4310 - val_loss: -3.1486\n",
      "Epoch 1643/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4701 - val_loss: -4.6204\n",
      "Epoch 1644/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4276 - val_loss: -4.8064\n",
      "Epoch 1645/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5139 - val_loss: -4.3333\n",
      "Epoch 1646/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2367 - val_loss: -3.9512\n",
      "Epoch 1647/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4624 - val_loss: -4.3492\n",
      "Epoch 1648/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4380 - val_loss: -4.3816\n",
      "Epoch 1649/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4076 - val_loss: -4.7625\n",
      "Epoch 1650/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4772 - val_loss: -3.9849\n",
      "Epoch 1651/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4486 - val_loss: -4.2878\n",
      "Epoch 1652/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4952 - val_loss: -4.2843\n",
      "Epoch 1653/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2867 - val_loss: -4.5102\n",
      "Epoch 1654/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4502 - val_loss: -4.5053\n",
      "Epoch 1655/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4978 - val_loss: -4.4210\n",
      "Epoch 1656/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4651 - val_loss: -4.1266\n",
      "Epoch 1657/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5039 - val_loss: -4.3821\n",
      "Epoch 1658/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3447 - val_loss: -4.4600\n",
      "Epoch 1659/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8775 - val_loss: -4.7281\n",
      "Epoch 1660/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3060 - val_loss: -4.3044\n",
      "Epoch 1661/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3877 - val_loss: -3.5214\n",
      "Epoch 1662/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2873 - val_loss: -4.3136\n",
      "Epoch 1663/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3912 - val_loss: -3.4922\n",
      "Epoch 1664/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4180 - val_loss: -4.4742\n",
      "Epoch 1665/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3808 - val_loss: -2.6880\n",
      "Epoch 1666/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4402 - val_loss: -4.2290\n",
      "Epoch 1667/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4536 - val_loss: -3.2057\n",
      "Epoch 1668/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4904 - val_loss: -3.8775\n",
      "Epoch 1669/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4841 - val_loss: -4.4448\n",
      "Epoch 1670/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4693 - val_loss: -4.6686\n",
      "Epoch 1671/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2747 - val_loss: -4.7416\n",
      "Epoch 1672/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4099 - val_loss: -4.2220\n",
      "Epoch 1673/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3821 - val_loss: -4.5531\n",
      "Epoch 1674/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4299 - val_loss: -3.2017\n",
      "Epoch 1675/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4366 - val_loss: -4.4668\n",
      "Epoch 1676/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.4108 - val_loss: -4.2039\n",
      "Epoch 1677/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4864 - val_loss: -2.9486\n",
      "Epoch 1678/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4242 - val_loss: -4.6244\n",
      "Epoch 1679/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3371 - val_loss: -4.6362\n",
      "Epoch 1680/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4984 - val_loss: -4.1046\n",
      "Epoch 1681/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4591 - val_loss: -4.5865\n",
      "Epoch 1682/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4915 - val_loss: 1.8677\n",
      "Epoch 1683/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4140 - val_loss: -3.2195\n",
      "Epoch 1684/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4651 - val_loss: -3.7135\n",
      "Epoch 1685/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4657 - val_loss: -3.6682\n",
      "Epoch 1686/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4515 - val_loss: -2.7008\n",
      "Epoch 1687/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4680 - val_loss: -3.3508\n",
      "Epoch 1688/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1005 - val_loss: -3.4533\n",
      "Epoch 1689/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9376 - val_loss: -4.0324\n",
      "Epoch 1690/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3312 - val_loss: -4.5412\n",
      "Epoch 1691/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4341 - val_loss: -4.3828\n",
      "Epoch 1692/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4382 - val_loss: -4.2157\n",
      "Epoch 1693/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4662 - val_loss: -4.1669\n",
      "Epoch 1694/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4041 - val_loss: -3.9872\n",
      "Epoch 1695/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4439 - val_loss: -4.0874\n",
      "Epoch 1696/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4607 - val_loss: -2.1934\n",
      "Epoch 1697/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3586 - val_loss: -4.3425\n",
      "Epoch 1698/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4759 - val_loss: -3.4095\n",
      "Epoch 1699/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4288 - val_loss: -4.3438\n",
      "Epoch 1700/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3664 - val_loss: -4.5576\n",
      "Epoch 1701/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5095 - val_loss: -4.2982\n",
      "Epoch 1702/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8538 - val_loss: -3.9352\n",
      "Epoch 1703/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2607 - val_loss: -4.1659\n",
      "Epoch 1704/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2618 - val_loss: -3.2821\n",
      "Epoch 1705/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2603 - val_loss: -2.4996\n",
      "Epoch 1706/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3448 - val_loss: -3.7324\n",
      "Epoch 1707/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4632 - val_loss: -3.6791\n",
      "Epoch 1708/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3282 - val_loss: -4.5474\n",
      "Epoch 1709/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3664 - val_loss: -4.1384\n",
      "Epoch 1710/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4061 - val_loss: -4.6150\n",
      "Epoch 1711/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3912 - val_loss: -4.4394\n",
      "Epoch 1712/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5089 - val_loss: -4.1832\n",
      "Epoch 1713/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4029 - val_loss: -2.8869\n",
      "Epoch 1714/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4974 - val_loss: -3.9213\n",
      "Epoch 1715/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1253 - val_loss: -4.1559\n",
      "Epoch 1716/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4944 - val_loss: -3.8249\n",
      "Epoch 1717/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4154 - val_loss: -3.6085\n",
      "Epoch 1718/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5305 - val_loss: -3.9463\n",
      "Epoch 1719/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5491 - val_loss: -4.6193\n",
      "Epoch 1720/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2374 - val_loss: -4.5401\n",
      "Epoch 1721/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4102 - val_loss: -4.3132\n",
      "Epoch 1722/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3994 - val_loss: -4.2434\n",
      "Epoch 1723/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4149 - val_loss: -4.2287\n",
      "Epoch 1724/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3716 - val_loss: -4.7041\n",
      "Epoch 1725/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4411 - val_loss: -4.2352\n",
      "Epoch 1726/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4488 - val_loss: -2.2468\n",
      "Epoch 1727/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3983 - val_loss: -4.3396\n",
      "Epoch 1728/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4600 - val_loss: -4.1852\n",
      "Epoch 1729/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5086 - val_loss: -2.6572\n",
      "Epoch 1730/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4654 - val_loss: -4.1604\n",
      "Epoch 1731/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3343 - val_loss: -4.5727\n",
      "Epoch 1732/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4644 - val_loss: -4.0719\n",
      "Epoch 1733/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2928 - val_loss: -3.5545\n",
      "Epoch 1734/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4678 - val_loss: -1.3961\n",
      "Epoch 1735/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3846 - val_loss: -4.4197\n",
      "Epoch 1736/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5054 - val_loss: -2.7751\n",
      "Epoch 1737/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4118 - val_loss: -4.4495\n",
      "Epoch 1738/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4268 - val_loss: -3.8405\n",
      "Epoch 1739/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4462 - val_loss: -3.1771\n",
      "Epoch 1740/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4930 - val_loss: -2.9512\n",
      "Epoch 1741/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4972 - val_loss: -4.0359\n",
      "Epoch 1742/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4763 - val_loss: -3.6733\n",
      "Epoch 1743/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1176 - val_loss: -3.7908\n",
      "Epoch 1744/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2266 - val_loss: -4.6855\n",
      "Epoch 1745/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4479 - val_loss: -3.2420\n",
      "Epoch 1746/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4645 - val_loss: -2.9834\n",
      "Epoch 1747/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3833 - val_loss: -4.7282\n",
      "Epoch 1748/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4385 - val_loss: -2.3203\n",
      "Epoch 1749/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2353 - val_loss: -4.5565\n",
      "Epoch 1750/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4502 - val_loss: -2.9640\n",
      "Epoch 1751/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3387 - val_loss: -4.5112\n",
      "Epoch 1752/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4848 - val_loss: -4.7101\n",
      "Epoch 1753/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5008 - val_loss: -3.5875\n",
      "Epoch 1754/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3818 - val_loss: -3.7257\n",
      "Epoch 1755/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3219 - val_loss: -4.3707\n",
      "Epoch 1756/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4861 - val_loss: -4.5170\n",
      "Epoch 1757/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4824 - val_loss: -4.2386\n",
      "Epoch 1758/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4533 - val_loss: -4.0356\n",
      "Epoch 1759/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4618 - val_loss: -4.3889\n",
      "Epoch 1760/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3485 - val_loss: -4.0920\n",
      "Epoch 1761/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4883 - val_loss: 1.2555\n",
      "Epoch 1762/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4300 - val_loss: -3.1564\n",
      "Epoch 1763/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5207 - val_loss: -4.0611\n",
      "Epoch 1764/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5184 - val_loss: -3.8695\n",
      "Epoch 1765/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8290 - val_loss: -3.8258\n",
      "Epoch 1766/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8760 - val_loss: -3.9050\n",
      "Epoch 1767/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2921 - val_loss: -3.3259\n",
      "Epoch 1768/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3804 - val_loss: -3.7692\n",
      "Epoch 1769/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3675 - val_loss: -3.1749\n",
      "Epoch 1770/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3836 - val_loss: -4.3079\n",
      "Epoch 1771/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8129 - val_loss: -4.3241\n",
      "Epoch 1772/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1326 - val_loss: -2.8147\n",
      "Epoch 1773/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2400 - val_loss: -4.3188\n",
      "Epoch 1774/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4072 - val_loss: -4.4282\n",
      "Epoch 1775/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3761 - val_loss: -3.6855\n",
      "Epoch 1776/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4026 - val_loss: -4.3483\n",
      "Epoch 1777/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3938 - val_loss: -2.8536\n",
      "Epoch 1778/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4726 - val_loss: -4.6345\n",
      "Epoch 1779/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3688 - val_loss: -3.8339\n",
      "Epoch 1780/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4742 - val_loss: -3.9084\n",
      "Epoch 1781/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4978 - val_loss: -4.3859\n",
      "Epoch 1782/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4907 - val_loss: -4.3309\n",
      "Epoch 1783/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4989 - val_loss: -3.6705\n",
      "Epoch 1784/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3498 - val_loss: -4.6016\n",
      "Epoch 1785/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2839 - val_loss: -4.3898\n",
      "Epoch 1786/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5166 - val_loss: -3.0159\n",
      "Epoch 1787/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4279 - val_loss: -3.5826\n",
      "Epoch 1788/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.8964 - val_loss: -4.5016\n",
      "Epoch 1789/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9980 - val_loss: -4.0111\n",
      "Epoch 1790/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1366 - val_loss: -3.7428\n",
      "Epoch 1791/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2084 - val_loss: -4.0731\n",
      "Epoch 1792/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2792 - val_loss: -4.7069\n",
      "Epoch 1793/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3448 - val_loss: -4.2946\n",
      "Epoch 1794/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3858 - val_loss: -4.3424\n",
      "Epoch 1795/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2838 - val_loss: -3.7666\n",
      "Epoch 1796/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4031 - val_loss: -4.7296\n",
      "Epoch 1797/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3825 - val_loss: -3.9273\n",
      "Epoch 1798/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4540 - val_loss: -4.7479\n",
      "Epoch 1799/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4206 - val_loss: -4.2363\n",
      "Epoch 1800/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3695 - val_loss: -4.2872\n",
      "Epoch 1801/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3544 - val_loss: -3.2305\n",
      "Epoch 1802/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4371 - val_loss: -3.3542\n",
      "Epoch 1803/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4236 - val_loss: -3.9127\n",
      "Epoch 1804/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4103 - val_loss: -4.2922\n",
      "Epoch 1805/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4314 - val_loss: -4.1026\n",
      "Epoch 1806/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4366 - val_loss: -3.4342\n",
      "Epoch 1807/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3444 - val_loss: -4.3561\n",
      "Epoch 1808/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5056 - val_loss: -4.5127\n",
      "Epoch 1809/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4055 - val_loss: -4.1384\n",
      "Epoch 1810/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5042 - val_loss: -4.4281\n",
      "Epoch 1811/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2684 - val_loss: -4.2605\n",
      "Epoch 1812/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4709 - val_loss: -3.5895\n",
      "Epoch 1813/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4365 - val_loss: -2.2948\n",
      "Epoch 1814/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2988 - val_loss: -4.0969\n",
      "Epoch 1815/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5116 - val_loss: -3.8762\n",
      "Epoch 1816/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4485 - val_loss: -4.2832\n",
      "Epoch 1817/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4612 - val_loss: -3.4017\n",
      "Epoch 1818/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3725 - val_loss: -4.0167\n",
      "Epoch 1819/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5373 - val_loss: -4.4107\n",
      "Epoch 1820/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4689 - val_loss: -4.2414\n",
      "Epoch 1821/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5062 - val_loss: -4.3846\n",
      "Epoch 1822/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4078 - val_loss: -4.8642\n",
      "Epoch 1823/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5004 - val_loss: -4.0835\n",
      "Epoch 1824/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5012 - val_loss: -4.7895\n",
      "Epoch 1825/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4292 - val_loss: -4.5890\n",
      "Epoch 1826/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4938 - val_loss: -4.2516\n",
      "Epoch 1827/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5259 - val_loss: -2.1455\n",
      "Epoch 1828/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5127 - val_loss: -3.4322\n",
      "Epoch 1829/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4892 - val_loss: -4.5279\n",
      "Epoch 1830/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3812 - val_loss: -3.6311\n",
      "Epoch 1831/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4745 - val_loss: -4.0754\n",
      "Epoch 1832/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5136 - val_loss: -4.2564\n",
      "Epoch 1833/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4512 - val_loss: -3.5824\n",
      "Epoch 1834/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5116 - val_loss: -4.0105\n",
      "Epoch 1835/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2451 - val_loss: -4.5538\n",
      "Epoch 1836/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4721 - val_loss: -3.9034\n",
      "Epoch 1837/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5012 - val_loss: 1.7000\n",
      "Epoch 1838/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4932 - val_loss: -1.2154\n",
      "Epoch 1839/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5162 - val_loss: -2.8528\n",
      "Epoch 1840/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3632 - val_loss: -3.6778\n",
      "Epoch 1841/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5556 - val_loss: -3.9389\n",
      "Epoch 1842/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5216 - val_loss: -2.9463\n",
      "Epoch 1843/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4028 - val_loss: -2.5527\n",
      "Epoch 1844/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5229 - val_loss: -3.8289\n",
      "Epoch 1845/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5395 - val_loss: -4.0834\n",
      "Epoch 1846/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4639 - val_loss: -4.4710\n",
      "Epoch 1847/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5254 - val_loss: -4.4224\n",
      "Epoch 1848/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1027 - val_loss: -3.9362\n",
      "Epoch 1849/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7996 - val_loss: -4.3495\n",
      "Epoch 1850/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3165 - val_loss: -2.6208\n",
      "Epoch 1851/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3604 - val_loss: -4.1119\n",
      "Epoch 1852/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4137 - val_loss: -3.7596\n",
      "Epoch 1853/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4523 - val_loss: -3.6215\n",
      "Epoch 1854/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4804 - val_loss: -3.8435\n",
      "Epoch 1855/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5063 - val_loss: -2.3484\n",
      "Epoch 1856/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4655 - val_loss: -4.5520\n",
      "Epoch 1857/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4039 - val_loss: -3.1199\n",
      "Epoch 1858/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5404 - val_loss: -3.2354\n",
      "Epoch 1859/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3573 - val_loss: -4.4626\n",
      "Epoch 1860/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5016 - val_loss: -4.0138\n",
      "Epoch 1861/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4655 - val_loss: -3.8279\n",
      "Epoch 1862/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4811 - val_loss: -1.6649\n",
      "Epoch 1863/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4126 - val_loss: -3.6998\n",
      "Epoch 1864/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5327 - val_loss: -4.4607\n",
      "Epoch 1865/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2829 - val_loss: -2.8843\n",
      "Epoch 1866/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4618 - val_loss: -3.0108\n",
      "Epoch 1867/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4098 - val_loss: -4.2909\n",
      "Epoch 1868/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5235 - val_loss: -4.0153\n",
      "Epoch 1869/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4807 - val_loss: -4.2848\n",
      "Epoch 1870/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4356 - val_loss: -2.8913\n",
      "Epoch 1871/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5078 - val_loss: -4.1003\n",
      "Epoch 1872/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4578 - val_loss: -4.5997\n",
      "Epoch 1873/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5355 - val_loss: -3.2259\n",
      "Epoch 1874/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5443 - val_loss: -4.2857\n",
      "Epoch 1875/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4771 - val_loss: -4.1170\n",
      "Epoch 1876/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 21s 181ms/step - loss: -4.4358 - val_loss: -3.7071\n",
      "Epoch 1877/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8319 - val_loss: -3.5101\n",
      "Epoch 1878/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1228 - val_loss: -3.7899\n",
      "Epoch 1879/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2468 - val_loss: -4.2449\n",
      "Epoch 1880/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3429 - val_loss: -3.8283\n",
      "Epoch 1881/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2746 - val_loss: -4.6955\n",
      "Epoch 1882/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3539 - val_loss: -3.4911\n",
      "Epoch 1883/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4193 - val_loss: -3.6639\n",
      "Epoch 1884/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3462 - val_loss: -4.5356\n",
      "Epoch 1885/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3697 - val_loss: -3.8070\n",
      "Epoch 1886/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4351 - val_loss: -3.6808\n",
      "Epoch 1887/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3907 - val_loss: -2.3289\n",
      "Epoch 1888/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3272 - val_loss: -3.8674\n",
      "Epoch 1889/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4544 - val_loss: -4.4323\n",
      "Epoch 1890/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.4780 - val_loss: -3.2573\n",
      "Epoch 1891/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4765 - val_loss: -1.3689\n",
      "Epoch 1892/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3419 - val_loss: -3.8213\n",
      "Epoch 1893/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4896 - val_loss: -4.1705\n",
      "Epoch 1894/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4290 - val_loss: -4.7903\n",
      "Epoch 1895/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4976 - val_loss: -2.6346\n",
      "Epoch 1896/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5165 - val_loss: -3.5974\n",
      "Epoch 1897/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4926 - val_loss: -4.5835\n",
      "Epoch 1898/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2714 - val_loss: -4.6596\n",
      "Epoch 1899/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5141 - val_loss: -0.1580\n",
      "Epoch 1900/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3807"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_180_layer_call_fn, lstm_cell_180_layer_call_and_return_conditional_losses, lstm_cell_181_layer_call_fn, lstm_cell_181_layer_call_and_return_conditional_losses, lstm_cell_182_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 242ms/step - loss: -5.3807 - val_loss: -4.9258\n",
      "Epoch 1901/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5002 - val_loss: -4.1331\n",
      "Epoch 1902/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4136 - val_loss: -3.8914\n",
      "Epoch 1903/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5215 - val_loss: -4.4536\n",
      "Epoch 1904/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4871 - val_loss: -4.7210\n",
      "Epoch 1905/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4253 - val_loss: -4.4285\n",
      "Epoch 1906/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4706 - val_loss: -3.0409\n",
      "Epoch 1907/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4816 - val_loss: -3.6758\n",
      "Epoch 1908/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4887 - val_loss: -3.7534\n",
      "Epoch 1909/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4526 - val_loss: -3.4567\n",
      "Epoch 1910/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7400 - val_loss: -3.3152\n",
      "Epoch 1911/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3403 - val_loss: -4.5321\n",
      "Epoch 1912/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1681 - val_loss: -4.2199\n",
      "Epoch 1913/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2800 - val_loss: -0.3871\n",
      "Epoch 1914/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2926 - val_loss: -3.0995\n",
      "Epoch 1915/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3762 - val_loss: -4.4821\n",
      "Epoch 1916/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4446 - val_loss: -3.4711\n",
      "Epoch 1917/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9352 - val_loss: -4.8024\n",
      "Epoch 1918/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4104 - val_loss: -4.2598\n",
      "Epoch 1919/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4303 - val_loss: -3.4209\n",
      "Epoch 1920/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.4240 - val_loss: -4.5626\n",
      "Epoch 1921/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4037 - val_loss: -4.0871\n",
      "Epoch 1922/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3965 - val_loss: -4.0308\n",
      "Epoch 1923/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4728 - val_loss: -3.6072\n",
      "Epoch 1924/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4932 - val_loss: -3.1624\n",
      "Epoch 1925/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4410 - val_loss: -4.0807\n",
      "Epoch 1926/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4810 - val_loss: -0.8010\n",
      "Epoch 1927/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4843 - val_loss: -4.2962\n",
      "Epoch 1928/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5187 - val_loss: -4.0930\n",
      "Epoch 1929/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2650 - val_loss: -3.8328\n",
      "Epoch 1930/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5208 - val_loss: -1.4755\n",
      "Epoch 1931/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4952 - val_loss: -3.0692\n",
      "Epoch 1932/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4815 - val_loss: -3.7450\n",
      "Epoch 1933/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4663 - val_loss: -4.4496\n",
      "Epoch 1934/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4620 - val_loss: -4.5636\n",
      "Epoch 1935/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.5177 - val_loss: -4.0379\n",
      "Epoch 1936/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5197 - val_loss: -4.3353\n",
      "Epoch 1937/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4643 - val_loss: -4.1152\n",
      "Epoch 1938/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.5259 - val_loss: -0.4763\n",
      "Epoch 1939/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4472 - val_loss: -3.2740\n",
      "Epoch 1940/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4879 - val_loss: -4.4169\n",
      "Epoch 1941/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5602 - val_loss: -3.1260\n",
      "Epoch 1942/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4648 - val_loss: -4.4033\n",
      "Epoch 1943/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.5210 - val_loss: -3.2385\n",
      "Epoch 1944/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4419 - val_loss: -3.1786\n",
      "Epoch 1945/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4022 - val_loss: -4.3513\n",
      "Epoch 1946/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4397 - val_loss: -4.9177\n",
      "Epoch 1947/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4584 - val_loss: -4.4390\n",
      "Epoch 1948/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3842 - val_loss: -4.5064\n",
      "Epoch 1949/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4964 - val_loss: -4.1478\n",
      "Epoch 1950/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3970 - val_loss: -4.4765\n",
      "Epoch 1951/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5158 - val_loss: -3.4992\n",
      "Epoch 1952/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5182 - val_loss: -2.9252\n",
      "Epoch 1953/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9180 - val_loss: -4.6030\n",
      "Epoch 1954/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7809 - val_loss: -3.7825\n",
      "Epoch 1955/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0340 - val_loss: -4.2372\n",
      "Epoch 1956/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9916 - val_loss: -4.2559\n",
      "Epoch 1957/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1493 - val_loss: -4.5354\n",
      "Epoch 1958/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2161 - val_loss: -4.0983\n",
      "Epoch 1959/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1728 - val_loss: -4.0613\n",
      "Epoch 1960/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2619 - val_loss: -4.2536\n",
      "Epoch 1961/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2766 - val_loss: -3.7473\n",
      "Epoch 1962/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1363 - val_loss: -4.0279\n",
      "Epoch 1963/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2560 - val_loss: -3.5570\n",
      "Epoch 1964/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3051 - val_loss: -2.4668\n",
      "Epoch 1965/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3344 - val_loss: -4.1965\n",
      "Epoch 1966/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2192 - val_loss: -3.3608\n",
      "Epoch 1967/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3732 - val_loss: -4.2758\n",
      "Epoch 1968/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3512 - val_loss: -1.9977\n",
      "Epoch 1969/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3232 - val_loss: -4.0597\n",
      "Epoch 1970/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3193 - val_loss: -3.6159\n",
      "Epoch 1971/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3471 - val_loss: -3.5728\n",
      "Epoch 1972/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3823 - val_loss: -3.9169\n",
      "Epoch 1973/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3579 - val_loss: -4.2650\n",
      "Epoch 1974/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4606 - val_loss: -2.2620\n",
      "Epoch 1975/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3584 - val_loss: -4.2278\n",
      "Epoch 1976/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9423 - val_loss: -4.4774\n",
      "Epoch 1977/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0095 - val_loss: -4.5528\n",
      "Epoch 1978/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2749 - val_loss: -4.5145\n",
      "Epoch 1979/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3254 - val_loss: -1.5311\n",
      "Epoch 1980/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3721 - val_loss: -4.4205\n",
      "Epoch 1981/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.4118 - val_loss: -4.6959\n",
      "Epoch 1982/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3918 - val_loss: -1.7069\n",
      "Epoch 1983/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.4262 - val_loss: -4.5152\n",
      "Epoch 1984/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3509 - val_loss: -4.7727\n",
      "Epoch 1985/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3719 - val_loss: -3.8847\n",
      "Epoch 1986/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4383 - val_loss: -4.4175\n",
      "Epoch 1987/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4309 - val_loss: -3.8142\n",
      "Epoch 1988/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4220 - val_loss: -2.1843\n",
      "Epoch 1989/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0890 - val_loss: -4.3516\n",
      "Epoch 1990/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1926 - val_loss: -4.6632\n",
      "Epoch 1991/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3617 - val_loss: -3.1359\n",
      "Epoch 1992/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4404 - val_loss: -3.8370\n",
      "Epoch 1993/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3962 - val_loss: -4.7501\n",
      "Epoch 1994/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4265 - val_loss: -4.8207\n",
      "Epoch 1995/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4570 - val_loss: -4.7975\n",
      "Epoch 1996/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3759 - val_loss: -4.2438\n",
      "Epoch 1997/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3633 - val_loss: -4.4641\n",
      "Epoch 1998/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.5180 - val_loss: 1.9736\n",
      "Epoch 1999/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.3604 - val_loss: -4.3286\n",
      "Epoch 2000/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.4802 - val_loss: -3.7213\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 6s 36ms/step - loss: -5.0747\n",
      "WARNING:tensorflow:Layer lstm_94 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_94 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 6s 37ms/step - loss: -5.3994\n",
      "WARNING:tensorflow:Layer lstm_99 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_99 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.0012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 34s 261ms/step - loss: -1.0012 - val_loss: 0.3250\n",
      "Epoch 2/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2534 - val_loss: 0.4871\n",
      "Epoch 3/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -1.2557 - val_loss: 0.5731\n",
      "Epoch 4/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2578 - val_loss: 0.5764\n",
      "Epoch 5/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2591 - val_loss: 0.4818\n",
      "Epoch 6/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2605 - val_loss: 0.5085\n",
      "Epoch 7/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.2598"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -1.2598 - val_loss: 0.3030\n",
      "Epoch 8/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2609 - val_loss: 0.5086\n",
      "Epoch 9/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2608 - val_loss: 0.6784\n",
      "Epoch 10/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -1.2615 - val_loss: 0.4185\n",
      "Epoch 11/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.2613"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 256ms/step - loss: -1.2613 - val_loss: 0.2706\n",
      "Epoch 12/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2619 - val_loss: 0.5401\n",
      "Epoch 13/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.2617"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -1.2617 - val_loss: 0.2686\n",
      "Epoch 14/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2606 - val_loss: 0.7627\n",
      "Epoch 15/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2599 - val_loss: 0.5222\n",
      "Epoch 16/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2618 - val_loss: 0.6802\n",
      "Epoch 17/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -1.2616 - val_loss: 0.6253\n",
      "Epoch 18/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -1.2647 - val_loss: 0.8049\n",
      "Epoch 19/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -1.9913"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -1.9913 - val_loss: -0.5364\n",
      "Epoch 20/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -2.8177"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -2.8177 - val_loss: -2.6009\n",
      "Epoch 21/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.0416 - val_loss: -2.1706\n",
      "Epoch 22/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.2800"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -3.2800 - val_loss: -3.0837\n",
      "Epoch 23/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.4861"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -3.4861 - val_loss: -3.1362\n",
      "Epoch 24/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.6822 - val_loss: -3.0851\n",
      "Epoch 25/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8052 - val_loss: -2.6710\n",
      "Epoch 26/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -3.8382"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 256ms/step - loss: -3.8382 - val_loss: -3.5514\n",
      "Epoch 27/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -3.8214 - val_loss: -3.4100\n",
      "Epoch 28/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -3.9527 - val_loss: -3.5014\n",
      "Epoch 29/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9566 - val_loss: -3.4703\n",
      "Epoch 30/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.0254 - val_loss: -2.3339\n",
      "Epoch 31/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.0612"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -4.0612 - val_loss: -3.6599\n",
      "Epoch 32/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.0642 - val_loss: -3.0830\n",
      "Epoch 33/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1522 - val_loss: -3.6225\n",
      "Epoch 34/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.1401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -4.1401 - val_loss: -3.7183\n",
      "Epoch 35/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1891 - val_loss: -3.2372\n",
      "Epoch 36/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.2230"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -4.2230 - val_loss: -3.7220\n",
      "Epoch 37/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1881 - val_loss: -3.1074\n",
      "Epoch 38/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2155 - val_loss: -2.9540\n",
      "Epoch 39/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.2694"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -4.2694 - val_loss: -3.8743\n",
      "Epoch 40/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.2749 - val_loss: -3.8454\n",
      "Epoch 41/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1311 - val_loss: -2.2899\n",
      "Epoch 42/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.3372 - val_loss: -3.8225\n",
      "Epoch 43/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2959 - val_loss: -3.6915\n",
      "Epoch 44/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3008 - val_loss: -3.7442\n",
      "Epoch 45/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.3817"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -4.3817 - val_loss: -3.9182\n",
      "Epoch 46/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3007 - val_loss: -3.6665\n",
      "Epoch 47/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3866 - val_loss: -3.8931\n",
      "Epoch 48/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3947 - val_loss: -3.5491\n",
      "Epoch 49/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4082"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -4.4082 - val_loss: -3.9701\n",
      "Epoch 50/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.3763"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 252ms/step - loss: -4.3763 - val_loss: -3.9905\n",
      "Epoch 51/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3741 - val_loss: -3.8918\n",
      "Epoch 52/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.4643 - val_loss: -3.9445\n",
      "Epoch 53/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.4633 - val_loss: -3.4230\n",
      "Epoch 54/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2724 - val_loss: -3.8796\n",
      "Epoch 55/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4796"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -4.4796 - val_loss: -4.0122\n",
      "Epoch 56/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4657 - val_loss: -3.9076\n",
      "Epoch 57/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4803 - val_loss: -3.9601\n",
      "Epoch 58/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5221 - val_loss: -3.8785\n",
      "Epoch 59/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4722 - val_loss: -3.9022\n",
      "Epoch 60/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4402 - val_loss: -3.6693\n",
      "Epoch 61/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4154 - val_loss: -3.8245\n",
      "Epoch 62/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5253 - val_loss: -3.9768\n",
      "Epoch 63/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4943 - val_loss: -3.4979\n",
      "Epoch 64/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4710 - val_loss: -3.8236\n",
      "Epoch 65/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5597 - val_loss: -2.4164\n",
      "Epoch 66/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4296 - val_loss: -3.8848\n",
      "Epoch 67/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5450 - val_loss: -3.7649\n",
      "Epoch 68/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6003"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 256ms/step - loss: -4.6003 - val_loss: -4.0198\n",
      "Epoch 69/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.4469 - val_loss: -3.3242\n",
      "Epoch 70/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5609 - val_loss: -1.9379\n",
      "Epoch 71/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.4366"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -4.4366 - val_loss: -4.0376\n",
      "Epoch 72/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5325 - val_loss: -3.6156\n",
      "Epoch 73/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5292 - val_loss: -3.8782\n",
      "Epoch 74/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5613 - val_loss: -3.3467\n",
      "Epoch 75/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4003 - val_loss: -3.7026\n",
      "Epoch 76/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6128"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -4.6128 - val_loss: -4.0808\n",
      "Epoch 77/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.4791 - val_loss: -3.8920\n",
      "Epoch 78/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4784 - val_loss: -2.1141\n",
      "Epoch 79/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4873 - val_loss: -4.0592\n",
      "Epoch 80/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5511 - val_loss: -2.9416\n",
      "Epoch 81/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4866 - val_loss: -3.4326\n",
      "Epoch 82/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5578 - val_loss: -3.1044\n",
      "Epoch 83/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6054 - val_loss: -4.0091\n",
      "Epoch 84/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6242 - val_loss: -3.9870\n",
      "Epoch 85/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6361 - val_loss: -3.8860\n",
      "Epoch 86/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6568 - val_loss: -3.0449\n",
      "Epoch 87/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4982 - val_loss: -3.4482\n",
      "Epoch 88/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5037 - val_loss: -3.8848\n",
      "Epoch 89/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6057 - val_loss: -3.5441\n",
      "Epoch 90/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2835 - val_loss: -3.8234\n",
      "Epoch 91/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3915 - val_loss: -3.4642\n",
      "Epoch 92/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5296 - val_loss: 2.0053\n",
      "Epoch 93/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3405 - val_loss: -3.7643\n",
      "Epoch 94/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5167 - val_loss: -3.7359\n",
      "Epoch 95/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4569 - val_loss: -3.2594\n",
      "Epoch 96/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6117"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -4.6117 - val_loss: -4.0810\n",
      "Epoch 97/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5330 - val_loss: -3.9104\n",
      "Epoch 98/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6314 - val_loss: -3.5348\n",
      "Epoch 99/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4102 - val_loss: -3.4207\n",
      "Epoch 100/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.6369 - val_loss: -3.1424\n",
      "Epoch 101/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3570 - val_loss: -4.0801\n",
      "Epoch 102/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6404 - val_loss: -3.0871\n",
      "Epoch 103/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4425 - val_loss: -3.4355\n",
      "Epoch 104/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6663 - val_loss: -3.1218\n",
      "Epoch 105/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5620 - val_loss: -3.1231\n",
      "Epoch 106/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6706"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 256ms/step - loss: -4.6706 - val_loss: -4.1437\n",
      "Epoch 107/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6299"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -4.6299 - val_loss: -4.1759\n",
      "Epoch 108/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6224 - val_loss: -3.7411\n",
      "Epoch 109/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6861 - val_loss: -3.8323\n",
      "Epoch 110/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.6398 - val_loss: -3.6735\n",
      "Epoch 111/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6813 - val_loss: -2.2163\n",
      "Epoch 112/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5332 - val_loss: -2.8570\n",
      "Epoch 113/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6491 - val_loss: -3.6475\n",
      "Epoch 114/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7347 - val_loss: -4.0635\n",
      "Epoch 115/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6554 - val_loss: -3.3019\n",
      "Epoch 116/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6635 - val_loss: -4.0148\n",
      "Epoch 117/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5695 - val_loss: -3.9995\n",
      "Epoch 118/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.6836"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -4.6836 - val_loss: -4.2004\n",
      "Epoch 119/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6740 - val_loss: -1.0071\n",
      "Epoch 120/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7148 - val_loss: -4.1602\n",
      "Epoch 121/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6069 - val_loss: -3.9784\n",
      "Epoch 122/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6724 - val_loss: -3.5254\n",
      "Epoch 123/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6883 - val_loss: -3.8405\n",
      "Epoch 124/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6999 - val_loss: -2.3558\n",
      "Epoch 125/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6998 - val_loss: -4.1276\n",
      "Epoch 126/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6796 - val_loss: -3.7655\n",
      "Epoch 127/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.4315 - val_loss: -3.4597\n",
      "Epoch 128/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4856 - val_loss: -4.0087\n",
      "Epoch 129/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4869 - val_loss: -3.5546\n",
      "Epoch 130/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6496 - val_loss: -3.7587\n",
      "Epoch 131/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6019 - val_loss: -4.0284\n",
      "Epoch 132/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6436 - val_loss: -4.0687\n",
      "Epoch 133/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7584 - val_loss: -1.6885\n",
      "Epoch 134/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.6824 - val_loss: -3.5391\n",
      "Epoch 135/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5886 - val_loss: -2.5866\n",
      "Epoch 136/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.6930 - val_loss: -4.0647\n",
      "Epoch 137/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6696 - val_loss: -4.0572\n",
      "Epoch 138/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7396 - val_loss: -4.1008\n",
      "Epoch 139/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7106 - val_loss: -4.0734\n",
      "Epoch 140/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6790 - val_loss: -3.6098\n",
      "Epoch 141/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7992 - val_loss: -3.8635\n",
      "Epoch 142/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.6286 - val_loss: -3.7168\n",
      "Epoch 143/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5686 - val_loss: -4.0510\n",
      "Epoch 144/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6772 - val_loss: -3.9630\n",
      "Epoch 145/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6594 - val_loss: -4.0906\n",
      "Epoch 146/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6958 - val_loss: -3.8274\n",
      "Epoch 147/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6773 - val_loss: -3.6048\n",
      "Epoch 148/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7136 - val_loss: -4.1777\n",
      "Epoch 149/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6976 - val_loss: -2.1399\n",
      "Epoch 150/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7040 - val_loss: -4.0124\n",
      "Epoch 151/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7077 - val_loss: -4.0652\n",
      "Epoch 152/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7264 - val_loss: -3.6371\n",
      "Epoch 153/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6810 - val_loss: -3.5746\n",
      "Epoch 154/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7738"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 241ms/step - loss: -4.7738 - val_loss: -4.2404\n",
      "Epoch 155/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6371 - val_loss: -3.9085\n",
      "Epoch 156/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7399 - val_loss: -2.9003\n",
      "Epoch 157/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7085 - val_loss: -3.8248\n",
      "Epoch 158/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7911 - val_loss: -4.1195\n",
      "Epoch 159/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7359 - val_loss: -2.6938\n",
      "Epoch 160/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7017 - val_loss: 1.2678\n",
      "Epoch 161/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6884 - val_loss: -3.6123\n",
      "Epoch 162/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7766 - val_loss: -3.6683\n",
      "Epoch 163/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6862 - val_loss: -2.4773\n",
      "Epoch 164/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8133 - val_loss: -4.1179\n",
      "Epoch 165/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6739 - val_loss: -3.9509\n",
      "Epoch 166/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8105 - val_loss: -3.4648\n",
      "Epoch 167/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7766 - val_loss: -4.0834\n",
      "Epoch 168/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3255 - val_loss: -4.0395\n",
      "Epoch 169/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7473 - val_loss: -3.3574\n",
      "Epoch 170/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7427 - val_loss: -3.4141\n",
      "Epoch 171/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7985 - val_loss: -3.9598\n",
      "Epoch 172/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7485 - val_loss: -2.3043\n",
      "Epoch 173/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6126 - val_loss: -4.1287\n",
      "Epoch 174/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8169 - val_loss: -3.8491\n",
      "Epoch 175/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5782 - val_loss: -4.0071\n",
      "Epoch 176/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8049 - val_loss: -4.0800\n",
      "Epoch 177/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6706 - val_loss: -1.9486\n",
      "Epoch 178/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7019 - val_loss: -4.0527\n",
      "Epoch 179/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7891 - val_loss: -3.8787\n",
      "Epoch 180/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5528 - val_loss: -4.0415\n",
      "Epoch 181/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8776 - val_loss: -0.2988\n",
      "Epoch 182/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7828 - val_loss: -4.2034\n",
      "Epoch 183/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7851 - val_loss: -2.1843\n",
      "Epoch 184/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7514 - val_loss: -3.6810\n",
      "Epoch 185/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8325 - val_loss: -3.5416\n",
      "Epoch 186/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7137 - val_loss: -3.7460\n",
      "Epoch 187/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8156 - val_loss: -0.5576\n",
      "Epoch 188/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6673 - val_loss: -4.1421\n",
      "Epoch 189/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6104 - val_loss: -1.8955\n",
      "Epoch 190/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.8472 - val_loss: -3.7354\n",
      "Epoch 191/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6693 - val_loss: -3.5140\n",
      "Epoch 192/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7128 - val_loss: -3.6441\n",
      "Epoch 193/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2959 - val_loss: -4.1933\n",
      "Epoch 194/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7756 - val_loss: -4.1344\n",
      "Epoch 195/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6513 - val_loss: -3.3898\n",
      "Epoch 196/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7864 - val_loss: -2.1879\n",
      "Epoch 197/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4434 - val_loss: -4.1832\n",
      "Epoch 198/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7221 - val_loss: -3.8451\n",
      "Epoch 199/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6966 - val_loss: -3.8467\n",
      "Epoch 200/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8015 - val_loss: -4.0808\n",
      "Epoch 201/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7402 - val_loss: -3.3398\n",
      "Epoch 202/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6964 - val_loss: -3.4975\n",
      "Epoch 203/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8375 - val_loss: -3.9339\n",
      "Epoch 204/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7448 - val_loss: -3.4844\n",
      "Epoch 205/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.5660"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 254ms/step - loss: -4.5660 - val_loss: -4.2872\n",
      "Epoch 206/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7645 - val_loss: -3.9413\n",
      "Epoch 207/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7539 - val_loss: -4.1456\n",
      "Epoch 208/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7759 - val_loss: -4.0436\n",
      "Epoch 209/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8750 - val_loss: -3.0070\n",
      "Epoch 210/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8037 - val_loss: -3.1899\n",
      "Epoch 211/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8624 - val_loss: -2.6478\n",
      "Epoch 212/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5764 - val_loss: -4.2515\n",
      "Epoch 213/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8834 - val_loss: -3.7731\n",
      "Epoch 214/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7874 - val_loss: -4.1794\n",
      "Epoch 215/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7743 - val_loss: -4.0872\n",
      "Epoch 216/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8026 - val_loss: -3.3831\n",
      "Epoch 217/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7426 - val_loss: -4.2581\n",
      "Epoch 218/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8595 - val_loss: -4.2768\n",
      "Epoch 219/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9189 - val_loss: -3.7136\n",
      "Epoch 220/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7582 - val_loss: -3.3588\n",
      "Epoch 221/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8500 - val_loss: -4.2016\n",
      "Epoch 222/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8067 - val_loss: -4.2313\n",
      "Epoch 223/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8803 - val_loss: -4.1053\n",
      "Epoch 224/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5969 - val_loss: -2.7731\n",
      "Epoch 225/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7448 - val_loss: -4.0020\n",
      "Epoch 226/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.8493"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -4.8493 - val_loss: -4.3231\n",
      "Epoch 227/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7963 - val_loss: -4.0298\n",
      "Epoch 228/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8343 - val_loss: -4.1251\n",
      "Epoch 229/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8640 - val_loss: -2.9640\n",
      "Epoch 230/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3457 - val_loss: -4.0983\n",
      "Epoch 231/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7623 - val_loss: -4.2031\n",
      "Epoch 232/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8410 - val_loss: -4.0123\n",
      "Epoch 233/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7864 - val_loss: -4.0791\n",
      "Epoch 234/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8528 - val_loss: -3.9504\n",
      "Epoch 235/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6766 - val_loss: -3.4003\n",
      "Epoch 236/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8301 - val_loss: -3.9083\n",
      "Epoch 237/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7165 - val_loss: -3.9327\n",
      "Epoch 238/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8979 - val_loss: -3.9834\n",
      "Epoch 239/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7564 - val_loss: -4.1257\n",
      "Epoch 240/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8117 - val_loss: -2.4790\n",
      "Epoch 241/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8851 - val_loss: -3.8928\n",
      "Epoch 242/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8810 - val_loss: -3.9591\n",
      "Epoch 243/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.4924 - val_loss: -4.1348\n",
      "Epoch 244/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8851 - val_loss: -3.9277\n",
      "Epoch 245/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8737 - val_loss: -3.1630\n",
      "Epoch 246/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9164 - val_loss: -3.0285\n",
      "Epoch 247/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8027 - val_loss: -3.5046\n",
      "Epoch 248/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8456 - val_loss: -4.1175\n",
      "Epoch 249/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7161 - val_loss: -4.1630\n",
      "Epoch 250/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9254 - val_loss: -3.6698\n",
      "Epoch 251/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8586 - val_loss: -3.8357\n",
      "Epoch 252/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7591 - val_loss: -3.9852\n",
      "Epoch 253/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9477 - val_loss: -4.2967\n",
      "Epoch 254/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7869 - val_loss: -4.2116\n",
      "Epoch 255/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9534 - val_loss: -4.1672\n",
      "Epoch 256/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7893 - val_loss: -3.4330\n",
      "Epoch 257/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8792 - val_loss: -3.4394\n",
      "Epoch 258/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3108 - val_loss: -4.3095\n",
      "Epoch 259/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8904 - val_loss: -3.0214\n",
      "Epoch 260/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8189 - val_loss: -4.2783\n",
      "Epoch 261/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9701 - val_loss: -3.9033\n",
      "Epoch 262/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8045 - val_loss: -4.0761\n",
      "Epoch 263/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9240 - val_loss: -4.2834\n",
      "Epoch 264/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9058 - val_loss: -4.1709\n",
      "Epoch 265/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5828 - val_loss: -3.8287\n",
      "Epoch 266/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9556 - val_loss: -3.4271\n",
      "Epoch 267/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.8760 - val_loss: -2.9805\n",
      "Epoch 268/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9088 - val_loss: -3.8886\n",
      "Epoch 269/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8506 - val_loss: -1.0640\n",
      "Epoch 270/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8218 - val_loss: -4.1262\n",
      "Epoch 271/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8817 - val_loss: -2.8635\n",
      "Epoch 272/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9402"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -4.9402 - val_loss: -4.3348\n",
      "Epoch 273/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8867 - val_loss: -4.0858\n",
      "Epoch 274/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.8603 - val_loss: -4.1515\n",
      "Epoch 275/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.7602 - val_loss: -3.1151\n",
      "Epoch 276/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.5966 - val_loss: -3.2151\n",
      "Epoch 277/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7549 - val_loss: -3.4978\n",
      "Epoch 278/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.7464 - val_loss: -4.0838\n",
      "Epoch 279/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8250 - val_loss: -4.1257\n",
      "Epoch 280/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8295 - val_loss: -3.8181\n",
      "Epoch 281/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9152 - val_loss: -2.7988\n",
      "Epoch 282/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8292 - val_loss: -4.0542\n",
      "Epoch 283/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7141 - val_loss: -3.7660\n",
      "Epoch 284/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8552 - val_loss: -3.6058\n",
      "Epoch 285/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7011 - val_loss: -3.9480\n",
      "Epoch 286/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9559 - val_loss: -3.1275\n",
      "Epoch 287/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9469 - val_loss: -4.0979\n",
      "Epoch 288/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.7237"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 253ms/step - loss: -4.7237 - val_loss: -4.3550\n",
      "Epoch 289/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -4.9392"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 238ms/step - loss: -4.9392 - val_loss: -4.3839\n",
      "Epoch 290/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9329 - val_loss: -3.3886\n",
      "Epoch 291/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8576 - val_loss: -3.9341\n",
      "Epoch 292/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8270 - val_loss: -3.9236\n",
      "Epoch 293/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9858 - val_loss: -3.9433\n",
      "Epoch 294/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8966 - val_loss: -3.7300\n",
      "Epoch 295/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8974 - val_loss: -4.1928\n",
      "Epoch 296/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8288 - val_loss: -3.3528\n",
      "Epoch 297/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9371 - val_loss: -3.4475\n",
      "Epoch 298/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0310 - val_loss: -4.1499\n",
      "Epoch 299/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8145 - val_loss: -3.8491\n",
      "Epoch 300/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8804 - val_loss: -3.9584\n",
      "Epoch 301/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9081 - val_loss: -3.3791\n",
      "Epoch 302/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9118 - val_loss: -4.2140\n",
      "Epoch 303/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1183 - val_loss: -3.9056\n",
      "Epoch 304/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6273 - val_loss: -4.1849\n",
      "Epoch 305/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7639 - val_loss: -4.2080\n",
      "Epoch 306/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9215 - val_loss: -3.9404\n",
      "Epoch 307/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9126 - val_loss: -4.1799\n",
      "Epoch 308/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9126 - val_loss: -1.0910\n",
      "Epoch 309/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8333 - val_loss: -4.1522\n",
      "Epoch 310/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9615 - val_loss: -3.7352\n",
      "Epoch 311/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7665 - val_loss: -3.6619\n",
      "Epoch 312/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9031 - val_loss: -2.0066\n",
      "Epoch 313/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8938 - val_loss: -4.1473\n",
      "Epoch 314/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9868 - val_loss: -4.0971\n",
      "Epoch 315/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9441 - val_loss: -4.2147\n",
      "Epoch 316/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.0752 - val_loss: -2.4169\n",
      "Epoch 317/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2904 - val_loss: -3.4151\n",
      "Epoch 318/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5337 - val_loss: -4.0620\n",
      "Epoch 319/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6436 - val_loss: -3.9184\n",
      "Epoch 320/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6486 - val_loss: -4.2800\n",
      "Epoch 321/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7587 - val_loss: -4.2279\n",
      "Epoch 322/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6420 - val_loss: -4.0818\n",
      "Epoch 323/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7873 - val_loss: -3.8168\n",
      "Epoch 324/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8230 - val_loss: -3.8063\n",
      "Epoch 325/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7406 - val_loss: -4.0875\n",
      "Epoch 326/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8747 - val_loss: -4.1642\n",
      "Epoch 327/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8383 - val_loss: -4.2848\n",
      "Epoch 328/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8767 - val_loss: -4.1010\n",
      "Epoch 329/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7645 - val_loss: -4.1891\n",
      "Epoch 330/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8545 - val_loss: -3.6191\n",
      "Epoch 331/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9101 - val_loss: -3.9088\n",
      "Epoch 332/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9086 - val_loss: -3.7343\n",
      "Epoch 333/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8710 - val_loss: -3.5742\n",
      "Epoch 334/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9060 - val_loss: -4.0692\n",
      "Epoch 335/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7126 - val_loss: -3.7998\n",
      "Epoch 336/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9281 - val_loss: -4.0398\n",
      "Epoch 337/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8349 - val_loss: -2.6873\n",
      "Epoch 338/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9011 - val_loss: -3.9063\n",
      "Epoch 339/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9170 - val_loss: -3.8243\n",
      "Epoch 340/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8386 - val_loss: -2.6774\n",
      "Epoch 341/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9567 - val_loss: -3.6349\n",
      "Epoch 342/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8945 - val_loss: -3.7590\n",
      "Epoch 343/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9239 - val_loss: -4.2474\n",
      "Epoch 344/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7160 - val_loss: -3.8498\n",
      "Epoch 345/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9185 - val_loss: -3.7349\n",
      "Epoch 346/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9644 - val_loss: -3.5521\n",
      "Epoch 347/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9781 - val_loss: -4.0277\n",
      "Epoch 348/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8638 - val_loss: -3.2153\n",
      "Epoch 349/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8958 - val_loss: -3.6038\n",
      "Epoch 350/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9339 - val_loss: -3.9355\n",
      "Epoch 351/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8263 - val_loss: -4.1176\n",
      "Epoch 352/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0063 - val_loss: -3.3874\n",
      "Epoch 353/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9093 - val_loss: -3.5981\n",
      "Epoch 354/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9648 - val_loss: -4.2234\n",
      "Epoch 355/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9753 - val_loss: -3.2297\n",
      "Epoch 356/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0043 - val_loss: -4.2124\n",
      "Epoch 357/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8331 - val_loss: -3.6038\n",
      "Epoch 358/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9525 - val_loss: -3.7221\n",
      "Epoch 359/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9588 - val_loss: -3.7230\n",
      "Epoch 360/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9436 - val_loss: -4.1646\n",
      "Epoch 361/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0114 - val_loss: -4.3622\n",
      "Epoch 362/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.0760 - val_loss: -2.8915\n",
      "Epoch 363/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -3.9231 - val_loss: -4.1145\n",
      "Epoch 364/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5690 - val_loss: -3.9204\n",
      "Epoch 365/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7376 - val_loss: -3.8731\n",
      "Epoch 366/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7417 - val_loss: -3.6687\n",
      "Epoch 367/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6814 - val_loss: -4.0794\n",
      "Epoch 368/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8187 - val_loss: -4.3537\n",
      "Epoch 369/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8052 - val_loss: -4.3521\n",
      "Epoch 370/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7636 - val_loss: -3.8941\n",
      "Epoch 371/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9029 - val_loss: -4.1114\n",
      "Epoch 372/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7320 - val_loss: -3.8117\n",
      "Epoch 373/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8126 - val_loss: -3.9717\n",
      "Epoch 374/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8704 - val_loss: -4.1637\n",
      "Epoch 375/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7719 - val_loss: -3.9951\n",
      "Epoch 376/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8891 - val_loss: -3.6505\n",
      "Epoch 377/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8003 - val_loss: -3.9378\n",
      "Epoch 378/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9343 - val_loss: -3.7115\n",
      "Epoch 379/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9179 - val_loss: -3.8292\n",
      "Epoch 380/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9314 - val_loss: -3.1086\n",
      "Epoch 381/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9931 - val_loss: -2.4065\n",
      "Epoch 382/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8391 - val_loss: -3.3701\n",
      "Epoch 383/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9236 - val_loss: -4.0364\n",
      "Epoch 384/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9440 - val_loss: -3.9947\n",
      "Epoch 385/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9406 - val_loss: -4.0474\n",
      "Epoch 386/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9703 - val_loss: -3.9235\n",
      "Epoch 387/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9473 - val_loss: -4.2439\n",
      "Epoch 388/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9625 - val_loss: -3.0246\n",
      "Epoch 389/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9255 - val_loss: -4.0987\n",
      "Epoch 390/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9309 - val_loss: -3.2622\n",
      "Epoch 391/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9120 - val_loss: -3.6882\n",
      "Epoch 392/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.0408 - val_loss: -3.4019\n",
      "Epoch 393/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8029 - val_loss: -4.1281\n",
      "Epoch 394/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9772 - val_loss: -1.9112\n",
      "Epoch 395/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9319 - val_loss: -3.1352\n",
      "Epoch 396/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0431 - val_loss: -2.6003\n",
      "Epoch 397/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -3.5496 - val_loss: -3.1826\n",
      "Epoch 398/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.1605 - val_loss: -2.6226\n",
      "Epoch 399/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.4320 - val_loss: -2.7065\n",
      "Epoch 400/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5937 - val_loss: -3.9932\n",
      "Epoch 401/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6133 - val_loss: -4.0496\n",
      "Epoch 402/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6543 - val_loss: -3.9682\n",
      "Epoch 403/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6913 - val_loss: -2.9879\n",
      "Epoch 404/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7453 - val_loss: -3.5864\n",
      "Epoch 405/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8215 - val_loss: -3.6918\n",
      "Epoch 406/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7806 - val_loss: -4.2412\n",
      "Epoch 407/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7998 - val_loss: -4.1836\n",
      "Epoch 408/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7611 - val_loss: -3.9357\n",
      "Epoch 409/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7704 - val_loss: -4.2663\n",
      "Epoch 410/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7686 - val_loss: -4.1801\n",
      "Epoch 411/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8350 - val_loss: -3.5577\n",
      "Epoch 412/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9136 - val_loss: -4.0311\n",
      "Epoch 413/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7575 - val_loss: -2.5573\n",
      "Epoch 414/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7650 - val_loss: -4.2550\n",
      "Epoch 415/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8309 - val_loss: -4.1692\n",
      "Epoch 416/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9072 - val_loss: -3.8829\n",
      "Epoch 417/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8847 - val_loss: -4.3817\n",
      "Epoch 418/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8841 - val_loss: -4.1399\n",
      "Epoch 419/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9314 - val_loss: -4.0081\n",
      "Epoch 420/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5860 - val_loss: -3.7826\n",
      "Epoch 421/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8059 - val_loss: -4.3221\n",
      "Epoch 422/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8132 - val_loss: -4.2936\n",
      "Epoch 423/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9651 - val_loss: -2.9735\n",
      "Epoch 424/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9109 - val_loss: -3.9586\n",
      "Epoch 425/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9436 - val_loss: -2.2722\n",
      "Epoch 426/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9165 - val_loss: -4.1372\n",
      "Epoch 427/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8440 - val_loss: -4.3537\n",
      "Epoch 428/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0248 - val_loss: -1.8939\n",
      "Epoch 429/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9166 - val_loss: -4.0161\n",
      "Epoch 430/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9536 - val_loss: -4.2780\n",
      "Epoch 431/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8882 - val_loss: -3.0930\n",
      "Epoch 432/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0325 - val_loss: -4.3326\n",
      "Epoch 433/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9532 - val_loss: -4.3662\n",
      "Epoch 434/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9522 - val_loss: -3.8080\n",
      "Epoch 435/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9778 - val_loss: -4.1253\n",
      "Epoch 436/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7632 - val_loss: -3.4857\n",
      "Epoch 437/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9263 - val_loss: -4.3688\n",
      "Epoch 438/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8359 - val_loss: -4.3180\n",
      "Epoch 439/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7786 - val_loss: -4.2859\n",
      "Epoch 440/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9598 - val_loss: -4.3033\n",
      "Epoch 441/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9218 - val_loss: -3.6584\n",
      "Epoch 442/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8111 - val_loss: -4.3041\n",
      "Epoch 443/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9795 - val_loss: -2.6296\n",
      "Epoch 444/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9196 - val_loss: -4.1532\n",
      "Epoch 445/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9184 - val_loss: -4.3451\n",
      "Epoch 446/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9872 - val_loss: -4.2858\n",
      "Epoch 447/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9060 - val_loss: -4.1369\n",
      "Epoch 448/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9658 - val_loss: -3.8853\n",
      "Epoch 449/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9244 - val_loss: -1.5076\n",
      "Epoch 450/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9729 - val_loss: -3.8466\n",
      "Epoch 451/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8307 - val_loss: -4.3218\n",
      "Epoch 452/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0484 - val_loss: -4.1599\n",
      "Epoch 453/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9548 - val_loss: -4.0701\n",
      "Epoch 454/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9465 - val_loss: -4.3554\n",
      "Epoch 455/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9278 - val_loss: -4.0904\n",
      "Epoch 456/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.0372"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 258ms/step - loss: -5.0372 - val_loss: -4.5472\n",
      "Epoch 457/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9222 - val_loss: -3.5701\n",
      "Epoch 458/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0765 - val_loss: -3.6508\n",
      "Epoch 459/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7504 - val_loss: -2.7938\n",
      "Epoch 460/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0308 - val_loss: -3.9866\n",
      "Epoch 461/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9632 - val_loss: -3.9991\n",
      "Epoch 462/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0337 - val_loss: -4.3342\n",
      "Epoch 463/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0220 - val_loss: -3.5921\n",
      "Epoch 464/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9307 - val_loss: -4.0829\n",
      "Epoch 465/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.5954 - val_loss: -3.4069\n",
      "Epoch 466/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7143 - val_loss: -3.9466\n",
      "Epoch 467/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8748 - val_loss: -4.0515\n",
      "Epoch 468/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8921 - val_loss: -4.0561\n",
      "Epoch 469/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7693 - val_loss: -4.3160\n",
      "Epoch 470/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9555 - val_loss: -3.3765\n",
      "Epoch 471/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9547 - val_loss: -4.3810\n",
      "Epoch 472/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9928 - val_loss: -4.0522\n",
      "Epoch 473/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9740 - val_loss: -3.7917\n",
      "Epoch 474/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8992 - val_loss: -3.7460\n",
      "Epoch 475/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9263 - val_loss: -4.1684\n",
      "Epoch 476/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.0296 - val_loss: -4.2995\n",
      "Epoch 477/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9406 - val_loss: -4.1922\n",
      "Epoch 478/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7679 - val_loss: -4.1426\n",
      "Epoch 479/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8911 - val_loss: -4.2200\n",
      "Epoch 480/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8675 - val_loss: -4.1094\n",
      "Epoch 481/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9368 - val_loss: -2.8151\n",
      "Epoch 482/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8638 - val_loss: -3.9486\n",
      "Epoch 483/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9422 - val_loss: -4.1900\n",
      "Epoch 484/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0086 - val_loss: -3.4190\n",
      "Epoch 485/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9139 - val_loss: -3.1521\n",
      "Epoch 486/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9346 - val_loss: -3.7147\n",
      "Epoch 487/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.8851 - val_loss: -4.0503\n",
      "Epoch 488/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6627 - val_loss: -4.2980\n",
      "Epoch 489/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9022 - val_loss: -4.2177\n",
      "Epoch 490/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8803 - val_loss: -3.4363\n",
      "Epoch 491/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9531 - val_loss: -4.3597\n",
      "Epoch 492/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4991 - val_loss: -3.9828\n",
      "Epoch 493/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9709 - val_loss: -3.8634\n",
      "Epoch 494/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8275 - val_loss: -4.3520\n",
      "Epoch 495/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9719 - val_loss: -4.1696\n",
      "Epoch 496/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0238 - val_loss: -3.4087\n",
      "Epoch 497/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8822 - val_loss: -3.7215\n",
      "Epoch 498/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9906 - val_loss: -4.1337\n",
      "Epoch 499/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0083 - val_loss: -4.1987\n",
      "Epoch 500/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8865 - val_loss: -4.3742\n",
      "Epoch 501/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9987 - val_loss: -3.4225\n",
      "Epoch 502/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0060 - val_loss: -3.4019\n",
      "Epoch 503/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9299 - val_loss: -4.1630\n",
      "Epoch 504/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0445 - val_loss: -4.4515\n",
      "Epoch 505/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0199 - val_loss: -4.3204\n",
      "Epoch 506/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9866 - val_loss: -4.4153\n",
      "Epoch 507/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0725 - val_loss: -4.0265\n",
      "Epoch 508/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8646 - val_loss: -4.1344\n",
      "Epoch 509/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9692 - val_loss: -2.9025\n",
      "Epoch 510/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0036 - val_loss: -3.9843\n",
      "Epoch 511/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9623 - val_loss: -4.2537\n",
      "Epoch 512/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0583 - val_loss: -4.1871\n",
      "Epoch 513/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9984 - val_loss: -3.2558\n",
      "Epoch 514/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1039 - val_loss: -4.1088\n",
      "Epoch 515/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0443 - val_loss: -3.4184\n",
      "Epoch 516/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9745 - val_loss: -4.1423\n",
      "Epoch 517/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1158 - val_loss: -3.0688\n",
      "Epoch 518/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0473 - val_loss: -4.3699\n",
      "Epoch 519/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0638 - val_loss: -4.3938\n",
      "Epoch 520/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9707 - val_loss: -4.2228\n",
      "Epoch 521/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6234 - val_loss: -4.0783\n",
      "Epoch 522/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0737 - val_loss: -4.3677\n",
      "Epoch 523/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0225 - val_loss: -4.0084\n",
      "Epoch 524/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1028 - val_loss: -2.7452\n",
      "Epoch 525/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0397 - val_loss: -4.3147\n",
      "Epoch 526/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0224 - val_loss: -4.0968\n",
      "Epoch 527/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1282 - val_loss: -4.4453\n",
      "Epoch 528/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0958 - val_loss: -4.0787\n",
      "Epoch 529/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0072 - val_loss: -4.2974\n",
      "Epoch 530/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0269 - val_loss: -4.3784\n",
      "Epoch 531/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1347 - val_loss: -3.1846\n",
      "Epoch 532/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.8897 - val_loss: -4.1779\n",
      "Epoch 533/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8790 - val_loss: -3.4373\n",
      "Epoch 534/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9766 - val_loss: -3.4915\n",
      "Epoch 535/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9288 - val_loss: -4.1663\n",
      "Epoch 536/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9812 - val_loss: -4.3268\n",
      "Epoch 537/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0294 - val_loss: -4.3955\n",
      "Epoch 538/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9581 - val_loss: -4.2319\n",
      "Epoch 539/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9825 - val_loss: -2.7689\n",
      "Epoch 540/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0526 - val_loss: -0.0576\n",
      "Epoch 541/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9591 - val_loss: -4.4143\n",
      "Epoch 542/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0138 - val_loss: -4.4229\n",
      "Epoch 543/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8653 - val_loss: -4.0660\n",
      "Epoch 544/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0592 - val_loss: -3.0002\n",
      "Epoch 545/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0172 - val_loss: -4.1894\n",
      "Epoch 546/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9995 - val_loss: -2.8589\n",
      "Epoch 547/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0476 - val_loss: -4.4719\n",
      "Epoch 548/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9279 - val_loss: -3.6015\n",
      "Epoch 549/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1057 - val_loss: -4.4027\n",
      "Epoch 550/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1090 - val_loss: -4.4832\n",
      "Epoch 551/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1040 - val_loss: -4.2082\n",
      "Epoch 552/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9720 - val_loss: -3.8320\n",
      "Epoch 553/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1297 - val_loss: -4.0647\n",
      "Epoch 554/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0902 - val_loss: -4.3322\n",
      "Epoch 555/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0293 - val_loss: -4.0138\n",
      "Epoch 556/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6201 - val_loss: -3.8944\n",
      "Epoch 557/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0086 - val_loss: -4.1090\n",
      "Epoch 558/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0559 - val_loss: -3.9524\n",
      "Epoch 559/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0445 - val_loss: -4.0637\n",
      "Epoch 560/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0654 - val_loss: -4.1472\n",
      "Epoch 561/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.0682 - val_loss: -2.9404\n",
      "Epoch 562/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0220 - val_loss: -3.9175\n",
      "Epoch 563/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.0351 - val_loss: -3.5436\n",
      "Epoch 564/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1345 - val_loss: -3.8283\n",
      "Epoch 565/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0891 - val_loss: -4.1782\n",
      "Epoch 566/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0864 - val_loss: -4.1595\n",
      "Epoch 567/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0990 - val_loss: -4.0116\n",
      "Epoch 568/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0081 - val_loss: -4.4371\n",
      "Epoch 569/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1393 - val_loss: -3.7202\n",
      "Epoch 570/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0898 - val_loss: -4.1882\n",
      "Epoch 571/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1019"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 240ms/step - loss: -5.1019 - val_loss: -4.6011\n",
      "Epoch 572/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1397 - val_loss: -2.0235\n",
      "Epoch 573/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0498 - val_loss: -4.4870\n",
      "Epoch 574/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9819 - val_loss: -3.3698\n",
      "Epoch 575/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0747 - val_loss: -4.4444\n",
      "Epoch 576/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0664 - val_loss: -4.1827\n",
      "Epoch 577/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1223 - val_loss: -3.0322\n",
      "Epoch 578/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0976 - val_loss: -3.8980\n",
      "Epoch 579/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1086 - val_loss: -3.9266\n",
      "Epoch 580/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0377 - val_loss: -3.6811\n",
      "Epoch 581/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1590 - val_loss: -3.6835\n",
      "Epoch 582/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0765 - val_loss: -2.0123\n",
      "Epoch 583/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8532 - val_loss: -4.1250\n",
      "Epoch 584/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2091 - val_loss: -3.8921\n",
      "Epoch 585/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0717 - val_loss: -4.3839\n",
      "Epoch 586/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1261 - val_loss: -4.2552\n",
      "Epoch 587/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1538 - val_loss: -3.1635\n",
      "Epoch 588/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.3559 - val_loss: -3.2503\n",
      "Epoch 589/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7932 - val_loss: -3.5887\n",
      "Epoch 590/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.3741 - val_loss: -4.1392\n",
      "Epoch 591/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5657 - val_loss: -3.9783\n",
      "Epoch 592/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6983 - val_loss: -4.1911\n",
      "Epoch 593/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7031 - val_loss: -4.2934\n",
      "Epoch 594/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7855 - val_loss: -3.9224\n",
      "Epoch 595/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8293 - val_loss: -4.4161\n",
      "Epoch 596/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8097 - val_loss: -4.3178\n",
      "Epoch 597/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9028 - val_loss: -3.9493\n",
      "Epoch 598/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8739 - val_loss: -4.3552\n",
      "Epoch 599/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9081 - val_loss: -4.1925\n",
      "Epoch 600/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8520 - val_loss: -4.1365\n",
      "Epoch 601/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8528 - val_loss: -3.8110\n",
      "Epoch 602/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7566 - val_loss: -3.1695\n",
      "Epoch 603/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8276 - val_loss: -3.8194\n",
      "Epoch 604/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9261 - val_loss: -4.2036\n",
      "Epoch 605/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6625 - val_loss: -4.2339\n",
      "Epoch 606/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9345 - val_loss: -3.2102\n",
      "Epoch 607/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9051 - val_loss: -3.5513\n",
      "Epoch 608/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9301 - val_loss: -4.2161\n",
      "Epoch 609/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9948 - val_loss: -3.9718\n",
      "Epoch 610/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9004 - val_loss: -3.8267\n",
      "Epoch 611/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9397 - val_loss: -2.6465\n",
      "Epoch 612/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9775 - val_loss: -4.1793\n",
      "Epoch 613/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8312 - val_loss: -4.4079\n",
      "Epoch 614/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9494 - val_loss: -4.2855\n",
      "Epoch 615/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9524 - val_loss: -3.5011\n",
      "Epoch 616/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9092 - val_loss: -4.1997\n",
      "Epoch 617/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9698 - val_loss: -4.5366\n",
      "Epoch 618/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9725 - val_loss: -4.2547\n",
      "Epoch 619/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8694 - val_loss: -1.6621\n",
      "Epoch 620/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9811 - val_loss: -4.4431\n",
      "Epoch 621/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9182 - val_loss: -3.7615\n",
      "Epoch 622/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9762 - val_loss: -4.1866\n",
      "Epoch 623/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0408 - val_loss: -3.8367\n",
      "Epoch 624/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0223 - val_loss: -4.3556\n",
      "Epoch 625/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0693 - val_loss: -4.0710\n",
      "Epoch 626/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0699 - val_loss: -3.9778\n",
      "Epoch 627/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7923 - val_loss: -4.0646\n",
      "Epoch 628/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9624 - val_loss: -3.4365\n",
      "Epoch 629/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8862 - val_loss: -4.3365\n",
      "Epoch 630/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0126 - val_loss: -4.1622\n",
      "Epoch 631/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.9552 - val_loss: -3.7197\n",
      "Epoch 632/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8977 - val_loss: -4.1813\n",
      "Epoch 633/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9783 - val_loss: -4.0008\n",
      "Epoch 634/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0894 - val_loss: -3.9779\n",
      "Epoch 635/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0087 - val_loss: -4.0907\n",
      "Epoch 636/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0689 - val_loss: -4.2776\n",
      "Epoch 637/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9235 - val_loss: -4.1082\n",
      "Epoch 638/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0946 - val_loss: -4.0788\n",
      "Epoch 639/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0332 - val_loss: -3.7412\n",
      "Epoch 640/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0283 - val_loss: -4.3187\n",
      "Epoch 641/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1112 - val_loss: -4.2182\n",
      "Epoch 642/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9914 - val_loss: -4.0301\n",
      "Epoch 643/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0411 - val_loss: -4.1001\n",
      "Epoch 644/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9560 - val_loss: -4.0339\n",
      "Epoch 645/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0689 - val_loss: -3.4074\n",
      "Epoch 646/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1136 - val_loss: -4.5147\n",
      "Epoch 647/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0391 - val_loss: -3.3603\n",
      "Epoch 648/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9154 - val_loss: -4.3578\n",
      "Epoch 649/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1859 - val_loss: -3.5284\n",
      "Epoch 650/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.4121 - val_loss: -3.9998\n",
      "Epoch 651/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1082 - val_loss: -4.4301\n",
      "Epoch 652/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9860 - val_loss: -4.0720\n",
      "Epoch 653/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0897 - val_loss: -3.9584\n",
      "Epoch 654/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0285 - val_loss: -3.8983\n",
      "Epoch 655/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1453 - val_loss: -3.3259\n",
      "Epoch 656/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0460 - val_loss: -2.4204\n",
      "Epoch 657/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1392 - val_loss: -4.4550\n",
      "Epoch 658/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0771 - val_loss: -3.5084\n",
      "Epoch 659/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1347 - val_loss: -2.2131\n",
      "Epoch 660/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0043 - val_loss: -3.0768\n",
      "Epoch 661/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4186 - val_loss: -4.4643\n",
      "Epoch 662/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9811 - val_loss: -4.2112\n",
      "Epoch 663/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0460 - val_loss: -4.3765\n",
      "Epoch 664/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0862 - val_loss: -4.0196\n",
      "Epoch 665/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1102 - val_loss: -4.2435\n",
      "Epoch 666/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0356 - val_loss: -3.9522\n",
      "Epoch 667/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1062 - val_loss: -4.5568\n",
      "Epoch 668/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0359 - val_loss: -4.2642\n",
      "Epoch 669/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1462 - val_loss: -4.2686\n",
      "Epoch 670/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1795 - val_loss: -2.4525\n",
      "Epoch 671/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0573 - val_loss: -2.9446\n",
      "Epoch 672/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0063 - val_loss: -3.6273\n",
      "Epoch 673/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0981 - val_loss: -4.0748\n",
      "Epoch 674/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1440 - val_loss: -4.3865\n",
      "Epoch 675/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0102 - val_loss: -4.4844\n",
      "Epoch 676/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0872 - val_loss: -3.0200\n",
      "Epoch 677/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9407 - val_loss: -1.6772\n",
      "Epoch 678/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1024 - val_loss: -3.8443\n",
      "Epoch 679/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7868 - val_loss: -2.5979\n",
      "Epoch 680/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.2840 - val_loss: -3.7458\n",
      "Epoch 681/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0166 - val_loss: -4.1952\n",
      "Epoch 682/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0601 - val_loss: -4.1595\n",
      "Epoch 683/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0093 - val_loss: -0.7802\n",
      "Epoch 684/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9843 - val_loss: -4.4025\n",
      "Epoch 685/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0877 - val_loss: -4.1426\n",
      "Epoch 686/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7408 - val_loss: -3.9845\n",
      "Epoch 687/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1213 - val_loss: -4.3000\n",
      "Epoch 688/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0964 - val_loss: -3.1861\n",
      "Epoch 689/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1278 - val_loss: -4.3585\n",
      "Epoch 690/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1197 - val_loss: -3.8896\n",
      "Epoch 691/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5944 - val_loss: -3.9326\n",
      "Epoch 692/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0323 - val_loss: -4.1310\n",
      "Epoch 693/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.6053 - val_loss: -4.3431\n",
      "Epoch 694/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1458 - val_loss: -1.8501\n",
      "Epoch 695/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.1523 - val_loss: -4.2032\n",
      "Epoch 696/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0321 - val_loss: -4.1971\n",
      "Epoch 697/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1597 - val_loss: -2.1269\n",
      "Epoch 698/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0031 - val_loss: -4.2107\n",
      "Epoch 699/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1773 - val_loss: -3.7651\n",
      "Epoch 700/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7336 - val_loss: -4.1078\n",
      "Epoch 701/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0568 - val_loss: -3.3079\n",
      "Epoch 702/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1090 - val_loss: -3.3321\n",
      "Epoch 703/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9475 - val_loss: -4.4743\n",
      "Epoch 704/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1273 - val_loss: -4.1744\n",
      "Epoch 705/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1633 - val_loss: -3.4981\n",
      "Epoch 706/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0166 - val_loss: -2.7342\n",
      "Epoch 707/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0728 - val_loss: -4.3639\n",
      "Epoch 708/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0003 - val_loss: -4.2038\n",
      "Epoch 709/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -5.1246 - val_loss: -2.8930\n",
      "Epoch 710/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1241 - val_loss: -3.9614\n",
      "Epoch 711/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9578 - val_loss: -4.1787\n",
      "Epoch 712/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1671 - val_loss: -2.8348\n",
      "Epoch 713/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1456 - val_loss: -4.3046\n",
      "Epoch 714/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1151 - val_loss: -3.4460\n",
      "Epoch 715/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0737 - val_loss: -4.1746\n",
      "Epoch 716/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1775 - val_loss: -4.3124\n",
      "Epoch 717/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1802 - val_loss: -2.8156\n",
      "Epoch 718/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0170 - val_loss: -4.2515\n",
      "Epoch 719/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1781 - val_loss: -4.3167\n",
      "Epoch 720/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1789 - val_loss: -4.0619\n",
      "Epoch 721/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1501 - val_loss: -4.1726\n",
      "Epoch 722/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2415 - val_loss: -2.9393\n",
      "Epoch 723/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0385 - val_loss: -3.0027\n",
      "Epoch 724/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1928 - val_loss: -2.9730\n",
      "Epoch 725/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0216 - val_loss: -4.3895\n",
      "Epoch 726/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1911 - val_loss: -3.9236\n",
      "Epoch 727/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1806 - val_loss: -4.1150\n",
      "Epoch 728/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2114 - val_loss: -4.1284\n",
      "Epoch 729/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1345 - val_loss: -4.2762\n",
      "Epoch 730/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1671 - val_loss: -2.7344\n",
      "Epoch 731/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1472 - val_loss: -4.1562\n",
      "Epoch 732/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1595 - val_loss: -4.0694\n",
      "Epoch 733/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1877 - val_loss: -3.7933\n",
      "Epoch 734/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2250 - val_loss: -4.1625\n",
      "Epoch 735/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1329 - val_loss: -3.0085\n",
      "Epoch 736/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1300 - val_loss: -4.4886\n",
      "Epoch 737/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2338 - val_loss: -2.2602\n",
      "Epoch 738/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7267 - val_loss: -4.2381\n",
      "Epoch 739/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1784 - val_loss: -3.5950\n",
      "Epoch 740/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1699 - val_loss: -4.2193\n",
      "Epoch 741/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1665 - val_loss: -4.5497\n",
      "Epoch 742/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1743 - val_loss: -4.1535\n",
      "Epoch 743/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2322 - val_loss: -3.9844\n",
      "Epoch 744/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1575 - val_loss: -4.3274\n",
      "Epoch 745/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1626 - val_loss: -4.4998\n",
      "Epoch 746/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1259 - val_loss: -3.5948\n",
      "Epoch 747/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2096 - val_loss: -3.9016\n",
      "Epoch 748/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2055 - val_loss: -3.0335\n",
      "Epoch 749/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2212 - val_loss: -4.1521\n",
      "Epoch 750/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1454 - val_loss: -4.5354\n",
      "Epoch 751/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2367 - val_loss: -4.0641\n",
      "Epoch 752/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0648 - val_loss: -4.3756\n",
      "Epoch 753/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1796 - val_loss: -3.8827\n",
      "Epoch 754/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2326 - val_loss: -3.0896\n",
      "Epoch 755/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1477 - val_loss: -3.7609\n",
      "Epoch 756/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1157 - val_loss: -4.1219\n",
      "Epoch 757/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2806 - val_loss: -3.9124\n",
      "Epoch 758/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1537 - val_loss: -3.0070\n",
      "Epoch 759/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1312 - val_loss: -4.4099\n",
      "Epoch 760/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1879 - val_loss: -3.6765\n",
      "Epoch 761/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2381 - val_loss: 0.2424\n",
      "Epoch 762/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9190 - val_loss: -4.2223\n",
      "Epoch 763/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5818 - val_loss: -4.2687\n",
      "Epoch 764/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7880 - val_loss: -4.0653\n",
      "Epoch 765/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9539 - val_loss: -4.0214\n",
      "Epoch 766/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9962 - val_loss: -4.3051\n",
      "Epoch 767/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9584 - val_loss: -4.2712\n",
      "Epoch 768/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9802 - val_loss: -3.6643\n",
      "Epoch 769/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1506 - val_loss: -4.4575\n",
      "Epoch 770/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0036 - val_loss: -3.7235\n",
      "Epoch 771/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0739 - val_loss: -4.4145\n",
      "Epoch 772/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1812 - val_loss: -4.3498\n",
      "Epoch 773/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1045 - val_loss: -3.8096\n",
      "Epoch 774/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0884 - val_loss: -4.5480\n",
      "Epoch 775/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0955 - val_loss: -4.3720\n",
      "Epoch 776/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1246 - val_loss: -4.4118\n",
      "Epoch 777/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1334 - val_loss: -4.2980\n",
      "Epoch 778/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2355 - val_loss: -3.8946\n",
      "Epoch 779/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6595 - val_loss: -4.4753\n",
      "Epoch 780/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1486 - val_loss: -4.2126\n",
      "Epoch 781/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1864 - val_loss: -0.7104\n",
      "Epoch 782/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1804 - val_loss: -3.9459\n",
      "Epoch 783/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0735 - val_loss: -4.1590\n",
      "Epoch 784/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2149 - val_loss: -4.3341\n",
      "Epoch 785/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0729 - val_loss: -4.2925\n",
      "Epoch 786/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1293 - val_loss: -4.0742\n",
      "Epoch 787/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1571 - val_loss: -3.8464\n",
      "Epoch 788/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1498 - val_loss: -4.3374\n",
      "Epoch 789/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2254 - val_loss: -4.5297\n",
      "Epoch 790/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.1695"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 255ms/step - loss: -5.1695 - val_loss: -4.6054\n",
      "Epoch 791/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2699 - val_loss: -4.5223\n",
      "Epoch 792/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2119 - val_loss: -4.2661\n",
      "Epoch 793/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2056 - val_loss: -4.1982\n",
      "Epoch 794/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0615 - val_loss: -4.3527\n",
      "Epoch 795/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1782 - val_loss: -4.1372\n",
      "Epoch 796/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2096 - val_loss: -4.0448\n",
      "Epoch 797/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1544 - val_loss: -4.2467\n",
      "Epoch 798/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7240 - val_loss: -4.3016\n",
      "Epoch 799/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0421 - val_loss: -4.0613\n",
      "Epoch 800/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1788 - val_loss: -4.1798\n",
      "Epoch 801/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0328 - val_loss: -4.4740\n",
      "Epoch 802/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1649 - val_loss: -2.7976\n",
      "Epoch 803/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2968 - val_loss: -4.4201\n",
      "Epoch 804/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1047 - val_loss: -3.3813\n",
      "Epoch 805/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2646 - val_loss: -3.5443\n",
      "Epoch 806/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2456 - val_loss: -3.9547\n",
      "Epoch 807/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0972 - val_loss: -4.5269\n",
      "Epoch 808/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2524 - val_loss: -4.2035\n",
      "Epoch 809/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2345 - val_loss: -3.7348\n",
      "Epoch 810/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2349 - val_loss: -4.2584\n",
      "Epoch 811/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2248 - val_loss: -2.9541\n",
      "Epoch 812/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2783 - val_loss: -2.7102\n",
      "Epoch 813/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1773 - val_loss: -4.0387\n",
      "Epoch 814/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2392 - val_loss: -4.4680\n",
      "Epoch 815/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2193 - val_loss: -4.2185\n",
      "Epoch 816/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2393 - val_loss: -4.4105\n",
      "Epoch 817/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2136 - val_loss: -3.9882\n",
      "Epoch 818/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2287 - val_loss: -3.9441\n",
      "Epoch 819/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2533 - val_loss: -4.0529\n",
      "Epoch 820/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0894 - val_loss: -4.5200\n",
      "Epoch 821/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1359 - val_loss: -4.1625\n",
      "Epoch 822/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1197 - val_loss: -4.3431\n",
      "Epoch 823/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2562 - val_loss: -4.1426\n",
      "Epoch 824/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2702 - val_loss: -3.6313\n",
      "Epoch 825/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.4801 - val_loss: -3.4141\n",
      "Epoch 826/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3159 - val_loss: -3.9014\n",
      "Epoch 827/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6388 - val_loss: -4.1873\n",
      "Epoch 828/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8012 - val_loss: -3.5438\n",
      "Epoch 829/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9212 - val_loss: -3.7762\n",
      "Epoch 830/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9689 - val_loss: -4.1831\n",
      "Epoch 831/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9815 - val_loss: -4.4081\n",
      "Epoch 832/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8018 - val_loss: -2.7687\n",
      "Epoch 833/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0583 - val_loss: -3.6692\n",
      "Epoch 834/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9513 - val_loss: -3.6629\n",
      "Epoch 835/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0439 - val_loss: -4.2738\n",
      "Epoch 836/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0122 - val_loss: -4.4147\n",
      "Epoch 837/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1336 - val_loss: -4.0954\n",
      "Epoch 838/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0917 - val_loss: -4.1381\n",
      "Epoch 839/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0737 - val_loss: -3.9170\n",
      "Epoch 840/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9724 - val_loss: -3.6561\n",
      "Epoch 841/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2192 - val_loss: 0.4457\n",
      "Epoch 842/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0128 - val_loss: -3.8722\n",
      "Epoch 843/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1606 - val_loss: -4.5445\n",
      "Epoch 844/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0821 - val_loss: -3.5306\n",
      "Epoch 845/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2469 - val_loss: -4.1093\n",
      "Epoch 846/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1876 - val_loss: -3.9536\n",
      "Epoch 847/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1666 - val_loss: -4.0126\n",
      "Epoch 848/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1854 - val_loss: -4.1694\n",
      "Epoch 849/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2306 - val_loss: -4.1972\n",
      "Epoch 850/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1712 - val_loss: -4.0491\n",
      "Epoch 851/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1231 - val_loss: -3.5766\n",
      "Epoch 852/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1208 - val_loss: -2.5776\n",
      "Epoch 853/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1773 - val_loss: -3.1197\n",
      "Epoch 854/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2446 - val_loss: -4.3171\n",
      "Epoch 855/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1984 - val_loss: -4.1454\n",
      "Epoch 856/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2221 - val_loss: -4.3420\n",
      "Epoch 857/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2609 - val_loss: -4.0423\n",
      "Epoch 858/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2445 - val_loss: -3.6723\n",
      "Epoch 859/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8688 - val_loss: -3.9352\n",
      "Epoch 860/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0994 - val_loss: -2.7876\n",
      "Epoch 861/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2458 - val_loss: -2.3903\n",
      "Epoch 862/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1473 - val_loss: -4.5103\n",
      "Epoch 863/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2370 - val_loss: -4.0605\n",
      "Epoch 864/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2310 - val_loss: -1.0152\n",
      "Epoch 865/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2185 - val_loss: -4.2884\n",
      "Epoch 866/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3005 - val_loss: -3.8093\n",
      "Epoch 867/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2565 - val_loss: -0.6425\n",
      "Epoch 868/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2281 - val_loss: -4.1159\n",
      "Epoch 869/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1965 - val_loss: -4.5196\n",
      "Epoch 870/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2643 - val_loss: -4.1143\n",
      "Epoch 871/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2482 - val_loss: -4.0069\n",
      "Epoch 872/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2054 - val_loss: -4.2815\n",
      "Epoch 873/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2527 - val_loss: -4.0295\n",
      "Epoch 874/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2337 - val_loss: -4.2542\n",
      "Epoch 875/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1956 - val_loss: -4.4659\n",
      "Epoch 876/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2538 - val_loss: -3.8431\n",
      "Epoch 877/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2289 - val_loss: -4.2960\n",
      "Epoch 878/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1113 - val_loss: -4.4400\n",
      "Epoch 879/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3140 - val_loss: -4.1453\n",
      "Epoch 880/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1696 - val_loss: -4.4239\n",
      "Epoch 881/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2691 - val_loss: -2.5703\n",
      "Epoch 882/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2735 - val_loss: -3.6026\n",
      "Epoch 883/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2861 - val_loss: -4.0992\n",
      "Epoch 884/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1419 - val_loss: -3.5327\n",
      "Epoch 885/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2905 - val_loss: -4.0907\n",
      "Epoch 886/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2836 - val_loss: -3.6557\n",
      "Epoch 887/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1579 - val_loss: -1.4713\n",
      "Epoch 888/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3237"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 28s 239ms/step - loss: -5.3237 - val_loss: -4.7183\n",
      "Epoch 889/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8305 - val_loss: -4.3384\n",
      "Epoch 890/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2339 - val_loss: -4.0622\n",
      "Epoch 891/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2668 - val_loss: -3.3743\n",
      "Epoch 892/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2817 - val_loss: -2.5706\n",
      "Epoch 893/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2357 - val_loss: -4.0994\n",
      "Epoch 894/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2652 - val_loss: -3.9573\n",
      "Epoch 895/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2195 - val_loss: -3.4234\n",
      "Epoch 896/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0965 - val_loss: -4.6284\n",
      "Epoch 897/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3099 - val_loss: -3.4003\n",
      "Epoch 898/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3100 - val_loss: -2.9357\n",
      "Epoch 899/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2308 - val_loss: -4.3939\n",
      "Epoch 900/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1572 - val_loss: -3.8806\n",
      "Epoch 901/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1548 - val_loss: -4.4366\n",
      "Epoch 902/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2894 - val_loss: -3.9949\n",
      "Epoch 903/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1424 - val_loss: -3.4158\n",
      "Epoch 904/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1972 - val_loss: -4.4256\n",
      "Epoch 905/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2910 - val_loss: -4.4073\n",
      "Epoch 906/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3162 - val_loss: -4.2191\n",
      "Epoch 907/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3141 - val_loss: -4.1240\n",
      "Epoch 908/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2086 - val_loss: -3.7441\n",
      "Epoch 909/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2498 - val_loss: -4.4147\n",
      "Epoch 910/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3276 - val_loss: -4.2662\n",
      "Epoch 911/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3138 - val_loss: -4.4666\n",
      "Epoch 912/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2861 - val_loss: -3.3315\n",
      "Epoch 913/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1983 - val_loss: -4.3357\n",
      "Epoch 914/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3083 - val_loss: -2.3296\n",
      "Epoch 915/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1762 - val_loss: -3.7690\n",
      "Epoch 916/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2804 - val_loss: -3.5808\n",
      "Epoch 917/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3214 - val_loss: -3.3524\n",
      "Epoch 918/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3174 - val_loss: -3.8101\n",
      "Epoch 919/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3265 - val_loss: -4.4085\n",
      "Epoch 920/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2960 - val_loss: -3.9651\n",
      "Epoch 921/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6601 - val_loss: -4.3468\n",
      "Epoch 922/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1883 - val_loss: -3.9483\n",
      "Epoch 923/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1014 - val_loss: -4.3497\n",
      "Epoch 924/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1958 - val_loss: -3.6908\n",
      "Epoch 925/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2686 - val_loss: -4.2404\n",
      "Epoch 926/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2112 - val_loss: -3.7521\n",
      "Epoch 927/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2385 - val_loss: -3.8701\n",
      "Epoch 928/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9772 - val_loss: -3.2543\n",
      "Epoch 929/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2435 - val_loss: -4.1464\n",
      "Epoch 930/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1762 - val_loss: -4.0535\n",
      "Epoch 931/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1640 - val_loss: -4.5034\n",
      "Epoch 932/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2719 - val_loss: -3.6550\n",
      "Epoch 933/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2920 - val_loss: -4.6237\n",
      "Epoch 934/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1589 - val_loss: -4.5179\n",
      "Epoch 935/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3184 - val_loss: -2.9391\n",
      "Epoch 936/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2975 - val_loss: -3.3353\n",
      "Epoch 937/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1879 - val_loss: -4.5748\n",
      "Epoch 938/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0942 - val_loss: -4.2594\n",
      "Epoch 939/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3057 - val_loss: -4.3816\n",
      "Epoch 940/2000\n",
      "118/118 [==============================] - ETA: 0s - loss: -5.3521"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_190_layer_call_fn, lstm_cell_190_layer_call_and_return_conditional_losses, lstm_cell_191_layer_call_fn, lstm_cell_191_layer_call_and_return_conditional_losses, lstm_cell_192_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 30s 256ms/step - loss: -5.3521 - val_loss: -4.8641\n",
      "Epoch 941/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2023 - val_loss: -4.4105\n",
      "Epoch 942/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2925 - val_loss: -3.2011\n",
      "Epoch 943/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3157 - val_loss: -3.5118\n",
      "Epoch 944/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3244 - val_loss: -2.9679\n",
      "Epoch 945/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6084 - val_loss: -4.3565\n",
      "Epoch 946/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.2146 - val_loss: -4.0334\n",
      "Epoch 947/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2052 - val_loss: -2.9732\n",
      "Epoch 948/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1988 - val_loss: -3.9856\n",
      "Epoch 949/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2225 - val_loss: -3.4539\n",
      "Epoch 950/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1783 - val_loss: -3.8461\n",
      "Epoch 951/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2681 - val_loss: -3.6604\n",
      "Epoch 952/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2947 - val_loss: -3.4794\n",
      "Epoch 953/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1942 - val_loss: -4.2566\n",
      "Epoch 954/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2788 - val_loss: -3.2983\n",
      "Epoch 955/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2875 - val_loss: -3.7755\n",
      "Epoch 956/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2651 - val_loss: -3.8376\n",
      "Epoch 957/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1828 - val_loss: -4.2062\n",
      "Epoch 958/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3250 - val_loss: -4.0936\n",
      "Epoch 959/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3254 - val_loss: -4.3242\n",
      "Epoch 960/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2809 - val_loss: -4.1369\n",
      "Epoch 961/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3242 - val_loss: -4.3486\n",
      "Epoch 962/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3753 - val_loss: -2.0692\n",
      "Epoch 963/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1873 - val_loss: -3.6899\n",
      "Epoch 964/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.7776 - val_loss: -3.2522\n",
      "Epoch 965/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4137 - val_loss: -3.5275\n",
      "Epoch 966/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7789 - val_loss: -3.8414\n",
      "Epoch 967/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9476 - val_loss: -3.8493\n",
      "Epoch 968/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.0190 - val_loss: -4.0144\n",
      "Epoch 969/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0634 - val_loss: -3.5882\n",
      "Epoch 970/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0667 - val_loss: -4.2090\n",
      "Epoch 971/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0906 - val_loss: -4.5564\n",
      "Epoch 972/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1405 - val_loss: -4.1449\n",
      "Epoch 973/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0115 - val_loss: -4.4252\n",
      "Epoch 974/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1508 - val_loss: -3.6811\n",
      "Epoch 975/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1170 - val_loss: -4.2047\n",
      "Epoch 976/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8905 - val_loss: -3.7008\n",
      "Epoch 977/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0711 - val_loss: -3.1618\n",
      "Epoch 978/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0461 - val_loss: -4.3117\n",
      "Epoch 979/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1974 - val_loss: -4.5546\n",
      "Epoch 980/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1820 - val_loss: -4.3081\n",
      "Epoch 981/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2052 - val_loss: -4.1917\n",
      "Epoch 982/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2084 - val_loss: -4.3524\n",
      "Epoch 983/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2317 - val_loss: -3.3638\n",
      "Epoch 984/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2214 - val_loss: -4.3428\n",
      "Epoch 985/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1263 - val_loss: -4.1255\n",
      "Epoch 986/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2583 - val_loss: -4.3871\n",
      "Epoch 987/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1630 - val_loss: -4.1335\n",
      "Epoch 988/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1593 - val_loss: -4.0749\n",
      "Epoch 989/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2461 - val_loss: -4.4004\n",
      "Epoch 990/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.1310 - val_loss: -4.1951\n",
      "Epoch 991/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2629 - val_loss: -2.4185\n",
      "Epoch 992/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1080 - val_loss: -4.1309\n",
      "Epoch 993/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2225 - val_loss: -4.3882\n",
      "Epoch 994/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8586 - val_loss: -3.9746\n",
      "Epoch 995/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1651 - val_loss: -1.5625\n",
      "Epoch 996/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1327 - val_loss: -4.0952\n",
      "Epoch 997/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3215 - val_loss: -2.8398\n",
      "Epoch 998/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2788 - val_loss: -4.0016\n",
      "Epoch 999/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1459 - val_loss: -4.2587\n",
      "Epoch 1000/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1608 - val_loss: -3.3769\n",
      "Epoch 1001/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3747 - val_loss: -3.8962\n",
      "Epoch 1002/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7716 - val_loss: -3.5978\n",
      "Epoch 1003/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8285 - val_loss: -3.3285\n",
      "Epoch 1004/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0853 - val_loss: -2.5896\n",
      "Epoch 1005/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7282 - val_loss: -3.8030\n",
      "Epoch 1006/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0143 - val_loss: -4.2885\n",
      "Epoch 1007/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0865 - val_loss: -4.3095\n",
      "Epoch 1008/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1189 - val_loss: -4.4056\n",
      "Epoch 1009/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1392 - val_loss: -4.2189\n",
      "Epoch 1010/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1990 - val_loss: -3.4986\n",
      "Epoch 1011/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1963 - val_loss: -3.2024\n",
      "Epoch 1012/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0756 - val_loss: -4.4231\n",
      "Epoch 1013/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2657 - val_loss: -4.6990\n",
      "Epoch 1014/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1257 - val_loss: -3.8825\n",
      "Epoch 1015/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2174 - val_loss: -3.9919\n",
      "Epoch 1016/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2894 - val_loss: -3.4823\n",
      "Epoch 1017/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2086 - val_loss: -2.5715\n",
      "Epoch 1018/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2013 - val_loss: -4.2953\n",
      "Epoch 1019/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1731 - val_loss: -3.9990\n",
      "Epoch 1020/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4045 - val_loss: -3.9046\n",
      "Epoch 1021/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0316 - val_loss: -4.3671\n",
      "Epoch 1022/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0927 - val_loss: -3.8223\n",
      "Epoch 1023/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2313 - val_loss: -4.5067\n",
      "Epoch 1024/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1849 - val_loss: -4.1460\n",
      "Epoch 1025/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3212 - val_loss: -3.2671\n",
      "Epoch 1026/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1872 - val_loss: -4.5579\n",
      "Epoch 1027/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2065 - val_loss: -3.7455\n",
      "Epoch 1028/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2936 - val_loss: -4.0465\n",
      "Epoch 1029/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2226 - val_loss: -3.5901\n",
      "Epoch 1030/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2398 - val_loss: -4.3462\n",
      "Epoch 1031/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2035 - val_loss: -4.4857\n",
      "Epoch 1032/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2269 - val_loss: -4.0259\n",
      "Epoch 1033/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3136 - val_loss: -4.2182\n",
      "Epoch 1034/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2418 - val_loss: -3.9145\n",
      "Epoch 1035/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4643 - val_loss: -3.6980\n",
      "Epoch 1036/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0023 - val_loss: -3.3259\n",
      "Epoch 1037/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1887 - val_loss: -4.1559\n",
      "Epoch 1038/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0266 - val_loss: -4.1167\n",
      "Epoch 1039/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2032 - val_loss: -4.6171\n",
      "Epoch 1040/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2674 - val_loss: -0.9558\n",
      "Epoch 1041/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -5.2221 - val_loss: -4.2286\n",
      "Epoch 1042/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2877 - val_loss: -3.9074\n",
      "Epoch 1043/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2809 - val_loss: -4.0425\n",
      "Epoch 1044/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7040 - val_loss: -3.8459\n",
      "Epoch 1045/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2690 - val_loss: -2.8009\n",
      "Epoch 1046/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2194 - val_loss: -4.2414\n",
      "Epoch 1047/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2267 - val_loss: -4.1915\n",
      "Epoch 1048/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2219 - val_loss: -4.0785\n",
      "Epoch 1049/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2348 - val_loss: -3.5898\n",
      "Epoch 1050/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2726 - val_loss: -4.1392\n",
      "Epoch 1051/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2168 - val_loss: -4.3249\n",
      "Epoch 1052/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1926 - val_loss: -4.4877\n",
      "Epoch 1053/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9639 - val_loss: -4.3267\n",
      "Epoch 1054/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3175 - val_loss: -3.2620\n",
      "Epoch 1055/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2357 - val_loss: -4.3810\n",
      "Epoch 1056/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3244 - val_loss: -4.2725\n",
      "Epoch 1057/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2696 - val_loss: -3.8805\n",
      "Epoch 1058/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2592 - val_loss: -4.4270\n",
      "Epoch 1059/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2190 - val_loss: -4.1562\n",
      "Epoch 1060/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3002 - val_loss: -3.9508\n",
      "Epoch 1061/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2918 - val_loss: -3.5676\n",
      "Epoch 1062/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3010 - val_loss: -4.1002\n",
      "Epoch 1063/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3711 - val_loss: -3.8618\n",
      "Epoch 1064/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3148 - val_loss: -4.4860\n",
      "Epoch 1065/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9778 - val_loss: -3.8863\n",
      "Epoch 1066/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1836 - val_loss: -4.2155\n",
      "Epoch 1067/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2647 - val_loss: -4.0672\n",
      "Epoch 1068/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3130 - val_loss: -4.4964\n",
      "Epoch 1069/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8274 - val_loss: -3.3693\n",
      "Epoch 1070/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1631 - val_loss: -4.1734\n",
      "Epoch 1071/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3028 - val_loss: -4.0912\n",
      "Epoch 1072/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2883 - val_loss: -3.7227\n",
      "Epoch 1073/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3396 - val_loss: -4.1598\n",
      "Epoch 1074/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2912 - val_loss: -4.0842\n",
      "Epoch 1075/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2345 - val_loss: -4.2946\n",
      "Epoch 1076/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3114 - val_loss: -4.3865\n",
      "Epoch 1077/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3678 - val_loss: -4.5747\n",
      "Epoch 1078/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2015 - val_loss: -3.9548\n",
      "Epoch 1079/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2196 - val_loss: -4.4661\n",
      "Epoch 1080/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3422 - val_loss: -4.5304\n",
      "Epoch 1081/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2958 - val_loss: -4.2256\n",
      "Epoch 1082/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2438 - val_loss: -4.6314\n",
      "Epoch 1083/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3299 - val_loss: -3.9741\n",
      "Epoch 1084/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3999 - val_loss: -4.1213\n",
      "Epoch 1085/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3373 - val_loss: -4.4996\n",
      "Epoch 1086/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3555 - val_loss: -3.6609\n",
      "Epoch 1087/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1878 - val_loss: -3.9817\n",
      "Epoch 1088/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3620 - val_loss: -3.8038\n",
      "Epoch 1089/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3631 - val_loss: -3.6197\n",
      "Epoch 1090/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3299 - val_loss: -3.7876\n",
      "Epoch 1091/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2881 - val_loss: -4.6221\n",
      "Epoch 1092/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3794 - val_loss: -3.2598\n",
      "Epoch 1093/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2709 - val_loss: -2.9894\n",
      "Epoch 1094/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2877 - val_loss: -3.7365\n",
      "Epoch 1095/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3538 - val_loss: -3.4236\n",
      "Epoch 1096/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3006 - val_loss: -3.9865\n",
      "Epoch 1097/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3064 - val_loss: -4.0285\n",
      "Epoch 1098/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3294 - val_loss: -4.5044\n",
      "Epoch 1099/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3796 - val_loss: -3.6951\n",
      "Epoch 1100/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9784 - val_loss: -3.4550\n",
      "Epoch 1101/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9523 - val_loss: -4.5164\n",
      "Epoch 1102/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2400 - val_loss: -4.5505\n",
      "Epoch 1103/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.3115 - val_loss: -4.0502\n",
      "Epoch 1104/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3092 - val_loss: -2.7888\n",
      "Epoch 1105/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3478 - val_loss: -4.3574\n",
      "Epoch 1106/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1801 - val_loss: -4.4647\n",
      "Epoch 1107/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.4025 - val_loss: -2.2347\n",
      "Epoch 1108/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2461 - val_loss: -3.8764\n",
      "Epoch 1109/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3148 - val_loss: -4.4419\n",
      "Epoch 1110/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3426 - val_loss: -4.4614\n",
      "Epoch 1111/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2698 - val_loss: -3.8104\n",
      "Epoch 1112/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0306 - val_loss: -3.5796\n",
      "Epoch 1113/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3402 - val_loss: -4.5671\n",
      "Epoch 1114/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2768 - val_loss: -4.2089\n",
      "Epoch 1115/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3477 - val_loss: -3.4479\n",
      "Epoch 1116/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3079 - val_loss: -4.0500\n",
      "Epoch 1117/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3068 - val_loss: -3.8538\n",
      "Epoch 1118/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2909 - val_loss: -3.6576\n",
      "Epoch 1119/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3249 - val_loss: -4.4491\n",
      "Epoch 1120/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4612 - val_loss: -1.7149\n",
      "Epoch 1121/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2933 - val_loss: -3.4738\n",
      "Epoch 1122/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3110 - val_loss: -4.1477\n",
      "Epoch 1123/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3440 - val_loss: -4.6642\n",
      "Epoch 1124/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3295 - val_loss: -4.4892\n",
      "Epoch 1125/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3778 - val_loss: 0.0234\n",
      "Epoch 1126/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.1644 - val_loss: -4.2816\n",
      "Epoch 1127/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2806 - val_loss: -4.7525\n",
      "Epoch 1128/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3665 - val_loss: -2.2691\n",
      "Epoch 1129/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3136 - val_loss: -2.4463\n",
      "Epoch 1130/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3711 - val_loss: -4.3323\n",
      "Epoch 1131/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8229 - val_loss: -4.1664\n",
      "Epoch 1132/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2906 - val_loss: -4.5185\n",
      "Epoch 1133/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3903 - val_loss: -3.4844\n",
      "Epoch 1134/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3420 - val_loss: -4.4787\n",
      "Epoch 1135/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.3731 - val_loss: -4.3059\n",
      "Epoch 1136/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4165 - val_loss: -3.1754\n",
      "Epoch 1137/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3411 - val_loss: -4.5952\n",
      "Epoch 1138/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3620 - val_loss: -3.3298\n",
      "Epoch 1139/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3772 - val_loss: -1.0835\n",
      "Epoch 1140/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2515 - val_loss: -4.2742\n",
      "Epoch 1141/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2987 - val_loss: -3.7231\n",
      "Epoch 1142/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9341 - val_loss: -3.4181\n",
      "Epoch 1143/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2177 - val_loss: -4.2213\n",
      "Epoch 1144/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8337 - val_loss: -3.9676\n",
      "Epoch 1145/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9514 - val_loss: -4.1530\n",
      "Epoch 1146/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0470 - val_loss: -4.3942\n",
      "Epoch 1147/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0139 - val_loss: -4.0399\n",
      "Epoch 1148/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1506 - val_loss: -3.0516\n",
      "Epoch 1149/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0501 - val_loss: -4.1120\n",
      "Epoch 1150/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1319 - val_loss: -2.8356\n",
      "Epoch 1151/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1293 - val_loss: -4.1825\n",
      "Epoch 1152/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1359 - val_loss: -3.9620\n",
      "Epoch 1153/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1538 - val_loss: -4.0291\n",
      "Epoch 1154/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.1966 - val_loss: -4.3908\n",
      "Epoch 1155/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2126 - val_loss: -2.3620\n",
      "Epoch 1156/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1312 - val_loss: -4.4005\n",
      "Epoch 1157/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0767 - val_loss: -2.6144\n",
      "Epoch 1158/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1558 - val_loss: -3.6865\n",
      "Epoch 1159/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1725 - val_loss: -4.3416\n",
      "Epoch 1160/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2580 - val_loss: -4.4857\n",
      "Epoch 1161/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2195 - val_loss: -4.2758\n",
      "Epoch 1162/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0366 - val_loss: -4.3444\n",
      "Epoch 1163/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3067 - val_loss: -4.2992\n",
      "Epoch 1164/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1920 - val_loss: -4.2863\n",
      "Epoch 1165/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1877 - val_loss: -4.6921\n",
      "Epoch 1166/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3014 - val_loss: -4.6368\n",
      "Epoch 1167/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2041 - val_loss: -3.7856\n",
      "Epoch 1168/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2340 - val_loss: -3.2728\n",
      "Epoch 1169/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.3067 - val_loss: -4.6418\n",
      "Epoch 1170/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3037 - val_loss: -4.2523\n",
      "Epoch 1171/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2215 - val_loss: -2.4194\n",
      "Epoch 1172/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2662 - val_loss: -3.7258\n",
      "Epoch 1173/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.2679 - val_loss: -2.4849\n",
      "Epoch 1174/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2987 - val_loss: -4.4630\n",
      "Epoch 1175/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5311 - val_loss: -4.5394\n",
      "Epoch 1176/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.1380 - val_loss: -3.0854\n",
      "Epoch 1177/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.1971 - val_loss: -3.6410\n",
      "Epoch 1178/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.2414 - val_loss: -4.2612\n",
      "Epoch 1179/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2188 - val_loss: -4.5390\n",
      "Epoch 1180/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2491 - val_loss: -4.5342\n",
      "Epoch 1181/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1636 - val_loss: -4.3190\n",
      "Epoch 1182/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2060 - val_loss: -4.1877\n",
      "Epoch 1183/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3460 - val_loss: -3.9452\n",
      "Epoch 1184/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3412 - val_loss: -4.4732\n",
      "Epoch 1185/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3098 - val_loss: -4.2600\n",
      "Epoch 1186/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.3776 - val_loss: -4.0430\n",
      "Epoch 1187/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2701 - val_loss: -4.5083\n",
      "Epoch 1188/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2689 - val_loss: -4.6243\n",
      "Epoch 1189/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.2989 - val_loss: -4.6977\n",
      "Epoch 1190/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.2930 - val_loss: -4.4992\n",
      "Epoch 1191/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -5.3903 - val_loss: -4.5561\n",
      "Epoch 1192/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.2008 - val_loss: -4.6044\n",
      "Epoch 1193/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2743 - val_loss: -4.1414\n",
      "Epoch 1194/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3117 - val_loss: -3.9514\n",
      "Epoch 1195/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.3934 - val_loss: -3.9198\n",
      "Epoch 1196/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.1933 - val_loss: -4.3635\n",
      "Epoch 1197/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.2478 - val_loss: -3.0567\n",
      "Epoch 1198/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.4145 - val_loss: -4.4765\n",
      "Epoch 1199/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.6847 - val_loss: -1.3483\n",
      "Epoch 1200/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.1119 - val_loss: -2.3148\n",
      "Epoch 1201/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -2.8058 - val_loss: -2.6333\n",
      "Epoch 1202/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -3.2811 - val_loss: -3.4329\n",
      "Epoch 1203/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.5900 - val_loss: -3.6521\n",
      "Epoch 1204/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -3.7876 - val_loss: -3.8517\n",
      "Epoch 1205/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9462 - val_loss: -3.8974\n",
      "Epoch 1206/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9953 - val_loss: -3.8147\n",
      "Epoch 1207/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0218 - val_loss: -4.0399\n",
      "Epoch 1208/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1601 - val_loss: -4.1610\n",
      "Epoch 1209/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2758 - val_loss: -3.5495\n",
      "Epoch 1210/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3399 - val_loss: -3.8944\n",
      "Epoch 1211/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.3419 - val_loss: -3.9427\n",
      "Epoch 1212/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4907 - val_loss: -3.1793\n",
      "Epoch 1213/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5235 - val_loss: -3.8225\n",
      "Epoch 1214/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4742 - val_loss: -4.1328\n",
      "Epoch 1215/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.5547 - val_loss: -1.4802\n",
      "Epoch 1216/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4942 - val_loss: -4.0105\n",
      "Epoch 1217/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5740 - val_loss: -4.0602\n",
      "Epoch 1218/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6397 - val_loss: -4.1456\n",
      "Epoch 1219/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6937 - val_loss: -3.7277\n",
      "Epoch 1220/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7168 - val_loss: -4.0753\n",
      "Epoch 1221/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5949 - val_loss: -3.8549\n",
      "Epoch 1222/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6634 - val_loss: -4.3971\n",
      "Epoch 1223/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7125 - val_loss: -4.2025\n",
      "Epoch 1224/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7607 - val_loss: -4.3424\n",
      "Epoch 1225/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7504 - val_loss: -3.1953\n",
      "Epoch 1226/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7729 - val_loss: -4.0305\n",
      "Epoch 1227/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7917 - val_loss: -4.3045\n",
      "Epoch 1228/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6305 - val_loss: -3.9719\n",
      "Epoch 1229/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6990 - val_loss: -3.0376\n",
      "Epoch 1230/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7462 - val_loss: -4.0545\n",
      "Epoch 1231/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8421 - val_loss: -3.2594\n",
      "Epoch 1232/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8009 - val_loss: -3.9949\n",
      "Epoch 1233/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8124 - val_loss: -3.8830\n",
      "Epoch 1234/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8300 - val_loss: -4.1923\n",
      "Epoch 1235/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8744 - val_loss: -3.0619\n",
      "Epoch 1236/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8118 - val_loss: -4.2299\n",
      "Epoch 1237/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9672 - val_loss: -4.0164\n",
      "Epoch 1238/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7737 - val_loss: -4.3745\n",
      "Epoch 1239/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7844 - val_loss: -4.2746\n",
      "Epoch 1240/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9465 - val_loss: -2.8277\n",
      "Epoch 1241/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8238 - val_loss: -4.0964\n",
      "Epoch 1242/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8984 - val_loss: -4.1645\n",
      "Epoch 1243/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7886 - val_loss: -3.9999\n",
      "Epoch 1244/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3330 - val_loss: -4.3382\n",
      "Epoch 1245/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.8830 - val_loss: -2.2883\n",
      "Epoch 1246/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.3763 - val_loss: -2.7343\n",
      "Epoch 1247/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -2.8355 - val_loss: -3.3247\n",
      "Epoch 1248/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.2607 - val_loss: -3.2661\n",
      "Epoch 1249/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.5814 - val_loss: -3.7388\n",
      "Epoch 1250/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8469 - val_loss: -3.0752\n",
      "Epoch 1251/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9595 - val_loss: -4.3152\n",
      "Epoch 1252/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9295 - val_loss: -4.2456\n",
      "Epoch 1253/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.6785 - val_loss: -3.5995\n",
      "Epoch 1254/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.0093 - val_loss: -4.1707\n",
      "Epoch 1255/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.0949 - val_loss: -3.4580\n",
      "Epoch 1256/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0596 - val_loss: -2.8109\n",
      "Epoch 1257/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -2.3394 - val_loss: -0.9912\n",
      "Epoch 1258/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.3042 - val_loss: -0.8244\n",
      "Epoch 1259/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.0908 - val_loss: -1.5161\n",
      "Epoch 1260/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.5016 - val_loss: -1.9982\n",
      "Epoch 1261/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.6041 - val_loss: -2.4066\n",
      "Epoch 1262/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.7108 - val_loss: -3.0056\n",
      "Epoch 1263/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.6935 - val_loss: -3.1807\n",
      "Epoch 1264/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.8026 - val_loss: -3.2720\n",
      "Epoch 1265/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8547 - val_loss: -3.2650\n",
      "Epoch 1266/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.8719 - val_loss: -3.5643\n",
      "Epoch 1267/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9734 - val_loss: -3.4867\n",
      "Epoch 1268/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9824 - val_loss: -3.6020\n",
      "Epoch 1269/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8732 - val_loss: -3.4366\n",
      "Epoch 1270/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.0088 - val_loss: -3.5287\n",
      "Epoch 1271/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0875 - val_loss: -3.0884\n",
      "Epoch 1272/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0885 - val_loss: -3.6329\n",
      "Epoch 1273/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1441 - val_loss: -3.7947\n",
      "Epoch 1274/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -3.9973 - val_loss: -3.7064\n",
      "Epoch 1275/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1874 - val_loss: -3.7683\n",
      "Epoch 1276/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0694 - val_loss: -2.9789\n",
      "Epoch 1277/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.1845 - val_loss: -3.0787\n",
      "Epoch 1278/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1943 - val_loss: -3.6956\n",
      "Epoch 1279/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2374 - val_loss: -3.7789\n",
      "Epoch 1280/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2251 - val_loss: -3.5758\n",
      "Epoch 1281/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2554 - val_loss: -2.6531\n",
      "Epoch 1282/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1254 - val_loss: -3.6327\n",
      "Epoch 1283/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.2615 - val_loss: -3.3710\n",
      "Epoch 1284/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2413 - val_loss: -3.2368\n",
      "Epoch 1285/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3017 - val_loss: -3.8664\n",
      "Epoch 1286/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1443 - val_loss: -1.1550\n",
      "Epoch 1287/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3397 - val_loss: -3.9958\n",
      "Epoch 1288/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2315 - val_loss: -3.4928\n",
      "Epoch 1289/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.3424 - val_loss: -3.8375\n",
      "Epoch 1290/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2854 - val_loss: -3.7043\n",
      "Epoch 1291/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3411 - val_loss: -3.8222\n",
      "Epoch 1292/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.3290 - val_loss: -3.8689\n",
      "Epoch 1293/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2830 - val_loss: -3.6071\n",
      "Epoch 1294/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3149 - val_loss: -3.8082\n",
      "Epoch 1295/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3640 - val_loss: -3.5032\n",
      "Epoch 1296/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2381 - val_loss: -3.6308\n",
      "Epoch 1297/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2515 - val_loss: -3.1141\n",
      "Epoch 1298/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3538 - val_loss: -3.9764\n",
      "Epoch 1299/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3907 - val_loss: -3.6422\n",
      "Epoch 1300/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2751 - val_loss: -4.0793\n",
      "Epoch 1301/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3353 - val_loss: -3.7905\n",
      "Epoch 1302/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3967 - val_loss: -2.8165\n",
      "Epoch 1303/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3275 - val_loss: -3.6503\n",
      "Epoch 1304/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3544 - val_loss: -3.8962\n",
      "Epoch 1305/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.3738 - val_loss: -3.9964\n",
      "Epoch 1306/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4338 - val_loss: -3.6335\n",
      "Epoch 1307/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.2532 - val_loss: -3.8952\n",
      "Epoch 1308/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.3275 - val_loss: -3.8872\n",
      "Epoch 1309/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4497 - val_loss: -3.9491\n",
      "Epoch 1310/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3990 - val_loss: -4.0165\n",
      "Epoch 1311/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4171 - val_loss: -4.0065\n",
      "Epoch 1312/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3450 - val_loss: -3.8824\n",
      "Epoch 1313/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3964 - val_loss: -3.8296\n",
      "Epoch 1314/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4636 - val_loss: -3.9353\n",
      "Epoch 1315/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4147 - val_loss: -3.9247\n",
      "Epoch 1316/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3436 - val_loss: -3.7781\n",
      "Epoch 1317/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.4179 - val_loss: -3.9248\n",
      "Epoch 1318/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4500 - val_loss: -3.9359\n",
      "Epoch 1319/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0759 - val_loss: -3.5499\n",
      "Epoch 1320/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3633 - val_loss: -3.9475\n",
      "Epoch 1321/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3954 - val_loss: -3.7412\n",
      "Epoch 1322/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3721 - val_loss: -4.0159\n",
      "Epoch 1323/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5179 - val_loss: -1.9762\n",
      "Epoch 1324/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3915 - val_loss: -4.0733\n",
      "Epoch 1325/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4539 - val_loss: -3.9634\n",
      "Epoch 1326/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4127 - val_loss: -3.6187\n",
      "Epoch 1327/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4385 - val_loss: -3.9656\n",
      "Epoch 1328/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.4382 - val_loss: -3.9003\n",
      "Epoch 1329/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3375 - val_loss: -3.6303\n",
      "Epoch 1330/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4526 - val_loss: -3.7953\n",
      "Epoch 1331/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4479 - val_loss: -4.1241\n",
      "Epoch 1332/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4776 - val_loss: -4.0033\n",
      "Epoch 1333/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4629 - val_loss: -3.7168\n",
      "Epoch 1334/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.4889 - val_loss: -4.0385\n",
      "Epoch 1335/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.9957 - val_loss: -3.2133\n",
      "Epoch 1336/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.4630 - val_loss: -3.8972\n",
      "Epoch 1337/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4514 - val_loss: -4.1446\n",
      "Epoch 1338/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4854 - val_loss: -4.0787\n",
      "Epoch 1339/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4702 - val_loss: -3.8854\n",
      "Epoch 1340/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5227 - val_loss: -2.7803\n",
      "Epoch 1341/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5104 - val_loss: -3.5221\n",
      "Epoch 1342/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5421 - val_loss: -3.9582\n",
      "Epoch 1343/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4412 - val_loss: -3.6859\n",
      "Epoch 1344/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4553 - val_loss: -3.8695\n",
      "Epoch 1345/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5502 - val_loss: -4.0572\n",
      "Epoch 1346/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4806 - val_loss: -3.3882\n",
      "Epoch 1347/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3506 - val_loss: -3.2330\n",
      "Epoch 1348/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.5210 - val_loss: -3.8446\n",
      "Epoch 1349/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.4475 - val_loss: -3.7846\n",
      "Epoch 1350/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3525 - val_loss: -3.8075\n",
      "Epoch 1351/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4855 - val_loss: -3.8365\n",
      "Epoch 1352/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5316 - val_loss: -3.7615\n",
      "Epoch 1353/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5129 - val_loss: -4.2033\n",
      "Epoch 1354/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4914 - val_loss: -4.0864\n",
      "Epoch 1355/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3490 - val_loss: -4.0672\n",
      "Epoch 1356/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4010 - val_loss: -3.5782\n",
      "Epoch 1357/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.3237 - val_loss: -4.0291\n",
      "Epoch 1358/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3762 - val_loss: -3.6247\n",
      "Epoch 1359/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4423 - val_loss: -3.7253\n",
      "Epoch 1360/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5835 - val_loss: -3.9414\n",
      "Epoch 1361/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5577 - val_loss: -3.3027\n",
      "Epoch 1362/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4496 - val_loss: -3.8364\n",
      "Epoch 1363/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3633 - val_loss: -3.5457\n",
      "Epoch 1364/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5347 - val_loss: -3.0612\n",
      "Epoch 1365/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5351 - val_loss: -3.7168\n",
      "Epoch 1366/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3240 - val_loss: -4.0237\n",
      "Epoch 1367/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5353 - val_loss: -3.8325\n",
      "Epoch 1368/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9256 - val_loss: -3.8667\n",
      "Epoch 1369/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5232 - val_loss: -3.5998\n",
      "Epoch 1370/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5786 - val_loss: -3.9348\n",
      "Epoch 1371/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5147 - val_loss: -3.6753\n",
      "Epoch 1372/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5539 - val_loss: -3.9777\n",
      "Epoch 1373/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5388 - val_loss: -3.9709\n",
      "Epoch 1374/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4555 - val_loss: -3.9053\n",
      "Epoch 1375/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.4209 - val_loss: -3.9996\n",
      "Epoch 1376/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4432 - val_loss: -3.9520\n",
      "Epoch 1377/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4977 - val_loss: -2.8157\n",
      "Epoch 1378/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5730 - val_loss: -4.1831\n",
      "Epoch 1379/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6138 - val_loss: -3.9735\n",
      "Epoch 1380/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5196 - val_loss: -3.4304\n",
      "Epoch 1381/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5208 - val_loss: -4.0169\n",
      "Epoch 1382/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4059 - val_loss: -3.6151\n",
      "Epoch 1383/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5265 - val_loss: -4.0607\n",
      "Epoch 1384/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5270 - val_loss: -3.7728\n",
      "Epoch 1385/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3518 - val_loss: -3.6889\n",
      "Epoch 1386/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5364 - val_loss: -4.1077\n",
      "Epoch 1387/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6276 - val_loss: -3.3414\n",
      "Epoch 1388/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4825 - val_loss: -4.0017\n",
      "Epoch 1389/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6084 - val_loss: -3.1384\n",
      "Epoch 1390/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6150 - val_loss: -3.0056\n",
      "Epoch 1391/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4598 - val_loss: -3.8593\n",
      "Epoch 1392/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5124 - val_loss: -3.9155\n",
      "Epoch 1393/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5777 - val_loss: -4.0408\n",
      "Epoch 1394/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6411 - val_loss: -1.8696\n",
      "Epoch 1395/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.5675 - val_loss: -3.7677\n",
      "Epoch 1396/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5250 - val_loss: -4.2295\n",
      "Epoch 1397/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5621 - val_loss: -4.0656\n",
      "Epoch 1398/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5485 - val_loss: -3.6248\n",
      "Epoch 1399/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6439 - val_loss: -4.1001\n",
      "Epoch 1400/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5640 - val_loss: -3.8987\n",
      "Epoch 1401/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6513 - val_loss: -3.4905\n",
      "Epoch 1402/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4891 - val_loss: -3.7732\n",
      "Epoch 1403/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6166 - val_loss: -3.7222\n",
      "Epoch 1404/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4481 - val_loss: -3.4027\n",
      "Epoch 1405/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6626 - val_loss: -3.8874\n",
      "Epoch 1406/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5774 - val_loss: -3.4731\n",
      "Epoch 1407/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5017 - val_loss: -4.2274\n",
      "Epoch 1408/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5843 - val_loss: -4.0309\n",
      "Epoch 1409/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5180 - val_loss: -3.7954\n",
      "Epoch 1410/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6409 - val_loss: -3.9809\n",
      "Epoch 1411/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6227 - val_loss: -3.8733\n",
      "Epoch 1412/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4928 - val_loss: -2.6052\n",
      "Epoch 1413/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.9860 - val_loss: -3.9956\n",
      "Epoch 1414/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.6394 - val_loss: -3.9312\n",
      "Epoch 1415/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6672 - val_loss: -3.2438\n",
      "Epoch 1416/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6043 - val_loss: -3.9673\n",
      "Epoch 1417/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4527 - val_loss: -3.5237\n",
      "Epoch 1418/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6237 - val_loss: -3.7091\n",
      "Epoch 1419/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.5439 - val_loss: -3.5779\n",
      "Epoch 1420/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4668 - val_loss: -4.1349\n",
      "Epoch 1421/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6759 - val_loss: -3.7597\n",
      "Epoch 1422/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4892 - val_loss: -3.2753\n",
      "Epoch 1423/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5750 - val_loss: -3.9441\n",
      "Epoch 1424/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6264 - val_loss: -4.1625\n",
      "Epoch 1425/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5850 - val_loss: -2.4384\n",
      "Epoch 1426/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6285 - val_loss: -4.2197\n",
      "Epoch 1427/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4901 - val_loss: -3.6801\n",
      "Epoch 1428/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5080 - val_loss: -4.0719\n",
      "Epoch 1429/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6076 - val_loss: -4.0071\n",
      "Epoch 1430/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6613 - val_loss: -4.1001\n",
      "Epoch 1431/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6711 - val_loss: -4.0529\n",
      "Epoch 1432/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6631 - val_loss: -4.2183\n",
      "Epoch 1433/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5338 - val_loss: -2.6634\n",
      "Epoch 1434/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6366 - val_loss: -3.9752\n",
      "Epoch 1435/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6416 - val_loss: -4.0961\n",
      "Epoch 1436/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6123 - val_loss: -3.7452\n",
      "Epoch 1437/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4665 - val_loss: -3.7768\n",
      "Epoch 1438/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6198 - val_loss: -4.0589\n",
      "Epoch 1439/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7105 - val_loss: -2.9612\n",
      "Epoch 1440/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5669 - val_loss: -4.1065\n",
      "Epoch 1441/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.4062 - val_loss: -3.5659\n",
      "Epoch 1442/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6844 - val_loss: -4.1977\n",
      "Epoch 1443/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6296 - val_loss: -4.1616\n",
      "Epoch 1444/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6395 - val_loss: -3.5738\n",
      "Epoch 1445/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6935 - val_loss: -4.2498\n",
      "Epoch 1446/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6638 - val_loss: -4.1259\n",
      "Epoch 1447/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5765 - val_loss: -3.6400\n",
      "Epoch 1448/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7330 - val_loss: -4.0846\n",
      "Epoch 1449/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.5679 - val_loss: -3.4795\n",
      "Epoch 1450/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6225 - val_loss: -3.9843\n",
      "Epoch 1451/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7682 - val_loss: -4.2294\n",
      "Epoch 1452/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.0230 - val_loss: -3.8637\n",
      "Epoch 1453/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6772 - val_loss: -4.0829\n",
      "Epoch 1454/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5944 - val_loss: -3.8846\n",
      "Epoch 1455/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7002 - val_loss: -4.2192\n",
      "Epoch 1456/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6886 - val_loss: -3.2798\n",
      "Epoch 1457/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7074 - val_loss: -4.1449\n",
      "Epoch 1458/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6395 - val_loss: -3.9880\n",
      "Epoch 1459/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7070 - val_loss: -3.9830\n",
      "Epoch 1460/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6116 - val_loss: -3.9709\n",
      "Epoch 1461/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5375 - val_loss: -3.7497\n",
      "Epoch 1462/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6610 - val_loss: -4.0179\n",
      "Epoch 1463/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7068 - val_loss: -4.0516\n",
      "Epoch 1464/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5678 - val_loss: -3.9398\n",
      "Epoch 1465/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7207 - val_loss: -3.9507\n",
      "Epoch 1466/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6104 - val_loss: -4.2550\n",
      "Epoch 1467/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7228 - val_loss: -2.5183\n",
      "Epoch 1468/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7079 - val_loss: -3.5990\n",
      "Epoch 1469/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7017 - val_loss: -4.1094\n",
      "Epoch 1470/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6636 - val_loss: -4.0375\n",
      "Epoch 1471/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.4070 - val_loss: -3.5444\n",
      "Epoch 1472/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7680 - val_loss: -3.7861\n",
      "Epoch 1473/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7531 - val_loss: -4.1338\n",
      "Epoch 1474/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7306 - val_loss: -4.0314\n",
      "Epoch 1475/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6532 - val_loss: -4.1006\n",
      "Epoch 1476/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6835 - val_loss: -4.1984\n",
      "Epoch 1477/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6906 - val_loss: -4.0882\n",
      "Epoch 1478/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7495 - val_loss: -3.6723\n",
      "Epoch 1479/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6445 - val_loss: -4.1375\n",
      "Epoch 1480/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7334 - val_loss: -3.6013\n",
      "Epoch 1481/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5880 - val_loss: -4.1675\n",
      "Epoch 1482/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7927 - val_loss: -4.2320\n",
      "Epoch 1483/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6188 - val_loss: -3.8773\n",
      "Epoch 1484/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7293 - val_loss: -3.8968\n",
      "Epoch 1485/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7348 - val_loss: -4.3726\n",
      "Epoch 1486/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6288 - val_loss: -3.5979\n",
      "Epoch 1487/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7164 - val_loss: -3.9482\n",
      "Epoch 1488/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7661 - val_loss: -3.8227\n",
      "Epoch 1489/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6449 - val_loss: -4.1487\n",
      "Epoch 1490/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7384 - val_loss: -3.8590\n",
      "Epoch 1491/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7560 - val_loss: -3.9326\n",
      "Epoch 1492/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6979 - val_loss: -3.9276\n",
      "Epoch 1493/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7380 - val_loss: -4.1983\n",
      "Epoch 1494/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7426 - val_loss: -3.7611\n",
      "Epoch 1495/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.6151 - val_loss: -3.8427\n",
      "Epoch 1496/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7940 - val_loss: -4.1856\n",
      "Epoch 1497/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4925 - val_loss: -3.6961\n",
      "Epoch 1498/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7591 - val_loss: -4.1453\n",
      "Epoch 1499/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6805 - val_loss: -4.0010\n",
      "Epoch 1500/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7510 - val_loss: -3.7667\n",
      "Epoch 1501/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7524 - val_loss: -3.8446\n",
      "Epoch 1502/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7706 - val_loss: -4.0802\n",
      "Epoch 1503/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7492 - val_loss: -4.2005\n",
      "Epoch 1504/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7983 - val_loss: -3.9855\n",
      "Epoch 1505/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8099 - val_loss: -3.3241\n",
      "Epoch 1506/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8189 - val_loss: -3.3040\n",
      "Epoch 1507/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7674 - val_loss: -4.3901\n",
      "Epoch 1508/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6682 - val_loss: -4.2211\n",
      "Epoch 1509/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7402 - val_loss: -4.3483\n",
      "Epoch 1510/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7919 - val_loss: -4.3209\n",
      "Epoch 1511/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7517 - val_loss: -4.2452\n",
      "Epoch 1512/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7258 - val_loss: -3.2546\n",
      "Epoch 1513/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6438 - val_loss: -4.0867\n",
      "Epoch 1514/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3181 - val_loss: -3.9863\n",
      "Epoch 1515/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8092 - val_loss: -4.1077\n",
      "Epoch 1516/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7506 - val_loss: -3.6010\n",
      "Epoch 1517/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7077 - val_loss: -3.5767\n",
      "Epoch 1518/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7584 - val_loss: -4.0781\n",
      "Epoch 1519/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6566 - val_loss: -3.9875\n",
      "Epoch 1520/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7695 - val_loss: -3.3562\n",
      "Epoch 1521/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8701 - val_loss: -3.6942\n",
      "Epoch 1522/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7647 - val_loss: -4.1052\n",
      "Epoch 1523/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7655 - val_loss: -3.4164\n",
      "Epoch 1524/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6537 - val_loss: -3.7567\n",
      "Epoch 1525/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.6925 - val_loss: -3.6362\n",
      "Epoch 1526/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8072 - val_loss: -3.9092\n",
      "Epoch 1527/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6737 - val_loss: -3.6605\n",
      "Epoch 1528/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6272 - val_loss: -3.8258\n",
      "Epoch 1529/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6917 - val_loss: -3.3921\n",
      "Epoch 1530/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5627 - val_loss: -4.0885\n",
      "Epoch 1531/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7800 - val_loss: -3.5614\n",
      "Epoch 1532/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7007 - val_loss: -4.2008\n",
      "Epoch 1533/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7575 - val_loss: -4.0183\n",
      "Epoch 1534/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.0493 - val_loss: -3.9583\n",
      "Epoch 1535/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8428 - val_loss: -3.8815\n",
      "Epoch 1536/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7546 - val_loss: -4.1555\n",
      "Epoch 1537/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7606 - val_loss: -4.1878\n",
      "Epoch 1538/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7384 - val_loss: -4.1965\n",
      "Epoch 1539/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7551 - val_loss: -4.0360\n",
      "Epoch 1540/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6034 - val_loss: -3.4707\n",
      "Epoch 1541/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7299 - val_loss: -4.3563\n",
      "Epoch 1542/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7797 - val_loss: -3.7128\n",
      "Epoch 1543/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7475 - val_loss: -3.8795\n",
      "Epoch 1544/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7893 - val_loss: -4.1453\n",
      "Epoch 1545/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8622 - val_loss: -4.3218\n",
      "Epoch 1546/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6188 - val_loss: -3.3690\n",
      "Epoch 1547/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.7322 - val_loss: -3.4299\n",
      "Epoch 1548/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7659 - val_loss: -4.2401\n",
      "Epoch 1549/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8440 - val_loss: -3.3129\n",
      "Epoch 1550/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8353 - val_loss: -4.4245\n",
      "Epoch 1551/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3837 - val_loss: -4.1278\n",
      "Epoch 1552/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8022 - val_loss: -4.2347\n",
      "Epoch 1553/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8471 - val_loss: -4.3069\n",
      "Epoch 1554/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8340 - val_loss: -2.6125\n",
      "Epoch 1555/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7627 - val_loss: -3.4303\n",
      "Epoch 1556/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8160 - val_loss: -4.1183\n",
      "Epoch 1557/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8652 - val_loss: -4.2519\n",
      "Epoch 1558/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7644 - val_loss: -4.0078\n",
      "Epoch 1559/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7665 - val_loss: -4.3892\n",
      "Epoch 1560/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8143 - val_loss: -3.9348\n",
      "Epoch 1561/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8707 - val_loss: -3.4125\n",
      "Epoch 1562/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7387 - val_loss: -3.0181\n",
      "Epoch 1563/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5998 - val_loss: -3.9240\n",
      "Epoch 1564/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7728 - val_loss: -4.1345\n",
      "Epoch 1565/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7951 - val_loss: -4.3242\n",
      "Epoch 1566/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8587 - val_loss: -4.4591\n",
      "Epoch 1567/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8160 - val_loss: -4.1579\n",
      "Epoch 1568/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7907 - val_loss: -3.8181\n",
      "Epoch 1569/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7594 - val_loss: -4.1841\n",
      "Epoch 1570/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.1492 - val_loss: -3.5241\n",
      "Epoch 1571/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7154 - val_loss: -3.6559\n",
      "Epoch 1572/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7160 - val_loss: -3.5350\n",
      "Epoch 1573/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8688 - val_loss: -4.3694\n",
      "Epoch 1574/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8578 - val_loss: -3.9715\n",
      "Epoch 1575/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7448 - val_loss: -3.7668\n",
      "Epoch 1576/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8525 - val_loss: -3.9951\n",
      "Epoch 1577/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8924 - val_loss: -4.2347\n",
      "Epoch 1578/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6847 - val_loss: -3.1445\n",
      "Epoch 1579/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7546 - val_loss: -3.9144\n",
      "Epoch 1580/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8317 - val_loss: -3.9829\n",
      "Epoch 1581/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8027 - val_loss: -2.8653\n",
      "Epoch 1582/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8173 - val_loss: -3.8026\n",
      "Epoch 1583/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7965 - val_loss: -3.8769\n",
      "Epoch 1584/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8359 - val_loss: -4.3190\n",
      "Epoch 1585/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8481 - val_loss: -4.1472\n",
      "Epoch 1586/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6366 - val_loss: -3.8223\n",
      "Epoch 1587/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8848 - val_loss: -2.9946\n",
      "Epoch 1588/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.6020 - val_loss: -4.0013\n",
      "Epoch 1589/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8372 - val_loss: -4.2772\n",
      "Epoch 1590/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7868 - val_loss: -3.2172\n",
      "Epoch 1591/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8304 - val_loss: -3.4610\n",
      "Epoch 1592/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2574 - val_loss: -3.4325\n",
      "Epoch 1593/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7768 - val_loss: -2.8384\n",
      "Epoch 1594/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8104 - val_loss: -4.0996\n",
      "Epoch 1595/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8488 - val_loss: -4.1636\n",
      "Epoch 1596/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7617 - val_loss: -4.0348\n",
      "Epoch 1597/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8844 - val_loss: -3.8960\n",
      "Epoch 1598/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9028 - val_loss: -4.3705\n",
      "Epoch 1599/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7965 - val_loss: -4.4080\n",
      "Epoch 1600/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8220 - val_loss: -1.4254\n",
      "Epoch 1601/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7646 - val_loss: -4.3163\n",
      "Epoch 1602/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9089 - val_loss: -4.4188\n",
      "Epoch 1603/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8104 - val_loss: -4.4023\n",
      "Epoch 1604/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8948 - val_loss: -4.5122\n",
      "Epoch 1605/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9031 - val_loss: -3.8290\n",
      "Epoch 1606/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8795 - val_loss: -4.4189\n",
      "Epoch 1607/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7398 - val_loss: 0.9466\n",
      "Epoch 1608/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.9086 - val_loss: -4.1140\n",
      "Epoch 1609/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7963 - val_loss: -3.7422\n",
      "Epoch 1610/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9142 - val_loss: -4.0347\n",
      "Epoch 1611/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7915 - val_loss: -4.4071\n",
      "Epoch 1612/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9330 - val_loss: -4.1629\n",
      "Epoch 1613/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7887 - val_loss: -4.5847\n",
      "Epoch 1614/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8628 - val_loss: -4.4508\n",
      "Epoch 1615/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.9241 - val_loss: -2.9720\n",
      "Epoch 1616/2000\n",
      "118/118 [==============================] - 22s 187ms/step - loss: -4.7489 - val_loss: -4.2084\n",
      "Epoch 1617/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8667 - val_loss: -4.1560\n",
      "Epoch 1618/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8784 - val_loss: -3.6169\n",
      "Epoch 1619/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8931 - val_loss: -2.5940\n",
      "Epoch 1620/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9100 - val_loss: -4.4275\n",
      "Epoch 1621/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.2485 - val_loss: -4.2723\n",
      "Epoch 1622/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8855 - val_loss: -4.1969\n",
      "Epoch 1623/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9150 - val_loss: -4.4150\n",
      "Epoch 1624/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7859 - val_loss: -3.9388\n",
      "Epoch 1625/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8725 - val_loss: -3.3057\n",
      "Epoch 1626/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8926 - val_loss: -2.5162\n",
      "Epoch 1627/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7841 - val_loss: -4.3023\n",
      "Epoch 1628/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8677 - val_loss: -3.4717\n",
      "Epoch 1629/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8998 - val_loss: -4.2413\n",
      "Epoch 1630/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9118 - val_loss: -4.0023\n",
      "Epoch 1631/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.7709 - val_loss: -4.1537\n",
      "Epoch 1632/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9500 - val_loss: -4.3803\n",
      "Epoch 1633/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6940 - val_loss: -3.9953\n",
      "Epoch 1634/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8788 - val_loss: -2.9505\n",
      "Epoch 1635/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9198 - val_loss: -4.3612\n",
      "Epoch 1636/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8527 - val_loss: -3.1087\n",
      "Epoch 1637/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.7978 - val_loss: -4.4686\n",
      "Epoch 1638/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9070 - val_loss: -4.2416\n",
      "Epoch 1639/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8613 - val_loss: -4.1674\n",
      "Epoch 1640/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8903 - val_loss: -3.6798\n",
      "Epoch 1641/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8934 - val_loss: -2.8692\n",
      "Epoch 1642/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8441 - val_loss: -3.9703\n",
      "Epoch 1643/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9714 - val_loss: -4.2846\n",
      "Epoch 1644/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7708 - val_loss: -4.3129\n",
      "Epoch 1645/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8801 - val_loss: -3.9945\n",
      "Epoch 1646/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7906 - val_loss: -3.9595\n",
      "Epoch 1647/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8826 - val_loss: -4.1188\n",
      "Epoch 1648/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7841 - val_loss: -4.1721\n",
      "Epoch 1649/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9019 - val_loss: -3.7521\n",
      "Epoch 1650/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9017 - val_loss: -4.3204\n",
      "Epoch 1651/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8933 - val_loss: -4.3247\n",
      "Epoch 1652/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7981 - val_loss: -4.1089\n",
      "Epoch 1653/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9608 - val_loss: -3.6816\n",
      "Epoch 1654/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6564 - val_loss: -4.2372\n",
      "Epoch 1655/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8439 - val_loss: -4.3498\n",
      "Epoch 1656/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7767 - val_loss: -4.0419\n",
      "Epoch 1657/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8857 - val_loss: -1.8752\n",
      "Epoch 1658/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8719 - val_loss: -2.9402\n",
      "Epoch 1659/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9328 - val_loss: -4.0036\n",
      "Epoch 1660/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9210 - val_loss: -3.9804\n",
      "Epoch 1661/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8391 - val_loss: -4.3226\n",
      "Epoch 1662/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9004 - val_loss: -4.2921\n",
      "Epoch 1663/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8651 - val_loss: -4.2814\n",
      "Epoch 1664/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9028 - val_loss: -4.4570\n",
      "Epoch 1665/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9123 - val_loss: -3.6051\n",
      "Epoch 1666/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9044 - val_loss: -3.4425\n",
      "Epoch 1667/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8260 - val_loss: -4.3335\n",
      "Epoch 1668/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8881 - val_loss: -3.9010\n",
      "Epoch 1669/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8618 - val_loss: -4.1077\n",
      "Epoch 1670/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8835 - val_loss: -4.1227\n",
      "Epoch 1671/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9197 - val_loss: -4.1206\n",
      "Epoch 1672/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9464 - val_loss: -3.7866\n",
      "Epoch 1673/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5755 - val_loss: -4.1755\n",
      "Epoch 1674/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9371 - val_loss: -4.0105\n",
      "Epoch 1675/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8615 - val_loss: -3.8455\n",
      "Epoch 1676/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7676 - val_loss: -2.9585\n",
      "Epoch 1677/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9854 - val_loss: -3.1800\n",
      "Epoch 1678/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9160 - val_loss: -4.0298\n",
      "Epoch 1679/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9180 - val_loss: -4.2368\n",
      "Epoch 1680/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.0205 - val_loss: -2.2158\n",
      "Epoch 1681/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8596 - val_loss: -3.6493\n",
      "Epoch 1682/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6023 - val_loss: -3.6386\n",
      "Epoch 1683/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7628 - val_loss: -3.9376\n",
      "Epoch 1684/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7024 - val_loss: -3.2431\n",
      "Epoch 1685/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8687 - val_loss: -3.8322\n",
      "Epoch 1686/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8696 - val_loss: -4.1777\n",
      "Epoch 1687/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7866 - val_loss: -4.1839\n",
      "Epoch 1688/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8631 - val_loss: -4.1608\n",
      "Epoch 1689/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8743 - val_loss: -3.2460\n",
      "Epoch 1690/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8887 - val_loss: -4.2878\n",
      "Epoch 1691/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8722 - val_loss: -3.6347\n",
      "Epoch 1692/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9148 - val_loss: -4.2176\n",
      "Epoch 1693/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8226 - val_loss: -4.2844\n",
      "Epoch 1694/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9645 - val_loss: -3.7506\n",
      "Epoch 1695/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.5585 - val_loss: -4.1612\n",
      "Epoch 1696/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9405 - val_loss: -4.2248\n",
      "Epoch 1697/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8997 - val_loss: -4.0007\n",
      "Epoch 1698/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9655 - val_loss: -4.2125\n",
      "Epoch 1699/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7721 - val_loss: -4.0774\n",
      "Epoch 1700/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9716 - val_loss: -4.3207\n",
      "Epoch 1701/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0388 - val_loss: -4.2278\n",
      "Epoch 1702/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9624 - val_loss: -4.2684\n",
      "Epoch 1703/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9358 - val_loss: -4.1132\n",
      "Epoch 1704/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8792 - val_loss: -3.7873\n",
      "Epoch 1705/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9424 - val_loss: -4.4069\n",
      "Epoch 1706/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6637 - val_loss: -4.1865\n",
      "Epoch 1707/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8851 - val_loss: -3.7842\n",
      "Epoch 1708/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9135 - val_loss: -3.7138\n",
      "Epoch 1709/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9088 - val_loss: -4.2456\n",
      "Epoch 1710/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.3639 - val_loss: -2.8104\n",
      "Epoch 1711/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.4498 - val_loss: -4.1225\n",
      "Epoch 1712/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9180 - val_loss: -4.0763\n",
      "Epoch 1713/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8412 - val_loss: -4.0651\n",
      "Epoch 1714/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9261 - val_loss: -4.0749\n",
      "Epoch 1715/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.5961 - val_loss: -4.3214\n",
      "Epoch 1716/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8509 - val_loss: -3.5016\n",
      "Epoch 1717/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8645 - val_loss: -4.1426\n",
      "Epoch 1718/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9504 - val_loss: -4.1182\n",
      "Epoch 1719/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9608 - val_loss: -4.5315\n",
      "Epoch 1720/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9252 - val_loss: -4.0355\n",
      "Epoch 1721/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8892 - val_loss: -4.0372\n",
      "Epoch 1722/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.8819 - val_loss: -3.6857\n",
      "Epoch 1723/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6234 - val_loss: -0.3456\n",
      "Epoch 1724/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -3.3875 - val_loss: -3.7253\n",
      "Epoch 1725/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3750 - val_loss: -4.0589\n",
      "Epoch 1726/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6445 - val_loss: -4.1186\n",
      "Epoch 1727/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8316 - val_loss: -3.9802\n",
      "Epoch 1728/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8662 - val_loss: -3.4475\n",
      "Epoch 1729/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9057 - val_loss: -4.2526\n",
      "Epoch 1730/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9381 - val_loss: -3.9424\n",
      "Epoch 1731/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9072 - val_loss: -4.3911\n",
      "Epoch 1732/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7467 - val_loss: -4.1565\n",
      "Epoch 1733/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9342 - val_loss: -2.8513\n",
      "Epoch 1734/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9442 - val_loss: -4.2523\n",
      "Epoch 1735/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8635 - val_loss: -4.0766\n",
      "Epoch 1736/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9160 - val_loss: -3.4638\n",
      "Epoch 1737/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9846 - val_loss: -4.0732\n",
      "Epoch 1738/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8531 - val_loss: -4.4255\n",
      "Epoch 1739/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9179 - val_loss: -2.2022\n",
      "Epoch 1740/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7076 - val_loss: -0.5932\n",
      "Epoch 1741/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7406 - val_loss: -3.8766\n",
      "Epoch 1742/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9635 - val_loss: -3.9439\n",
      "Epoch 1743/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9285 - val_loss: -4.3634\n",
      "Epoch 1744/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.8217 - val_loss: -4.3658\n",
      "Epoch 1745/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8920 - val_loss: -3.4478\n",
      "Epoch 1746/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8769 - val_loss: -4.1024\n",
      "Epoch 1747/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9508 - val_loss: -3.9141\n",
      "Epoch 1748/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9938 - val_loss: -4.2217\n",
      "Epoch 1749/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8136 - val_loss: -3.8012\n",
      "Epoch 1750/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9919 - val_loss: -4.3993\n",
      "Epoch 1751/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8759 - val_loss: -4.0481\n",
      "Epoch 1752/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9319 - val_loss: -4.3327\n",
      "Epoch 1753/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9797 - val_loss: -3.6908\n",
      "Epoch 1754/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0267 - val_loss: -3.5432\n",
      "Epoch 1755/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8622 - val_loss: -4.1234\n",
      "Epoch 1756/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9467 - val_loss: -4.5511\n",
      "Epoch 1757/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0246 - val_loss: -4.1878\n",
      "Epoch 1758/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8828 - val_loss: -4.5638\n",
      "Epoch 1759/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9665 - val_loss: -4.1833\n",
      "Epoch 1760/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9773 - val_loss: -4.2163\n",
      "Epoch 1761/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9281 - val_loss: -4.5310\n",
      "Epoch 1762/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0116 - val_loss: -4.3293\n",
      "Epoch 1763/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8966 - val_loss: -4.0693\n",
      "Epoch 1764/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0606 - val_loss: -4.2043\n",
      "Epoch 1765/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8992 - val_loss: -4.5103\n",
      "Epoch 1766/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4991 - val_loss: -3.9116\n",
      "Epoch 1767/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8334 - val_loss: -3.8193\n",
      "Epoch 1768/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9746 - val_loss: -3.9353\n",
      "Epoch 1769/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0193 - val_loss: -4.3556\n",
      "Epoch 1770/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9204 - val_loss: -3.9583\n",
      "Epoch 1771/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9867 - val_loss: -3.4459\n",
      "Epoch 1772/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9745 - val_loss: -3.9841\n",
      "Epoch 1773/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0141 - val_loss: -4.1426\n",
      "Epoch 1774/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9315 - val_loss: -4.0877\n",
      "Epoch 1775/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9649 - val_loss: -4.4241\n",
      "Epoch 1776/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8219 - val_loss: -4.1231\n",
      "Epoch 1777/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9578 - val_loss: -3.9743\n",
      "Epoch 1778/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9542 - val_loss: -4.1538\n",
      "Epoch 1779/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9145 - val_loss: -4.2151\n",
      "Epoch 1780/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -3.4949 - val_loss: -2.8472\n",
      "Epoch 1781/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.2407 - val_loss: -3.9293\n",
      "Epoch 1782/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6816 - val_loss: -3.1438\n",
      "Epoch 1783/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7442 - val_loss: -4.0252\n",
      "Epoch 1784/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8404 - val_loss: -4.4007\n",
      "Epoch 1785/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7363 - val_loss: -4.0646\n",
      "Epoch 1786/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8709 - val_loss: -3.9595\n",
      "Epoch 1787/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.9354 - val_loss: -4.3016\n",
      "Epoch 1788/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8719 - val_loss: -3.9923\n",
      "Epoch 1789/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9421 - val_loss: -4.1097\n",
      "Epoch 1790/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8230 - val_loss: -3.8879\n",
      "Epoch 1791/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8919 - val_loss: -4.1116\n",
      "Epoch 1792/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9441 - val_loss: -3.7223\n",
      "Epoch 1793/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0063 - val_loss: -4.2916\n",
      "Epoch 1794/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8480 - val_loss: -3.7818\n",
      "Epoch 1795/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9729 - val_loss: -3.3935\n",
      "Epoch 1796/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7963 - val_loss: -3.8699\n",
      "Epoch 1797/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9632 - val_loss: -4.3398\n",
      "Epoch 1798/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9005 - val_loss: -4.4926\n",
      "Epoch 1799/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.0814 - val_loss: -3.9549\n",
      "Epoch 1800/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8638 - val_loss: -3.6829\n",
      "Epoch 1801/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9984 - val_loss: -3.6256\n",
      "Epoch 1802/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9651 - val_loss: -4.3432\n",
      "Epoch 1803/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7560 - val_loss: -3.9577\n",
      "Epoch 1804/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9327 - val_loss: -3.7676\n",
      "Epoch 1805/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0408 - val_loss: -3.7239\n",
      "Epoch 1806/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0279 - val_loss: -4.0838\n",
      "Epoch 1807/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8519 - val_loss: -3.8014\n",
      "Epoch 1808/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.5895 - val_loss: -4.1480\n",
      "Epoch 1809/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9608 - val_loss: -3.5166\n",
      "Epoch 1810/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0003 - val_loss: -3.8136\n",
      "Epoch 1811/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -3.7397 - val_loss: -3.3673\n",
      "Epoch 1812/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7921 - val_loss: -3.8718\n",
      "Epoch 1813/2000\n",
      "118/118 [==============================] - 21s 181ms/step - loss: -4.9409 - val_loss: -3.8948\n",
      "Epoch 1814/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9985 - val_loss: -4.6007\n",
      "Epoch 1815/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9976 - val_loss: -4.1852\n",
      "Epoch 1816/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0084 - val_loss: -4.0006\n",
      "Epoch 1817/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9560 - val_loss: -4.1176\n",
      "Epoch 1818/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9936 - val_loss: -3.1049\n",
      "Epoch 1819/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8992 - val_loss: -4.2393\n",
      "Epoch 1820/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9884 - val_loss: -4.0728\n",
      "Epoch 1821/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9601 - val_loss: -4.2826\n",
      "Epoch 1822/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0519 - val_loss: -4.3626\n",
      "Epoch 1823/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0284 - val_loss: -4.6076\n",
      "Epoch 1824/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0579 - val_loss: -3.6735\n",
      "Epoch 1825/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9761 - val_loss: -3.4193\n",
      "Epoch 1826/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7835 - val_loss: -4.1347\n",
      "Epoch 1827/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6642 - val_loss: -3.5021\n",
      "Epoch 1828/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.8118 - val_loss: -3.9443\n",
      "Epoch 1829/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7711 - val_loss: -3.6150\n",
      "Epoch 1830/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9012 - val_loss: -1.4042\n",
      "Epoch 1831/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9720 - val_loss: -4.1706\n",
      "Epoch 1832/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9764 - val_loss: -4.4896\n",
      "Epoch 1833/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8100 - val_loss: -3.6547\n",
      "Epoch 1834/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9904 - val_loss: -4.3471\n",
      "Epoch 1835/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7628 - val_loss: -4.3060\n",
      "Epoch 1836/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0247 - val_loss: -4.1175\n",
      "Epoch 1837/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8991 - val_loss: -4.2087\n",
      "Epoch 1838/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0025 - val_loss: -2.9542\n",
      "Epoch 1839/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0398 - val_loss: -4.2768\n",
      "Epoch 1840/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8404 - val_loss: -4.0205\n",
      "Epoch 1841/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9918 - val_loss: -4.1411\n",
      "Epoch 1842/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0051 - val_loss: -4.0993\n",
      "Epoch 1843/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9857 - val_loss: -4.5291\n",
      "Epoch 1844/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0473 - val_loss: -4.0027\n",
      "Epoch 1845/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0643 - val_loss: -4.2813\n",
      "Epoch 1846/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9832 - val_loss: -4.2526\n",
      "Epoch 1847/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8398 - val_loss: -4.1525\n",
      "Epoch 1848/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0091 - val_loss: -3.2837\n",
      "Epoch 1849/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0084 - val_loss: -4.0502\n",
      "Epoch 1850/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9457 - val_loss: -4.0696\n",
      "Epoch 1851/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0137 - val_loss: -3.8731\n",
      "Epoch 1852/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0673 - val_loss: -3.1976\n",
      "Epoch 1853/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0117 - val_loss: -3.8445\n",
      "Epoch 1854/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0458 - val_loss: -4.1644\n",
      "Epoch 1855/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0135 - val_loss: -4.0351\n",
      "Epoch 1856/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0481 - val_loss: -4.3916\n",
      "Epoch 1857/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9484 - val_loss: -3.6490\n",
      "Epoch 1858/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9718 - val_loss: -3.9247\n",
      "Epoch 1859/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.9747 - val_loss: -4.3295\n",
      "Epoch 1860/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9995 - val_loss: -4.3539\n",
      "Epoch 1861/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0558 - val_loss: -2.8139\n",
      "Epoch 1862/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0081 - val_loss: -4.2787\n",
      "Epoch 1863/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0549 - val_loss: -3.6871\n",
      "Epoch 1864/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0186 - val_loss: -4.0427\n",
      "Epoch 1865/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9221 - val_loss: -3.3914\n",
      "Epoch 1866/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0425 - val_loss: -4.4189\n",
      "Epoch 1867/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0320 - val_loss: -3.7322\n",
      "Epoch 1868/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0195 - val_loss: -3.9144\n",
      "Epoch 1869/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0375 - val_loss: -3.7860\n",
      "Epoch 1870/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0924 - val_loss: -4.0346\n",
      "Epoch 1871/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9818 - val_loss: -3.7156\n",
      "Epoch 1872/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9792 - val_loss: -4.3938\n",
      "Epoch 1873/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0199 - val_loss: -4.3314\n",
      "Epoch 1874/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0809 - val_loss: -4.0072\n",
      "Epoch 1875/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9965 - val_loss: -4.2910\n",
      "Epoch 1876/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9611 - val_loss: -3.8698\n",
      "Epoch 1877/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9923 - val_loss: -4.3076\n",
      "Epoch 1878/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.1392 - val_loss: -3.5706\n",
      "Epoch 1879/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8653 - val_loss: -4.1402\n",
      "Epoch 1880/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0265 - val_loss: -3.3004\n",
      "Epoch 1881/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0439 - val_loss: -4.0754\n",
      "Epoch 1882/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9987 - val_loss: -4.0150\n",
      "Epoch 1883/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0916 - val_loss: -4.1377\n",
      "Epoch 1884/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -4.9989 - val_loss: -4.4293\n",
      "Epoch 1885/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0558 - val_loss: -3.5642\n",
      "Epoch 1886/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9939 - val_loss: -4.3007\n",
      "Epoch 1887/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8830 - val_loss: -3.8691\n",
      "Epoch 1888/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0984 - val_loss: -4.1956\n",
      "Epoch 1889/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0988 - val_loss: -4.4192\n",
      "Epoch 1890/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0272 - val_loss: -4.4271\n",
      "Epoch 1891/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.6014 - val_loss: -4.3800\n",
      "Epoch 1892/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0801 - val_loss: -4.0929\n",
      "Epoch 1893/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0120 - val_loss: -3.9291\n",
      "Epoch 1894/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0612 - val_loss: -4.2757\n",
      "Epoch 1895/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0073 - val_loss: -4.1496\n",
      "Epoch 1896/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0739 - val_loss: -4.0537\n",
      "Epoch 1897/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0305 - val_loss: -1.8819\n",
      "Epoch 1898/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0049 - val_loss: -3.2783\n",
      "Epoch 1899/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0593 - val_loss: -2.4727\n",
      "Epoch 1900/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8636 - val_loss: -4.6233\n",
      "Epoch 1901/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -2.7575 - val_loss: -2.5840\n",
      "Epoch 1902/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.0624 - val_loss: -3.3199\n",
      "Epoch 1903/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -3.5619 - val_loss: -3.3408\n",
      "Epoch 1904/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -3.8877 - val_loss: -2.8645\n",
      "Epoch 1905/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.1398 - val_loss: -3.4546\n",
      "Epoch 1906/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.3305 - val_loss: -3.7623\n",
      "Epoch 1907/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.4383 - val_loss: -3.5267\n",
      "Epoch 1908/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.5198 - val_loss: -3.7630\n",
      "Epoch 1909/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6176 - val_loss: -3.8564\n",
      "Epoch 1910/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6352 - val_loss: -3.7268\n",
      "Epoch 1911/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.6709 - val_loss: -3.6717\n",
      "Epoch 1912/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.6873 - val_loss: -3.9893\n",
      "Epoch 1913/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7261 - val_loss: -3.8488\n",
      "Epoch 1914/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7237 - val_loss: -3.2463\n",
      "Epoch 1915/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7226 - val_loss: -3.9235\n",
      "Epoch 1916/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7508 - val_loss: -4.1868\n",
      "Epoch 1917/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7367 - val_loss: -4.2555\n",
      "Epoch 1918/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7500 - val_loss: -4.2633\n",
      "Epoch 1919/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8198 - val_loss: -2.2404\n",
      "Epoch 1920/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6715 - val_loss: -4.3813\n",
      "Epoch 1921/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8426 - val_loss: -3.9589\n",
      "Epoch 1922/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.7004 - val_loss: -4.1301\n",
      "Epoch 1923/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -4.8113 - val_loss: -2.4594\n",
      "Epoch 1924/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8747 - val_loss: -4.1010\n",
      "Epoch 1925/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8505 - val_loss: -4.4498\n",
      "Epoch 1926/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.6892 - val_loss: -2.8668\n",
      "Epoch 1927/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8414 - val_loss: -4.3777\n",
      "Epoch 1928/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8968 - val_loss: 0.5624\n",
      "Epoch 1929/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8457 - val_loss: -4.3070\n",
      "Epoch 1930/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8941 - val_loss: -3.3923\n",
      "Epoch 1931/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8565 - val_loss: -4.4171\n",
      "Epoch 1932/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8967 - val_loss: -3.8065\n",
      "Epoch 1933/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9114 - val_loss: -4.0397\n",
      "Epoch 1934/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.7809 - val_loss: -3.9003\n",
      "Epoch 1935/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8719 - val_loss: -4.1906\n",
      "Epoch 1936/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3936 - val_loss: -3.3358\n",
      "Epoch 1937/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.3537 - val_loss: -3.9304\n",
      "Epoch 1938/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9070 - val_loss: -2.7622\n",
      "Epoch 1939/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9374 - val_loss: -1.6327\n",
      "Epoch 1940/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8985 - val_loss: -4.2657\n",
      "Epoch 1941/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8874 - val_loss: -3.7605\n",
      "Epoch 1942/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9382 - val_loss: -3.2829\n",
      "Epoch 1943/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8635 - val_loss: -4.4186\n",
      "Epoch 1944/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8658 - val_loss: -3.8987\n",
      "Epoch 1945/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9386 - val_loss: -2.9749\n",
      "Epoch 1946/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8976 - val_loss: -4.2803\n",
      "Epoch 1947/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9836 - val_loss: -2.6129\n",
      "Epoch 1948/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9023 - val_loss: -3.8764\n",
      "Epoch 1949/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8001 - val_loss: -3.3898\n",
      "Epoch 1950/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8289 - val_loss: -3.7214\n",
      "Epoch 1951/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8597 - val_loss: -3.4565\n",
      "Epoch 1952/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9568 - val_loss: -4.3122\n",
      "Epoch 1953/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9457 - val_loss: -4.4011\n",
      "Epoch 1954/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9283 - val_loss: -4.4761\n",
      "Epoch 1955/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9768 - val_loss: -4.2742\n",
      "Epoch 1956/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9857 - val_loss: -4.2232\n",
      "Epoch 1957/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9341 - val_loss: -4.5414\n",
      "Epoch 1958/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.8213 - val_loss: -3.1898\n",
      "Epoch 1959/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.7777 - val_loss: -4.0889\n",
      "Epoch 1960/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9047 - val_loss: -4.2230\n",
      "Epoch 1961/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9794 - val_loss: -3.4488\n",
      "Epoch 1962/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.8954 - val_loss: -3.8987\n",
      "Epoch 1963/2000\n",
      "118/118 [==============================] - 22s 186ms/step - loss: -5.0070 - val_loss: -4.2488\n",
      "Epoch 1964/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8315 - val_loss: -4.5094\n",
      "Epoch 1965/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9755 - val_loss: -4.2724\n",
      "Epoch 1966/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9369 - val_loss: -4.2807\n",
      "Epoch 1967/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0332 - val_loss: -4.5300\n",
      "Epoch 1968/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9841 - val_loss: -4.1515\n",
      "Epoch 1969/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9332 - val_loss: -4.0732\n",
      "Epoch 1970/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0573 - val_loss: -3.6750\n",
      "Epoch 1971/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9400 - val_loss: -4.3237\n",
      "Epoch 1972/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0032 - val_loss: -3.8942\n",
      "Epoch 1973/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9049 - val_loss: -3.5353\n",
      "Epoch 1974/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0596 - val_loss: -2.8478\n",
      "Epoch 1975/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8907 - val_loss: -4.5080\n",
      "Epoch 1976/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0039 - val_loss: -4.4145\n",
      "Epoch 1977/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9651 - val_loss: -4.1273\n",
      "Epoch 1978/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9873 - val_loss: -4.2279\n",
      "Epoch 1979/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9689 - val_loss: -4.2291\n",
      "Epoch 1980/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9929 - val_loss: -2.4581\n",
      "Epoch 1981/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9092 - val_loss: -4.3029\n",
      "Epoch 1982/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0238 - val_loss: -1.3395\n",
      "Epoch 1983/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9641 - val_loss: -4.1155\n",
      "Epoch 1984/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0205 - val_loss: -4.2449\n",
      "Epoch 1985/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9909 - val_loss: -3.6896\n",
      "Epoch 1986/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.9391 - val_loss: -4.1426\n",
      "Epoch 1987/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0311 - val_loss: -3.7447\n",
      "Epoch 1988/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.9442 - val_loss: -4.2348\n",
      "Epoch 1989/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0213 - val_loss: -4.3195\n",
      "Epoch 1990/2000\n",
      "118/118 [==============================] - 22s 182ms/step - loss: -4.9372 - val_loss: -3.5554\n",
      "Epoch 1991/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0254 - val_loss: -3.8374\n",
      "Epoch 1992/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -5.0454 - val_loss: -4.1280\n",
      "Epoch 1993/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -4.7414 - val_loss: -4.5167\n",
      "Epoch 1994/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0404 - val_loss: -4.6500\n",
      "Epoch 1995/2000\n",
      "118/118 [==============================] - 22s 185ms/step - loss: -5.0147 - val_loss: -4.3504\n",
      "Epoch 1996/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -5.0880 - val_loss: -3.1847\n",
      "Epoch 1997/2000\n",
      "118/118 [==============================] - 22s 184ms/step - loss: -4.8653 - val_loss: -3.7756\n",
      "Epoch 1998/2000\n",
      "118/118 [==============================] - 21s 182ms/step - loss: -5.0727 - val_loss: -3.3485\n",
      "Epoch 1999/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9899 - val_loss: -4.3548\n",
      "Epoch 2000/2000\n",
      "118/118 [==============================] - 22s 183ms/step - loss: -4.9874 - val_loss: -4.3337\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 6s 37ms/step - loss: -4.8975\n",
      "WARNING:tensorflow:Layer lstm_99 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_99 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 6s 36ms/step - loss: -5.3082\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from LSTMutils import MeanVarianceLogLikelyhoodLoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "NumEnsemble = 10\n",
    "SequenceLength = 250\n",
    "validation_split = 0.1\n",
    "batch_size = 64\n",
    "NumEpochs = 2000\n",
    "\n",
    "df = pd.read_csv(r\"../TrainingData/SimulatedTrainingSet10000.csv\",sep=',',header=0)\n",
    "\n",
    "labels = df.iloc[:,0]\n",
    "df_data = df.iloc[:,1:]\n",
    "\n",
    "df_train, df_val = train_test_split(df_data, test_size=validation_split, train_size=1-validation_split, random_state=42, shuffle=True, stratify=labels)\n",
    "\n",
    "# normalise time series data\n",
    "min_value, max_value = df_train.min().min(), df_train.max().max()\n",
    "df_norm_train = (df_train - min_value)/(max_value - min_value)\n",
    "df_norm_test = (df_test - min_value)/(max_value - min_value)\n",
    "df_norm_val = (df_val - min_value)/(max_value - min_value)\n",
    "\n",
    "models = list()\n",
    "\n",
    "for i in range(NumEnsemble):\n",
    "    \n",
    "    df_norm_train = df_norm_train.sample(frac=1)\n",
    "    df_norm_val = df_norm_val.sample(frac=1)\n",
    "    \n",
    "    X_train = df_norm_train.iloc[:,:SequenceLength].values\n",
    "    y_train = df_norm_train.iloc[:,SequenceLength-1].values\n",
    "    X_train = np.expand_dims(X_train, 2)\n",
    "    y_train = np.broadcast_to(y_train[:,None], (y_train.shape[0],SequenceLength))\n",
    "    y_train = np.expand_dims(y_train, 2)\n",
    "\n",
    "    X_val = df_norm_val.iloc[:,:SequenceLength].values\n",
    "    y_val = df_norm_val.iloc[:,SequenceLength-1].values\n",
    "    X_val = np.expand_dims(X_val, 2)\n",
    "    y_val = np.broadcast_to(y_val[:,None], (y_val.shape[0],SequenceLength))\n",
    "    y_val = np.expand_dims(y_val, 2)\n",
    "    \n",
    "    print(\"model number \" + str(i+1))\n",
    "    \n",
    "    model = keras.models.Sequential([keras.layers.LSTM(50, input_shape=(SequenceLength,1), return_sequences=True, stateful=False)\n",
    "                                     , keras.layers.LSTM(200, return_sequences=True, stateful=False)\n",
    "                                     , keras.layers.LSTM(50, return_sequences=True, stateful=False)\n",
    "                                     , keras.layers.LSTM(10, return_sequences=True, stateful=False)\n",
    "                                     , keras.layers.LSTM(2, activation='softplus', return_sequences=True, stateful=False)])\n",
    "\n",
    "    \n",
    "\n",
    "    checkpoint_filepath = r\"../Models/SimulatedDataEnsembleModels/SimulatedDataEnsembleModel\" + str(i+1)\n",
    "    \n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "        \n",
    "    model.compile(optimizer=\"adam\",loss = MeanVarianceLogLikelyhoodLoss)\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, validation_split=validation_split, epochs=NumEpochs, callbacks=[model_checkpoint_callback,keras.callbacks.TerminateOnNaN()])\n",
    "\n",
    "    loss_values = history.history['loss']\n",
    "    val_loss_values = history.history['val_loss']\n",
    "    epochs = range(1, len(loss_values)+1)\n",
    "    plt.plot(epochs, loss_values, label='Training Loss')\n",
    "    plt.plot(epochs, val_loss_values, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    model.evaluate(X_train,y_train,batch_size=batch_size) #min loss shold be -6.908\n",
    "    \n",
    "    bestModel = keras.models.load_model(checkpoint_filepath, custom_objects={\"MeanVarianceLogLikelyhoodLoss\": MeanVarianceLogLikelyhoodLoss})\n",
    "    bestModel.evaluate(X_train, y_train, batch_size=batch_size)\n",
    "    print(str(i+1)+\" models complete\")\n",
    "    \n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529d2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
