{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d68f83",
   "metadata": {},
   "source": [
    "<a id='SimulatedDataEnsembleModelsTop'></a>\n",
    "# Pretrain an LSTM Network using Simulated Data\n",
    "\n",
    "Investigating whether starting with a model pretrained on simulated data improves performance\n",
    "\n",
    "- Architecture, informed by limited [hyperparameter tuning](SimulatedDataPretrainedModelTuner.ipynb#SimulatedDataPretrainedModelTunerTop) with keras tuner, is:\n",
    "    - input layer - 50\n",
    "    - hidden layer - 200\n",
    "    - hidden layer - 50\n",
    "    - hidden layer - 10\n",
    "    - output layer - 2 (softplus activation, to ensure variance predictions are positive)\n",
    "- batch size = 64\n",
    "- Number of base learners in ensemble = 10\n",
    "- 3:1:1 train:validation:test dataset split\n",
    "- 2000 epochs\n",
    "\n",
    "The model from the epoch which gives the best validation loss is saved for future use, for transfer learning using the experimental dataset\n",
    "\n",
    "Minimum loss is -ln(minimum_variance)/2 = -6.91 (for a minimum variance chosen to be 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from LSTMutils import MeanVarianceLogLikelyhoodLoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import LSTMutils\n",
    "\n",
    "# input parameters\n",
    "SequenceLength = 250\n",
    "validation_split = 0.1\n",
    "batch_size = 64\n",
    "NumEpochs = 2\n",
    "\n",
    "#set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# read simulated dataset\n",
    "ExperimentalData = LSTMutils.ExperimentalData(ExperimentalDataFilePath=\"../TrainingData/SimulatedTrainingSet10000.csv\",SequenceLength=SequenceLength)\n",
    "unused, concentrations, df_data, unused = ExperimentalData.ReadData()\n",
    "concentrations=concentrations.apply(pd.to_numeric)\n",
    "\n",
    "# split data into stratified train and test sets, size defined by the test_split variable\n",
    "# the split will always be the same provided the data is in the same order, the same random_state is used,\n",
    "# and strangely the labels used for stratification are always the same type (str is used here)\n",
    "df_train, df_val = train_test_split(df_data, test_size=validation_split, train_size=1-validation_split, random_state=42, shuffle=True, stratify=concentrations)\n",
    "  \n",
    "# normalise time series data\n",
    "df_norm_train, df_norm_val, unused = ExperimentalData.NormalizeData(df_train,df_val)\n",
    "    \n",
    "# Define y as the last element in X, and ensure X and y are the correct shape\n",
    "X_train, y_train = ExperimentalData.Shape(df_norm_train)\n",
    "X_val, y_val = ExperimentalData.Shape(df_norm_val)\n",
    "\n",
    "# define network architecture\n",
    "model = keras.models.Sequential([keras.layers.LSTM(50, input_shape=(SequenceLength,1), return_sequences=True, stateful=False)\n",
    "                                 , keras.layers.LSTM(200, return_sequences=True, stateful=False)\n",
    "                                 , keras.layers.LSTM(50, return_sequences=True, stateful=False)\n",
    "                                 , keras.layers.LSTM(10, return_sequences=True, stateful=False)\n",
    "                                 , keras.layers.LSTM(2, activation='softplus', return_sequences=True, stateful=False)])\n",
    "\n",
    "# save the model at the epoch which gives the lowest loss predictions on the validataion dataset\n",
    "checkpoint_filepath = r\"../Models/SimulatedDataPretrainedModel\"\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss = MeanVarianceLogLikelyhoodLoss)\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_val,y_val), epochs=NumEpochs, callbacks=[model_checkpoint_callback,keras.callbacks.TerminateOnNaN()])\n",
    "\n",
    "# plot loss vs epochs\n",
    "Evaluation = LSTMutils.ModelTrainingEvaluation()\n",
    "Evaluation.PlotLossHistory(history)\n",
    "\n",
    "# load and evaluate the best model, in terms of validation loss\n",
    "bestModel = keras.models.load_model(checkpoint_filepath, custom_objects={\"MeanVarianceLogLikelyhoodLoss\": MeanVarianceLogLikelyhoodLoss})\n",
    "bestModel.evaluate(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e6252f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
