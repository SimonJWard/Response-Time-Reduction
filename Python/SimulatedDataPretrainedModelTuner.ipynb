{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c0f2c5",
   "metadata": {},
   "source": [
    "<a id='SimulatedDataPretrainedModelTunerTop'></a>\n",
    "# Hyperameter Tuning of Sinlge LSTM Pretrained Model using Simulated Data with Keras Tuner\n",
    "\n",
    "Hyperparameter tuning of single LSTM model with the simulated dataset, using keras tuner.\n",
    "\n",
    "- Tunable parameters are:\n",
    "    - number of nodes in each layer (5-500)\n",
    "    - number of hidden layers (2-6)\n",
    "    - batch size (64-1024)\n",
    "- Optional tunable parameters are:\n",
    "    - regularization\n",
    "    - dropout\n",
    "    - custom learning rate\n",
    "\n",
    "Hyperparameter tuning parameters\n",
    "- max epoch = 1000\n",
    "- random search\n",
    "- early stopping after 50 epochs of no improvement, or validation loss is nan\n",
    "\n",
    "The hyperparameters and validation loss from the epoch and model which resulted in the best validation loss is saved, informing the hyperparameters of the [model](SimulatedDataPretrainedModel.ipynb#ESimulatedDataPretrainedModelTop)\n",
    "\n",
    "Minimum loss is -ln(minimum_variance)/2 = -6.91 (for a minimum variance chosen to be 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from LSTMutils import MeanVarianceLogLikelyhoodLoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import LSTMutils\n",
    "import keras_tuner as kt\n",
    "\n",
    "# input parameters\n",
    "SequenceLength = 250\n",
    "validation_split = 0.1\n",
    "NumEpochs = 1000\n",
    "AddDropout = False\n",
    "AddRegularization = False\n",
    "CustomLearningRate = False\n",
    "EarlyStoppingPatience = 50\n",
    "NumTopResults = 30\n",
    "\n",
    "#set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# read simulated dataset\n",
    "ExperimentalData = LSTMutils.ExperimentalData(ExperimentalDataFilePath=\"../TrainingData/SimulatedTrainingSet10000.csv\",SequenceLength=SequenceLength)\n",
    "unused, concentrations, df_data, unused = ExperimentalData.ReadData()\n",
    "concentrations=concentrations.apply(pd.to_numeric)\n",
    "\n",
    "# split data into stratified train and test sets, size defined by the test_split variable\n",
    "# the split will always be the same provided the data is in the same order, the same random_state is used,\n",
    "# and strangely the labels used for stratification are always the same type (str is used here)\n",
    "df_train, df_val = train_test_split(df_data, test_size=validation_split, train_size=1-validation_split, random_state=42, shuffle=True, stratify=concentrations)\n",
    "\n",
    "# normalise time series data\n",
    "df_norm_train, df_norm_val, unused = ExperimentalData.NormalizeData(df_train,df_val)\n",
    "\n",
    "# Define y as the last element in X, and ensure X and y are the correct shape\n",
    "X_train, y_train = ExperimentalData.Shape(df_norm_train)\n",
    "X_val, y_val = ExperimentalData.Shape(df_norm_val)\n",
    "\n",
    "# implementation informed by https://github.com/keras-team/keras-tuner/issues/122\n",
    "# set up class to enable batch size to be searched with the rest of the hyperparameters\n",
    "class LSTMHyperModel(kt.HyperModel):\n",
    "    \n",
    "    def build(self,hp):\n",
    "        \n",
    "        # if needed, add L2 regularizer\n",
    "        if AddRegularization:\n",
    "            Regularizer = keras.regularizers.L2(hp.Choice(\"L2Regularizer1\", [0., 1e-4]))\n",
    "        else:\n",
    "            Regularizer = None        \n",
    "        \n",
    "        # define netwrork architecture of input and first hidden layer\n",
    "        model = keras.models.Sequential(keras.layers.LSTM(hp.Choice(\"layer_1_units\", [10,20,50,100,200,500])\n",
    "                                                          , input_shape=(SequenceLength,1), return_sequences=True\n",
    "                                                          , kernel_regularizer=Regularizer))\n",
    "\n",
    "        # if needed add dropout\n",
    "        if AddDropout:\n",
    "            if hp.Boolean(\"layer_1_dropout\"):\n",
    "                model.add(keras.layers.Dropout(rate=0.25))\n",
    "\n",
    "        # define network architecture of subsequent hidden layers \n",
    "        for i in range(hp.Int(\"LSTM Layers\", min_value = 1, max_value = 5)):\n",
    "            model.add(keras.layers.LSTM(hp.Choice(f\"layer_{i+2}_units\", [5,10,20,50,100,200,500]), return_sequences=True))#, kernel_regularizer=keras.regularizers.L2(hp.Choice(f\"L2Regularizer{i+2}\", [0., 1e-4]))))\n",
    "\n",
    "            # if needed add dropout\n",
    "            if AddDropout:\n",
    "                if hp.Boolean(\"layer_1_dropout\"):\n",
    "                    model.add(keras.layers.Dropout(rate=0.25))\n",
    "                \n",
    "        # if needed specify learning rate schedule            \n",
    "        if CustomLearningRate:\n",
    "            learning_rate = hp.Float(\"lr\", min_value=1e-5, max_value=1e-2, sampling=\"log\")\n",
    "            \n",
    "        # add output layer with softmax activation to ensure variance predictions are positive\n",
    "        model.add(keras.layers.LSTM(2, activation='softplus',return_sequences=True))\n",
    "\n",
    "        # adam optimizer and custom negative log likelyhood loss\n",
    "        model.compile(optimizer=\"adam\",loss = MeanVarianceLogLikelyhoodLoss)\n",
    "    \n",
    "        return model\n",
    "\n",
    "    # define custom fit function called by keras tuner that includes batch size as a hyperparameter    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,batch_size=hp.Choice(\"batch_size\",[64,128,256,512,1024]),**kwargs)\n",
    "\n",
    "# set up tuner\n",
    "tuner = kt.BayesianOptimization(\n",
    "    LSTMHyperModel(),\n",
    "    objective='val_loss',\n",
    "    overwrite = False,\n",
    "    max_trials = 300,\n",
    "    seed = 42,\n",
    "    directory = \"../Models/SimulatedPretrainedModelTuner\")\n",
    "\n",
    "# run random hyperparameter search, with early stopping if there is no decrease in validation loss\n",
    "# in 'EarlyStoppingPatience' epochs, or the loss is nan\n",
    "tuner.search(X_train, y_train, epochs=NumEpochs, validation_data=(X_val,y_val)\n",
    "             , callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',patience=EarlyStoppingPatience)\n",
    "             ,keras.callbacks.TerminateOnNaN()])\n",
    "\n",
    "# get and evaluate model from the epoch that achieved the best validation loss\n",
    "best_model = tuner.get_best_models()[0]\n",
    "best_model.summary()\n",
    "best_model.save(\"../Models/SimulatedPretrainedModelTuner\")\n",
    "\n",
    "# show summary of top 'NumTopResults' candidate sets of hyperparameters, ordered in terms of lowest validation loss\n",
    "tuner.results_summary(NumTopResults)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
